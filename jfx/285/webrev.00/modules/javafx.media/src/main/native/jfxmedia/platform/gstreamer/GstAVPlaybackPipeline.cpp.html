<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New modules/javafx.media/src/main/native/jfxmedia/platform/gstreamer/GstAVPlaybackPipeline.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2010, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.  Oracle designates this
  8  * particular file as subject to the &quot;Classpath&quot; exception as provided
  9  * by Oracle in the LICENSE file that accompanied this code.
 10  *
 11  * This code is distributed in the hope that it will be useful, but WITHOUT
 12  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 13  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 14  * version 2 for more details (a copy is included in the LICENSE file that
 15  * accompanied this code).
 16  *
 17  * You should have received a copy of the GNU General Public License version
 18  * 2 along with this work; if not, write to the Free Software Foundation,
 19  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 20  *
 21  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 22  * or visit www.oracle.com if you need additional information or have any
 23  * questions.
 24  */
 25 
 26 #include &quot;GstAVPlaybackPipeline.h&quot;
 27 
 28 #include &quot;GstVideoFrame.h&quot;
 29 #include &lt;gst/gst.h&gt;
 30 #include &lt;gst/app/gstappsink.h&gt;
 31 #include &lt;PipelineManagement/VideoTrack.h&gt;
 32 #include &lt;MediaManagement/Media.h&gt;
 33 #include &lt;jni/Logger.h&gt;
 34 #include &lt;Common/VSMemory.h&gt;
 35 #include &lt;Utils/LowLevelPerf.h&gt;
 36 
 37 #define MAX_SIZE_BUFFERS_LIMIT 25
 38 #define MAX_SIZE_BUFFERS_INC   5
 39 
 40 //*************************************************************************************************
 41 //********** class CGstAVPlaybackPipeline
 42 //*************************************************************************************************
 43 
 44 /**
 45  * CGstAVPlaybackPipeline::CGstAVPlaybackPipeline()
 46  *
 47  * Constructor
 48  *
 49  * @param   elements    GStreamer container of elements
 50  * @param   pOptions    options for the pipeline
 51  */
 52 CGstAVPlaybackPipeline::CGstAVPlaybackPipeline(const GstElementContainer&amp; elements, int audioFlags, CPipelineOptions* pOptions)
 53 :   CGstAudioPlaybackPipeline(elements, audioFlags, pOptions)
 54 {
 55     LOGGER_LOGMSG(LOGGER_DEBUG, &quot;CGstAVPlaybackPipeline::CGstAVPlaybackPipeline()&quot;);
 56     m_videoDecoderSrcProbeHID = 0L;
 57     m_EncodedVideoFrameRate = 24.0F;
 58     m_SendFrameSizeEvent = TRUE;
 59     m_FrameWidth = 0;
 60     m_FrameHeight = 0;
 61     m_videoCodecErrorCode = ERROR_NONE;
 62     m_bStaticPipeline = false; // For now all video pipelines are dynamic
 63 }
 64 
 65 /**
 66  * CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()
 67  *
 68  * Destructor
 69  */
 70 CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()
 71 {
 72 #if JFXMEDIA_DEBUG
 73     g_print (&quot;CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()\n&quot;);
 74 #endif
 75     LOGGER_LOGMSG(LOGGER_DEBUG, &quot;CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()&quot;);
 76 }
 77 
 78 /**
 79  * CGstAVPlaybackPipeline::Init()
 80  *
 81  * Initialize the pipeline.
 82  */
 83 uint32_t CGstAVPlaybackPipeline::Init()
 84 {
 85     g_signal_connect(m_Elements[AV_DEMUXER], &quot;pad-added&quot;, G_CALLBACK (on_pad_added), this);
 86     g_signal_connect(m_Elements[AV_DEMUXER], &quot;no-more-pads&quot;, G_CALLBACK (no_more_pads), this);
 87     g_signal_connect(m_Elements[AUDIO_QUEUE], &quot;overrun&quot;, G_CALLBACK (queue_overrun), this);
 88     g_signal_connect(m_Elements[VIDEO_QUEUE], &quot;overrun&quot;, G_CALLBACK (queue_overrun), this);
 89     g_signal_connect(m_Elements[AUDIO_QUEUE], &quot;underrun&quot;, G_CALLBACK (queue_underrun), this);
 90     g_signal_connect(m_Elements[VIDEO_QUEUE], &quot;underrun&quot;, G_CALLBACK (queue_underrun), this);
 91 
 92     return CGstAudioPlaybackPipeline::Init();
 93 }
 94 
 95 uint32_t CGstAVPlaybackPipeline::PostBuildInit()
 96 {
 97     if (m_bHasVideo &amp;&amp; !m_bVideoInitDone)
 98     {
 99 #if ENABLE_APP_SINK &amp;&amp; !ENABLE_NATIVE_SINK
100         //Tell it to push signals to us in sync mode so that audio and video are sync&#39;d
101         g_object_set (G_OBJECT (m_Elements[VIDEO_SINK]), &quot;emit-signals&quot;, TRUE, &quot;sync&quot;, TRUE, NULL);
102 
103         //Connect the callback
104         g_signal_connect (m_Elements[VIDEO_SINK], &quot;new-sample&quot;, G_CALLBACK (OnAppSinkHaveFrame), this);
105         g_signal_connect (m_Elements[VIDEO_SINK], &quot;new-preroll&quot;, G_CALLBACK (OnAppSinkPreroll), this);
106 #endif
107 
108         // Add a buffer probe on the sink pad of the decoder to capture frame rate
109         GstPad *pPad = gst_element_get_static_pad(m_Elements[VIDEO_DECODER], &quot;src&quot;);
110         if (NULL == pPad)
111             return ERROR_GSTREAMER_VIDEO_DECODER_SINK_PAD;
112         m_videoDecoderSrcProbeHID = gst_pad_add_probe(pPad, GST_PAD_PROBE_TYPE_BUFFER, (GstPadProbeCallback)VideoDecoderSrcProbe, this, NULL);
113         gst_object_unref(pPad);
114 
115         m_bVideoInitDone = true;
116     }
117 
118     return CGstAudioPlaybackPipeline::PostBuildInit();
119 }
120 
121 /**
122  * CGstAVPlaybackPipeline::Dispose()
123  *
124  * Disposes of resources held by this object. The pipeline should not be used
125  * once this method has been invoked.
126  */
127 void CGstAVPlaybackPipeline::Dispose()
128 {
129 #if JFXMEDIA_DEBUG
130     g_print (&quot;CGstAVPlaybackPipeline::Dispose()\n&quot;);
131 #endif
132 
133     if (m_bHasVideo &amp;&amp; m_bVideoInitDone)
134     {
135 #if ENABLE_APP_SINK &amp;&amp; !ENABLE_NATIVE_SINK
136         g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_SINK], (void*)G_CALLBACK(OnAppSinkHaveFrame), this);
137         g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_SINK], (void*)G_CALLBACK(OnAppSinkPreroll), this);
138 #endif
139     }
140 
141     g_signal_handlers_disconnect_by_func(m_Elements[AUDIO_QUEUE], (void*)G_CALLBACK(queue_overrun), this);
142     g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_QUEUE], (void*)G_CALLBACK(queue_overrun), this);
143     g_signal_handlers_disconnect_by_func(m_Elements[AUDIO_QUEUE], (void*)G_CALLBACK(queue_underrun), this);
144     g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_QUEUE], (void*)G_CALLBACK(queue_underrun), this);
145 
146     CGstAudioPlaybackPipeline::Dispose();
147 
148     if (!m_bHasAudio &amp;&amp; m_Elements[AUDIO_BIN] != NULL)
149         gst_object_unref(m_Elements[AUDIO_BIN]);
150 
151     if (!m_bHasVideo &amp;&amp; m_Elements[VIDEO_BIN] != NULL)
152         gst_object_unref(m_Elements[VIDEO_BIN]);
153 }
154 
155 bool CGstAVPlaybackPipeline::IsCodecSupported(GstCaps *pCaps)
156 {
157 #if TARGET_OS_WIN32
158     GstStructure *s = NULL;
159     const gchar *mimetype = NULL;
160 
161     if (pCaps)
162     {
163         s = gst_caps_get_structure (pCaps, 0);
164         if (s != NULL)
165         {
166             mimetype = gst_structure_get_name (s);
167             if (mimetype != NULL)
168             {
169                 if (strstr(mimetype, &quot;video/x-h264&quot;) != NULL) // H.264
170                 {
171                     gboolean is_supported = FALSE;
172                     g_object_set(m_Elements[VIDEO_DECODER], &quot;codec-id&quot;, (gint)CODEC_ID_AVC1, NULL); // Check for AVC1 (MP4). For HLS we should get error early
173                     g_object_get(m_Elements[VIDEO_DECODER], &quot;is-supported&quot;, &amp;is_supported, NULL);
174                     if (is_supported)
175                     {
176                         return TRUE;
177                     }
178                     else
179                     {
180                         m_videoCodecErrorCode = ERROR_MEDIA_H264_FORMAT_UNSUPPORTED;
181                         return FALSE;
182                     }
183                 }
184             }
185         }
186     }
187 
188     return CGstAudioPlaybackPipeline::IsCodecSupported(pCaps);
189 #else // TARGET_OS_WIN32
190     GstStructure *s = NULL;
191     const gchar *mimetype = NULL;
192 
193     if (pCaps)
194     {
195         s = gst_caps_get_structure (pCaps, 0);
196         if (s != NULL)
197         {
198             mimetype = gst_structure_get_name (s);
199             if (mimetype != NULL)
200             {
201                 if (strstr(mimetype, &quot;video/unsupported&quot;) != NULL)
202                 {
203                     m_videoCodecErrorCode = ERROR_MEDIA_VIDEO_FORMAT_UNSUPPORTED;
204                     return FALSE;
205                 }
206             }
207         }
208     }
209 
210     return CGstAudioPlaybackPipeline::IsCodecSupported(pCaps);
211 #endif // TRAGET_OS_WIN32
212 }
213 
214 bool CGstAVPlaybackPipeline::CheckCodecSupport()
215 {
216     if (!m_bHasVideo)
217     {
218         if (!CGstAudioPlaybackPipeline::CheckCodecSupport())
219         {
220             if (m_pEventDispatcher &amp;&amp; m_videoCodecErrorCode != ERROR_NONE)
221             {
222                 if (!m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(m_videoCodecErrorCode))
223                 {
224                     LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
225                 }
226 
227                 return FALSE;
228             }
229         }
230     }
231     else
232     {
233         return CGstAudioPlaybackPipeline::CheckCodecSupport();
234     }
235     return FALSE;
236 }
237 
238 /**
239  * CGstAVPlaybackPipeline::SetEncodedVideoFrameRate()
240  *
241  * Sets the encoded video frame rate data member.
242  */
243 void CGstAVPlaybackPipeline::SetEncodedVideoFrameRate(float frameRate)
244 {
245     m_EncodedVideoFrameRate = frameRate;
246 }
247 
248 /**
249  * CGstAVPlaybackPipeline::OnAppSinkHaveFrame()
250  *
251  * AppSink callback that receives frames from GStreamer.
252  *
253  * @param   pElem       GStreamer element that is calling this callback
254  * @param   pPipeline   Pointer to this class, passed back as user data
255  */
256 GstFlowReturn CGstAVPlaybackPipeline::OnAppSinkHaveFrame(GstElement* pElem, CGstAVPlaybackPipeline* pPipeline)
257 {
258     LOWLEVELPERF_RESETCOUNTER(&quot;FPS&quot;);
259 
260     //***** get the buffer from appsink
261     GstSample* pSample = gst_app_sink_pull_sample(GST_APP_SINK (pElem));
262     if (pSample == NULL)
263         return GST_FLOW_OK;
264 
265     GstBuffer* pBuffer = gst_sample_get_buffer(pSample);
266     if (pBuffer == NULL)
267     {
268         gst_sample_unref(pSample);
269         return GST_FLOW_OK;
270     }
271 
272     if (pPipeline-&gt;m_SendFrameSizeEvent || GST_BUFFER_IS_DISCONT(pBuffer))
273         OnAppSinkVideoFrameDiscont(pPipeline, pSample);
274 
275     //***** Create a VideoFrame object
276     CGstVideoFrame* pVideoFrame = new CGstVideoFrame();
277     if (!pVideoFrame-&gt;Init(pSample))
278     {
279         gst_sample_unref(pSample);
280         delete pVideoFrame;
281         return GST_FLOW_OK;
282     }
283 
284     if (pVideoFrame-&gt;IsValid() &amp;&amp; pPipeline-&gt;m_pEventDispatcher)
285     {
286         CPlayerEventDispatcher* pEventDispatcher = pPipeline-&gt;m_pEventDispatcher;
287 
288         // Send new frame which Java will delete later.
289         if (!pEventDispatcher-&gt;SendNewFrameEvent(pVideoFrame))
290         {
291             if(!pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_NEW_FRAME_EVENT))
292             {
293                 LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
294             }
295         }
296     }
297     else
298     {
299         delete pVideoFrame;
300         if (pPipeline-&gt;m_pEventDispatcher != NULL) {
301             pPipeline-&gt;m_pEventDispatcher-&gt;Warning(WARNING_GSTREAMER_INVALID_FRAME,
302                                                    &quot;Invalid frame&quot;);
303         }
304 
305     }
306 
307 // INLINE - gst_sample_unref()
308     gst_sample_unref (pSample);
309 
310     return GST_FLOW_OK;
311 }
312 
313 /**
314  * CGstAVPlaybackPipeline::OnAppSinkPreroll()
315  *
316  * Gets some initial information such as the first frame and the height and width.
317  *
318  * @param   elements    GStreamer container of elements
319  */
320 GstFlowReturn CGstAVPlaybackPipeline::OnAppSinkPreroll(GstElement* pElem, CGstAVPlaybackPipeline* pPipeline)
321 {
322     LOWLEVELPERF_EXECTIMESTOP(&quot;nativeInitNativeMediaManagerToVideoPreroll&quot;);
323 
324     //***** get the buffer from appsink
325     GstSample* pSample = gst_app_sink_pull_preroll(GST_APP_SINK (pElem));
326 
327     GstBuffer* pBuffer = gst_sample_get_buffer(pSample);
328     if (pBuffer == NULL)
329     {
330         gst_sample_unref(pSample);
331         return GST_FLOW_OK;
332     }
333 
334     if (pPipeline-&gt;m_SendFrameSizeEvent || GST_BUFFER_IS_DISCONT(pBuffer))
335         OnAppSinkVideoFrameDiscont(pPipeline, pSample);
336 
337     // Send frome 0 up to use as poster frame.
338     if(pPipeline-&gt;m_pEventDispatcher != NULL)
339     {
340         CGstVideoFrame* pVideoFrame = new CGstVideoFrame();
341         if (!pVideoFrame-&gt;Init(pSample))
342         {
343             // INLINE - gst_sample_unref()
344             gst_sample_unref (pSample);
345             delete pVideoFrame;
346             return GST_FLOW_OK;
347         }
348         if (pVideoFrame-&gt;IsValid()) {
349             if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendNewFrameEvent(pVideoFrame))
350             {
351                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_NEW_FRAME_EVENT))
352                 {
353                     LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
354                 }
355             }
356         } else {
357             delete pVideoFrame;
358             if (pPipeline-&gt;m_pEventDispatcher != NULL) {
359                 pPipeline-&gt;m_pEventDispatcher-&gt;Warning(WARNING_GSTREAMER_INVALID_FRAME, &quot;Invalid frame&quot;);
360             }
361         }
362     }
363 
364 // INLINE - gst_sample_unref()
365     gst_sample_unref (pSample);
366 
367     return GST_FLOW_OK;
368 }
369 
370 void CGstAVPlaybackPipeline::OnAppSinkVideoFrameDiscont(CGstAVPlaybackPipeline* pPipeline, GstSample *pSample)
371 {
372     gint width, height;
373 
374     GstCaps* caps = gst_sample_get_caps(pSample);
375     if (caps == NULL)
376         return;
377 
378     const GstStructure* str = gst_caps_get_structure(caps, 0);
379     if (str == NULL)
380         return;
381 
382     if (!gst_structure_get_int(str, &quot;width&quot;, &amp;width))
383     {
384         pPipeline-&gt;m_pEventDispatcher-&gt;Warning (WARNING_GSTREAMER_PIPELINE_FRAME_SIZE, (char*)&quot;width could not be retrieved from preroll GstBuffer&quot;);
385         width = 0;
386     }
387     if (!gst_structure_get_int(str, &quot;height&quot;, &amp;height))
388     {
389         pPipeline-&gt;m_pEventDispatcher-&gt;Warning (WARNING_GSTREAMER_PIPELINE_FRAME_SIZE, (char*)&quot;height could not be retrieved from preroll GstBuffer&quot;);
390         height = 0;
391     }
392 
393     if (pPipeline-&gt;m_SendFrameSizeEvent || width != pPipeline-&gt;m_FrameWidth || height != pPipeline-&gt;m_FrameHeight)
394     {
395         // Save values for possible later use.
396         pPipeline-&gt;m_FrameWidth = width;
397         pPipeline-&gt;m_FrameHeight = height;
398 
399         if (pPipeline-&gt;m_pEventDispatcher != NULL)
400         {
401             pPipeline-&gt;m_SendFrameSizeEvent = !pPipeline-&gt;m_pEventDispatcher-&gt;SendFrameSizeChangedEvent(pPipeline-&gt;m_FrameWidth, pPipeline-&gt;m_FrameHeight);
402             if (pPipeline-&gt;m_SendFrameSizeEvent)
403             {
404                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_FRAME_SIZE_CHANGED_EVENT))
405                 {
406                     LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
407                 }
408             }
409         }
410         else
411             pPipeline-&gt;m_SendFrameSizeEvent = TRUE;
412     }
413 }
414 
415 /**
416  * CGstAVPlaybackPipeline::CGstAVPlaybackPipeline()
417  *
418  *
419  *
420  * @param
421  */
422 void CGstAVPlaybackPipeline::on_pad_added(GstElement *element, GstPad *pad, CGstAVPlaybackPipeline *pPipeline)
423 {
424     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Enter();
425 
426     if (pPipeline-&gt;m_pBusCallbackContent-&gt;m_bIsDisposeInProgress)
427     {
428         pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
429         return;
430     }
431 
432     GstCaps *pCaps = gst_pad_get_current_caps(pad);
433     const GstStructure *pStructure = gst_caps_get_structure(pCaps, 0);
434     const gchar* pstrName = gst_structure_get_name(pStructure);
435     GstPad *pPad = NULL;
436     GstPadLinkReturn ret = GST_PAD_LINK_OK;
437     GstStateChangeReturn stateRet = GST_STATE_CHANGE_FAILURE;
438 
439     if (g_str_has_prefix(pstrName, &quot;audio&quot;))
440     {
441          // Ignore additional audio tracks if we already have one.
442          // Otherwise files with multiple audio track will fail to play, since
443          // we will not able to connect second audio track.
444          if (pPipeline-&gt;m_bHasAudio)
445          {
446             if (pCaps != NULL)
447                 gst_caps_unref(pCaps);
448 
449             pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
450             return;
451         }
452 
453         if (pPipeline-&gt;IsCodecSupported(pCaps))
454         {
455             pPad = gst_element_get_static_pad(pPipeline-&gt;m_Elements[AUDIO_BIN], &quot;sink&quot;);
456             gst_bin_add(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[AUDIO_BIN]);
457             stateRet = gst_element_set_state(pPipeline-&gt;m_Elements[AUDIO_BIN], GST_STATE_READY);
458             if (stateRet == GST_STATE_CHANGE_FAILURE)
459             {
460                 gst_object_ref(pPipeline-&gt;m_Elements[AUDIO_BIN]);
461                 gst_bin_remove(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[AUDIO_BIN]);
462                 // Remove handles, so we do not receive any more notifications about pads being added or
463                 // when we done adding new pads. Since we fail to switch bin state we got fatal error and
464                 // bus callback will move pipeline into GST_STATE_NULL while holding dispose lock and
465                 // demux (qtdemux) might deadlock since it will call on_pad_added or no_more_pads
466                 // and these callback will hold dispose lock as well.
467                 g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(on_pad_added), pPipeline);
468                 g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(no_more_pads), pPipeline);
469                 goto Error;
470             }
471             if (pPad != NULL)
472             {
473                 ret = gst_pad_link (pad, pPad);
474                 if (ret != GST_PAD_LINK_OK)
475                 {
476                     gst_element_set_state(pPipeline-&gt;m_Elements[AUDIO_BIN], GST_STATE_NULL);
477                     gst_object_ref(pPipeline-&gt;m_Elements[AUDIO_BIN]);
478                     gst_bin_remove(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[AUDIO_BIN]);
479                     // We might need to remove callbacks here as well, but it was not necessary before,
480                     // so to avoid any regression we will not do it here.
481                     goto Error;
482                 }
483             }
484             pPipeline-&gt;m_bHasAudio = true;
485             pPipeline-&gt;PostBuildInit();
486             gst_element_sync_state_with_parent(pPipeline-&gt;m_Elements[AUDIO_BIN]);
487         }
488     }
489     else if (g_str_has_prefix(pstrName, &quot;video&quot;))
490     {
491         if (pPipeline-&gt;IsCodecSupported(pCaps))
492         {
493             pPad = gst_element_get_static_pad(pPipeline-&gt;m_Elements[VIDEO_BIN], &quot;sink&quot;);
494             gst_bin_add (GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[VIDEO_BIN]);
495             stateRet = gst_element_set_state(pPipeline-&gt;m_Elements[VIDEO_BIN], GST_STATE_READY);
496             if (stateRet == GST_STATE_CHANGE_FAILURE)
497             {
498                 gst_object_ref(pPipeline-&gt;m_Elements[VIDEO_BIN]);
499                 gst_bin_remove(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[VIDEO_BIN]);
500                 g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(on_pad_added), pPipeline);
501                 g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(no_more_pads), pPipeline);
502                 goto Error;
503             }
504             if (pPad != NULL)
505             {
506                 ret = gst_pad_link (pad, pPad);
507                 if (ret != GST_PAD_LINK_OK)
508                 {
509                     gst_element_set_state(pPipeline-&gt;m_Elements[VIDEO_BIN], GST_STATE_NULL);
510                     gst_object_ref(pPipeline-&gt;m_Elements[VIDEO_BIN]);
511                     gst_bin_remove(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[VIDEO_BIN]);
512                     goto Error;
513                 }
514             }
515             pPipeline-&gt;m_bHasVideo = true;
516             pPipeline-&gt;PostBuildInit();
517             gst_element_sync_state_with_parent(pPipeline-&gt;m_Elements[VIDEO_BIN]);
518         }
519     }
520 
521 Error:
522     // Check if we have error set.
523     if (ret != GST_PAD_LINK_OK &amp;&amp; pPipeline-&gt;m_pEventDispatcher != NULL) {
524         // Handle special case for GST_PAD_LINK_NOFORMAT, which means format is not supported
525         if (ret == GST_PAD_LINK_NOFORMAT) {
526             if (g_str_has_prefix(pstrName, &quot;audio&quot;))
527             {
528                 pPipeline-&gt;m_audioCodecErrorCode = ERROR_MEDIA_AUDIO_FORMAT_UNSUPPORTED;
529             }
530             else if (g_str_has_prefix(pstrName, &quot;video&quot;))
531             {
532                 pPipeline-&gt;m_videoCodecErrorCode = ERROR_MEDIA_VIDEO_FORMAT_UNSUPPORTED;
533             }
534         } else {
535             GTimeVal now;
536             g_get_current_time (&amp;now);
537             if (g_str_has_prefix(pstrName, &quot;audio&quot;))
538             {
539                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerHaltEvent(&quot;Failed to link AV parser to audio bin!&quot;, (double)GST_TIMEVAL_TO_TIME (now)))
540                 {
541                     if(!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_PLAYER_HALT_EVENT))
542                     {
543                         LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
544                     }
545                 }
546             }
547             else if (g_str_has_prefix(pstrName, &quot;video&quot;))
548             {
549                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerHaltEvent(&quot;Failed to link AV parser to video bin!&quot;, (double)GST_TIMEVAL_TO_TIME (now)))
550                 {
551                     if(!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_PLAYER_HALT_EVENT))
552                     {
553                         LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
554                     }
555                 }
556             }
557         }
558     }
559 
560     if (pPad != NULL)
561         gst_object_unref(pPad);
562     if (pCaps != NULL)
563         gst_caps_unref(pCaps);
564 
565     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
566 }
567 
568 /**
569  * CGstAVPlaybackPipeline::no_more_pads()
570  *
571  *
572  *
573  * @param   elements    GStreamer container of elements
574  */
575 void CGstAVPlaybackPipeline::no_more_pads(GstElement *element, CGstAVPlaybackPipeline *pPipeline)
576 {
577     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Enter();
578 
579     if (pPipeline-&gt;m_pBusCallbackContent-&gt;m_bIsDisposeInProgress)
580     {
581         pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
582         return;
583     }
584 
585     g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(on_pad_added), pPipeline);
586     g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(no_more_pads), pPipeline);
587 
588     pPipeline-&gt;CheckCodecSupport();
589 
590     if (!pPipeline-&gt;m_bHasAudio)
591         pPipeline-&gt;m_bAudioSinkReady = true;
592     if (!pPipeline-&gt;m_bHasVideo)
593         pPipeline-&gt;m_bVideoSinkReady = true;
594 
595     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
596 }
597 
598 void CGstAVPlaybackPipeline::CheckQueueSize(GstElement *element)
599 {
600     guint current_level_buffers = 0;
601     guint max_size_buffers = 0;
602 
603     if (element == NULL)
604     {
605         g_object_get(m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
606         if (current_level_buffers &gt;= max_size_buffers)
607         {
608             element = m_Elements[VIDEO_QUEUE];
609         }
610         else
611         {
612             g_object_get(m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
613             if (current_level_buffers &gt;= max_size_buffers)
614                 element = m_Elements[AUDIO_QUEUE];
615         }
616 
617         if (element == NULL)
618             return;
619     }
620 
621     GstState state, pending_state;
622     gst_element_get_state(m_Elements[PIPELINE], &amp;state, &amp;pending_state, 0);
623 
624     gboolean inc_size_time = FALSE;
625     if (IsPlayerState(Unknown) || m_StallOnPause || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PLAYING) || (state == GST_STATE_PLAYING &amp;&amp; pending_state == GST_STATE_PAUSED))
626     {
627 
628         if (m_Elements[AUDIO_QUEUE] == element)
629         {
630             g_object_get(m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
631             if (current_level_buffers &lt; MAX_SIZE_BUFFERS_LIMIT)
632                 inc_size_time = TRUE;
633         }
634         else if (m_Elements[VIDEO_QUEUE] == element)
635         {
636             g_object_get(m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
637             if (current_level_buffers &lt; MAX_SIZE_BUFFERS_LIMIT)
638                 inc_size_time = TRUE;
639         }
640     }
641     else if ((state == GST_STATE_PLAYING &amp;&amp; pending_state == GST_STATE_VOID_PENDING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PLAYING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PAUSED))
642     {
643         // Do not increment queue if we playing and only have one track
644         if (!(m_bHasAudio &amp;&amp; m_bHasVideo))
645             return;
646 
647         if (m_Elements[AUDIO_QUEUE] == element)
648         {
649             g_object_get(m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
650             if (current_level_buffers == 0)
651                 inc_size_time = TRUE;
652         }
653         else if (m_Elements[VIDEO_QUEUE] == element)
654         {
655             g_object_get(m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
656             if (current_level_buffers == 0)
657                 inc_size_time = TRUE;
658         }
659     }
660 
661     if (inc_size_time)
662     {
663         g_object_get(element, &quot;max-size-buffers&quot;, &amp;max_size_buffers, NULL);
664         max_size_buffers += MAX_SIZE_BUFFERS_INC;
665         g_object_set(element, &quot;max-size-buffers&quot;, max_size_buffers, NULL);
666     }
667 }
668 
669 void CGstAVPlaybackPipeline::queue_overrun(GstElement *element, CGstAVPlaybackPipeline *pPipeline)
670 {
671     pPipeline-&gt;CheckQueueSize(element);
672 }
673 
674 void CGstAVPlaybackPipeline::queue_underrun(GstElement *element, CGstAVPlaybackPipeline *pPipeline)
675 {
676     if (pPipeline-&gt;m_pOptions-&gt;GetHLSModeEnabled())
677     {
678         if (pPipeline-&gt;m_Elements[AUDIO_QUEUE] == element)
679         {
680             GstStructure *s = gst_structure_new_empty(HLS_PB_MESSAGE_STALL);
681             GstMessage *msg = gst_message_new_application(GST_OBJECT(element), s);
682             gst_element_post_message(GST_ELEMENT(element), msg);
683         }
684     }
685     else
686     {
687         gboolean inc_size_time = FALSE;
688         guint current_level_buffers = 0;
689         guint max_size_buffers = 0;
690         GstState state, pending_state;
691         GstElement* inc_element = NULL;
692 
693         gst_element_get_state(pPipeline-&gt;m_Elements[PIPELINE], &amp;state, &amp;pending_state, 0);
694 
695         if ((state == GST_STATE_PLAYING &amp;&amp; pending_state == GST_STATE_VOID_PENDING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PLAYING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PAUSED))
696         {
697             if (pPipeline-&gt;m_Elements[AUDIO_QUEUE] == element)
698             {
699                 g_object_get(pPipeline-&gt;m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
700                 g_object_get(pPipeline-&gt;m_Elements[VIDEO_QUEUE], &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
701                 if (current_level_buffers == max_size_buffers)
702                 {
703                     inc_element = pPipeline-&gt;m_Elements[VIDEO_QUEUE];
704                     inc_size_time = TRUE;
705                 }
706             }
707             else if (pPipeline-&gt;m_Elements[VIDEO_QUEUE] == element)
708             {
709                 g_object_get(pPipeline-&gt;m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
710                 g_object_get(pPipeline-&gt;m_Elements[AUDIO_QUEUE], &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
711                 if (current_level_buffers == max_size_buffers)
712                 {
713                     inc_element = pPipeline-&gt;m_Elements[AUDIO_QUEUE];
714                     inc_size_time = TRUE;
715                 }
716             }
717         }
718 
719         if (inc_size_time)
720         {
721             g_object_get(inc_element, &quot;max-size-buffers&quot;, &amp;max_size_buffers, NULL);
722             max_size_buffers += MAX_SIZE_BUFFERS_INC;
723             g_object_set(inc_element, &quot;max-size-buffers&quot;, max_size_buffers, NULL);
724         }
725     }
726 }
727 
728 /**
729  * CGstAVPlaybackPipeline::VideoDecoderSrcProbe()
730  *
731  *
732  *
733  * @param
734  */
735 GstPadProbeReturn CGstAVPlaybackPipeline::VideoDecoderSrcProbe(GstPad* pPad, GstPadProbeInfo *pInfo, CGstAVPlaybackPipeline* pPipeline)
736 {
737     GstPadProbeReturn ret = GST_PAD_PROBE_OK;
738     GstCaps *pCaps = NULL;
739     GstPad *pSinkPad = NULL;
740 
741     if (pPipeline-&gt;m_pEventDispatcher)
742     {
743         GstStructure *pStructure = NULL;
744         bool hasAlpha = false;
745         gboolean enabled;
746 
747         string           strMimeType;
748         CTrack::Encoding encoding;
749         gint             width    = 0;
750         gint             height   = 0;
751         gint             fr_num   = 0;
752         gint             fr_denom = 1; // We don&#39;t want do divide by zero
753         gint trackID;
754 
755         // Make sure we got requested probe
756         if ((pInfo-&gt;type &amp; GST_PAD_PROBE_TYPE_BUFFER) != GST_PAD_PROBE_TYPE_BUFFER || pInfo-&gt;data == NULL)
757             goto exit;
758 
759         // Get resolution and framerate from src pad
760         if (NULL == (pCaps = gst_pad_get_current_caps(pPad)) ||
761             NULL == (pStructure = gst_caps_get_structure(pCaps, 0)))
762             goto exit;
763 
764         if (!gst_structure_get_int(pStructure, &quot;width&quot;, &amp;width) ||
765             !gst_structure_get_int(pStructure, &quot;height&quot;, &amp;height) ||
766             !gst_structure_get_fraction(pStructure, &quot;framerate&quot;, &amp;fr_num, &amp;fr_denom) ||
767             0 == fr_denom)
768                 goto exit;
769 
770         float frameRate = (float) fr_num / fr_denom;
771         pPipeline-&gt;SetEncodedVideoFrameRate(frameRate);
772 
773         // Get encoding and track ID from sink pad
774         if (pCaps != NULL)
775             gst_caps_unref(pCaps);
776 
777         pSinkPad = gst_element_get_static_pad(pPipeline-&gt;m_Elements[VIDEO_DECODER], &quot;sink&quot;);
778         if (NULL == pSinkPad ||
779             NULL == (pCaps = gst_pad_get_current_caps(pSinkPad)) ||
780             NULL == (pStructure = gst_caps_get_structure(pCaps, 0)))
781         {
782             goto exit;
783         }
784 
785         strMimeType = gst_structure_get_name(pStructure);
786 
787         if (strMimeType.find(&quot;video/x-h264&quot;) != string::npos) {
788             encoding = CTrack::H264;
789         } else {
790             encoding = CTrack::CUSTOM;
791         }
792 
793         if (!gst_structure_get_boolean(pStructure, &quot;track_enabled&quot;, &amp;enabled)) {
794             enabled = TRUE; // treat as enabled if field is not present
795         }
796 
797         if (!gst_structure_get_int(pStructure, &quot;track_id&quot;, &amp;trackID)) {
798             trackID = 1; // default to 1 for video track, in case container doesn&#39;t have track IDs
799         }
800 
801         // Create the video track.
802         CVideoTrack *p_VideoTrack = new CVideoTrack(
803             (int64_t)trackID,
804             strMimeType,
805             encoding,
806             (bool)enabled,
807             width, height,
808             frameRate,
809             hasAlpha);
810 
811         // Dispatch the track event.
812         if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendVideoTrackEvent(p_VideoTrack))
813         {
814             if(!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_VIDEO_TRACK_EVENT))
815             {
816                 LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
817             }
818         }
819 
820         delete p_VideoTrack;
821     }
822 
823     // Unregister the data probe.
824     ret = GST_PAD_PROBE_REMOVE;
825 
826 exit:
827     if (pCaps != NULL)
828         gst_caps_unref(pCaps);
829     if (pSinkPad != NULL)
830         gst_object_unref(pSinkPad);
831 
832     return ret;
833 }
    </pre>
  </body>
</html>
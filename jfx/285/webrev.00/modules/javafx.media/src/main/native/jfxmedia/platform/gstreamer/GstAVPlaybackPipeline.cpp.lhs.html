<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames modules/javafx.media/src/main/native/jfxmedia/platform/gstreamer/GstAVPlaybackPipeline.cpp</title>
    <link rel="stylesheet" href="../../../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
<a name="1" id="anc1"></a><span class="line-modified">  2  * Copyright (c) 2010, 2018, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.  Oracle designates this
  8  * particular file as subject to the &quot;Classpath&quot; exception as provided
  9  * by Oracle in the LICENSE file that accompanied this code.
 10  *
 11  * This code is distributed in the hope that it will be useful, but WITHOUT
 12  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 13  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 14  * version 2 for more details (a copy is included in the LICENSE file that
 15  * accompanied this code).
 16  *
 17  * You should have received a copy of the GNU General Public License version
 18  * 2 along with this work; if not, write to the Free Software Foundation,
 19  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 20  *
 21  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 22  * or visit www.oracle.com if you need additional information or have any
 23  * questions.
 24  */
 25 
 26 #include &quot;GstAVPlaybackPipeline.h&quot;
 27 
 28 #include &quot;GstVideoFrame.h&quot;
 29 #include &lt;gst/gst.h&gt;
 30 #include &lt;gst/app/gstappsink.h&gt;
 31 #include &lt;PipelineManagement/VideoTrack.h&gt;
 32 #include &lt;MediaManagement/Media.h&gt;
 33 #include &lt;jni/Logger.h&gt;
 34 #include &lt;Common/VSMemory.h&gt;
 35 #include &lt;Utils/LowLevelPerf.h&gt;
 36 
 37 #define MAX_SIZE_BUFFERS_LIMIT 25
 38 #define MAX_SIZE_BUFFERS_INC   5
 39 
 40 //*************************************************************************************************
 41 //********** class CGstAVPlaybackPipeline
 42 //*************************************************************************************************
 43 
 44 /**
 45  * CGstAVPlaybackPipeline::CGstAVPlaybackPipeline()
 46  *
 47  * Constructor
 48  *
 49  * @param   elements    GStreamer container of elements
 50  * @param   pOptions    options for the pipeline
 51  */
 52 CGstAVPlaybackPipeline::CGstAVPlaybackPipeline(const GstElementContainer&amp; elements, int audioFlags, CPipelineOptions* pOptions)
 53 :   CGstAudioPlaybackPipeline(elements, audioFlags, pOptions)
 54 {
 55     LOGGER_LOGMSG(LOGGER_DEBUG, &quot;CGstAVPlaybackPipeline::CGstAVPlaybackPipeline()&quot;);
 56     m_videoDecoderSrcProbeHID = 0L;
 57     m_EncodedVideoFrameRate = 24.0F;
 58     m_SendFrameSizeEvent = TRUE;
 59     m_FrameWidth = 0;
 60     m_FrameHeight = 0;
 61     m_videoCodecErrorCode = ERROR_NONE;
 62     m_bStaticPipeline = false; // For now all video pipelines are dynamic
 63 }
 64 
 65 /**
 66  * CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()
 67  *
 68  * Destructor
 69  */
 70 CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()
 71 {
 72 #if JFXMEDIA_DEBUG
 73     g_print (&quot;CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()\n&quot;);
 74 #endif
 75     LOGGER_LOGMSG(LOGGER_DEBUG, &quot;CGstAVPlaybackPipeline::~CGstAVPlaybackPipeline()&quot;);
 76 }
 77 
 78 /**
 79  * CGstAVPlaybackPipeline::Init()
 80  *
 81  * Initialize the pipeline.
 82  */
 83 uint32_t CGstAVPlaybackPipeline::Init()
 84 {
 85     g_signal_connect(m_Elements[AV_DEMUXER], &quot;pad-added&quot;, G_CALLBACK (on_pad_added), this);
 86     g_signal_connect(m_Elements[AV_DEMUXER], &quot;no-more-pads&quot;, G_CALLBACK (no_more_pads), this);
 87     g_signal_connect(m_Elements[AUDIO_QUEUE], &quot;overrun&quot;, G_CALLBACK (queue_overrun), this);
 88     g_signal_connect(m_Elements[VIDEO_QUEUE], &quot;overrun&quot;, G_CALLBACK (queue_overrun), this);
 89     g_signal_connect(m_Elements[AUDIO_QUEUE], &quot;underrun&quot;, G_CALLBACK (queue_underrun), this);
 90     g_signal_connect(m_Elements[VIDEO_QUEUE], &quot;underrun&quot;, G_CALLBACK (queue_underrun), this);
 91 
 92     return CGstAudioPlaybackPipeline::Init();
 93 }
 94 
 95 uint32_t CGstAVPlaybackPipeline::PostBuildInit()
 96 {
 97     if (m_bHasVideo &amp;&amp; !m_bVideoInitDone)
 98     {
 99 #if ENABLE_APP_SINK &amp;&amp; !ENABLE_NATIVE_SINK
100         //Tell it to push signals to us in sync mode so that audio and video are sync&#39;d
101         g_object_set (G_OBJECT (m_Elements[VIDEO_SINK]), &quot;emit-signals&quot;, TRUE, &quot;sync&quot;, TRUE, NULL);
102 
103         //Connect the callback
104         g_signal_connect (m_Elements[VIDEO_SINK], &quot;new-sample&quot;, G_CALLBACK (OnAppSinkHaveFrame), this);
105         g_signal_connect (m_Elements[VIDEO_SINK], &quot;new-preroll&quot;, G_CALLBACK (OnAppSinkPreroll), this);
106 #endif
107 
108         // Add a buffer probe on the sink pad of the decoder to capture frame rate
109         GstPad *pPad = gst_element_get_static_pad(m_Elements[VIDEO_DECODER], &quot;src&quot;);
110         if (NULL == pPad)
111             return ERROR_GSTREAMER_VIDEO_DECODER_SINK_PAD;
112         m_videoDecoderSrcProbeHID = gst_pad_add_probe(pPad, GST_PAD_PROBE_TYPE_BUFFER, (GstPadProbeCallback)VideoDecoderSrcProbe, this, NULL);
113         gst_object_unref(pPad);
114 
115         m_bVideoInitDone = true;
116     }
117 
118     return CGstAudioPlaybackPipeline::PostBuildInit();
119 }
120 
121 /**
122  * CGstAVPlaybackPipeline::Dispose()
123  *
124  * Disposes of resources held by this object. The pipeline should not be used
125  * once this method has been invoked.
126  */
127 void CGstAVPlaybackPipeline::Dispose()
128 {
129 #if JFXMEDIA_DEBUG
130     g_print (&quot;CGstAVPlaybackPipeline::Dispose()\n&quot;);
131 #endif
132 
133     if (m_bHasVideo &amp;&amp; m_bVideoInitDone)
134     {
135 #if ENABLE_APP_SINK &amp;&amp; !ENABLE_NATIVE_SINK
136         g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_SINK], (void*)G_CALLBACK(OnAppSinkHaveFrame), this);
137         g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_SINK], (void*)G_CALLBACK(OnAppSinkPreroll), this);
138 #endif
139     }
140 
141     g_signal_handlers_disconnect_by_func(m_Elements[AUDIO_QUEUE], (void*)G_CALLBACK(queue_overrun), this);
142     g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_QUEUE], (void*)G_CALLBACK(queue_overrun), this);
143     g_signal_handlers_disconnect_by_func(m_Elements[AUDIO_QUEUE], (void*)G_CALLBACK(queue_underrun), this);
144     g_signal_handlers_disconnect_by_func(m_Elements[VIDEO_QUEUE], (void*)G_CALLBACK(queue_underrun), this);
145 
146     CGstAudioPlaybackPipeline::Dispose();
147 
148     if (!m_bHasAudio &amp;&amp; m_Elements[AUDIO_BIN] != NULL)
149         gst_object_unref(m_Elements[AUDIO_BIN]);
150 
151     if (!m_bHasVideo &amp;&amp; m_Elements[VIDEO_BIN] != NULL)
152         gst_object_unref(m_Elements[VIDEO_BIN]);
153 }
154 
155 bool CGstAVPlaybackPipeline::IsCodecSupported(GstCaps *pCaps)
156 {
157 #if TARGET_OS_WIN32
158     GstStructure *s = NULL;
159     const gchar *mimetype = NULL;
160 
161     if (pCaps)
162     {
163         s = gst_caps_get_structure (pCaps, 0);
164         if (s != NULL)
165         {
166             mimetype = gst_structure_get_name (s);
167             if (mimetype != NULL)
168             {
169                 if (strstr(mimetype, &quot;video/x-h264&quot;) != NULL) // H.264
170                 {
171                     gboolean is_supported = FALSE;
172                     g_object_set(m_Elements[VIDEO_DECODER], &quot;codec-id&quot;, (gint)CODEC_ID_AVC1, NULL); // Check for AVC1 (MP4). For HLS we should get error early
173                     g_object_get(m_Elements[VIDEO_DECODER], &quot;is-supported&quot;, &amp;is_supported, NULL);
174                     if (is_supported)
175                     {
176                         return TRUE;
177                     }
178                     else
179                     {
180                         m_videoCodecErrorCode = ERROR_MEDIA_H264_FORMAT_UNSUPPORTED;
181                         return FALSE;
182                     }
183                 }
184             }
185         }
186     }
187 
188     return CGstAudioPlaybackPipeline::IsCodecSupported(pCaps);
189 #else // TARGET_OS_WIN32
190     GstStructure *s = NULL;
191     const gchar *mimetype = NULL;
192 
193     if (pCaps)
194     {
195         s = gst_caps_get_structure (pCaps, 0);
196         if (s != NULL)
197         {
198             mimetype = gst_structure_get_name (s);
199             if (mimetype != NULL)
200             {
201                 if (strstr(mimetype, &quot;video/unsupported&quot;) != NULL)
202                 {
203                     m_videoCodecErrorCode = ERROR_MEDIA_VIDEO_FORMAT_UNSUPPORTED;
204                     return FALSE;
205                 }
206             }
207         }
208     }
209 
210     return CGstAudioPlaybackPipeline::IsCodecSupported(pCaps);
211 #endif // TRAGET_OS_WIN32
212 }
213 
214 bool CGstAVPlaybackPipeline::CheckCodecSupport()
215 {
216     if (!m_bHasVideo)
217     {
218         if (!CGstAudioPlaybackPipeline::CheckCodecSupport())
219         {
220             if (m_pEventDispatcher &amp;&amp; m_videoCodecErrorCode != ERROR_NONE)
221             {
222                 if (!m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(m_videoCodecErrorCode))
223                 {
224                     LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
225                 }
226 
227                 return FALSE;
228             }
229         }
230     }
231     else
232     {
233         return CGstAudioPlaybackPipeline::CheckCodecSupport();
234     }
235     return FALSE;
236 }
237 
238 /**
239  * CGstAVPlaybackPipeline::SetEncodedVideoFrameRate()
240  *
241  * Sets the encoded video frame rate data member.
242  */
243 void CGstAVPlaybackPipeline::SetEncodedVideoFrameRate(float frameRate)
244 {
245     m_EncodedVideoFrameRate = frameRate;
246 }
247 
248 /**
249  * CGstAVPlaybackPipeline::OnAppSinkHaveFrame()
250  *
251  * AppSink callback that receives frames from GStreamer.
252  *
253  * @param   pElem       GStreamer element that is calling this callback
254  * @param   pPipeline   Pointer to this class, passed back as user data
255  */
256 GstFlowReturn CGstAVPlaybackPipeline::OnAppSinkHaveFrame(GstElement* pElem, CGstAVPlaybackPipeline* pPipeline)
257 {
258     LOWLEVELPERF_RESETCOUNTER(&quot;FPS&quot;);
259 
260     //***** get the buffer from appsink
261     GstSample* pSample = gst_app_sink_pull_sample(GST_APP_SINK (pElem));
262     if (pSample == NULL)
263         return GST_FLOW_OK;
264 
265     GstBuffer* pBuffer = gst_sample_get_buffer(pSample);
266     if (pBuffer == NULL)
267     {
268         gst_sample_unref(pSample);
269         return GST_FLOW_OK;
270     }
271 
272     if (pPipeline-&gt;m_SendFrameSizeEvent || GST_BUFFER_IS_DISCONT(pBuffer))
273         OnAppSinkVideoFrameDiscont(pPipeline, pSample);
274 
275     //***** Create a VideoFrame object
276     CGstVideoFrame* pVideoFrame = new CGstVideoFrame();
277     if (!pVideoFrame-&gt;Init(pSample))
278     {
279         gst_sample_unref(pSample);
280         delete pVideoFrame;
281         return GST_FLOW_OK;
282     }
283 
284     if (pVideoFrame-&gt;IsValid() &amp;&amp; pPipeline-&gt;m_pEventDispatcher)
285     {
286         CPlayerEventDispatcher* pEventDispatcher = pPipeline-&gt;m_pEventDispatcher;
287 
288         // Send new frame which Java will delete later.
289         if (!pEventDispatcher-&gt;SendNewFrameEvent(pVideoFrame))
290         {
291             if(!pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_NEW_FRAME_EVENT))
292             {
293                 LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
294             }
295         }
296     }
297     else
298     {
299         delete pVideoFrame;
300         if (pPipeline-&gt;m_pEventDispatcher != NULL) {
301             pPipeline-&gt;m_pEventDispatcher-&gt;Warning(WARNING_GSTREAMER_INVALID_FRAME,
302                                                    &quot;Invalid frame&quot;);
303         }
304 
305     }
306 
307 // INLINE - gst_sample_unref()
308     gst_sample_unref (pSample);
309 
310     return GST_FLOW_OK;
311 }
312 
313 /**
314  * CGstAVPlaybackPipeline::OnAppSinkPreroll()
315  *
316  * Gets some initial information such as the first frame and the height and width.
317  *
318  * @param   elements    GStreamer container of elements
319  */
320 GstFlowReturn CGstAVPlaybackPipeline::OnAppSinkPreroll(GstElement* pElem, CGstAVPlaybackPipeline* pPipeline)
321 {
322     LOWLEVELPERF_EXECTIMESTOP(&quot;nativeInitNativeMediaManagerToVideoPreroll&quot;);
323 
324     //***** get the buffer from appsink
325     GstSample* pSample = gst_app_sink_pull_preroll(GST_APP_SINK (pElem));
326 
327     GstBuffer* pBuffer = gst_sample_get_buffer(pSample);
328     if (pBuffer == NULL)
329     {
330         gst_sample_unref(pSample);
331         return GST_FLOW_OK;
332     }
333 
334     if (pPipeline-&gt;m_SendFrameSizeEvent || GST_BUFFER_IS_DISCONT(pBuffer))
335         OnAppSinkVideoFrameDiscont(pPipeline, pSample);
336 
337     // Send frome 0 up to use as poster frame.
338     if(pPipeline-&gt;m_pEventDispatcher != NULL)
339     {
340         CGstVideoFrame* pVideoFrame = new CGstVideoFrame();
341         if (!pVideoFrame-&gt;Init(pSample))
342         {
343             // INLINE - gst_sample_unref()
344             gst_sample_unref (pSample);
345             delete pVideoFrame;
346             return GST_FLOW_OK;
347         }
348         if (pVideoFrame-&gt;IsValid()) {
349             if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendNewFrameEvent(pVideoFrame))
350             {
351                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_NEW_FRAME_EVENT))
352                 {
353                     LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
354                 }
355             }
356         } else {
357             delete pVideoFrame;
358             if (pPipeline-&gt;m_pEventDispatcher != NULL) {
359                 pPipeline-&gt;m_pEventDispatcher-&gt;Warning(WARNING_GSTREAMER_INVALID_FRAME, &quot;Invalid frame&quot;);
360             }
361         }
362     }
363 
364 // INLINE - gst_sample_unref()
365     gst_sample_unref (pSample);
366 
367     return GST_FLOW_OK;
368 }
369 
370 void CGstAVPlaybackPipeline::OnAppSinkVideoFrameDiscont(CGstAVPlaybackPipeline* pPipeline, GstSample *pSample)
371 {
372     gint width, height;
373 
374     GstCaps* caps = gst_sample_get_caps(pSample);
375     if (caps == NULL)
376         return;
377 
378     const GstStructure* str = gst_caps_get_structure(caps, 0);
379     if (str == NULL)
380         return;
381 
382     if (!gst_structure_get_int(str, &quot;width&quot;, &amp;width))
383     {
384         pPipeline-&gt;m_pEventDispatcher-&gt;Warning (WARNING_GSTREAMER_PIPELINE_FRAME_SIZE, (char*)&quot;width could not be retrieved from preroll GstBuffer&quot;);
385         width = 0;
386     }
387     if (!gst_structure_get_int(str, &quot;height&quot;, &amp;height))
388     {
389         pPipeline-&gt;m_pEventDispatcher-&gt;Warning (WARNING_GSTREAMER_PIPELINE_FRAME_SIZE, (char*)&quot;height could not be retrieved from preroll GstBuffer&quot;);
390         height = 0;
391     }
392 
393     if (pPipeline-&gt;m_SendFrameSizeEvent || width != pPipeline-&gt;m_FrameWidth || height != pPipeline-&gt;m_FrameHeight)
394     {
395         // Save values for possible later use.
396         pPipeline-&gt;m_FrameWidth = width;
397         pPipeline-&gt;m_FrameHeight = height;
398 
399         if (pPipeline-&gt;m_pEventDispatcher != NULL)
400         {
401             pPipeline-&gt;m_SendFrameSizeEvent = !pPipeline-&gt;m_pEventDispatcher-&gt;SendFrameSizeChangedEvent(pPipeline-&gt;m_FrameWidth, pPipeline-&gt;m_FrameHeight);
402             if (pPipeline-&gt;m_SendFrameSizeEvent)
403             {
404                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_FRAME_SIZE_CHANGED_EVENT))
405                 {
406                     LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
407                 }
408             }
409         }
410         else
411             pPipeline-&gt;m_SendFrameSizeEvent = TRUE;
412     }
413 }
414 
415 /**
416  * CGstAVPlaybackPipeline::CGstAVPlaybackPipeline()
417  *
418  *
419  *
420  * @param
421  */
422 void CGstAVPlaybackPipeline::on_pad_added(GstElement *element, GstPad *pad, CGstAVPlaybackPipeline *pPipeline)
423 {
424     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Enter();
425 
426     if (pPipeline-&gt;m_pBusCallbackContent-&gt;m_bIsDisposeInProgress)
427     {
428         pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
429         return;
430     }
431 
432     GstCaps *pCaps = gst_pad_get_current_caps(pad);
433     const GstStructure *pStructure = gst_caps_get_structure(pCaps, 0);
434     const gchar* pstrName = gst_structure_get_name(pStructure);
435     GstPad *pPad = NULL;
436     GstPadLinkReturn ret = GST_PAD_LINK_OK;
<a name="2" id="anc2"></a>
437 
438     if (g_str_has_prefix(pstrName, &quot;audio&quot;))
439     {
440          // Ignore additional audio tracks if we already have one.
441          // Otherwise files with multiple audio track will fail to play, since
442          // we will not able to connect second audio track.
443          if (pPipeline-&gt;m_bHasAudio)
444          {
445             if (pCaps != NULL)
446                 gst_caps_unref(pCaps);
447 
448             pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
449             return;
450         }
451 
452         if (pPipeline-&gt;IsCodecSupported(pCaps))
453         {
454             pPad = gst_element_get_static_pad(pPipeline-&gt;m_Elements[AUDIO_BIN], &quot;sink&quot;);
455             gst_bin_add(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[AUDIO_BIN]);
<a name="3" id="anc3"></a><span class="line-modified">456             gst_element_set_state(pPipeline-&gt;m_Elements[AUDIO_BIN], GST_STATE_READY);</span>













457             if (pPad != NULL)
458             {
459                 ret = gst_pad_link (pad, pPad);
460                 if (ret != GST_PAD_LINK_OK)
461                 {
462                     gst_element_set_state(pPipeline-&gt;m_Elements[AUDIO_BIN], GST_STATE_NULL);
463                     gst_object_ref(pPipeline-&gt;m_Elements[AUDIO_BIN]);
464                     gst_bin_remove(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[AUDIO_BIN]);
<a name="4" id="anc4"></a>

465                     goto Error;
466                 }
467             }
468             pPipeline-&gt;m_bHasAudio = true;
469             pPipeline-&gt;PostBuildInit();
470             gst_element_sync_state_with_parent(pPipeline-&gt;m_Elements[AUDIO_BIN]);
471         }
472     }
473     else if (g_str_has_prefix(pstrName, &quot;video&quot;))
474     {
475         if (pPipeline-&gt;IsCodecSupported(pCaps))
476         {
477             pPad = gst_element_get_static_pad(pPipeline-&gt;m_Elements[VIDEO_BIN], &quot;sink&quot;);
478             gst_bin_add (GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[VIDEO_BIN]);
<a name="5" id="anc5"></a><span class="line-modified">479             gst_element_set_state(pPipeline-&gt;m_Elements[VIDEO_BIN], GST_STATE_READY);</span>








480             if (pPad != NULL)
481             {
482                 ret = gst_pad_link (pad, pPad);
483                 if (ret != GST_PAD_LINK_OK)
484                 {
485                     gst_element_set_state(pPipeline-&gt;m_Elements[VIDEO_BIN], GST_STATE_NULL);
486                     gst_object_ref(pPipeline-&gt;m_Elements[VIDEO_BIN]);
487                     gst_bin_remove(GST_BIN (pPipeline-&gt;m_Elements[PIPELINE]), pPipeline-&gt;m_Elements[VIDEO_BIN]);
488                     goto Error;
489                 }
490             }
491             pPipeline-&gt;m_bHasVideo = true;
492             pPipeline-&gt;PostBuildInit();
493             gst_element_sync_state_with_parent(pPipeline-&gt;m_Elements[VIDEO_BIN]);
494         }
495     }
496 
497 Error:
498     // Check if we have error set.
499     if (ret != GST_PAD_LINK_OK &amp;&amp; pPipeline-&gt;m_pEventDispatcher != NULL) {
500         // Handle special case for GST_PAD_LINK_NOFORMAT, which means format is not supported
501         if (ret == GST_PAD_LINK_NOFORMAT) {
502             if (g_str_has_prefix(pstrName, &quot;audio&quot;))
503             {
504                 pPipeline-&gt;m_audioCodecErrorCode = ERROR_MEDIA_AUDIO_FORMAT_UNSUPPORTED;
505             }
506             else if (g_str_has_prefix(pstrName, &quot;video&quot;))
507             {
508                 pPipeline-&gt;m_videoCodecErrorCode = ERROR_MEDIA_VIDEO_FORMAT_UNSUPPORTED;
509             }
510         } else {
511             GTimeVal now;
512             g_get_current_time (&amp;now);
513             if (g_str_has_prefix(pstrName, &quot;audio&quot;))
514             {
515                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerHaltEvent(&quot;Failed to link AV parser to audio bin!&quot;, (double)GST_TIMEVAL_TO_TIME (now)))
516                 {
517                     if(!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_PLAYER_HALT_EVENT))
518                     {
519                         LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
520                     }
521                 }
522             }
523             else if (g_str_has_prefix(pstrName, &quot;video&quot;))
524             {
525                 if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerHaltEvent(&quot;Failed to link AV parser to video bin!&quot;, (double)GST_TIMEVAL_TO_TIME (now)))
526                 {
527                     if(!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_PLAYER_HALT_EVENT))
528                     {
529                         LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
530                     }
531                 }
532             }
533         }
534     }
535 
536     if (pPad != NULL)
537         gst_object_unref(pPad);
538     if (pCaps != NULL)
539         gst_caps_unref(pCaps);
540 
541     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
542 }
543 
544 /**
545  * CGstAVPlaybackPipeline::no_more_pads()
546  *
547  *
548  *
549  * @param   elements    GStreamer container of elements
550  */
551 void CGstAVPlaybackPipeline::no_more_pads(GstElement *element, CGstAVPlaybackPipeline *pPipeline)
552 {
553     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Enter();
554 
555     if (pPipeline-&gt;m_pBusCallbackContent-&gt;m_bIsDisposeInProgress)
556     {
557         pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
558         return;
559     }
560 
561     g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(on_pad_added), pPipeline);
562     g_signal_handlers_disconnect_by_func(element, (void*)G_CALLBACK(no_more_pads), pPipeline);
563 
564     pPipeline-&gt;CheckCodecSupport();
565 
566     if (!pPipeline-&gt;m_bHasAudio)
567         pPipeline-&gt;m_bAudioSinkReady = true;
568     if (!pPipeline-&gt;m_bHasVideo)
569         pPipeline-&gt;m_bVideoSinkReady = true;
570 
571     pPipeline-&gt;m_pBusCallbackContent-&gt;m_DisposeLock-&gt;Exit();
572 }
573 
574 void CGstAVPlaybackPipeline::CheckQueueSize(GstElement *element)
575 {
576     guint current_level_buffers = 0;
577     guint max_size_buffers = 0;
578 
579     if (element == NULL)
580     {
581         g_object_get(m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
582         if (current_level_buffers &gt;= max_size_buffers)
583         {
584             element = m_Elements[VIDEO_QUEUE];
585         }
586         else
587         {
588             g_object_get(m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
589             if (current_level_buffers &gt;= max_size_buffers)
590                 element = m_Elements[AUDIO_QUEUE];
591         }
592 
593         if (element == NULL)
594             return;
595     }
596 
597     GstState state, pending_state;
598     gst_element_get_state(m_Elements[PIPELINE], &amp;state, &amp;pending_state, 0);
599 
600     gboolean inc_size_time = FALSE;
601     if (IsPlayerState(Unknown) || m_StallOnPause || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PLAYING) || (state == GST_STATE_PLAYING &amp;&amp; pending_state == GST_STATE_PAUSED))
602     {
603 
604         if (m_Elements[AUDIO_QUEUE] == element)
605         {
606             g_object_get(m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
607             if (current_level_buffers &lt; MAX_SIZE_BUFFERS_LIMIT)
608                 inc_size_time = TRUE;
609         }
610         else if (m_Elements[VIDEO_QUEUE] == element)
611         {
612             g_object_get(m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
613             if (current_level_buffers &lt; MAX_SIZE_BUFFERS_LIMIT)
614                 inc_size_time = TRUE;
615         }
616     }
617     else if ((state == GST_STATE_PLAYING &amp;&amp; pending_state == GST_STATE_VOID_PENDING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PLAYING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PAUSED))
618     {
619         // Do not increment queue if we playing and only have one track
620         if (!(m_bHasAudio &amp;&amp; m_bHasVideo))
621             return;
622 
623         if (m_Elements[AUDIO_QUEUE] == element)
624         {
625             g_object_get(m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
626             if (current_level_buffers == 0)
627                 inc_size_time = TRUE;
628         }
629         else if (m_Elements[VIDEO_QUEUE] == element)
630         {
631             g_object_get(m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
632             if (current_level_buffers == 0)
633                 inc_size_time = TRUE;
634         }
635     }
636 
637     if (inc_size_time)
638     {
639         g_object_get(element, &quot;max-size-buffers&quot;, &amp;max_size_buffers, NULL);
640         max_size_buffers += MAX_SIZE_BUFFERS_INC;
641         g_object_set(element, &quot;max-size-buffers&quot;, max_size_buffers, NULL);
642     }
643 }
644 
645 void CGstAVPlaybackPipeline::queue_overrun(GstElement *element, CGstAVPlaybackPipeline *pPipeline)
646 {
647     pPipeline-&gt;CheckQueueSize(element);
648 }
649 
650 void CGstAVPlaybackPipeline::queue_underrun(GstElement *element, CGstAVPlaybackPipeline *pPipeline)
651 {
652     if (pPipeline-&gt;m_pOptions-&gt;GetHLSModeEnabled())
653     {
654         if (pPipeline-&gt;m_Elements[AUDIO_QUEUE] == element)
655         {
656             GstStructure *s = gst_structure_new_empty(HLS_PB_MESSAGE_STALL);
657             GstMessage *msg = gst_message_new_application(GST_OBJECT(element), s);
658             gst_element_post_message(GST_ELEMENT(element), msg);
659         }
660     }
661     else
662     {
663         gboolean inc_size_time = FALSE;
664         guint current_level_buffers = 0;
665         guint max_size_buffers = 0;
666         GstState state, pending_state;
667         GstElement* inc_element = NULL;
668 
669         gst_element_get_state(pPipeline-&gt;m_Elements[PIPELINE], &amp;state, &amp;pending_state, 0);
670 
671         if ((state == GST_STATE_PLAYING &amp;&amp; pending_state == GST_STATE_VOID_PENDING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PLAYING) || (state == GST_STATE_PAUSED &amp;&amp; pending_state == GST_STATE_PAUSED))
672         {
673             if (pPipeline-&gt;m_Elements[AUDIO_QUEUE] == element)
674             {
675                 g_object_get(pPipeline-&gt;m_Elements[VIDEO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
676                 g_object_get(pPipeline-&gt;m_Elements[VIDEO_QUEUE], &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
677                 if (current_level_buffers == max_size_buffers)
678                 {
679                     inc_element = pPipeline-&gt;m_Elements[VIDEO_QUEUE];
680                     inc_size_time = TRUE;
681                 }
682             }
683             else if (pPipeline-&gt;m_Elements[VIDEO_QUEUE] == element)
684             {
685                 g_object_get(pPipeline-&gt;m_Elements[AUDIO_QUEUE], &quot;current-level-buffers&quot;, &amp;current_level_buffers, NULL);
686                 g_object_get(pPipeline-&gt;m_Elements[AUDIO_QUEUE], &quot;max_size_buffers&quot;, &amp;max_size_buffers, NULL);
687                 if (current_level_buffers == max_size_buffers)
688                 {
689                     inc_element = pPipeline-&gt;m_Elements[AUDIO_QUEUE];
690                     inc_size_time = TRUE;
691                 }
692             }
693         }
694 
695         if (inc_size_time)
696         {
697             g_object_get(inc_element, &quot;max-size-buffers&quot;, &amp;max_size_buffers, NULL);
698             max_size_buffers += MAX_SIZE_BUFFERS_INC;
699             g_object_set(inc_element, &quot;max-size-buffers&quot;, max_size_buffers, NULL);
700         }
701     }
702 }
703 
704 /**
705  * CGstAVPlaybackPipeline::VideoDecoderSrcProbe()
706  *
707  *
708  *
709  * @param
710  */
711 GstPadProbeReturn CGstAVPlaybackPipeline::VideoDecoderSrcProbe(GstPad* pPad, GstPadProbeInfo *pInfo, CGstAVPlaybackPipeline* pPipeline)
712 {
713     GstPadProbeReturn ret = GST_PAD_PROBE_OK;
714     GstCaps *pCaps = NULL;
715     GstPad *pSinkPad = NULL;
716 
717     if (pPipeline-&gt;m_pEventDispatcher)
718     {
719         GstStructure *pStructure = NULL;
720         bool hasAlpha = false;
721         gboolean enabled;
722 
723         string           strMimeType;
724         CTrack::Encoding encoding;
725         gint             width    = 0;
726         gint             height   = 0;
727         gint             fr_num   = 0;
728         gint             fr_denom = 1; // We don&#39;t want do divide by zero
729         gint trackID;
730 
731         // Make sure we got requested probe
732         if ((pInfo-&gt;type &amp; GST_PAD_PROBE_TYPE_BUFFER) != GST_PAD_PROBE_TYPE_BUFFER || pInfo-&gt;data == NULL)
733             goto exit;
734 
735         // Get resolution and framerate from src pad
736         if (NULL == (pCaps = gst_pad_get_current_caps(pPad)) ||
737             NULL == (pStructure = gst_caps_get_structure(pCaps, 0)))
738             goto exit;
739 
740         if (!gst_structure_get_int(pStructure, &quot;width&quot;, &amp;width) ||
741             !gst_structure_get_int(pStructure, &quot;height&quot;, &amp;height) ||
742             !gst_structure_get_fraction(pStructure, &quot;framerate&quot;, &amp;fr_num, &amp;fr_denom) ||
743             0 == fr_denom)
744                 goto exit;
745 
746         float frameRate = (float) fr_num / fr_denom;
747         pPipeline-&gt;SetEncodedVideoFrameRate(frameRate);
748 
749         // Get encoding and track ID from sink pad
750         if (pCaps != NULL)
751             gst_caps_unref(pCaps);
752 
753         pSinkPad = gst_element_get_static_pad(pPipeline-&gt;m_Elements[VIDEO_DECODER], &quot;sink&quot;);
754         if (NULL == pSinkPad ||
755             NULL == (pCaps = gst_pad_get_current_caps(pSinkPad)) ||
756             NULL == (pStructure = gst_caps_get_structure(pCaps, 0)))
757         {
758             goto exit;
759         }
760 
761         strMimeType = gst_structure_get_name(pStructure);
762 
763         if (strMimeType.find(&quot;video/x-h264&quot;) != string::npos) {
764             encoding = CTrack::H264;
765         } else {
766             encoding = CTrack::CUSTOM;
767         }
768 
769         if (!gst_structure_get_boolean(pStructure, &quot;track_enabled&quot;, &amp;enabled)) {
770             enabled = TRUE; // treat as enabled if field is not present
771         }
772 
773         if (!gst_structure_get_int(pStructure, &quot;track_id&quot;, &amp;trackID)) {
774             trackID = 1; // default to 1 for video track, in case container doesn&#39;t have track IDs
775         }
776 
777         // Create the video track.
778         CVideoTrack *p_VideoTrack = new CVideoTrack(
779             (int64_t)trackID,
780             strMimeType,
781             encoding,
782             (bool)enabled,
783             width, height,
784             frameRate,
785             hasAlpha);
786 
787         // Dispatch the track event.
788         if (!pPipeline-&gt;m_pEventDispatcher-&gt;SendVideoTrackEvent(p_VideoTrack))
789         {
790             if(!pPipeline-&gt;m_pEventDispatcher-&gt;SendPlayerMediaErrorEvent(ERROR_JNI_SEND_VIDEO_TRACK_EVENT))
791             {
792                 LOGGER_LOGMSG(LOGGER_ERROR, &quot;Cannot send media error event.\n&quot;);
793             }
794         }
795 
796         delete p_VideoTrack;
797     }
798 
799     // Unregister the data probe.
800     ret = GST_PAD_PROBE_REMOVE;
801 
802 exit:
803     if (pCaps != NULL)
804         gst_caps_unref(pCaps);
805     if (pSinkPad != NULL)
806         gst_object_unref(pSinkPad);
807 
808     return ret;
809 }
<a name="6" id="anc6"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="6" type="hidden" />
</body>
</html>
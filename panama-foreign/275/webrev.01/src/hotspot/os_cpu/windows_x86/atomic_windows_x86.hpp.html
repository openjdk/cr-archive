<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 26 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 27 
 28 #include &lt;intrin.h&gt;
 29 #include &quot;runtime/os.hpp&quot;
 30 
 31 // Note that in MSVC, volatile memory accesses are explicitly
 32 // guaranteed to have acquire release semantics (w.r.t. compiler
 33 // reordering) and therefore does not even need a compiler barrier
 34 // for normal acquire release accesses. And all generalized
 35 // bound calls like release_store go through Atomic::load
 36 // and Atomic::store which do volatile memory accesses.
 37 template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
 38 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
 39 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
 40 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
 41 
 42 template&lt;size_t byte_size&gt;
 43 struct Atomic::PlatformAdd {
 44   template&lt;typename D, typename I&gt;
 45   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 46 
 47   template&lt;typename D, typename I&gt;
 48   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {
 49     return add_and_fetch(dest, add_value, order) - add_value;
 50   }
 51 };
 52 
 53 // The Interlocked* APIs only take long and will not accept __int32. That is
 54 // acceptable on Windows, since long is a 32-bits integer type.
 55 
 56 #define DEFINE_INTRINSIC_ADD(IntrinsicName, IntrinsicType)                \
 57   template&lt;&gt;                                                              \
 58   template&lt;typename D, typename I&gt;                                        \
 59   inline D Atomic::PlatformAdd&lt;sizeof(IntrinsicType)&gt;::add_and_fetch(D volatile* dest, \
 60                                                                      I add_value, \
 61                                                                      atomic_memory_order order) const { \
 62     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(D));                    \
 63     return PrimitiveConversions::cast&lt;D&gt;(                                 \
 64       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \
 65                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(add_value))); \
 66   }
 67 
 68 DEFINE_INTRINSIC_ADD(InterlockedAdd,   long)
 69 DEFINE_INTRINSIC_ADD(InterlockedAdd64, __int64)
 70 
 71 #undef DEFINE_INTRINSIC_ADD
 72 
 73 #define DEFINE_INTRINSIC_XCHG(IntrinsicName, IntrinsicType)               \
 74   template&lt;&gt;                                                              \
 75   template&lt;typename T&gt;                                                    \
 76   inline T Atomic::PlatformXchg&lt;sizeof(IntrinsicType)&gt;::operator()(T volatile* dest, \
 77                                                                    T exchange_value, \
 78                                                                    atomic_memory_order order) const { \
 79     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \
 80     return PrimitiveConversions::cast&lt;T&gt;(                                 \
 81       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \
 82                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(exchange_value))); \
 83   }
 84 
 85 DEFINE_INTRINSIC_XCHG(InterlockedExchange,   long)
 86 DEFINE_INTRINSIC_XCHG(InterlockedExchange64, __int64)
 87 
 88 #undef DEFINE_INTRINSIC_XCHG
 89 
 90 // Note: the order of the parameters is different between
 91 // Atomic::PlatformCmpxchg&lt;*&gt;::operator() and the
 92 // InterlockedCompareExchange* API.
 93 
 94 #define DEFINE_INTRINSIC_CMPXCHG(IntrinsicName, IntrinsicType)            \
 95   template&lt;&gt;                                                              \
 96   template&lt;typename T&gt;                                                    \
 97   inline T Atomic::PlatformCmpxchg&lt;sizeof(IntrinsicType)&gt;::operator()(T volatile* dest, \
 98                                                                       T compare_value, \
 99                                                                       T exchange_value, \
100                                                                       atomic_memory_order order) const { \
101     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \
102     return PrimitiveConversions::cast&lt;T&gt;(                                 \
103       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \
104                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(exchange_value), \
105                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(compare_value))); \
106   }
107 
108 DEFINE_INTRINSIC_CMPXCHG(_InterlockedCompareExchange8, char) // Use the intrinsic as InterlockedCompareExchange8 does not exist
109 DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange,   long)
110 DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange64, __int64)
111 
112 #undef DEFINE_INTRINSIC_CMPXCHG
113 
114 #ifndef AMD64
115 
116 #pragma warning(disable: 4035) // Disables warnings reporting missing return statement
117 
118 template&lt;&gt;
119 template&lt;typename T&gt;
120 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
121   STATIC_ASSERT(8 == sizeof(T));
122   volatile T dest;
123   volatile T* pdest = &amp;dest;
124   __asm {
125     mov eax, src
126     fild     qword ptr [eax]
127     mov eax, pdest
128     fistp    qword ptr [eax]
129   }
130   return dest;
131 }
132 
133 template&lt;&gt;
134 template&lt;typename T&gt;
135 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
136                                                  T store_value) const {
137   STATIC_ASSERT(8 == sizeof(T));
138   volatile T* src = &amp;store_value;
139   __asm {
140     mov eax, src
141     fild     qword ptr [eax]
142     mov eax, dest
143     fistp    qword ptr [eax]
144   }
145 }
146 
147 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
148 
149 template&lt;&gt;
150 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
151 {
152   template &lt;typename T&gt;
153   void operator()(volatile T* p, T v) const {
154     __asm {
155       mov edx, p;
156       mov al, v;
157       xchg al, byte ptr [edx];
158     }
159   }
160 };
161 
162 template&lt;&gt;
163 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
164 {
165   template &lt;typename T&gt;
166   void operator()(volatile T* p, T v) const {
167     __asm {
168       mov edx, p;
169       mov ax, v;
170       xchg ax, word ptr [edx];
171     }
172   }
173 };
174 
175 template&lt;&gt;
176 struct Atomic::PlatformOrderedStore&lt;4, RELEASE_X_FENCE&gt;
177 {
178   template &lt;typename T&gt;
179   void operator()(volatile T* p, T v) const {
180     __asm {
181       mov edx, p;
182       mov eax, v;
183       xchg eax, dword ptr [edx];
184     }
185   }
186 };
187 #endif // AMD64
188 
189 #endif // OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
    </pre>
  </body>
</html>
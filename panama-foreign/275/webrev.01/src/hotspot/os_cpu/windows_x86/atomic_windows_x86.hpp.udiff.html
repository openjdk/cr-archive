<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Udiff src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../../os/windows/threadCritical_windows.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="os_windows_x86.cpp.udiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<hr />
<pre>
<span class="line-new-header">@@ -1,7 +1,7 @@</span>
  /*
<span class="udiff-line-modified-removed">-  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.</span>
<span class="udiff-line-modified-added">+  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   *
   * This code is free software; you can redistribute it and/or modify it
   * under the terms of the GNU General Public License version 2 only, as
   * published by the Free Software Foundation.
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -23,10 +23,11 @@</span>
   */
  
  #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
  #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
  
<span class="udiff-line-added">+ #include &lt;intrin.h&gt;</span>
  #include &quot;runtime/os.hpp&quot;
  
  // Note that in MSVC, volatile memory accesses are explicitly
  // guaranteed to have acquire release semantics (w.r.t. compiler
  // reordering) and therefore does not even need a compiler barrier
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -36,25 +37,10 @@</span>
  template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
  template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
  template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
  template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
  
<span class="udiff-line-removed">- // The following alternative implementations are needed because</span>
<span class="udiff-line-removed">- // Windows 95 doesn&#39;t support (some of) the corresponding Windows NT</span>
<span class="udiff-line-removed">- // calls. Furthermore, these versions allow inlining in the caller.</span>
<span class="udiff-line-removed">- // (More precisely: The documentation for InterlockedExchange says</span>
<span class="udiff-line-removed">- // it is supported for Windows 95. However, when single-stepping</span>
<span class="udiff-line-removed">- // through the assembly code we cannot step into the routine and</span>
<span class="udiff-line-removed">- // when looking at the routine address we see only garbage code.</span>
<span class="udiff-line-removed">- // Better safe then sorry!). Was bug 7/31/98 (gri).</span>
<span class="udiff-line-removed">- //</span>
<span class="udiff-line-removed">- // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not</span>
<span class="udiff-line-removed">- // necessary (and expensive). We should generate separate cases if</span>
<span class="udiff-line-removed">- // this becomes a performance problem.</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
<span class="udiff-line-removed">- </span>
  template&lt;size_t byte_size&gt;
  struct Atomic::PlatformAdd {
    template&lt;typename D, typename I&gt;
    D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
  
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -62,144 +48,74 @@</span>
    D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {
      return add_and_fetch(dest, add_value, order) - add_value;
    }
  };
  
<span class="udiff-line-modified-removed">- #ifdef AMD64</span>
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-modified-removed">- template&lt;typename D, typename I&gt;</span>
<span class="udiff-line-modified-removed">- inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="udiff-line-modified-removed">-                                                atomic_memory_order order) const {</span>
<span class="udiff-line-modified-removed">-   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, dest, add_value);</span>
<span class="udiff-line-modified-removed">- }</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-modified-removed">- template&lt;typename D, typename I&gt;</span>
<span class="udiff-line-modified-removed">- inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="udiff-line-modified-removed">-                                                atomic_memory_order order) const {</span>
<span class="udiff-line-modified-removed">-   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, dest, add_value);</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-removed">- </span>
<span class="udiff-line-removed">- #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \</span>
<span class="udiff-line-removed">-   template&lt;&gt;                                                            \</span>
<span class="udiff-line-removed">-   template&lt;typename T&gt;                                                  \</span>
<span class="udiff-line-removed">-   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \</span>
<span class="udiff-line-removed">-                                                       T exchange_value, \</span>
<span class="udiff-line-removed">-                                                       atomic_memory_order order) const { \</span>
<span class="udiff-line-removed">-     STATIC_ASSERT(ByteSize == sizeof(T));                               \</span>
<span class="udiff-line-removed">-     return xchg_using_helper&lt;StubType&gt;(StubName, dest, exchange_value); \</span>
<span class="udiff-line-modified-added">+ // The Interlocked* APIs only take long and will not accept __int32. That is</span>
<span class="udiff-line-modified-added">+ // acceptable on Windows, since long is a 32-bits integer type.</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ #define DEFINE_INTRINSIC_ADD(IntrinsicName, IntrinsicType)                \</span>
<span class="udiff-line-modified-added">+   template&lt;&gt;                                                              \</span>
<span class="udiff-line-modified-added">+   template&lt;typename D, typename I&gt;                                        \</span>
<span class="udiff-line-modified-added">+   inline D Atomic::PlatformAdd&lt;sizeof(IntrinsicType)&gt;::add_and_fetch(D volatile* dest, \</span>
<span class="udiff-line-modified-added">+                                                                      I add_value, \</span>
<span class="udiff-line-modified-added">+                                                                      atomic_memory_order order) const { \</span>
<span class="udiff-line-modified-added">+     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(D));                    \</span>
<span class="udiff-line-modified-added">+     return PrimitiveConversions::cast&lt;D&gt;(                                 \</span>
<span class="udiff-line-modified-added">+       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \</span>
<span class="udiff-line-modified-added">+                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(add_value))); \</span>
    }
  
<span class="udiff-line-modified-removed">- DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)</span>
<span class="udiff-line-modified-removed">- DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- #undef DEFINE_STUB_XCHG</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \</span>
<span class="udiff-line-modified-removed">-   template&lt;&gt;                                                               \</span>
<span class="udiff-line-modified-removed">-   template&lt;typename T&gt;                                                     \</span>
<span class="udiff-line-modified-removed">-   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \</span>
<span class="udiff-line-modified-removed">-                                                          T compare_value,  \</span>
<span class="udiff-line-modified-removed">-                                                          T exchange_value, \</span>
<span class="udiff-line-modified-removed">-                                                          atomic_memory_order order) const { \</span>
<span class="udiff-line-modified-removed">-     STATIC_ASSERT(ByteSize == sizeof(T));                                  \</span>
<span class="udiff-line-modified-removed">-     return cmpxchg_using_helper&lt;StubType&gt;(StubName, dest, compare_value, exchange_value); \</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_ADD(InterlockedAdd,   long)</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_ADD(InterlockedAdd64, __int64)</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ #undef DEFINE_INTRINSIC_ADD</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ #define DEFINE_INTRINSIC_XCHG(IntrinsicName, IntrinsicType)               \</span>
<span class="udiff-line-modified-added">+   template&lt;&gt;                                                              \</span>
<span class="udiff-line-modified-added">+   template&lt;typename T&gt;                                                    \</span>
<span class="udiff-line-modified-added">+   inline T Atomic::PlatformXchg&lt;sizeof(IntrinsicType)&gt;::operator()(T volatile* dest, \</span>
<span class="udiff-line-modified-added">+                                                                    T exchange_value, \</span>
<span class="udiff-line-modified-added">+                                                                    atomic_memory_order order) const { \</span>
<span class="udiff-line-modified-added">+     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \</span>
<span class="udiff-line-modified-added">+     return PrimitiveConversions::cast&lt;T&gt;(                                 \</span>
<span class="udiff-line-modified-added">+       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \</span>
<span class="udiff-line-added">+                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(exchange_value))); \</span>
    }
  
<span class="udiff-line-modified-removed">- DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)</span>
<span class="udiff-line-modified-removed">- DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)</span>
<span class="udiff-line-modified-removed">- DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- #undef DEFINE_STUB_CMPXCHG</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- #else // !AMD64</span>
<span class="udiff-line-modified-removed">- </span>
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-modified-removed">- template&lt;typename D, typename I&gt;</span>
<span class="udiff-line-modified-removed">- inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="udiff-line-modified-removed">-                                                atomic_memory_order order) const {</span>
<span class="udiff-line-modified-removed">-   STATIC_ASSERT(4 == sizeof(I));</span>
<span class="udiff-line-modified-removed">-   STATIC_ASSERT(4 == sizeof(D));</span>
<span class="udiff-line-modified-removed">-   __asm {</span>
<span class="udiff-line-modified-removed">-     mov edx, dest;</span>
<span class="udiff-line-modified-removed">-     mov eax, add_value;</span>
<span class="udiff-line-modified-removed">-     mov ecx, eax;</span>
<span class="udiff-line-modified-removed">-     lock xadd dword ptr [edx], eax;</span>
<span class="udiff-line-modified-removed">-     add eax, ecx;</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_XCHG(InterlockedExchange,   long)</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_XCHG(InterlockedExchange64, __int64)</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ #undef DEFINE_INTRINSIC_XCHG</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ // Note: the order of the parameters is different between</span>
<span class="udiff-line-modified-added">+ // Atomic::PlatformCmpxchg&lt;*&gt;::operator() and the</span>
<span class="udiff-line-modified-added">+ // InterlockedCompareExchange* API.</span>
<span class="udiff-line-modified-added">+ </span>
<span class="udiff-line-modified-added">+ #define DEFINE_INTRINSIC_CMPXCHG(IntrinsicName, IntrinsicType)            \</span>
<span class="udiff-line-modified-added">+   template&lt;&gt;                                                              \</span>
<span class="udiff-line-modified-added">+   template&lt;typename T&gt;                                                    \</span>
<span class="udiff-line-modified-added">+   inline T Atomic::PlatformCmpxchg&lt;sizeof(IntrinsicType)&gt;::operator()(T volatile* dest, \</span>
<span class="udiff-line-modified-added">+                                                                       T compare_value, \</span>
<span class="udiff-line-modified-added">+                                                                       T exchange_value, \</span>
<span class="udiff-line-modified-added">+                                                                       atomic_memory_order order) const { \</span>
<span class="udiff-line-modified-added">+     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \</span>
<span class="udiff-line-modified-added">+     return PrimitiveConversions::cast&lt;T&gt;(                                 \</span>
<span class="udiff-line-modified-added">+       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \</span>
<span class="udiff-line-modified-added">+                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(exchange_value), \</span>
<span class="udiff-line-added">+                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(compare_value))); \</span>
    }
<span class="udiff-line-removed">- }</span>
  
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-modified-removed">- template&lt;typename T&gt;</span>
<span class="udiff-line-modified-removed">- inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="udiff-line-removed">-                                              T exchange_value,</span>
<span class="udiff-line-removed">-                                              atomic_memory_order order) const {</span>
<span class="udiff-line-removed">-   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="udiff-line-removed">-   // alternative for InterlockedExchange</span>
<span class="udiff-line-removed">-   __asm {</span>
<span class="udiff-line-removed">-     mov eax, exchange_value;</span>
<span class="udiff-line-removed">-     mov ecx, dest;</span>
<span class="udiff-line-removed">-     xchg eax, dword ptr [ecx];</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_CMPXCHG(_InterlockedCompareExchange8, char) // Use the intrinsic as InterlockedCompareExchange8 does not exist</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange,   long)</span>
<span class="udiff-line-modified-added">+ DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange64, __int64)</span>
  
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-removed">- template&lt;typename T&gt;</span>
<span class="udiff-line-removed">- inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>
<span class="udiff-line-removed">-                                                 T compare_value,</span>
<span class="udiff-line-removed">-                                                 T exchange_value,</span>
<span class="udiff-line-removed">-                                                 atomic_memory_order order) const {</span>
<span class="udiff-line-removed">-   STATIC_ASSERT(1 == sizeof(T));</span>
<span class="udiff-line-removed">-   // alternative for InterlockedCompareExchange</span>
<span class="udiff-line-removed">-   __asm {</span>
<span class="udiff-line-removed">-     mov edx, dest</span>
<span class="udiff-line-removed">-     mov cl, exchange_value</span>
<span class="udiff-line-removed">-     mov al, compare_value</span>
<span class="udiff-line-removed">-     lock cmpxchg byte ptr [edx], cl</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-modified-added">+ #undef DEFINE_INTRINSIC_CMPXCHG</span>
  
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-removed">- template&lt;typename T&gt;</span>
<span class="udiff-line-removed">- inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="udiff-line-removed">-                                                 T compare_value,</span>
<span class="udiff-line-removed">-                                                 T exchange_value,</span>
<span class="udiff-line-removed">-                                                 atomic_memory_order order) const {</span>
<span class="udiff-line-removed">-   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="udiff-line-removed">-   // alternative for InterlockedCompareExchange</span>
<span class="udiff-line-removed">-   __asm {</span>
<span class="udiff-line-removed">-     mov edx, dest</span>
<span class="udiff-line-removed">-     mov ecx, exchange_value</span>
<span class="udiff-line-removed">-     mov eax, compare_value</span>
<span class="udiff-line-removed">-     lock cmpxchg dword ptr [edx], ecx</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-modified-added">+ #ifndef AMD64</span>
  
<span class="udiff-line-modified-removed">- template&lt;&gt;</span>
<span class="udiff-line-removed">- template&lt;typename T&gt;</span>
<span class="udiff-line-removed">- inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="udiff-line-removed">-                                                 T compare_value,</span>
<span class="udiff-line-removed">-                                                 T exchange_value,</span>
<span class="udiff-line-removed">-                                                 atomic_memory_order order) const {</span>
<span class="udiff-line-removed">-   STATIC_ASSERT(8 == sizeof(T));</span>
<span class="udiff-line-removed">-   int32_t ex_lo  = (int32_t)exchange_value;</span>
<span class="udiff-line-removed">-   int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );</span>
<span class="udiff-line-removed">-   int32_t cmp_lo = (int32_t)compare_value;</span>
<span class="udiff-line-removed">-   int32_t cmp_hi = *( ((int32_t*)&amp;compare_value) + 1 );</span>
<span class="udiff-line-removed">-   __asm {</span>
<span class="udiff-line-removed">-     push ebx</span>
<span class="udiff-line-removed">-     push edi</span>
<span class="udiff-line-removed">-     mov eax, cmp_lo</span>
<span class="udiff-line-removed">-     mov edx, cmp_hi</span>
<span class="udiff-line-removed">-     mov edi, dest</span>
<span class="udiff-line-removed">-     mov ebx, ex_lo</span>
<span class="udiff-line-removed">-     mov ecx, ex_hi</span>
<span class="udiff-line-removed">-     lock cmpxchg8b qword ptr [edi]</span>
<span class="udiff-line-removed">-     pop edi</span>
<span class="udiff-line-removed">-     pop ebx</span>
<span class="udiff-line-removed">-   }</span>
<span class="udiff-line-removed">- }</span>
<span class="udiff-line-modified-added">+ #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
  
  template&lt;&gt;
  template&lt;typename T&gt;
  inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
    STATIC_ASSERT(8 == sizeof(T));
</pre>
<hr />
<pre>
<span class="line-new-header">@@ -226,15 +142,12 @@</span>
      mov eax, dest
      fistp    qword ptr [eax]
    }
  }
  
<span class="udiff-line-removed">- #endif // AMD64</span>
<span class="udiff-line-removed">- </span>
  #pragma warning(default: 4035) // Enables warnings reporting missing return statement
  
<span class="udiff-line-removed">- #ifndef AMD64</span>
  template&lt;&gt;
  struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
  {
    template &lt;typename T&gt;
    void operator()(volatile T* p, T v) const {
</pre>
<center><a href="../../os/windows/threadCritical_windows.cpp.udiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="os_windows_x86.cpp.udiff.html" target="_top">next &gt;</a></center>  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../../os/windows/threadCritical_windows.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="os_windows_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/os_cpu/windows_x86/atomic_windows_x86.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 1999, 2019, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 26 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 27 

 28 #include &quot;runtime/os.hpp&quot;
 29 
 30 // Note that in MSVC, volatile memory accesses are explicitly
 31 // guaranteed to have acquire release semantics (w.r.t. compiler
 32 // reordering) and therefore does not even need a compiler barrier
 33 // for normal acquire release accesses. And all generalized
 34 // bound calls like release_store go through Atomic::load
 35 // and Atomic::store which do volatile memory accesses.
 36 template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
 37 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
 38 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
 39 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
 40 
<span class="line-removed"> 41 // The following alternative implementations are needed because</span>
<span class="line-removed"> 42 // Windows 95 doesn&#39;t support (some of) the corresponding Windows NT</span>
<span class="line-removed"> 43 // calls. Furthermore, these versions allow inlining in the caller.</span>
<span class="line-removed"> 44 // (More precisely: The documentation for InterlockedExchange says</span>
<span class="line-removed"> 45 // it is supported for Windows 95. However, when single-stepping</span>
<span class="line-removed"> 46 // through the assembly code we cannot step into the routine and</span>
<span class="line-removed"> 47 // when looking at the routine address we see only garbage code.</span>
<span class="line-removed"> 48 // Better safe then sorry!). Was bug 7/31/98 (gri).</span>
<span class="line-removed"> 49 //</span>
<span class="line-removed"> 50 // Performance note: On uniprocessors, the &#39;lock&#39; prefixes are not</span>
<span class="line-removed"> 51 // necessary (and expensive). We should generate separate cases if</span>
<span class="line-removed"> 52 // this becomes a performance problem.</span>
<span class="line-removed"> 53 </span>
<span class="line-removed"> 54 #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>
<span class="line-removed"> 55 </span>
 56 template&lt;size_t byte_size&gt;
 57 struct Atomic::PlatformAdd {
 58   template&lt;typename D, typename I&gt;
 59   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 60 
 61   template&lt;typename D, typename I&gt;
 62   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {
 63     return add_and_fetch(dest, add_value, order) - add_value;
 64   }
 65 };
 66 
<span class="line-modified"> 67 #ifdef AMD64</span>
<span class="line-modified"> 68 template&lt;&gt;</span>
<span class="line-modified"> 69 template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 70 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-modified"> 71                                                atomic_memory_order order) const {</span>
<span class="line-modified"> 72   return add_using_helper&lt;int32_t&gt;(os::atomic_add_func, dest, add_value);</span>
<span class="line-modified"> 73 }</span>
<span class="line-modified"> 74 </span>
<span class="line-modified"> 75 template&lt;&gt;</span>
<span class="line-modified"> 76 template&lt;typename D, typename I&gt;</span>
<span class="line-modified"> 77 inline D Atomic::PlatformAdd&lt;8&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-modified"> 78                                                atomic_memory_order order) const {</span>
<span class="line-modified"> 79   return add_using_helper&lt;int64_t&gt;(os::atomic_add_long_func, dest, add_value);</span>
<span class="line-removed"> 80 }</span>
<span class="line-removed"> 81 </span>
<span class="line-removed"> 82 #define DEFINE_STUB_XCHG(ByteSize, StubType, StubName)                  \</span>
<span class="line-removed"> 83   template&lt;&gt;                                                            \</span>
<span class="line-removed"> 84   template&lt;typename T&gt;                                                  \</span>
<span class="line-removed"> 85   inline T Atomic::PlatformXchg&lt;ByteSize&gt;::operator()(T volatile* dest, \</span>
<span class="line-removed"> 86                                                       T exchange_value, \</span>
<span class="line-removed"> 87                                                       atomic_memory_order order) const { \</span>
<span class="line-removed"> 88     STATIC_ASSERT(ByteSize == sizeof(T));                               \</span>
<span class="line-removed"> 89     return xchg_using_helper&lt;StubType&gt;(StubName, dest, exchange_value); \</span>
 90   }
 91 
<span class="line-modified"> 92 DEFINE_STUB_XCHG(4, int32_t, os::atomic_xchg_func)</span>
<span class="line-modified"> 93 DEFINE_STUB_XCHG(8, int64_t, os::atomic_xchg_long_func)</span>
<span class="line-modified"> 94 </span>
<span class="line-modified"> 95 #undef DEFINE_STUB_XCHG</span>
<span class="line-modified"> 96 </span>
<span class="line-modified"> 97 #define DEFINE_STUB_CMPXCHG(ByteSize, StubType, StubName)                  \</span>
<span class="line-modified"> 98   template&lt;&gt;                                                               \</span>
<span class="line-modified"> 99   template&lt;typename T&gt;                                                     \</span>
<span class="line-modified">100   inline T Atomic::PlatformCmpxchg&lt;ByteSize&gt;::operator()(T volatile* dest, \</span>
<span class="line-modified">101                                                          T compare_value,  \</span>
<span class="line-modified">102                                                          T exchange_value, \</span>
<span class="line-modified">103                                                          atomic_memory_order order) const { \</span>
<span class="line-modified">104     STATIC_ASSERT(ByteSize == sizeof(T));                                  \</span>
<span class="line-modified">105     return cmpxchg_using_helper&lt;StubType&gt;(StubName, dest, compare_value, exchange_value); \</span>

106   }
107 
<span class="line-modified">108 DEFINE_STUB_CMPXCHG(1, int8_t,  os::atomic_cmpxchg_byte_func)</span>
<span class="line-modified">109 DEFINE_STUB_CMPXCHG(4, int32_t, os::atomic_cmpxchg_func)</span>
<span class="line-modified">110 DEFINE_STUB_CMPXCHG(8, int64_t, os::atomic_cmpxchg_long_func)</span>
<span class="line-modified">111 </span>
<span class="line-modified">112 #undef DEFINE_STUB_CMPXCHG</span>
<span class="line-modified">113 </span>
<span class="line-modified">114 #else // !AMD64</span>
<span class="line-modified">115 </span>
<span class="line-modified">116 template&lt;&gt;</span>
<span class="line-modified">117 template&lt;typename D, typename I&gt;</span>
<span class="line-modified">118 inline D Atomic::PlatformAdd&lt;4&gt;::add_and_fetch(D volatile* dest, I add_value,</span>
<span class="line-modified">119                                                atomic_memory_order order) const {</span>
<span class="line-modified">120   STATIC_ASSERT(4 == sizeof(I));</span>
<span class="line-modified">121   STATIC_ASSERT(4 == sizeof(D));</span>
<span class="line-modified">122   __asm {</span>
<span class="line-modified">123     mov edx, dest;</span>
<span class="line-modified">124     mov eax, add_value;</span>
<span class="line-modified">125     mov ecx, eax;</span>
<span class="line-modified">126     lock xadd dword ptr [edx], eax;</span>
<span class="line-modified">127     add eax, ecx;</span>

128   }
<span class="line-removed">129 }</span>
130 
<span class="line-modified">131 template&lt;&gt;</span>
<span class="line-modified">132 template&lt;typename T&gt;</span>
<span class="line-modified">133 inline T Atomic::PlatformXchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">134                                              T exchange_value,</span>
<span class="line-removed">135                                              atomic_memory_order order) const {</span>
<span class="line-removed">136   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">137   // alternative for InterlockedExchange</span>
<span class="line-removed">138   __asm {</span>
<span class="line-removed">139     mov eax, exchange_value;</span>
<span class="line-removed">140     mov ecx, dest;</span>
<span class="line-removed">141     xchg eax, dword ptr [ecx];</span>
<span class="line-removed">142   }</span>
<span class="line-removed">143 }</span>
144 
<span class="line-modified">145 template&lt;&gt;</span>
<span class="line-removed">146 template&lt;typename T&gt;</span>
<span class="line-removed">147 inline T Atomic::PlatformCmpxchg&lt;1&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">148                                                 T compare_value,</span>
<span class="line-removed">149                                                 T exchange_value,</span>
<span class="line-removed">150                                                 atomic_memory_order order) const {</span>
<span class="line-removed">151   STATIC_ASSERT(1 == sizeof(T));</span>
<span class="line-removed">152   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">153   __asm {</span>
<span class="line-removed">154     mov edx, dest</span>
<span class="line-removed">155     mov cl, exchange_value</span>
<span class="line-removed">156     mov al, compare_value</span>
<span class="line-removed">157     lock cmpxchg byte ptr [edx], cl</span>
<span class="line-removed">158   }</span>
<span class="line-removed">159 }</span>
160 
<span class="line-modified">161 template&lt;&gt;</span>
<span class="line-removed">162 template&lt;typename T&gt;</span>
<span class="line-removed">163 inline T Atomic::PlatformCmpxchg&lt;4&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">164                                                 T compare_value,</span>
<span class="line-removed">165                                                 T exchange_value,</span>
<span class="line-removed">166                                                 atomic_memory_order order) const {</span>
<span class="line-removed">167   STATIC_ASSERT(4 == sizeof(T));</span>
<span class="line-removed">168   // alternative for InterlockedCompareExchange</span>
<span class="line-removed">169   __asm {</span>
<span class="line-removed">170     mov edx, dest</span>
<span class="line-removed">171     mov ecx, exchange_value</span>
<span class="line-removed">172     mov eax, compare_value</span>
<span class="line-removed">173     lock cmpxchg dword ptr [edx], ecx</span>
<span class="line-removed">174   }</span>
<span class="line-removed">175 }</span>
176 
<span class="line-modified">177 template&lt;&gt;</span>
<span class="line-removed">178 template&lt;typename T&gt;</span>
<span class="line-removed">179 inline T Atomic::PlatformCmpxchg&lt;8&gt;::operator()(T volatile* dest,</span>
<span class="line-removed">180                                                 T compare_value,</span>
<span class="line-removed">181                                                 T exchange_value,</span>
<span class="line-removed">182                                                 atomic_memory_order order) const {</span>
<span class="line-removed">183   STATIC_ASSERT(8 == sizeof(T));</span>
<span class="line-removed">184   int32_t ex_lo  = (int32_t)exchange_value;</span>
<span class="line-removed">185   int32_t ex_hi  = *( ((int32_t*)&amp;exchange_value) + 1 );</span>
<span class="line-removed">186   int32_t cmp_lo = (int32_t)compare_value;</span>
<span class="line-removed">187   int32_t cmp_hi = *( ((int32_t*)&amp;compare_value) + 1 );</span>
<span class="line-removed">188   __asm {</span>
<span class="line-removed">189     push ebx</span>
<span class="line-removed">190     push edi</span>
<span class="line-removed">191     mov eax, cmp_lo</span>
<span class="line-removed">192     mov edx, cmp_hi</span>
<span class="line-removed">193     mov edi, dest</span>
<span class="line-removed">194     mov ebx, ex_lo</span>
<span class="line-removed">195     mov ecx, ex_hi</span>
<span class="line-removed">196     lock cmpxchg8b qword ptr [edi]</span>
<span class="line-removed">197     pop edi</span>
<span class="line-removed">198     pop ebx</span>
<span class="line-removed">199   }</span>
<span class="line-removed">200 }</span>
201 
202 template&lt;&gt;
203 template&lt;typename T&gt;
204 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
205   STATIC_ASSERT(8 == sizeof(T));
206   volatile T dest;
207   volatile T* pdest = &amp;dest;
208   __asm {
209     mov eax, src
210     fild     qword ptr [eax]
211     mov eax, pdest
212     fistp    qword ptr [eax]
213   }
214   return dest;
215 }
216 
217 template&lt;&gt;
218 template&lt;typename T&gt;
219 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
220                                                  T store_value) const {
221   STATIC_ASSERT(8 == sizeof(T));
222   volatile T* src = &amp;store_value;
223   __asm {
224     mov eax, src
225     fild     qword ptr [eax]
226     mov eax, dest
227     fistp    qword ptr [eax]
228   }
229 }
230 
<span class="line-removed">231 #endif // AMD64</span>
<span class="line-removed">232 </span>
233 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
234 
<span class="line-removed">235 #ifndef AMD64</span>
236 template&lt;&gt;
237 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
238 {
239   template &lt;typename T&gt;
240   void operator()(volatile T* p, T v) const {
241     __asm {
242       mov edx, p;
243       mov al, v;
244       xchg al, byte ptr [edx];
245     }
246   }
247 };
248 
249 template&lt;&gt;
250 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
251 {
252   template &lt;typename T&gt;
253   void operator()(volatile T* p, T v) const {
254     __asm {
255       mov edx, p;
</pre>
</td>
<td>
<hr />
<pre>
  1 /*
<span class="line-modified">  2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.</span>
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 26 #define OS_CPU_WINDOWS_X86_ATOMIC_WINDOWS_X86_HPP
 27 
<span class="line-added"> 28 #include &lt;intrin.h&gt;</span>
 29 #include &quot;runtime/os.hpp&quot;
 30 
 31 // Note that in MSVC, volatile memory accesses are explicitly
 32 // guaranteed to have acquire release semantics (w.r.t. compiler
 33 // reordering) and therefore does not even need a compiler barrier
 34 // for normal acquire release accesses. And all generalized
 35 // bound calls like release_store go through Atomic::load
 36 // and Atomic::store which do volatile memory accesses.
 37 template&lt;&gt; inline void ScopedFence&lt;X_ACQUIRE&gt;::postfix()       { }
 38 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X&gt;::prefix()        { }
 39 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::prefix()  { }
 40 template&lt;&gt; inline void ScopedFence&lt;RELEASE_X_FENCE&gt;::postfix() { OrderAccess::fence(); }
 41 















 42 template&lt;size_t byte_size&gt;
 43 struct Atomic::PlatformAdd {
 44   template&lt;typename D, typename I&gt;
 45   D add_and_fetch(D volatile* dest, I add_value, atomic_memory_order order) const;
 46 
 47   template&lt;typename D, typename I&gt;
 48   D fetch_and_add(D volatile* dest, I add_value, atomic_memory_order order) const {
 49     return add_and_fetch(dest, add_value, order) - add_value;
 50   }
 51 };
 52 
<span class="line-modified"> 53 // The Interlocked* APIs only take long and will not accept __int32. That is</span>
<span class="line-modified"> 54 // acceptable on Windows, since long is a 32-bits integer type.</span>
<span class="line-modified"> 55 </span>
<span class="line-modified"> 56 #define DEFINE_INTRINSIC_ADD(IntrinsicName, IntrinsicType)                \</span>
<span class="line-modified"> 57   template&lt;&gt;                                                              \</span>
<span class="line-modified"> 58   template&lt;typename D, typename I&gt;                                        \</span>
<span class="line-modified"> 59   inline D Atomic::PlatformAdd&lt;sizeof(IntrinsicType)&gt;::add_and_fetch(D volatile* dest, \</span>
<span class="line-modified"> 60                                                                      I add_value, \</span>
<span class="line-modified"> 61                                                                      atomic_memory_order order) const { \</span>
<span class="line-modified"> 62     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(D));                    \</span>
<span class="line-modified"> 63     return PrimitiveConversions::cast&lt;D&gt;(                                 \</span>
<span class="line-modified"> 64       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \</span>
<span class="line-modified"> 65                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(add_value))); \</span>










 66   }
 67 
<span class="line-modified"> 68 DEFINE_INTRINSIC_ADD(InterlockedAdd,   long)</span>
<span class="line-modified"> 69 DEFINE_INTRINSIC_ADD(InterlockedAdd64, __int64)</span>
<span class="line-modified"> 70 </span>
<span class="line-modified"> 71 #undef DEFINE_INTRINSIC_ADD</span>
<span class="line-modified"> 72 </span>
<span class="line-modified"> 73 #define DEFINE_INTRINSIC_XCHG(IntrinsicName, IntrinsicType)               \</span>
<span class="line-modified"> 74   template&lt;&gt;                                                              \</span>
<span class="line-modified"> 75   template&lt;typename T&gt;                                                    \</span>
<span class="line-modified"> 76   inline T Atomic::PlatformXchg&lt;sizeof(IntrinsicType)&gt;::operator()(T volatile* dest, \</span>
<span class="line-modified"> 77                                                                    T exchange_value, \</span>
<span class="line-modified"> 78                                                                    atomic_memory_order order) const { \</span>
<span class="line-modified"> 79     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \</span>
<span class="line-modified"> 80     return PrimitiveConversions::cast&lt;T&gt;(                                 \</span>
<span class="line-modified"> 81       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \</span>
<span class="line-added"> 82                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(exchange_value))); \</span>
 83   }
 84 
<span class="line-modified"> 85 DEFINE_INTRINSIC_XCHG(InterlockedExchange,   long)</span>
<span class="line-modified"> 86 DEFINE_INTRINSIC_XCHG(InterlockedExchange64, __int64)</span>
<span class="line-modified"> 87 </span>
<span class="line-modified"> 88 #undef DEFINE_INTRINSIC_XCHG</span>
<span class="line-modified"> 89 </span>
<span class="line-modified"> 90 // Note: the order of the parameters is different between</span>
<span class="line-modified"> 91 // Atomic::PlatformCmpxchg&lt;*&gt;::operator() and the</span>
<span class="line-modified"> 92 // InterlockedCompareExchange* API.</span>
<span class="line-modified"> 93 </span>
<span class="line-modified"> 94 #define DEFINE_INTRINSIC_CMPXCHG(IntrinsicName, IntrinsicType)            \</span>
<span class="line-modified"> 95   template&lt;&gt;                                                              \</span>
<span class="line-modified"> 96   template&lt;typename T&gt;                                                    \</span>
<span class="line-modified"> 97   inline T Atomic::PlatformCmpxchg&lt;sizeof(IntrinsicType)&gt;::operator()(T volatile* dest, \</span>
<span class="line-modified"> 98                                                                       T compare_value, \</span>
<span class="line-modified"> 99                                                                       T exchange_value, \</span>
<span class="line-modified">100                                                                       atomic_memory_order order) const { \</span>
<span class="line-modified">101     STATIC_ASSERT(sizeof(IntrinsicType) == sizeof(T));                    \</span>
<span class="line-modified">102     return PrimitiveConversions::cast&lt;T&gt;(                                 \</span>
<span class="line-modified">103       IntrinsicName(reinterpret_cast&lt;IntrinsicType volatile *&gt;(dest),     \</span>
<span class="line-modified">104                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(exchange_value), \</span>
<span class="line-added">105                     PrimitiveConversions::cast&lt;IntrinsicType&gt;(compare_value))); \</span>
106   }

107 
<span class="line-modified">108 DEFINE_INTRINSIC_CMPXCHG(_InterlockedCompareExchange8, char) // Use the intrinsic as InterlockedCompareExchange8 does not exist</span>
<span class="line-modified">109 DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange,   long)</span>
<span class="line-modified">110 DEFINE_INTRINSIC_CMPXCHG(InterlockedCompareExchange64, __int64)</span>










111 
<span class="line-modified">112 #undef DEFINE_INTRINSIC_CMPXCHG</span>














113 
<span class="line-modified">114 #ifndef AMD64</span>














115 
<span class="line-modified">116 #pragma warning(disable: 4035) // Disables warnings reporting missing return statement</span>























117 
118 template&lt;&gt;
119 template&lt;typename T&gt;
120 inline T Atomic::PlatformLoad&lt;8&gt;::operator()(T const volatile* src) const {
121   STATIC_ASSERT(8 == sizeof(T));
122   volatile T dest;
123   volatile T* pdest = &amp;dest;
124   __asm {
125     mov eax, src
126     fild     qword ptr [eax]
127     mov eax, pdest
128     fistp    qword ptr [eax]
129   }
130   return dest;
131 }
132 
133 template&lt;&gt;
134 template&lt;typename T&gt;
135 inline void Atomic::PlatformStore&lt;8&gt;::operator()(T volatile* dest,
136                                                  T store_value) const {
137   STATIC_ASSERT(8 == sizeof(T));
138   volatile T* src = &amp;store_value;
139   __asm {
140     mov eax, src
141     fild     qword ptr [eax]
142     mov eax, dest
143     fistp    qword ptr [eax]
144   }
145 }
146 


147 #pragma warning(default: 4035) // Enables warnings reporting missing return statement
148 

149 template&lt;&gt;
150 struct Atomic::PlatformOrderedStore&lt;1, RELEASE_X_FENCE&gt;
151 {
152   template &lt;typename T&gt;
153   void operator()(volatile T* p, T v) const {
154     __asm {
155       mov edx, p;
156       mov al, v;
157       xchg al, byte ptr [edx];
158     }
159   }
160 };
161 
162 template&lt;&gt;
163 struct Atomic::PlatformOrderedStore&lt;2, RELEASE_X_FENCE&gt;
164 {
165   template &lt;typename T&gt;
166   void operator()(volatile T* p, T v) const {
167     __asm {
168       mov edx, p;
</pre>
</td>
</tr>
</table>
<center><a href="../../os/windows/threadCritical_windows.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="os_windows_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
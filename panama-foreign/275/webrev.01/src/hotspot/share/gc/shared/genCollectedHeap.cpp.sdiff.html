<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/genCollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="gcVMOperations.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="../shenandoah/shenandoahConcurrentMark.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/genCollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 300     if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 301       result = _young_gen-&gt;expand_and_allocate(size, is_tlab);
 302     }
 303   }
 304   assert(result == NULL || is_in_reserved(result), &quot;result not in heap&quot;);
 305   return result;
 306 }
 307 
 308 HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,
 309                                               bool is_tlab,
 310                                               bool* gc_overhead_limit_was_exceeded) {
 311   // In general gc_overhead_limit_was_exceeded should be false so
 312   // set it so here and reset it to true only if the gc time
 313   // limit is being exceeded as checked below.
 314   *gc_overhead_limit_was_exceeded = false;
 315 
 316   HeapWord* result = NULL;
 317 
 318   // Loop until the allocation is satisfied, or unsatisfied after GC.
 319   for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {
<span class="line-removed"> 320     HandleMark hm; // Discard any handles allocated in each iteration.</span>
 321 
 322     // First allocation attempt is lock-free.
 323     Generation *young = _young_gen;
 324     assert(young-&gt;supports_inline_contig_alloc(),
 325       &quot;Otherwise, must do alloc within heap lock&quot;);
 326     if (young-&gt;should_allocate(size, is_tlab)) {
 327       result = young-&gt;par_allocate(size, is_tlab);
 328       if (result != NULL) {
 329         assert(is_in_reserved(result), &quot;result not in heap&quot;);
 330         return result;
 331       }
 332     }
 333     uint gc_count_before;  // Read inside the Heap_lock locked region.
 334     {
 335       MutexLocker ml(Heap_lock);
 336       log_trace(gc, alloc)(&quot;GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation&quot;);
 337       // Note that only large objects get a shot at being
 338       // allocated in later generations.
 339       bool first_only = !should_try_older_generation_allocation(size);
 340 
</pre>
<hr />
<pre>
 460 
 461 void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,
 462                                           bool is_tlab, bool run_verification, bool clear_soft_refs,
 463                                           bool restore_marks_for_biased_locking) {
 464   FormatBuffer&lt;&gt; title(&quot;Collect gen: %s&quot;, gen-&gt;short_name());
 465   GCTraceTime(Trace, gc, phases) t1(title);
 466   TraceCollectorStats tcs(gen-&gt;counters());
 467   TraceMemoryManagerStats tmms(gen-&gt;gc_manager(), gc_cause());
 468 
 469   gen-&gt;stat_record()-&gt;invocations++;
 470   gen-&gt;stat_record()-&gt;accumulated_time.start();
 471 
 472   // Must be done anew before each collection because
 473   // a previous collection will do mangling and will
 474   // change top of some spaces.
 475   record_gen_tops_before_GC();
 476 
 477   log_trace(gc)(&quot;%s invoke=%d size=&quot; SIZE_FORMAT, heap()-&gt;is_young_gen(gen) ? &quot;Young&quot; : &quot;Old&quot;, gen-&gt;stat_record()-&gt;invocations, size * HeapWordSize);
 478 
 479   if (run_verification &amp;&amp; VerifyBeforeGC) {
<span class="line-removed"> 480     HandleMark hm;  // Discard invalid handles created during verification</span>
 481     Universe::verify(&quot;Before GC&quot;);
 482   }
 483   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());
 484 
 485   if (restore_marks_for_biased_locking) {
 486     // We perform this mark word preservation work lazily
 487     // because it&#39;s only at this point that we know whether we
 488     // absolutely have to do it; we want to avoid doing it for
 489     // scavenge-only collections where it&#39;s unnecessary
 490     BiasedLocking::preserve_marks();
 491   }
 492 
 493   // Do collection work
 494   {
 495     // Note on ref discovery: For what appear to be historical reasons,
 496     // GCH enables and disabled (by enqueing) refs discovery.
 497     // In the future this should be moved into the generation&#39;s
 498     // collect method so that ref discovery and enqueueing concerns
 499     // are local to a generation. The collect method could return
 500     // an appropriate indication in the case that notification on
 501     // the ref lock was needed. This will make the treatment of
 502     // weak refs more uniform (and indeed remove such concerns
 503     // from GCH). XXX
 504 
<span class="line-removed"> 505     HandleMark hm;  // Discard invalid handles created during gc</span>
 506     save_marks();   // save marks for all gens
 507     // We want to discover references, but not process them yet.
 508     // This mode is disabled in process_discovered_references if the
 509     // generation does some collection work, or in
 510     // enqueue_discovered_references if the generation returns
 511     // without doing any work.
 512     ReferenceProcessor* rp = gen-&gt;ref_processor();
 513     // If the discovery of (&quot;weak&quot;) refs in this generation is
 514     // atomic wrt other collectors in this configuration, we
 515     // are guaranteed to have empty discovered ref lists.
 516     if (rp-&gt;discovery_is_atomic()) {
 517       rp-&gt;enable_discovery();
 518       rp-&gt;setup_policy(clear_soft_refs);
 519     } else {
 520       // collect() below will enable discovery as appropriate
 521     }
 522     gen-&gt;collect(full, clear_soft_refs, size, is_tlab);
 523     if (!rp-&gt;enqueuing_is_done()) {
 524       rp-&gt;disable_discovery();
 525     } else {
 526       rp-&gt;set_enqueuing_is_done(false);
 527     }
 528     rp-&gt;verify_no_references_recorded();
 529   }
 530 
 531   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());
 532 
 533   gen-&gt;stat_record()-&gt;accumulated_time.stop();
 534 
 535   update_gc_stats(gen, full);
 536 
 537   if (run_verification &amp;&amp; VerifyAfterGC) {
<span class="line-removed"> 538     HandleMark hm;  // Discard invalid handles created during verification</span>
 539     Universe::verify(&quot;After GC&quot;);
 540   }
 541 }
 542 
 543 void GenCollectedHeap::do_collection(bool           full,
 544                                      bool           clear_all_soft_refs,
 545                                      size_t         size,
 546                                      bool           is_tlab,
 547                                      GenerationType max_generation) {
 548   ResourceMark rm;
 549   DEBUG_ONLY(Thread* my_thread = Thread::current();)
 550 
 551   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
 552   assert(my_thread-&gt;is_VM_thread() ||
 553          my_thread-&gt;is_ConcurrentGC_thread(),
 554          &quot;incorrect thread type capability&quot;);
 555   assert(Heap_lock-&gt;is_locked(),
 556          &quot;the requesting thread should have the Heap_lock&quot;);
 557   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
 558 
</pre>
</td>
<td>
<hr />
<pre>
 300     if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 301       result = _young_gen-&gt;expand_and_allocate(size, is_tlab);
 302     }
 303   }
 304   assert(result == NULL || is_in_reserved(result), &quot;result not in heap&quot;);
 305   return result;
 306 }
 307 
 308 HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,
 309                                               bool is_tlab,
 310                                               bool* gc_overhead_limit_was_exceeded) {
 311   // In general gc_overhead_limit_was_exceeded should be false so
 312   // set it so here and reset it to true only if the gc time
 313   // limit is being exceeded as checked below.
 314   *gc_overhead_limit_was_exceeded = false;
 315 
 316   HeapWord* result = NULL;
 317 
 318   // Loop until the allocation is satisfied, or unsatisfied after GC.
 319   for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {

 320 
 321     // First allocation attempt is lock-free.
 322     Generation *young = _young_gen;
 323     assert(young-&gt;supports_inline_contig_alloc(),
 324       &quot;Otherwise, must do alloc within heap lock&quot;);
 325     if (young-&gt;should_allocate(size, is_tlab)) {
 326       result = young-&gt;par_allocate(size, is_tlab);
 327       if (result != NULL) {
 328         assert(is_in_reserved(result), &quot;result not in heap&quot;);
 329         return result;
 330       }
 331     }
 332     uint gc_count_before;  // Read inside the Heap_lock locked region.
 333     {
 334       MutexLocker ml(Heap_lock);
 335       log_trace(gc, alloc)(&quot;GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation&quot;);
 336       // Note that only large objects get a shot at being
 337       // allocated in later generations.
 338       bool first_only = !should_try_older_generation_allocation(size);
 339 
</pre>
<hr />
<pre>
 459 
 460 void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,
 461                                           bool is_tlab, bool run_verification, bool clear_soft_refs,
 462                                           bool restore_marks_for_biased_locking) {
 463   FormatBuffer&lt;&gt; title(&quot;Collect gen: %s&quot;, gen-&gt;short_name());
 464   GCTraceTime(Trace, gc, phases) t1(title);
 465   TraceCollectorStats tcs(gen-&gt;counters());
 466   TraceMemoryManagerStats tmms(gen-&gt;gc_manager(), gc_cause());
 467 
 468   gen-&gt;stat_record()-&gt;invocations++;
 469   gen-&gt;stat_record()-&gt;accumulated_time.start();
 470 
 471   // Must be done anew before each collection because
 472   // a previous collection will do mangling and will
 473   // change top of some spaces.
 474   record_gen_tops_before_GC();
 475 
 476   log_trace(gc)(&quot;%s invoke=%d size=&quot; SIZE_FORMAT, heap()-&gt;is_young_gen(gen) ? &quot;Young&quot; : &quot;Old&quot;, gen-&gt;stat_record()-&gt;invocations, size * HeapWordSize);
 477 
 478   if (run_verification &amp;&amp; VerifyBeforeGC) {

 479     Universe::verify(&quot;Before GC&quot;);
 480   }
 481   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());
 482 
 483   if (restore_marks_for_biased_locking) {
 484     // We perform this mark word preservation work lazily
 485     // because it&#39;s only at this point that we know whether we
 486     // absolutely have to do it; we want to avoid doing it for
 487     // scavenge-only collections where it&#39;s unnecessary
 488     BiasedLocking::preserve_marks();
 489   }
 490 
 491   // Do collection work
 492   {
 493     // Note on ref discovery: For what appear to be historical reasons,
 494     // GCH enables and disabled (by enqueing) refs discovery.
 495     // In the future this should be moved into the generation&#39;s
 496     // collect method so that ref discovery and enqueueing concerns
 497     // are local to a generation. The collect method could return
 498     // an appropriate indication in the case that notification on
 499     // the ref lock was needed. This will make the treatment of
 500     // weak refs more uniform (and indeed remove such concerns
 501     // from GCH). XXX
 502 

 503     save_marks();   // save marks for all gens
 504     // We want to discover references, but not process them yet.
 505     // This mode is disabled in process_discovered_references if the
 506     // generation does some collection work, or in
 507     // enqueue_discovered_references if the generation returns
 508     // without doing any work.
 509     ReferenceProcessor* rp = gen-&gt;ref_processor();
 510     // If the discovery of (&quot;weak&quot;) refs in this generation is
 511     // atomic wrt other collectors in this configuration, we
 512     // are guaranteed to have empty discovered ref lists.
 513     if (rp-&gt;discovery_is_atomic()) {
 514       rp-&gt;enable_discovery();
 515       rp-&gt;setup_policy(clear_soft_refs);
 516     } else {
 517       // collect() below will enable discovery as appropriate
 518     }
 519     gen-&gt;collect(full, clear_soft_refs, size, is_tlab);
 520     if (!rp-&gt;enqueuing_is_done()) {
 521       rp-&gt;disable_discovery();
 522     } else {
 523       rp-&gt;set_enqueuing_is_done(false);
 524     }
 525     rp-&gt;verify_no_references_recorded();
 526   }
 527 
 528   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());
 529 
 530   gen-&gt;stat_record()-&gt;accumulated_time.stop();
 531 
 532   update_gc_stats(gen, full);
 533 
 534   if (run_verification &amp;&amp; VerifyAfterGC) {

 535     Universe::verify(&quot;After GC&quot;);
 536   }
 537 }
 538 
 539 void GenCollectedHeap::do_collection(bool           full,
 540                                      bool           clear_all_soft_refs,
 541                                      size_t         size,
 542                                      bool           is_tlab,
 543                                      GenerationType max_generation) {
 544   ResourceMark rm;
 545   DEBUG_ONLY(Thread* my_thread = Thread::current();)
 546 
 547   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
 548   assert(my_thread-&gt;is_VM_thread() ||
 549          my_thread-&gt;is_ConcurrentGC_thread(),
 550          &quot;incorrect thread type capability&quot;);
 551   assert(Heap_lock-&gt;is_locked(),
 552          &quot;the requesting thread should have the Heap_lock&quot;);
 553   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
 554 
</pre>
</td>
</tr>
</table>
<center><a href="gcVMOperations.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="../shenandoah/shenandoahConcurrentMark.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/parallel/psScavenge.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2002, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;aot/aotLoader.hpp&quot;
 27 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
 28 #include &quot;classfile/stringTable.hpp&quot;
 29 #include &quot;code/codeCache.hpp&quot;
 30 #include &quot;gc/parallel/parallelScavengeHeap.hpp&quot;
 31 #include &quot;gc/parallel/psAdaptiveSizePolicy.hpp&quot;
 32 #include &quot;gc/parallel/psClosure.inline.hpp&quot;
 33 #include &quot;gc/parallel/psCompactionManager.hpp&quot;
 34 #include &quot;gc/parallel/psParallelCompact.inline.hpp&quot;
 35 #include &quot;gc/parallel/psPromotionManager.inline.hpp&quot;
 36 #include &quot;gc/parallel/psRootType.hpp&quot;
 37 #include &quot;gc/parallel/psScavenge.inline.hpp&quot;
 38 #include &quot;gc/shared/gcCause.hpp&quot;
 39 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
 40 #include &quot;gc/shared/gcId.hpp&quot;
 41 #include &quot;gc/shared/gcLocker.hpp&quot;
 42 #include &quot;gc/shared/gcTimer.hpp&quot;
 43 #include &quot;gc/shared/gcTrace.hpp&quot;
 44 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
 45 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
 46 #include &quot;gc/shared/oopStorage.inline.hpp&quot;
 47 #include &quot;gc/shared/oopStorageSetParState.inline.hpp&quot;
 48 #include &quot;gc/shared/oopStorageParState.inline.hpp&quot;
 49 #include &quot;gc/shared/referencePolicy.hpp&quot;
 50 #include &quot;gc/shared/referenceProcessor.hpp&quot;
 51 #include &quot;gc/shared/referenceProcessorPhaseTimes.hpp&quot;
 52 #include &quot;gc/shared/scavengableNMethods.hpp&quot;
 53 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;
 54 #include &quot;gc/shared/taskTerminator.hpp&quot;
 55 #include &quot;gc/shared/weakProcessor.hpp&quot;
 56 #include &quot;gc/shared/workerPolicy.hpp&quot;
 57 #include &quot;gc/shared/workgroup.hpp&quot;
 58 #include &quot;memory/iterator.hpp&quot;
 59 #include &quot;memory/resourceArea.hpp&quot;
 60 #include &quot;memory/universe.hpp&quot;
 61 #include &quot;logging/log.hpp&quot;
 62 #include &quot;oops/access.inline.hpp&quot;
 63 #include &quot;oops/compressedOops.inline.hpp&quot;
 64 #include &quot;oops/oop.inline.hpp&quot;
 65 #include &quot;runtime/biasedLocking.hpp&quot;
 66 #include &quot;runtime/handles.inline.hpp&quot;
 67 #include &quot;runtime/threadCritical.hpp&quot;
 68 #include &quot;runtime/vmThread.hpp&quot;
 69 #include &quot;runtime/vmOperations.hpp&quot;
 70 #include &quot;services/memoryService.hpp&quot;
 71 #include &quot;utilities/stack.inline.hpp&quot;
 72 
 73 HeapWord*                     PSScavenge::_to_space_top_before_gc = NULL;
 74 int                           PSScavenge::_consecutive_skipped_scavenges = 0;
 75 SpanSubjectToDiscoveryClosure PSScavenge::_span_based_discoverer;
 76 ReferenceProcessor*           PSScavenge::_ref_processor = NULL;
 77 PSCardTable*                  PSScavenge::_card_table = NULL;
 78 bool                          PSScavenge::_survivor_overflow = false;
 79 uint                          PSScavenge::_tenuring_threshold = 0;
 80 HeapWord*                     PSScavenge::_young_generation_boundary = NULL;
 81 uintptr_t                     PSScavenge::_young_generation_boundary_compressed = 0;
 82 elapsedTimer                  PSScavenge::_accumulated_time;
 83 STWGCTimer                    PSScavenge::_gc_timer;
 84 ParallelScavengeTracer        PSScavenge::_gc_tracer;
 85 CollectorCounters*            PSScavenge::_counters = NULL;
 86 
 87 static void scavenge_roots_work(ParallelRootType::Value root_type, uint worker_id) {
 88   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
 89 
 90   PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);
 91   PSScavengeRootsClosure roots_closure(pm);
 92   PSPromoteRootsClosure  roots_to_old_closure(pm);
 93 
 94   switch (root_type) {
 95     case ParallelRootType::universe:
 96       Universe::oops_do(&amp;roots_closure);
 97       break;
 98 
 99     case ParallelRootType::object_synchronizer:
100       ObjectSynchronizer::oops_do(&amp;roots_closure);
101       break;
102 
103     case ParallelRootType::class_loader_data:
104       {
105         PSScavengeCLDClosure cld_closure(pm);
106         ClassLoaderDataGraph::cld_do(&amp;cld_closure);
107       }
108       break;
109 
110     case ParallelRootType::code_cache:
111       {
112         MarkingCodeBlobClosure code_closure(&amp;roots_to_old_closure, CodeBlobToOopClosure::FixRelocations);
113         ScavengableNMethods::nmethods_do(&amp;code_closure);
114         AOTLoader::oops_do(&amp;roots_closure);
115       }
116       break;
117 
118     case ParallelRootType::sentinel:
119     DEBUG_ONLY(default:) // DEBUG_ONLY hack will create compile error on release builds (-Wswitch) and runtime check on debug builds
120       fatal(&quot;Bad enumeration value: %u&quot;, root_type);
121       break;
122   }
123 
124   // Do the real work
125   pm-&gt;drain_stacks(false);
126 }
127 
128 static void steal_work(TaskTerminator&amp; terminator, uint worker_id) {
129   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
130 
131   PSPromotionManager* pm =
132     PSPromotionManager::gc_thread_promotion_manager(worker_id);
133   pm-&gt;drain_stacks(true);
134   guarantee(pm-&gt;stacks_empty(),
135             &quot;stacks should be empty at this point&quot;);
136 
137   while (true) {
138     ScannerTask task;
139     if (PSPromotionManager::steal_depth(worker_id, task)) {
140       TASKQUEUE_STATS_ONLY(pm-&gt;record_steal(task));
141       pm-&gt;process_popped_location_depth(task);
142       pm-&gt;drain_stacks_depth(true);
143     } else {
144       if (terminator.offer_termination()) {
145         break;
146       }
147     }
148   }
149   guarantee(pm-&gt;stacks_empty(), &quot;stacks should be empty at this point&quot;);
150 }
151 
152 // Define before use
153 class PSIsAliveClosure: public BoolObjectClosure {
154 public:
155   bool do_object_b(oop p) {
156     return (!PSScavenge::is_obj_in_young(p)) || p-&gt;is_forwarded();
157   }
158 };
159 
160 PSIsAliveClosure PSScavenge::_is_alive_closure;
161 
162 class PSKeepAliveClosure: public OopClosure {
163 protected:
164   MutableSpace* _to_space;
165   PSPromotionManager* _promotion_manager;
166 
167 public:
168   PSKeepAliveClosure(PSPromotionManager* pm) : _promotion_manager(pm) {
169     ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
170     _to_space = heap-&gt;young_gen()-&gt;to_space();
171 
172     assert(_promotion_manager != NULL, &quot;Sanity&quot;);
173   }
174 
175   template &lt;class T&gt; void do_oop_work(T* p) {
176     assert (oopDesc::is_oop(RawAccess&lt;IS_NOT_NULL&gt;::oop_load(p)),
177             &quot;expected an oop while scanning weak refs&quot;);
178 
179     // Weak refs may be visited more than once.
180     if (PSScavenge::should_scavenge(p, _to_space)) {
181       _promotion_manager-&gt;copy_and_push_safe_barrier&lt;/*promote_immediately=*/false&gt;(p);
182     }
183   }
184   virtual void do_oop(oop* p)       { PSKeepAliveClosure::do_oop_work(p); }
185   virtual void do_oop(narrowOop* p) { PSKeepAliveClosure::do_oop_work(p); }
186 };
187 
188 class PSEvacuateFollowersClosure: public VoidClosure {
189  private:
190   PSPromotionManager* _promotion_manager;
191  public:
192   PSEvacuateFollowersClosure(PSPromotionManager* pm) : _promotion_manager(pm) {}
193 
194   virtual void do_void() {
195     assert(_promotion_manager != NULL, &quot;Sanity&quot;);
196     _promotion_manager-&gt;drain_stacks(true);
197     guarantee(_promotion_manager-&gt;stacks_empty(),
198               &quot;stacks should be empty at this point&quot;);
199   }
200 };
201 
202 class PSRefProcTaskExecutor: public AbstractRefProcTaskExecutor {
203   virtual void execute(ProcessTask&amp; process_task, uint ergo_workers);
204 };
205 
206 class PSRefProcTask : public AbstractGangTask {
207   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
208   TaskTerminator _terminator;
209   ProcessTask&amp; _task;
210   uint _active_workers;
211 
212 public:
213   PSRefProcTask(ProcessTask&amp; task, uint active_workers)
214     : AbstractGangTask(&quot;PSRefProcTask&quot;),
215       _terminator(active_workers, PSPromotionManager::stack_array_depth()),
216       _task(task),
217       _active_workers(active_workers) {
218   }
219 
220   virtual void work(uint worker_id) {
221     PSPromotionManager* promotion_manager =
222       PSPromotionManager::gc_thread_promotion_manager(worker_id);
223     assert(promotion_manager != NULL, &quot;sanity check&quot;);
224     PSKeepAliveClosure keep_alive(promotion_manager);
225     PSEvacuateFollowersClosure evac_followers(promotion_manager);
226     PSIsAliveClosure is_alive;
227     _task.work(worker_id, is_alive, keep_alive, evac_followers);
228 
229     if (_task.marks_oops_alive() &amp;&amp; _active_workers &gt; 1) {
230       steal_work(_terminator, worker_id);
231     }
232   }
233 };
234 
235 void PSRefProcTaskExecutor::execute(ProcessTask&amp; process_task, uint ergo_workers) {
236   PSRefProcTask task(process_task, ergo_workers);
237   ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
238 }
239 
240 // This method contains all heap specific policy for invoking scavenge.
241 // PSScavenge::invoke_no_policy() will do nothing but attempt to
242 // scavenge. It will not clean up after failed promotions, bail out if
243 // we&#39;ve exceeded policy time limits, or any other special behavior.
244 // All such policy should be placed here.
245 //
246 // Note that this method should only be called from the vm_thread while
247 // at a safepoint!
248 bool PSScavenge::invoke() {
249   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
250   assert(Thread::current() == (Thread*)VMThread::vm_thread(), &quot;should be in vm thread&quot;);
251   assert(!ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;not reentrant&quot;);
252 
253   ParallelScavengeHeap* const heap = ParallelScavengeHeap::heap();
254   PSAdaptiveSizePolicy* policy = heap-&gt;size_policy();
255   IsGCActiveMark mark;
256 
257   const bool scavenge_done = PSScavenge::invoke_no_policy();
258   const bool need_full_gc = !scavenge_done ||
259     policy-&gt;should_full_GC(heap-&gt;old_gen()-&gt;free_in_bytes());
260   bool full_gc_done = false;
261 
262   if (UsePerfData) {
263     PSGCAdaptivePolicyCounters* const counters = heap-&gt;gc_policy_counters();
264     const int ffs_val = need_full_gc ? full_follows_scavenge : not_skipped;
265     counters-&gt;update_full_follows_scavenge(ffs_val);
266   }
267 
268   if (need_full_gc) {
269     GCCauseSetter gccs(heap, GCCause::_adaptive_size_policy);
270     SoftRefPolicy* srp = heap-&gt;soft_ref_policy();
271     const bool clear_all_softrefs = srp-&gt;should_clear_all_soft_refs();
272 
273     full_gc_done = PSParallelCompact::invoke_no_policy(clear_all_softrefs);
274   }
275 
276   return full_gc_done;
277 }
278 
279 class PSThreadRootsTaskClosure : public ThreadClosure {
280   uint _worker_id;
281 public:
282   PSThreadRootsTaskClosure(uint worker_id) : _worker_id(worker_id) { }
283   virtual void do_thread(Thread* thread) {
284     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
285 
286     PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(_worker_id);
287     PSScavengeRootsClosure roots_closure(pm);
288     MarkingCodeBlobClosure roots_in_blobs(&amp;roots_closure, CodeBlobToOopClosure::FixRelocations);
289 
290     thread-&gt;oops_do(&amp;roots_closure, &amp;roots_in_blobs);
291 
292     // Do the real work
293     pm-&gt;drain_stacks(false);
294   }
295 };
296 
297 class ScavengeRootsTask : public AbstractGangTask {
298   StrongRootsScope _strong_roots_scope; // needed for Threads::possibly_parallel_threads_do
299   OopStorageSetStrongParState&lt;false /* concurrent */, false /* is_const */&gt; _oop_storage_strong_par_state;
300   SequentialSubTasksDone _subtasks;
301   PSOldGen* _old_gen;
302   HeapWord* _gen_top;
303   uint _active_workers;
304   bool _is_empty;
305   TaskTerminator _terminator;
306 
307 public:
308   ScavengeRootsTask(PSOldGen* old_gen,
309                     HeapWord* gen_top,
310                     uint active_workers,
311                     bool is_empty) :
312       AbstractGangTask(&quot;ScavengeRootsTask&quot;),
313       _strong_roots_scope(active_workers),
314       _subtasks(),
315       _old_gen(old_gen),
316       _gen_top(gen_top),
317       _active_workers(active_workers),
318       _is_empty(is_empty),
319       _terminator(active_workers, PSPromotionManager::vm_thread_promotion_manager()-&gt;stack_array_depth()) {
320     _subtasks.set_n_threads(active_workers);
321     _subtasks.set_n_tasks(ParallelRootType::sentinel);
322   }
323 
324   virtual void work(uint worker_id) {
325     ResourceMark rm;
326 
327     if (!_is_empty) {
328       // There are only old-to-young pointers if there are objects
329       // in the old gen.
330 
331       assert(_old_gen != NULL, &quot;Sanity&quot;);
332       // There are no old-to-young pointers if the old gen is empty.
333       assert(!_old_gen-&gt;object_space()-&gt;is_empty(), &quot;Should not be called is there is no work&quot;);
334       assert(_old_gen-&gt;object_space()-&gt;contains(_gen_top) || _gen_top == _old_gen-&gt;object_space()-&gt;top(), &quot;Sanity&quot;);
335       assert(worker_id &lt; ParallelGCThreads, &quot;Sanity&quot;);
336 
337       {
338         PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);
339         PSCardTable* card_table = ParallelScavengeHeap::heap()-&gt;card_table();
340 
341         card_table-&gt;scavenge_contents_parallel(_old_gen-&gt;start_array(),
342                                                _old_gen-&gt;object_space(),
343                                                _gen_top,
344                                                pm,
345                                                worker_id,
346                                                _active_workers);
347 
348         // Do the real work
349         pm-&gt;drain_stacks(false);
350       }
351     }
352 
353     for (uint root_type = 0; _subtasks.try_claim_task(root_type); /* empty */ ) {
354       scavenge_roots_work(static_cast&lt;ParallelRootType::Value&gt;(root_type), worker_id);
355     }
356     _subtasks.all_tasks_completed();
357 
358     PSThreadRootsTaskClosure closure(worker_id);
359     Threads::possibly_parallel_threads_do(true /*parallel */, &amp;closure);
360 
361     // Scavenge OopStorages
362     {
363       PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);
364       PSScavengeRootsClosure closure(pm);
365       _oop_storage_strong_par_state.oops_do(&amp;closure);
366       // Do the real work
367       pm-&gt;drain_stacks(false);
368     }
369 
370     // If active_workers can exceed 1, add a steal_work().
371     // PSPromotionManager::drain_stacks_depth() does not fully drain its
372     // stacks and expects a steal_work() to complete the draining if
373     // ParallelGCThreads is &gt; 1.
374 
375     if (_active_workers &gt; 1) {
376       steal_work(_terminator, worker_id);
377     }
378   }
379 };
380 
381 // This method contains no policy. You should probably
382 // be calling invoke() instead.
383 bool PSScavenge::invoke_no_policy() {
384   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
385   assert(Thread::current() == (Thread*)VMThread::vm_thread(), &quot;should be in vm thread&quot;);
386 
387   _gc_timer.register_gc_start();
388 
389   TimeStamp scavenge_entry;
390   TimeStamp scavenge_midpoint;
391   TimeStamp scavenge_exit;
392 
393   scavenge_entry.update();
394 
395   if (GCLocker::check_active_before_gc()) {
396     return false;
397   }
398 
399   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
400   GCCause::Cause gc_cause = heap-&gt;gc_cause();
401 
402   // Check for potential problems.
403   if (!should_attempt_scavenge()) {
404     return false;
405   }
406 
407   GCIdMark gc_id_mark;
408   _gc_tracer.report_gc_start(heap-&gt;gc_cause(), _gc_timer.gc_start());
409 
410   bool promotion_failure_occurred = false;
411 
412   PSYoungGen* young_gen = heap-&gt;young_gen();
413   PSOldGen* old_gen = heap-&gt;old_gen();
414   PSAdaptiveSizePolicy* size_policy = heap-&gt;size_policy();
415 
416   heap-&gt;increment_total_collections();
417 
418   if (AdaptiveSizePolicy::should_update_eden_stats(gc_cause)) {
419     // Gather the feedback data for eden occupancy.
420     young_gen-&gt;eden_space()-&gt;accumulate_statistics();
421   }
422 
423   heap-&gt;print_heap_before_gc();
424   heap-&gt;trace_heap_before_gc(&amp;_gc_tracer);
425 
426   assert(!NeverTenure || _tenuring_threshold == markWord::max_age + 1, &quot;Sanity&quot;);
427   assert(!AlwaysTenure || _tenuring_threshold == 0, &quot;Sanity&quot;);
428 
429   // Fill in TLABs
430   heap-&gt;ensure_parsability(true);  // retire TLABs
431 
432   if (VerifyBeforeGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
<a name="1" id="anc1"></a>
433     Universe::verify(&quot;Before GC&quot;);
434   }
435 
436   {
437     ResourceMark rm;
<a name="2" id="anc2"></a>
438 
439     GCTraceCPUTime tcpu;
440     GCTraceTime(Info, gc) tm(&quot;Pause Young&quot;, NULL, gc_cause, true);
441     TraceCollectorStats tcs(counters());
442     TraceMemoryManagerStats tms(heap-&gt;young_gc_manager(), gc_cause);
443 
444     if (log_is_enabled(Debug, gc, heap, exit)) {
445       accumulated_time()-&gt;start();
446     }
447 
448     // Let the size policy know we&#39;re starting
449     size_policy-&gt;minor_collection_begin();
450 
451     // Verify the object start arrays.
452     if (VerifyObjectStartArray &amp;&amp;
453         VerifyBeforeGC) {
454       old_gen-&gt;verify_object_start_array();
455     }
456 
457     // Verify no unmarked old-&gt;young roots
458     if (VerifyRememberedSets) {
459       heap-&gt;card_table()-&gt;verify_all_young_refs_imprecise();
460     }
461 
462     assert(young_gen-&gt;to_space()-&gt;is_empty(),
463            &quot;Attempt to scavenge with live objects in to_space&quot;);
464     young_gen-&gt;to_space()-&gt;clear(SpaceDecorator::Mangle);
465 
466     save_to_space_top_before_gc();
467 
468 #if COMPILER2_OR_JVMCI
469     DerivedPointerTable::clear();
470 #endif
471 
472     reference_processor()-&gt;enable_discovery();
473     reference_processor()-&gt;setup_policy(false);
474 
475     const PreGenGCValues pre_gc_values = heap-&gt;get_pre_gc_values();
476 
477     // Reset our survivor overflow.
478     set_survivor_overflow(false);
479 
480     // We need to save the old top values before
481     // creating the promotion_manager. We pass the top
482     // values to the card_table, to prevent it from
483     // straying into the promotion labs.
484     HeapWord* old_top = old_gen-&gt;object_space()-&gt;top();
485 
486     const uint active_workers =
487       WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()-&gt;workers().total_workers(),
488                                         ParallelScavengeHeap::heap()-&gt;workers().active_workers(),
489                                         Threads::number_of_non_daemon_threads());
490     ParallelScavengeHeap::heap()-&gt;workers().update_active_workers(active_workers);
491 
492     PSPromotionManager::pre_scavenge();
493 
494     // We&#39;ll use the promotion manager again later.
495     PSPromotionManager* promotion_manager = PSPromotionManager::vm_thread_promotion_manager();
496     {
497       GCTraceTime(Debug, gc, phases) tm(&quot;Scavenge&quot;, &amp;_gc_timer);
498 
499       ScavengeRootsTask task(old_gen, old_top, active_workers, old_gen-&gt;object_space()-&gt;is_empty());
500       ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
501     }
502 
503     scavenge_midpoint.update();
504 
505     // Process reference objects discovered during scavenge
506     {
507       GCTraceTime(Debug, gc, phases) tm(&quot;Reference Processing&quot;, &amp;_gc_timer);
508 
509       reference_processor()-&gt;setup_policy(false); // not always_clear
510       reference_processor()-&gt;set_active_mt_degree(active_workers);
511       PSKeepAliveClosure keep_alive(promotion_manager);
512       PSEvacuateFollowersClosure evac_followers(promotion_manager);
513       ReferenceProcessorStats stats;
514       ReferenceProcessorPhaseTimes pt(&amp;_gc_timer, reference_processor()-&gt;max_num_queues());
515       if (reference_processor()-&gt;processing_is_mt()) {
516         PSRefProcTaskExecutor task_executor;
517         stats = reference_processor()-&gt;process_discovered_references(
518           &amp;_is_alive_closure, &amp;keep_alive, &amp;evac_followers, &amp;task_executor,
519           &amp;pt);
520       } else {
521         stats = reference_processor()-&gt;process_discovered_references(
522           &amp;_is_alive_closure, &amp;keep_alive, &amp;evac_followers, NULL, &amp;pt);
523       }
524 
525       _gc_tracer.report_gc_reference_stats(stats);
526       pt.print_all_references();
527     }
528 
529     assert(promotion_manager-&gt;stacks_empty(),&quot;stacks should be empty at this point&quot;);
530 
531     PSScavengeRootsClosure root_closure(promotion_manager);
532 
533     {
534       GCTraceTime(Debug, gc, phases) tm(&quot;Weak Processing&quot;, &amp;_gc_timer);
535       WeakProcessor::weak_oops_do(&amp;_is_alive_closure, &amp;root_closure);
536     }
537 
538     // Verify that usage of root_closure didn&#39;t copy any objects.
539     assert(promotion_manager-&gt;stacks_empty(),&quot;stacks should be empty at this point&quot;);
540 
541     // Finally, flush the promotion_manager&#39;s labs, and deallocate its stacks.
542     promotion_failure_occurred = PSPromotionManager::post_scavenge(_gc_tracer);
543     if (promotion_failure_occurred) {
544       clean_up_failed_promotion();
545       log_info(gc, promotion)(&quot;Promotion failed&quot;);
546     }
547 
548     _gc_tracer.report_tenuring_threshold(tenuring_threshold());
549 
550     // Let the size policy know we&#39;re done.  Note that we count promotion
551     // failure cleanup time as part of the collection (otherwise, we&#39;re
552     // implicitly saying it&#39;s mutator time).
553     size_policy-&gt;minor_collection_end(gc_cause);
554 
555     if (!promotion_failure_occurred) {
556       // Swap the survivor spaces.
557       young_gen-&gt;eden_space()-&gt;clear(SpaceDecorator::Mangle);
558       young_gen-&gt;from_space()-&gt;clear(SpaceDecorator::Mangle);
559       young_gen-&gt;swap_spaces();
560 
561       size_t survived = young_gen-&gt;from_space()-&gt;used_in_bytes();
562       size_t promoted = old_gen-&gt;used_in_bytes() - pre_gc_values.old_gen_used();
563       size_policy-&gt;update_averages(_survivor_overflow, survived, promoted);
564 
565       // A successful scavenge should restart the GC time limit count which is
566       // for full GC&#39;s.
567       size_policy-&gt;reset_gc_overhead_limit_count();
568       if (UseAdaptiveSizePolicy) {
569         // Calculate the new survivor size and tenuring threshold
570 
571         log_debug(gc, ergo)(&quot;AdaptiveSizeStart:  collection: %d &quot;, heap-&gt;total_collections());
572         log_trace(gc, ergo)(&quot;old_gen_capacity: &quot; SIZE_FORMAT &quot; young_gen_capacity: &quot; SIZE_FORMAT,
573                             old_gen-&gt;capacity_in_bytes(), young_gen-&gt;capacity_in_bytes());
574 
575         if (UsePerfData) {
576           PSGCAdaptivePolicyCounters* counters = heap-&gt;gc_policy_counters();
577           counters-&gt;update_old_eden_size(
578             size_policy-&gt;calculated_eden_size_in_bytes());
579           counters-&gt;update_old_promo_size(
580             size_policy-&gt;calculated_promo_size_in_bytes());
581           counters-&gt;update_old_capacity(old_gen-&gt;capacity_in_bytes());
582           counters-&gt;update_young_capacity(young_gen-&gt;capacity_in_bytes());
583           counters-&gt;update_survived(survived);
584           counters-&gt;update_promoted(promoted);
585           counters-&gt;update_survivor_overflowed(_survivor_overflow);
586         }
587 
588         size_t max_young_size = young_gen-&gt;max_gen_size();
589 
590         // Deciding a free ratio in the young generation is tricky, so if
591         // MinHeapFreeRatio or MaxHeapFreeRatio are in use (implicating
592         // that the old generation size may have been limited because of them) we
593         // should then limit our young generation size using NewRatio to have it
594         // follow the old generation size.
595         if (MinHeapFreeRatio != 0 || MaxHeapFreeRatio != 100) {
596           max_young_size = MIN2(old_gen-&gt;capacity_in_bytes() / NewRatio,
597                                 young_gen-&gt;max_gen_size());
598         }
599 
600         size_t survivor_limit =
601           size_policy-&gt;max_survivor_size(max_young_size);
602         _tenuring_threshold =
603           size_policy-&gt;compute_survivor_space_size_and_threshold(
604                                                            _survivor_overflow,
605                                                            _tenuring_threshold,
606                                                            survivor_limit);
607 
608        log_debug(gc, age)(&quot;Desired survivor size &quot; SIZE_FORMAT &quot; bytes, new threshold %u (max threshold &quot; UINTX_FORMAT &quot;)&quot;,
609                           size_policy-&gt;calculated_survivor_size_in_bytes(),
610                           _tenuring_threshold, MaxTenuringThreshold);
611 
612         if (UsePerfData) {
613           PSGCAdaptivePolicyCounters* counters = heap-&gt;gc_policy_counters();
614           counters-&gt;update_tenuring_threshold(_tenuring_threshold);
615           counters-&gt;update_survivor_size_counters();
616         }
617 
618         // Do call at minor collections?
619         // Don&#39;t check if the size_policy is ready at this
620         // level.  Let the size_policy check that internally.
621         if (UseAdaptiveGenerationSizePolicyAtMinorCollection &amp;&amp;
622             AdaptiveSizePolicy::should_update_eden_stats(gc_cause)) {
623           // Calculate optimal free space amounts
624           assert(young_gen-&gt;max_gen_size() &gt;
625                  young_gen-&gt;from_space()-&gt;capacity_in_bytes() +
626                  young_gen-&gt;to_space()-&gt;capacity_in_bytes(),
627                  &quot;Sizes of space in young gen are out-of-bounds&quot;);
628 
629           size_t young_live = young_gen-&gt;used_in_bytes();
630           size_t eden_live = young_gen-&gt;eden_space()-&gt;used_in_bytes();
631           size_t cur_eden = young_gen-&gt;eden_space()-&gt;capacity_in_bytes();
632           size_t max_old_gen_size = old_gen-&gt;max_gen_size();
633           size_t max_eden_size = max_young_size -
634             young_gen-&gt;from_space()-&gt;capacity_in_bytes() -
635             young_gen-&gt;to_space()-&gt;capacity_in_bytes();
636 
637           // Used for diagnostics
638           size_policy-&gt;clear_generation_free_space_flags();
639 
640           size_policy-&gt;compute_eden_space_size(young_live,
641                                                eden_live,
642                                                cur_eden,
643                                                max_eden_size,
644                                                false /* not full gc*/);
645 
646           size_policy-&gt;check_gc_overhead_limit(eden_live,
647                                                max_old_gen_size,
648                                                max_eden_size,
649                                                false /* not full gc*/,
650                                                gc_cause,
651                                                heap-&gt;soft_ref_policy());
652 
653           size_policy-&gt;decay_supplemental_growth(false /* not full gc*/);
654         }
655         // Resize the young generation at every collection
656         // even if new sizes have not been calculated.  This is
657         // to allow resizes that may have been inhibited by the
658         // relative location of the &quot;to&quot; and &quot;from&quot; spaces.
659 
660         // Resizing the old gen at young collections can cause increases
661         // that don&#39;t feed back to the generation sizing policy until
662         // a full collection.  Don&#39;t resize the old gen here.
663 
664         heap-&gt;resize_young_gen(size_policy-&gt;calculated_eden_size_in_bytes(),
665                         size_policy-&gt;calculated_survivor_size_in_bytes());
666 
667         log_debug(gc, ergo)(&quot;AdaptiveSizeStop: collection: %d &quot;, heap-&gt;total_collections());
668       }
669 
670       // Update the structure of the eden. With NUMA-eden CPU hotplugging or offlining can
671       // cause the change of the heap layout. Make sure eden is reshaped if that&#39;s the case.
672       // Also update() will case adaptive NUMA chunk resizing.
673       assert(young_gen-&gt;eden_space()-&gt;is_empty(), &quot;eden space should be empty now&quot;);
674       young_gen-&gt;eden_space()-&gt;update();
675 
676       heap-&gt;gc_policy_counters()-&gt;update_counters();
677 
678       heap-&gt;resize_all_tlabs();
679 
680       assert(young_gen-&gt;to_space()-&gt;is_empty(), &quot;to space should be empty now&quot;);
681     }
682 
683 #if COMPILER2_OR_JVMCI
684     DerivedPointerTable::update_pointers();
685 #endif
686 
687     NOT_PRODUCT(reference_processor()-&gt;verify_no_references_recorded());
688 
689     // Re-verify object start arrays
690     if (VerifyObjectStartArray &amp;&amp;
691         VerifyAfterGC) {
692       old_gen-&gt;verify_object_start_array();
693     }
694 
695     // Verify all old -&gt; young cards are now precise
696     if (VerifyRememberedSets) {
697       // Precise verification will give false positives. Until this is fixed,
698       // use imprecise verification.
699       // heap-&gt;card_table()-&gt;verify_all_young_refs_precise();
700       heap-&gt;card_table()-&gt;verify_all_young_refs_imprecise();
701     }
702 
703     if (log_is_enabled(Debug, gc, heap, exit)) {
704       accumulated_time()-&gt;stop();
705     }
706 
707     heap-&gt;print_heap_change(pre_gc_values);
708 
709     // Track memory usage and detect low memory
710     MemoryService::track_memory_usage();
711     heap-&gt;update_counters();
712   }
713 
714   if (VerifyAfterGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
<a name="3" id="anc3"></a>
715     Universe::verify(&quot;After GC&quot;);
716   }
717 
718   heap-&gt;print_heap_after_gc();
719   heap-&gt;trace_heap_after_gc(&amp;_gc_tracer);
720 
721   scavenge_exit.update();
722 
723   log_debug(gc, task, time)(&quot;VM-Thread &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT,
724                             scavenge_entry.ticks(), scavenge_midpoint.ticks(),
725                             scavenge_exit.ticks());
726 
727   AdaptiveSizePolicyOutput::print(size_policy, heap-&gt;total_collections());
728 
729   _gc_timer.register_gc_end();
730 
731   _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());
732 
733   return !promotion_failure_occurred;
734 }
735 
736 // This method iterates over all objects in the young generation,
737 // removing all forwarding references. It then restores any preserved marks.
738 void PSScavenge::clean_up_failed_promotion() {
739   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
740   PSYoungGen* young_gen = heap-&gt;young_gen();
741 
742   RemoveForwardedPointerClosure remove_fwd_ptr_closure;
743   young_gen-&gt;object_iterate(&amp;remove_fwd_ptr_closure);
744 
745   PSPromotionManager::restore_preserved_marks();
746 
747   // Reset the PromotionFailureALot counters.
748   NOT_PRODUCT(heap-&gt;reset_promotion_should_fail();)
749 }
750 
751 bool PSScavenge::should_attempt_scavenge() {
752   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
753   PSGCAdaptivePolicyCounters* counters = heap-&gt;gc_policy_counters();
754 
755   if (UsePerfData) {
756     counters-&gt;update_scavenge_skipped(not_skipped);
757   }
758 
759   PSYoungGen* young_gen = heap-&gt;young_gen();
760   PSOldGen* old_gen = heap-&gt;old_gen();
761 
762   // Do not attempt to promote unless to_space is empty
763   if (!young_gen-&gt;to_space()-&gt;is_empty()) {
764     _consecutive_skipped_scavenges++;
765     if (UsePerfData) {
766       counters-&gt;update_scavenge_skipped(to_space_not_empty);
767     }
768     return false;
769   }
770 
771   // Test to see if the scavenge will likely fail.
772   PSAdaptiveSizePolicy* policy = heap-&gt;size_policy();
773 
774   // A similar test is done in the policy&#39;s should_full_GC().  If this is
775   // changed, decide if that test should also be changed.
776   size_t avg_promoted = (size_t) policy-&gt;padded_average_promoted_in_bytes();
777   size_t promotion_estimate = MIN2(avg_promoted, young_gen-&gt;used_in_bytes());
778   bool result = promotion_estimate &lt; old_gen-&gt;free_in_bytes();
779 
780   log_trace(ergo)(&quot;%s scavenge: average_promoted &quot; SIZE_FORMAT &quot; padded_average_promoted &quot; SIZE_FORMAT &quot; free in old gen &quot; SIZE_FORMAT,
781                 result ? &quot;Do&quot; : &quot;Skip&quot;, (size_t) policy-&gt;average_promoted_in_bytes(),
782                 (size_t) policy-&gt;padded_average_promoted_in_bytes(),
783                 old_gen-&gt;free_in_bytes());
784   if (young_gen-&gt;used_in_bytes() &lt; (size_t) policy-&gt;padded_average_promoted_in_bytes()) {
785     log_trace(ergo)(&quot; padded_promoted_average is greater than maximum promotion = &quot; SIZE_FORMAT, young_gen-&gt;used_in_bytes());
786   }
787 
788   if (result) {
789     _consecutive_skipped_scavenges = 0;
790   } else {
791     _consecutive_skipped_scavenges++;
792     if (UsePerfData) {
793       counters-&gt;update_scavenge_skipped(promoted_too_large);
794     }
795   }
796   return result;
797 }
798 
799 // Adaptive size policy support.
800 void PSScavenge::set_young_generation_boundary(HeapWord* v) {
801   _young_generation_boundary = v;
802   if (UseCompressedOops) {
803     _young_generation_boundary_compressed = (uintptr_t)CompressedOops::encode((oop)v);
804   }
805 }
806 
807 void PSScavenge::initialize() {
808   // Arguments must have been parsed
809 
810   if (AlwaysTenure || NeverTenure) {
811     assert(MaxTenuringThreshold == 0 || MaxTenuringThreshold == markWord::max_age + 1,
812            &quot;MaxTenuringThreshold should be 0 or markWord::max_age + 1, but is %d&quot;, (int) MaxTenuringThreshold);
813     _tenuring_threshold = MaxTenuringThreshold;
814   } else {
815     // We want to smooth out our startup times for the AdaptiveSizePolicy
816     _tenuring_threshold = (UseAdaptiveSizePolicy) ? InitialTenuringThreshold :
817                                                     MaxTenuringThreshold;
818   }
819 
820   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
821   PSYoungGen* young_gen = heap-&gt;young_gen();
822   PSOldGen* old_gen = heap-&gt;old_gen();
823 
824   // Set boundary between young_gen and old_gen
825   assert(old_gen-&gt;reserved().end() &lt;= young_gen-&gt;eden_space()-&gt;bottom(),
826          &quot;old above young&quot;);
827   set_young_generation_boundary(young_gen-&gt;eden_space()-&gt;bottom());
828 
829   // Initialize ref handling object for scavenging.
830   _span_based_discoverer.set_span(young_gen-&gt;reserved());
831   _ref_processor =
832     new ReferenceProcessor(&amp;_span_based_discoverer,
833                            ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1), // mt processing
834                            ParallelGCThreads,          // mt processing degree
835                            true,                       // mt discovery
836                            ParallelGCThreads,          // mt discovery degree
837                            true,                       // atomic_discovery
838                            NULL,                       // header provides liveness info
839                            false);
840 
841   // Cache the cardtable
842   _card_table = heap-&gt;card_table();
843 
844   _counters = new CollectorCounters(&quot;Parallel young collection pauses&quot;, 0);
845 }
<a name="4" id="anc4"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="4" type="hidden" />
</body>
</html>
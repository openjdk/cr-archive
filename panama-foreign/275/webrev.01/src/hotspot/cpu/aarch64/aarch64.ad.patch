diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -3705,11 +3705,11 @@
   stack_direction(TOWARDS_LOW);
 
   // These three registers define part of the calling convention
   // between compiled code and the interpreter.
 
-  // Inline Cache Register or methodOop for I2C.
+  // Inline Cache Register or Method for I2C.
   inline_cache_reg(R12);
 
   // Method Oop Register when calling interpreter.
   interpreter_method_oop_reg(R12);
 
@@ -4056,10 +4056,22 @@
   op_cost(0);
   format %{ %}
   interface(CONST_INTER);
 %}
 
+operand immL_positive_bitmaskI()
+%{
+  predicate((n->get_long() != 0)
+            && ((julong)n->get_long() < 0x80000000ULL)
+            && is_power_of_2(n->get_long() + 1));
+  match(ConL);
+
+  op_cost(0);
+  format %{ %}
+  interface(CONST_INTER);
+%}
+
 // Scale values for scaled offset addressing modes (up to long but not quad)
 operand immIScale()
 %{
   predicate(0 <= n->get_int() && (n->get_int() <= 3));
   match(ConI);
@@ -12152,10 +12164,54 @@
           as_Register($src$$reg), lshift, width);
   %}
   ins_pipe(ialu_reg_shift);
 %}
 
+// This pattern is automatically generated from aarch64_ad.m4
+// DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE
+
+// We can use ubfiz when masking by a positive number and then left shifting the result.
+// We know that the mask is positive because immI_bitmask guarantees it.
+instruct ubfizwIConvI2L(iRegLNoSp dst, iRegIorL2I src, immI lshift, immI_bitmask mask)
+%{
+  match(Set dst (ConvI2L (LShiftI (AndI src mask) lshift)));
+  predicate((exact_log2(n->in(1)->in(1)->in(2)->get_int() + 1) + (n->in(1)->in(2)->get_int() & 31)) <= 31);
+
+  ins_cost(INSN_COST);
+  format %{ "ubfizw $dst, $src, $lshift, $mask" %}
+  ins_encode %{
+    int lshift = $lshift$$constant & 31;
+    intptr_t mask = $mask$$constant;
+    int width = exact_log2(mask+1);
+    __ ubfizw(as_Register($dst$$reg),
+          as_Register($src$$reg), lshift, width);
+  %}
+  ins_pipe(ialu_reg_shift);
+%}
+
+// This pattern is automatically generated from aarch64_ad.m4
+// DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE
+
+// We can use ubfiz when masking by a positive number and then left shifting the result.
+// We know that the mask is positive because immL_bitmask guarantees it.
+instruct ubfizLConvL2I(iRegINoSp dst, iRegL src, immI lshift, immL_positive_bitmaskI mask)
+%{
+  match(Set dst (ConvL2I (LShiftL (AndL src mask) lshift)));
+  predicate((exact_log2_long(n->in(1)->in(1)->in(2)->get_long() + 1) + (n->in(1)->in(2)->get_int() & 63)) <= 31);
+
+  ins_cost(INSN_COST);
+  format %{ "ubfiz $dst, $src, $lshift, $mask" %}
+  ins_encode %{
+    int lshift = $lshift$$constant & 63;
+    intptr_t mask = $mask$$constant;
+    int width = exact_log2_long(mask+1);
+    __ ubfiz(as_Register($dst$$reg),
+          as_Register($src$$reg), lshift, width);
+  %}
+  ins_pipe(ialu_reg_shift);
+%}
+
 
 // This pattern is automatically generated from aarch64_ad.m4
 // DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE
 
 // If there is a convert I to L block between and AndI and a LShiftL, we can also match ubfiz
@@ -12174,10 +12230,46 @@
              as_Register($src$$reg), lshift, width);
   %}
   ins_pipe(ialu_reg_shift);
 %}
 
+// This pattern is automatically generated from aarch64_ad.m4
+// DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE
+
+// If there is a convert L to I block between and AndL and a LShiftI, we can also match ubfiz
+instruct ubfizLConvL2Ix(iRegINoSp dst, iRegL src, immI lshift, immL_positive_bitmaskI mask)
+%{
+  match(Set dst (LShiftI (ConvL2I (AndL src mask)) lshift));
+  predicate((exact_log2_long(n->in(1)->in(1)->in(2)->get_long() + 1) + (n->in(2)->get_int() & 31)) <= 31);
+
+  ins_cost(INSN_COST);
+  format %{ "ubfiz $dst, $src, $lshift, $mask" %}
+  ins_encode %{
+    int lshift = $lshift$$constant & 31;
+    intptr_t mask = $mask$$constant;
+    int width = exact_log2(mask+1);
+    __ ubfiz(as_Register($dst$$reg),
+             as_Register($src$$reg), lshift, width);
+  %}
+  ins_pipe(ialu_reg_shift);
+%}
+
+// This pattern is automatically generated from aarch64_ad.m4
+// DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE
+
+// Can skip int2long conversions after AND with small bitmask
+instruct ubfizIConvI2LAndI(iRegLNoSp dst, iRegI src, immI_bitmask msk)
+%{
+  match(Set dst (ConvI2L (AndI src msk)));
+  ins_cost(INSN_COST);
+  format %{ "ubfiz $dst, $src, 0, exact_log2($msk + 1) " %}
+  ins_encode %{
+    __ ubfiz(as_Register($dst$$reg), as_Register($src$$reg), 0, exact_log2($msk$$constant + 1));
+  %}
+  ins_pipe(ialu_reg_shift);
+%}
+
 
 // Rotations 
 // This pattern is automatically generated from aarch64_ad.m4
 // DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE
 instruct extrOrL(iRegLNoSp dst, iRegL src1, iRegL src2, immI lshift, immI rshift, rFlagsReg cr)
@@ -15481,17 +15573,17 @@
 
 // Tail Call; Jump from runtime stub to Java code.
 // Also known as an 'interprocedural jump'.
 // Target of jump will eventually return to caller.
 // TailJump below removes the return address.
-instruct TailCalljmpInd(iRegPNoSp jump_target, inline_cache_RegP method_oop)
+instruct TailCalljmpInd(iRegPNoSp jump_target, inline_cache_RegP method_ptr)
 %{
-  match(TailCall jump_target method_oop);
+  match(TailCall jump_target method_ptr);
 
   ins_cost(CALL_COST);
 
-  format %{ "br $jump_target\t# $method_oop holds method oop" %}
+  format %{ "br $jump_target\t# $method_ptr holds method" %}
 
   ins_encode(aarch64_enc_tail_call(jump_target));
 
   ins_pipe(pipe_class_call);
 %}

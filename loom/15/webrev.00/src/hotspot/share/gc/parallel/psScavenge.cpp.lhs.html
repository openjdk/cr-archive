<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/parallel/psScavenge.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2002, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;aot/aotLoader.hpp&quot;
 27 #include &quot;classfile/classLoaderDataGraph.hpp&quot;
 28 #include &quot;classfile/stringTable.hpp&quot;
 29 #include &quot;code/codeCache.hpp&quot;
 30 #include &quot;gc/parallel/parallelScavengeHeap.hpp&quot;
 31 #include &quot;gc/parallel/psAdaptiveSizePolicy.hpp&quot;
 32 #include &quot;gc/parallel/psClosure.inline.hpp&quot;
 33 #include &quot;gc/parallel/psCompactionManager.hpp&quot;
 34 #include &quot;gc/parallel/psParallelCompact.inline.hpp&quot;
 35 #include &quot;gc/parallel/psPromotionManager.inline.hpp&quot;
 36 #include &quot;gc/parallel/psRootType.hpp&quot;
 37 #include &quot;gc/parallel/psScavenge.inline.hpp&quot;
 38 #include &quot;gc/shared/gcCause.hpp&quot;
 39 #include &quot;gc/shared/gcHeapSummary.hpp&quot;
 40 #include &quot;gc/shared/gcId.hpp&quot;
 41 #include &quot;gc/shared/gcLocker.hpp&quot;
 42 #include &quot;gc/shared/gcTimer.hpp&quot;
 43 #include &quot;gc/shared/gcTrace.hpp&quot;
 44 #include &quot;gc/shared/gcTraceTime.inline.hpp&quot;
 45 #include &quot;gc/shared/isGCActiveMark.hpp&quot;
 46 #include &quot;gc/shared/oopStorage.inline.hpp&quot;
 47 #include &quot;gc/shared/oopStorageSetParState.inline.hpp&quot;
 48 #include &quot;gc/shared/oopStorageParState.inline.hpp&quot;
 49 #include &quot;gc/shared/referencePolicy.hpp&quot;
 50 #include &quot;gc/shared/referenceProcessor.hpp&quot;
 51 #include &quot;gc/shared/referenceProcessorPhaseTimes.hpp&quot;
 52 #include &quot;gc/shared/scavengableNMethods.hpp&quot;
 53 #include &quot;gc/shared/spaceDecorator.inline.hpp&quot;
 54 #include &quot;gc/shared/taskTerminator.hpp&quot;
 55 #include &quot;gc/shared/weakProcessor.hpp&quot;
 56 #include &quot;gc/shared/workerPolicy.hpp&quot;
 57 #include &quot;gc/shared/workgroup.hpp&quot;
 58 #include &quot;memory/iterator.hpp&quot;
 59 #include &quot;memory/resourceArea.hpp&quot;
 60 #include &quot;memory/universe.hpp&quot;
 61 #include &quot;logging/log.hpp&quot;
 62 #include &quot;oops/access.inline.hpp&quot;
 63 #include &quot;oops/compressedOops.inline.hpp&quot;
 64 #include &quot;oops/oop.inline.hpp&quot;
 65 #include &quot;runtime/biasedLocking.hpp&quot;
 66 #include &quot;runtime/handles.inline.hpp&quot;
 67 #include &quot;runtime/threadCritical.hpp&quot;
 68 #include &quot;runtime/vmThread.hpp&quot;
 69 #include &quot;runtime/vmOperations.hpp&quot;
 70 #include &quot;services/memoryService.hpp&quot;
 71 #include &quot;utilities/stack.inline.hpp&quot;
 72 
 73 HeapWord*                     PSScavenge::_to_space_top_before_gc = NULL;
 74 int                           PSScavenge::_consecutive_skipped_scavenges = 0;
 75 SpanSubjectToDiscoveryClosure PSScavenge::_span_based_discoverer;
 76 ReferenceProcessor*           PSScavenge::_ref_processor = NULL;
 77 PSCardTable*                  PSScavenge::_card_table = NULL;
 78 bool                          PSScavenge::_survivor_overflow = false;
 79 uint                          PSScavenge::_tenuring_threshold = 0;
 80 HeapWord*                     PSScavenge::_young_generation_boundary = NULL;
 81 uintptr_t                     PSScavenge::_young_generation_boundary_compressed = 0;
 82 elapsedTimer                  PSScavenge::_accumulated_time;
 83 STWGCTimer                    PSScavenge::_gc_timer;
 84 ParallelScavengeTracer        PSScavenge::_gc_tracer;
 85 CollectorCounters*            PSScavenge::_counters = NULL;
 86 
 87 static void scavenge_roots_work(ParallelRootType::Value root_type, uint worker_id) {
 88   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
 89 
 90   PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);
 91   PSScavengeRootsClosure roots_closure(pm);
 92   PSPromoteRootsClosure  roots_to_old_closure(pm);
 93 
 94   switch (root_type) {
 95     case ParallelRootType::universe:
 96       Universe::oops_do(&amp;roots_closure);
 97       break;
 98 
 99     case ParallelRootType::object_synchronizer:
100       ObjectSynchronizer::oops_do(&amp;roots_closure);
101       break;
102 
103     case ParallelRootType::class_loader_data:
104       {
105         PSScavengeCLDClosure cld_closure(pm);
106         ClassLoaderDataGraph::cld_do(&amp;cld_closure);
107       }
108       break;
109 
110     case ParallelRootType::code_cache:
111       {
<a name="1" id="anc1"></a><span class="line-modified">112         MarkingCodeBlobClosure code_closure(&amp;roots_to_old_closure, CodeBlobToOopClosure::FixRelocations);</span>
113         ScavengableNMethods::nmethods_do(&amp;code_closure);
114         AOTLoader::oops_do(&amp;roots_closure);
115       }
116       break;
117 
118     case ParallelRootType::sentinel:
119     DEBUG_ONLY(default:) // DEBUG_ONLY hack will create compile error on release builds (-Wswitch) and runtime check on debug builds
120       fatal(&quot;Bad enumeration value: %u&quot;, root_type);
121       break;
122   }
123 
124   // Do the real work
125   pm-&gt;drain_stacks(false);
126 }
127 
128 static void steal_work(TaskTerminator&amp; terminator, uint worker_id) {
129   assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
130 
131   PSPromotionManager* pm =
132     PSPromotionManager::gc_thread_promotion_manager(worker_id);
133   pm-&gt;drain_stacks(true);
134   guarantee(pm-&gt;stacks_empty(),
135             &quot;stacks should be empty at this point&quot;);
136 
137   while (true) {
138     ScannerTask task;
139     if (PSPromotionManager::steal_depth(worker_id, task)) {
140       TASKQUEUE_STATS_ONLY(pm-&gt;record_steal(task));
141       pm-&gt;process_popped_location_depth(task);
142       pm-&gt;drain_stacks_depth(true);
143     } else {
144       if (terminator.offer_termination()) {
145         break;
146       }
147     }
148   }
149   guarantee(pm-&gt;stacks_empty(), &quot;stacks should be empty at this point&quot;);
150 }
151 
152 // Define before use
153 class PSIsAliveClosure: public BoolObjectClosure {
154 public:
155   bool do_object_b(oop p) {
156     return (!PSScavenge::is_obj_in_young(p)) || p-&gt;is_forwarded();
157   }
158 };
159 
160 PSIsAliveClosure PSScavenge::_is_alive_closure;
161 
162 class PSKeepAliveClosure: public OopClosure {
163 protected:
164   MutableSpace* _to_space;
165   PSPromotionManager* _promotion_manager;
166 
167 public:
168   PSKeepAliveClosure(PSPromotionManager* pm) : _promotion_manager(pm) {
169     ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
170     _to_space = heap-&gt;young_gen()-&gt;to_space();
171 
172     assert(_promotion_manager != NULL, &quot;Sanity&quot;);
173   }
174 
175   template &lt;class T&gt; void do_oop_work(T* p) {
176     assert (oopDesc::is_oop(RawAccess&lt;IS_NOT_NULL&gt;::oop_load(p)),
177             &quot;expected an oop while scanning weak refs&quot;);
178 
179     // Weak refs may be visited more than once.
180     if (PSScavenge::should_scavenge(p, _to_space)) {
181       _promotion_manager-&gt;copy_and_push_safe_barrier&lt;/*promote_immediately=*/false&gt;(p);
182     }
183   }
184   virtual void do_oop(oop* p)       { PSKeepAliveClosure::do_oop_work(p); }
185   virtual void do_oop(narrowOop* p) { PSKeepAliveClosure::do_oop_work(p); }
186 };
187 
188 class PSEvacuateFollowersClosure: public VoidClosure {
189  private:
190   PSPromotionManager* _promotion_manager;
191  public:
192   PSEvacuateFollowersClosure(PSPromotionManager* pm) : _promotion_manager(pm) {}
193 
194   virtual void do_void() {
195     assert(_promotion_manager != NULL, &quot;Sanity&quot;);
196     _promotion_manager-&gt;drain_stacks(true);
197     guarantee(_promotion_manager-&gt;stacks_empty(),
198               &quot;stacks should be empty at this point&quot;);
199   }
200 };
201 
202 class PSRefProcTaskExecutor: public AbstractRefProcTaskExecutor {
203   virtual void execute(ProcessTask&amp; process_task, uint ergo_workers);
204 };
205 
206 class PSRefProcTask : public AbstractGangTask {
207   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
208   TaskTerminator _terminator;
209   ProcessTask&amp; _task;
210   uint _active_workers;
211 
212 public:
213   PSRefProcTask(ProcessTask&amp; task, uint active_workers)
214     : AbstractGangTask(&quot;PSRefProcTask&quot;),
215       _terminator(active_workers, PSPromotionManager::stack_array_depth()),
216       _task(task),
217       _active_workers(active_workers) {
218   }
219 
220   virtual void work(uint worker_id) {
221     PSPromotionManager* promotion_manager =
222       PSPromotionManager::gc_thread_promotion_manager(worker_id);
223     assert(promotion_manager != NULL, &quot;sanity check&quot;);
224     PSKeepAliveClosure keep_alive(promotion_manager);
225     PSEvacuateFollowersClosure evac_followers(promotion_manager);
226     PSIsAliveClosure is_alive;
227     _task.work(worker_id, is_alive, keep_alive, evac_followers);
228 
229     if (_task.marks_oops_alive() &amp;&amp; _active_workers &gt; 1) {
230       steal_work(_terminator, worker_id);
231     }
232   }
233 };
234 
235 void PSRefProcTaskExecutor::execute(ProcessTask&amp; process_task, uint ergo_workers) {
236   PSRefProcTask task(process_task, ergo_workers);
237   ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
238 }
239 
240 // This method contains all heap specific policy for invoking scavenge.
241 // PSScavenge::invoke_no_policy() will do nothing but attempt to
242 // scavenge. It will not clean up after failed promotions, bail out if
243 // we&#39;ve exceeded policy time limits, or any other special behavior.
244 // All such policy should be placed here.
245 //
246 // Note that this method should only be called from the vm_thread while
247 // at a safepoint!
248 bool PSScavenge::invoke() {
249   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
250   assert(Thread::current() == (Thread*)VMThread::vm_thread(), &quot;should be in vm thread&quot;);
251   assert(!ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;not reentrant&quot;);
252 
253   ParallelScavengeHeap* const heap = ParallelScavengeHeap::heap();
254   PSAdaptiveSizePolicy* policy = heap-&gt;size_policy();
255   IsGCActiveMark mark;
256 
257   const bool scavenge_done = PSScavenge::invoke_no_policy();
258   const bool need_full_gc = !scavenge_done ||
259     policy-&gt;should_full_GC(heap-&gt;old_gen()-&gt;free_in_bytes());
260   bool full_gc_done = false;
261 
262   if (UsePerfData) {
263     PSGCAdaptivePolicyCounters* const counters = heap-&gt;gc_policy_counters();
264     const int ffs_val = need_full_gc ? full_follows_scavenge : not_skipped;
265     counters-&gt;update_full_follows_scavenge(ffs_val);
266   }
267 
268   if (need_full_gc) {
269     GCCauseSetter gccs(heap, GCCause::_adaptive_size_policy);
270     SoftRefPolicy* srp = heap-&gt;soft_ref_policy();
271     const bool clear_all_softrefs = srp-&gt;should_clear_all_soft_refs();
272 
273     full_gc_done = PSParallelCompact::invoke_no_policy(clear_all_softrefs);
274   }
275 
276   return full_gc_done;
277 }
278 
279 class PSThreadRootsTaskClosure : public ThreadClosure {
280   uint _worker_id;
281 public:
282   PSThreadRootsTaskClosure(uint worker_id) : _worker_id(worker_id) { }
283   virtual void do_thread(Thread* thread) {
284     assert(ParallelScavengeHeap::heap()-&gt;is_gc_active(), &quot;called outside gc&quot;);
285 
286     PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(_worker_id);
287     PSScavengeRootsClosure roots_closure(pm);
<a name="2" id="anc2"></a><span class="line-modified">288     MarkingCodeBlobClosure roots_in_blobs(&amp;roots_closure, CodeBlobToOopClosure::FixRelocations);</span>
289 
290     thread-&gt;oops_do(&amp;roots_closure, &amp;roots_in_blobs);
291 
292     // Do the real work
293     pm-&gt;drain_stacks(false);
294   }
295 };
296 
297 class ScavengeRootsTask : public AbstractGangTask {
298   StrongRootsScope _strong_roots_scope; // needed for Threads::possibly_parallel_threads_do
299   OopStorageSetStrongParState&lt;false /* concurrent */, false /* is_const */&gt; _oop_storage_strong_par_state;
300   SequentialSubTasksDone _subtasks;
301   PSOldGen* _old_gen;
302   HeapWord* _gen_top;
303   uint _active_workers;
304   bool _is_empty;
305   TaskTerminator _terminator;
306 
307 public:
308   ScavengeRootsTask(PSOldGen* old_gen,
309                     HeapWord* gen_top,
310                     uint active_workers,
311                     bool is_empty) :
312       AbstractGangTask(&quot;ScavengeRootsTask&quot;),
313       _strong_roots_scope(active_workers),
314       _subtasks(),
315       _old_gen(old_gen),
316       _gen_top(gen_top),
317       _active_workers(active_workers),
318       _is_empty(is_empty),
319       _terminator(active_workers, PSPromotionManager::vm_thread_promotion_manager()-&gt;stack_array_depth()) {
320     _subtasks.set_n_threads(active_workers);
321     _subtasks.set_n_tasks(ParallelRootType::sentinel);
322   }
323 
324   virtual void work(uint worker_id) {
325     ResourceMark rm;
326 
327     if (!_is_empty) {
328       // There are only old-to-young pointers if there are objects
329       // in the old gen.
330 
331       assert(_old_gen != NULL, &quot;Sanity&quot;);
332       // There are no old-to-young pointers if the old gen is empty.
333       assert(!_old_gen-&gt;object_space()-&gt;is_empty(), &quot;Should not be called is there is no work&quot;);
334       assert(_old_gen-&gt;object_space()-&gt;contains(_gen_top) || _gen_top == _old_gen-&gt;object_space()-&gt;top(), &quot;Sanity&quot;);
335       assert(worker_id &lt; ParallelGCThreads, &quot;Sanity&quot;);
336 
337       {
338         PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);
339         PSCardTable* card_table = ParallelScavengeHeap::heap()-&gt;card_table();
340 
341         card_table-&gt;scavenge_contents_parallel(_old_gen-&gt;start_array(),
342                                                _old_gen-&gt;object_space(),
343                                                _gen_top,
344                                                pm,
345                                                worker_id,
346                                                _active_workers);
347 
348         // Do the real work
349         pm-&gt;drain_stacks(false);
350       }
351     }
352 
353     for (uint root_type = 0; _subtasks.try_claim_task(root_type); /* empty */ ) {
354       scavenge_roots_work(static_cast&lt;ParallelRootType::Value&gt;(root_type), worker_id);
355     }
356     _subtasks.all_tasks_completed();
357 
358     PSThreadRootsTaskClosure closure(worker_id);
359     Threads::possibly_parallel_threads_do(true /*parallel */, &amp;closure);
360 
361     // Scavenge OopStorages
362     {
363       PSPromotionManager* pm = PSPromotionManager::gc_thread_promotion_manager(worker_id);
364       PSScavengeRootsClosure closure(pm);
365       _oop_storage_strong_par_state.oops_do(&amp;closure);
366       // Do the real work
367       pm-&gt;drain_stacks(false);
368     }
369 
370     // If active_workers can exceed 1, add a steal_work().
371     // PSPromotionManager::drain_stacks_depth() does not fully drain its
372     // stacks and expects a steal_work() to complete the draining if
373     // ParallelGCThreads is &gt; 1.
374 
375     if (_active_workers &gt; 1) {
376       steal_work(_terminator, worker_id);
377     }
378   }
379 };
380 
381 // This method contains no policy. You should probably
382 // be calling invoke() instead.
383 bool PSScavenge::invoke_no_policy() {
384   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
385   assert(Thread::current() == (Thread*)VMThread::vm_thread(), &quot;should be in vm thread&quot;);
386 
387   _gc_timer.register_gc_start();
388 
389   TimeStamp scavenge_entry;
390   TimeStamp scavenge_midpoint;
391   TimeStamp scavenge_exit;
392 
393   scavenge_entry.update();
394 
395   if (GCLocker::check_active_before_gc()) {
396     return false;
397   }
398 
399   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
400   GCCause::Cause gc_cause = heap-&gt;gc_cause();
401 
402   // Check for potential problems.
403   if (!should_attempt_scavenge()) {
404     return false;
405   }
406 
407   GCIdMark gc_id_mark;
408   _gc_tracer.report_gc_start(heap-&gt;gc_cause(), _gc_timer.gc_start());
409 
410   bool promotion_failure_occurred = false;
411 
412   PSYoungGen* young_gen = heap-&gt;young_gen();
413   PSOldGen* old_gen = heap-&gt;old_gen();
414   PSAdaptiveSizePolicy* size_policy = heap-&gt;size_policy();
415 
416   heap-&gt;increment_total_collections();
417 
418   if (AdaptiveSizePolicy::should_update_eden_stats(gc_cause)) {
419     // Gather the feedback data for eden occupancy.
420     young_gen-&gt;eden_space()-&gt;accumulate_statistics();
421   }
422 
423   heap-&gt;print_heap_before_gc();
424   heap-&gt;trace_heap_before_gc(&amp;_gc_tracer);
425 
426   assert(!NeverTenure || _tenuring_threshold == markWord::max_age + 1, &quot;Sanity&quot;);
427   assert(!AlwaysTenure || _tenuring_threshold == 0, &quot;Sanity&quot;);
428 
429   // Fill in TLABs
430   heap-&gt;ensure_parsability(true);  // retire TLABs
431 
432   if (VerifyBeforeGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
433     HandleMark hm;  // Discard invalid handles created during verification
434     Universe::verify(&quot;Before GC&quot;);
435   }
436 
437   {
438     ResourceMark rm;
439     HandleMark hm;
440 
441     GCTraceCPUTime tcpu;
442     GCTraceTime(Info, gc) tm(&quot;Pause Young&quot;, NULL, gc_cause, true);
443     TraceCollectorStats tcs(counters());
444     TraceMemoryManagerStats tms(heap-&gt;young_gc_manager(), gc_cause);
445 
446     if (log_is_enabled(Debug, gc, heap, exit)) {
447       accumulated_time()-&gt;start();
448     }
449 
450     // Let the size policy know we&#39;re starting
451     size_policy-&gt;minor_collection_begin();
452 
453     // Verify the object start arrays.
454     if (VerifyObjectStartArray &amp;&amp;
455         VerifyBeforeGC) {
456       old_gen-&gt;verify_object_start_array();
457     }
458 
459     // Verify no unmarked old-&gt;young roots
460     if (VerifyRememberedSets) {
461       heap-&gt;card_table()-&gt;verify_all_young_refs_imprecise();
462     }
463 
464     assert(young_gen-&gt;to_space()-&gt;is_empty(),
465            &quot;Attempt to scavenge with live objects in to_space&quot;);
466     young_gen-&gt;to_space()-&gt;clear(SpaceDecorator::Mangle);
467 
468     save_to_space_top_before_gc();
469 
470 #if COMPILER2_OR_JVMCI
471     DerivedPointerTable::clear();
472 #endif
473 
474     reference_processor()-&gt;enable_discovery();
475     reference_processor()-&gt;setup_policy(false);
476 
477     const PreGenGCValues pre_gc_values = heap-&gt;get_pre_gc_values();
478 
479     // Reset our survivor overflow.
480     set_survivor_overflow(false);
481 
482     // We need to save the old top values before
483     // creating the promotion_manager. We pass the top
484     // values to the card_table, to prevent it from
485     // straying into the promotion labs.
486     HeapWord* old_top = old_gen-&gt;object_space()-&gt;top();
487 
488     const uint active_workers =
489       WorkerPolicy::calc_active_workers(ParallelScavengeHeap::heap()-&gt;workers().total_workers(),
490                                         ParallelScavengeHeap::heap()-&gt;workers().active_workers(),
491                                         Threads::number_of_non_daemon_threads());
492     ParallelScavengeHeap::heap()-&gt;workers().update_active_workers(active_workers);
493 
494     PSPromotionManager::pre_scavenge();
495 
496     // We&#39;ll use the promotion manager again later.
497     PSPromotionManager* promotion_manager = PSPromotionManager::vm_thread_promotion_manager();
498     {
499       GCTraceTime(Debug, gc, phases) tm(&quot;Scavenge&quot;, &amp;_gc_timer);
500 
501       ScavengeRootsTask task(old_gen, old_top, active_workers, old_gen-&gt;object_space()-&gt;is_empty());
502       ParallelScavengeHeap::heap()-&gt;workers().run_task(&amp;task);
503     }
504 
505     scavenge_midpoint.update();
506 
507     // Process reference objects discovered during scavenge
508     {
509       GCTraceTime(Debug, gc, phases) tm(&quot;Reference Processing&quot;, &amp;_gc_timer);
510 
511       reference_processor()-&gt;setup_policy(false); // not always_clear
512       reference_processor()-&gt;set_active_mt_degree(active_workers);
513       PSKeepAliveClosure keep_alive(promotion_manager);
514       PSEvacuateFollowersClosure evac_followers(promotion_manager);
515       ReferenceProcessorStats stats;
516       ReferenceProcessorPhaseTimes pt(&amp;_gc_timer, reference_processor()-&gt;max_num_queues());
517       if (reference_processor()-&gt;processing_is_mt()) {
518         PSRefProcTaskExecutor task_executor;
519         stats = reference_processor()-&gt;process_discovered_references(
520           &amp;_is_alive_closure, &amp;keep_alive, &amp;evac_followers, &amp;task_executor,
521           &amp;pt);
522       } else {
523         stats = reference_processor()-&gt;process_discovered_references(
524           &amp;_is_alive_closure, &amp;keep_alive, &amp;evac_followers, NULL, &amp;pt);
525       }
526 
527       _gc_tracer.report_gc_reference_stats(stats);
528       pt.print_all_references();
529     }
530 
531     assert(promotion_manager-&gt;stacks_empty(),&quot;stacks should be empty at this point&quot;);
532 
533     PSScavengeRootsClosure root_closure(promotion_manager);
534 
535     {
536       GCTraceTime(Debug, gc, phases) tm(&quot;Weak Processing&quot;, &amp;_gc_timer);
537       WeakProcessor::weak_oops_do(&amp;_is_alive_closure, &amp;root_closure);
538     }
539 
540     // Verify that usage of root_closure didn&#39;t copy any objects.
541     assert(promotion_manager-&gt;stacks_empty(),&quot;stacks should be empty at this point&quot;);
542 
543     // Finally, flush the promotion_manager&#39;s labs, and deallocate its stacks.
544     promotion_failure_occurred = PSPromotionManager::post_scavenge(_gc_tracer);
545     if (promotion_failure_occurred) {
546       clean_up_failed_promotion();
547       log_info(gc, promotion)(&quot;Promotion failed&quot;);
548     }
549 
550     _gc_tracer.report_tenuring_threshold(tenuring_threshold());
551 
552     // Let the size policy know we&#39;re done.  Note that we count promotion
553     // failure cleanup time as part of the collection (otherwise, we&#39;re
554     // implicitly saying it&#39;s mutator time).
555     size_policy-&gt;minor_collection_end(gc_cause);
556 
557     if (!promotion_failure_occurred) {
558       // Swap the survivor spaces.
559       young_gen-&gt;eden_space()-&gt;clear(SpaceDecorator::Mangle);
560       young_gen-&gt;from_space()-&gt;clear(SpaceDecorator::Mangle);
561       young_gen-&gt;swap_spaces();
562 
563       size_t survived = young_gen-&gt;from_space()-&gt;used_in_bytes();
564       size_t promoted = old_gen-&gt;used_in_bytes() - pre_gc_values.old_gen_used();
565       size_policy-&gt;update_averages(_survivor_overflow, survived, promoted);
566 
567       // A successful scavenge should restart the GC time limit count which is
568       // for full GC&#39;s.
569       size_policy-&gt;reset_gc_overhead_limit_count();
570       if (UseAdaptiveSizePolicy) {
571         // Calculate the new survivor size and tenuring threshold
572 
573         log_debug(gc, ergo)(&quot;AdaptiveSizeStart:  collection: %d &quot;, heap-&gt;total_collections());
574         log_trace(gc, ergo)(&quot;old_gen_capacity: &quot; SIZE_FORMAT &quot; young_gen_capacity: &quot; SIZE_FORMAT,
575                             old_gen-&gt;capacity_in_bytes(), young_gen-&gt;capacity_in_bytes());
576 
577         if (UsePerfData) {
578           PSGCAdaptivePolicyCounters* counters = heap-&gt;gc_policy_counters();
579           counters-&gt;update_old_eden_size(
580             size_policy-&gt;calculated_eden_size_in_bytes());
581           counters-&gt;update_old_promo_size(
582             size_policy-&gt;calculated_promo_size_in_bytes());
583           counters-&gt;update_old_capacity(old_gen-&gt;capacity_in_bytes());
584           counters-&gt;update_young_capacity(young_gen-&gt;capacity_in_bytes());
585           counters-&gt;update_survived(survived);
586           counters-&gt;update_promoted(promoted);
587           counters-&gt;update_survivor_overflowed(_survivor_overflow);
588         }
589 
590         size_t max_young_size = young_gen-&gt;max_gen_size();
591 
592         // Deciding a free ratio in the young generation is tricky, so if
593         // MinHeapFreeRatio or MaxHeapFreeRatio are in use (implicating
594         // that the old generation size may have been limited because of them) we
595         // should then limit our young generation size using NewRatio to have it
596         // follow the old generation size.
597         if (MinHeapFreeRatio != 0 || MaxHeapFreeRatio != 100) {
598           max_young_size = MIN2(old_gen-&gt;capacity_in_bytes() / NewRatio,
599                                 young_gen-&gt;max_gen_size());
600         }
601 
602         size_t survivor_limit =
603           size_policy-&gt;max_survivor_size(max_young_size);
604         _tenuring_threshold =
605           size_policy-&gt;compute_survivor_space_size_and_threshold(
606                                                            _survivor_overflow,
607                                                            _tenuring_threshold,
608                                                            survivor_limit);
609 
610        log_debug(gc, age)(&quot;Desired survivor size &quot; SIZE_FORMAT &quot; bytes, new threshold %u (max threshold &quot; UINTX_FORMAT &quot;)&quot;,
611                           size_policy-&gt;calculated_survivor_size_in_bytes(),
612                           _tenuring_threshold, MaxTenuringThreshold);
613 
614         if (UsePerfData) {
615           PSGCAdaptivePolicyCounters* counters = heap-&gt;gc_policy_counters();
616           counters-&gt;update_tenuring_threshold(_tenuring_threshold);
617           counters-&gt;update_survivor_size_counters();
618         }
619 
620         // Do call at minor collections?
621         // Don&#39;t check if the size_policy is ready at this
622         // level.  Let the size_policy check that internally.
623         if (UseAdaptiveGenerationSizePolicyAtMinorCollection &amp;&amp;
624             AdaptiveSizePolicy::should_update_eden_stats(gc_cause)) {
625           // Calculate optimal free space amounts
626           assert(young_gen-&gt;max_gen_size() &gt;
627                  young_gen-&gt;from_space()-&gt;capacity_in_bytes() +
628                  young_gen-&gt;to_space()-&gt;capacity_in_bytes(),
629                  &quot;Sizes of space in young gen are out-of-bounds&quot;);
630 
631           size_t young_live = young_gen-&gt;used_in_bytes();
632           size_t eden_live = young_gen-&gt;eden_space()-&gt;used_in_bytes();
633           size_t cur_eden = young_gen-&gt;eden_space()-&gt;capacity_in_bytes();
634           size_t max_old_gen_size = old_gen-&gt;max_gen_size();
635           size_t max_eden_size = max_young_size -
636             young_gen-&gt;from_space()-&gt;capacity_in_bytes() -
637             young_gen-&gt;to_space()-&gt;capacity_in_bytes();
638 
639           // Used for diagnostics
640           size_policy-&gt;clear_generation_free_space_flags();
641 
642           size_policy-&gt;compute_eden_space_size(young_live,
643                                                eden_live,
644                                                cur_eden,
645                                                max_eden_size,
646                                                false /* not full gc*/);
647 
648           size_policy-&gt;check_gc_overhead_limit(eden_live,
649                                                max_old_gen_size,
650                                                max_eden_size,
651                                                false /* not full gc*/,
652                                                gc_cause,
653                                                heap-&gt;soft_ref_policy());
654 
655           size_policy-&gt;decay_supplemental_growth(false /* not full gc*/);
656         }
657         // Resize the young generation at every collection
658         // even if new sizes have not been calculated.  This is
659         // to allow resizes that may have been inhibited by the
660         // relative location of the &quot;to&quot; and &quot;from&quot; spaces.
661 
662         // Resizing the old gen at young collections can cause increases
663         // that don&#39;t feed back to the generation sizing policy until
664         // a full collection.  Don&#39;t resize the old gen here.
665 
666         heap-&gt;resize_young_gen(size_policy-&gt;calculated_eden_size_in_bytes(),
667                         size_policy-&gt;calculated_survivor_size_in_bytes());
668 
669         log_debug(gc, ergo)(&quot;AdaptiveSizeStop: collection: %d &quot;, heap-&gt;total_collections());
670       }
671 
672       // Update the structure of the eden. With NUMA-eden CPU hotplugging or offlining can
673       // cause the change of the heap layout. Make sure eden is reshaped if that&#39;s the case.
674       // Also update() will case adaptive NUMA chunk resizing.
675       assert(young_gen-&gt;eden_space()-&gt;is_empty(), &quot;eden space should be empty now&quot;);
676       young_gen-&gt;eden_space()-&gt;update();
677 
678       heap-&gt;gc_policy_counters()-&gt;update_counters();
679 
680       heap-&gt;resize_all_tlabs();
681 
682       assert(young_gen-&gt;to_space()-&gt;is_empty(), &quot;to space should be empty now&quot;);
683     }
684 
685 #if COMPILER2_OR_JVMCI
686     DerivedPointerTable::update_pointers();
687 #endif
688 
689     NOT_PRODUCT(reference_processor()-&gt;verify_no_references_recorded());
690 
691     // Re-verify object start arrays
692     if (VerifyObjectStartArray &amp;&amp;
693         VerifyAfterGC) {
694       old_gen-&gt;verify_object_start_array();
695     }
696 
697     // Verify all old -&gt; young cards are now precise
698     if (VerifyRememberedSets) {
699       // Precise verification will give false positives. Until this is fixed,
700       // use imprecise verification.
701       // heap-&gt;card_table()-&gt;verify_all_young_refs_precise();
702       heap-&gt;card_table()-&gt;verify_all_young_refs_imprecise();
703     }
704 
705     if (log_is_enabled(Debug, gc, heap, exit)) {
706       accumulated_time()-&gt;stop();
707     }
708 
709     heap-&gt;print_heap_change(pre_gc_values);
710 
711     // Track memory usage and detect low memory
712     MemoryService::track_memory_usage();
713     heap-&gt;update_counters();
714   }
715 
716   if (VerifyAfterGC &amp;&amp; heap-&gt;total_collections() &gt;= VerifyGCStartAt) {
717     HandleMark hm;  // Discard invalid handles created during verification
718     Universe::verify(&quot;After GC&quot;);
719   }
720 
721   heap-&gt;print_heap_after_gc();
722   heap-&gt;trace_heap_after_gc(&amp;_gc_tracer);
723 
724   scavenge_exit.update();
725 
726   log_debug(gc, task, time)(&quot;VM-Thread &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT &quot; &quot; JLONG_FORMAT,
727                             scavenge_entry.ticks(), scavenge_midpoint.ticks(),
728                             scavenge_exit.ticks());
729 
730   AdaptiveSizePolicyOutput::print(size_policy, heap-&gt;total_collections());
731 
732   _gc_timer.register_gc_end();
733 
734   _gc_tracer.report_gc_end(_gc_timer.gc_end(), _gc_timer.time_partitions());
735 
736   return !promotion_failure_occurred;
737 }
738 
739 // This method iterates over all objects in the young generation,
740 // removing all forwarding references. It then restores any preserved marks.
741 void PSScavenge::clean_up_failed_promotion() {
742   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
743   PSYoungGen* young_gen = heap-&gt;young_gen();
744 
745   RemoveForwardedPointerClosure remove_fwd_ptr_closure;
746   young_gen-&gt;object_iterate(&amp;remove_fwd_ptr_closure);
747 
748   PSPromotionManager::restore_preserved_marks();
749 
750   // Reset the PromotionFailureALot counters.
751   NOT_PRODUCT(heap-&gt;reset_promotion_should_fail();)
752 }
753 
754 bool PSScavenge::should_attempt_scavenge() {
755   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
756   PSGCAdaptivePolicyCounters* counters = heap-&gt;gc_policy_counters();
757 
758   if (UsePerfData) {
759     counters-&gt;update_scavenge_skipped(not_skipped);
760   }
761 
762   PSYoungGen* young_gen = heap-&gt;young_gen();
763   PSOldGen* old_gen = heap-&gt;old_gen();
764 
765   // Do not attempt to promote unless to_space is empty
766   if (!young_gen-&gt;to_space()-&gt;is_empty()) {
767     _consecutive_skipped_scavenges++;
768     if (UsePerfData) {
769       counters-&gt;update_scavenge_skipped(to_space_not_empty);
770     }
771     return false;
772   }
773 
774   // Test to see if the scavenge will likely fail.
775   PSAdaptiveSizePolicy* policy = heap-&gt;size_policy();
776 
777   // A similar test is done in the policy&#39;s should_full_GC().  If this is
778   // changed, decide if that test should also be changed.
779   size_t avg_promoted = (size_t) policy-&gt;padded_average_promoted_in_bytes();
780   size_t promotion_estimate = MIN2(avg_promoted, young_gen-&gt;used_in_bytes());
781   bool result = promotion_estimate &lt; old_gen-&gt;free_in_bytes();
782 
783   log_trace(ergo)(&quot;%s scavenge: average_promoted &quot; SIZE_FORMAT &quot; padded_average_promoted &quot; SIZE_FORMAT &quot; free in old gen &quot; SIZE_FORMAT,
784                 result ? &quot;Do&quot; : &quot;Skip&quot;, (size_t) policy-&gt;average_promoted_in_bytes(),
785                 (size_t) policy-&gt;padded_average_promoted_in_bytes(),
786                 old_gen-&gt;free_in_bytes());
787   if (young_gen-&gt;used_in_bytes() &lt; (size_t) policy-&gt;padded_average_promoted_in_bytes()) {
788     log_trace(ergo)(&quot; padded_promoted_average is greater than maximum promotion = &quot; SIZE_FORMAT, young_gen-&gt;used_in_bytes());
789   }
790 
791   if (result) {
792     _consecutive_skipped_scavenges = 0;
793   } else {
794     _consecutive_skipped_scavenges++;
795     if (UsePerfData) {
796       counters-&gt;update_scavenge_skipped(promoted_too_large);
797     }
798   }
799   return result;
800 }
801 
802 // Adaptive size policy support.
803 void PSScavenge::set_young_generation_boundary(HeapWord* v) {
804   _young_generation_boundary = v;
805   if (UseCompressedOops) {
806     _young_generation_boundary_compressed = (uintptr_t)CompressedOops::encode((oop)v);
807   }
808 }
809 
810 void PSScavenge::initialize() {
811   // Arguments must have been parsed
812 
813   if (AlwaysTenure || NeverTenure) {
814     assert(MaxTenuringThreshold == 0 || MaxTenuringThreshold == markWord::max_age + 1,
815            &quot;MaxTenuringThreshold should be 0 or markWord::max_age + 1, but is %d&quot;, (int) MaxTenuringThreshold);
816     _tenuring_threshold = MaxTenuringThreshold;
817   } else {
818     // We want to smooth out our startup times for the AdaptiveSizePolicy
819     _tenuring_threshold = (UseAdaptiveSizePolicy) ? InitialTenuringThreshold :
820                                                     MaxTenuringThreshold;
821   }
822 
823   ParallelScavengeHeap* heap = ParallelScavengeHeap::heap();
824   PSYoungGen* young_gen = heap-&gt;young_gen();
825   PSOldGen* old_gen = heap-&gt;old_gen();
826 
827   // Set boundary between young_gen and old_gen
828   assert(old_gen-&gt;reserved().end() &lt;= young_gen-&gt;eden_space()-&gt;bottom(),
829          &quot;old above young&quot;);
830   set_young_generation_boundary(young_gen-&gt;eden_space()-&gt;bottom());
831 
832   // Initialize ref handling object for scavenging.
833   _span_based_discoverer.set_span(young_gen-&gt;reserved());
834   _ref_processor =
835     new ReferenceProcessor(&amp;_span_based_discoverer,
836                            ParallelRefProcEnabled &amp;&amp; (ParallelGCThreads &gt; 1), // mt processing
837                            ParallelGCThreads,          // mt processing degree
838                            true,                       // mt discovery
839                            ParallelGCThreads,          // mt discovery degree
840                            true,                       // atomic_discovery
841                            NULL,                       // header provides liveness info
842                            false);
843 
844   // Cache the cardtable
845   _card_table = heap-&gt;card_table();
846 
847   _counters = new CollectorCounters(&quot;Parallel young collection pauses&quot;, 0);
848 }
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>
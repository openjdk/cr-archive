<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="../s390/interp_masm_s390.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #ifndef _WINDOWS
  27 #include &quot;alloca.h&quot;
  28 #endif
  29 #include &quot;asm/macroAssembler.hpp&quot;
  30 #include &quot;asm/macroAssembler.inline.hpp&quot;

  31 #include &quot;code/debugInfoRec.hpp&quot;
  32 #include &quot;code/icBuffer.hpp&quot;
  33 #include &quot;code/nativeInst.hpp&quot;
  34 #include &quot;code/vtableStubs.hpp&quot;
  35 #include &quot;gc/shared/collectedHeap.hpp&quot;
  36 #include &quot;gc/shared/gcLocker.hpp&quot;
  37 #include &quot;gc/shared/barrierSet.hpp&quot;
  38 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  39 #include &quot;interpreter/interpreter.hpp&quot;
  40 #include &quot;logging/log.hpp&quot;
  41 #include &quot;memory/resourceArea.hpp&quot;
  42 #include &quot;memory/universe.hpp&quot;
  43 #include &quot;oops/compiledICHolder.hpp&quot;
  44 #include &quot;oops/klass.inline.hpp&quot;
  45 #include &quot;runtime/safepointMechanism.hpp&quot;
  46 #include &quot;runtime/sharedRuntime.hpp&quot;
  47 #include &quot;runtime/vframeArray.hpp&quot;
  48 #include &quot;runtime/vm_version.hpp&quot;
  49 #include &quot;utilities/align.hpp&quot;
  50 #include &quot;utilities/formatBuffer.hpp&quot;
</pre>
<hr />
<pre>
 476     case T_SHORT:
 477     case T_INT:
 478       if (int_args &lt; Argument::n_int_register_parameters_j) {
 479         regs[i].set1(INT_ArgReg[int_args++]-&gt;as_VMReg());
 480       } else {
 481         regs[i].set1(VMRegImpl::stack2reg(stk_args));
 482         stk_args += 2;
 483       }
 484       break;
 485     case T_VOID:
 486       // halves of T_LONG or T_DOUBLE
 487       assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);
 488       regs[i].set_bad();
 489       break;
 490     case T_LONG:
 491       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 492       // fall through
 493     case T_OBJECT:
 494     case T_ARRAY:
 495     case T_ADDRESS:

 496       if (int_args &lt; Argument::n_int_register_parameters_j) {
 497         regs[i].set2(INT_ArgReg[int_args++]-&gt;as_VMReg());
 498       } else {
 499         regs[i].set2(VMRegImpl::stack2reg(stk_args));
 500         stk_args += 2;
 501       }
 502       break;
 503     case T_FLOAT:
 504       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 505         regs[i].set1(FP_ArgReg[fp_args++]-&gt;as_VMReg());
 506       } else {
 507         regs[i].set1(VMRegImpl::stack2reg(stk_args));
 508         stk_args += 2;
 509       }
 510       break;
 511     case T_DOUBLE:
 512       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 513       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 514         regs[i].set2(FP_ArgReg[fp_args++]-&gt;as_VMReg());
 515       } else {
 516         regs[i].set2(VMRegImpl::stack2reg(stk_args));
 517         stk_args += 2;
 518       }
 519       break;
 520     default:
 521       ShouldNotReachHere();
 522       break;
 523     }
 524   }
 525 
 526   return align_up(stk_args, 2);
 527 }
 528 


















































































 529 // Patch the callers callsite with entry to compiled code if it exists.
 530 static void patch_callers_callsite(MacroAssembler *masm) {
 531   Label L;
 532   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 533   __ jcc(Assembler::equal, L);
 534 
 535   // Save the current stack pointer
 536   __ mov(r13, rsp);
 537   // Schedule the branch target address early.
 538   // Call into the VM to patch the caller, then jump to compiled callee
 539   // rax isn&#39;t live so capture return address while we easily can
 540   __ movptr(rax, Address(rsp, 0));
 541 
 542   // align stack so push_CPU_state doesn&#39;t fault
 543   __ andptr(rsp, -(StackAlignmentInBytes));
 544   __ push_CPU_state();
 545   __ vzeroupper();
 546   // VM needs caller&#39;s callsite
 547   // VM needs target method
 548   // This needs to be a long call since we will relocate this adapter to
</pre>
<hr />
<pre>
 551   // Allocate argument register save area
 552   if (frame::arg_reg_save_area_bytes != 0) {
 553     __ subptr(rsp, frame::arg_reg_save_area_bytes);
 554   }
 555   __ mov(c_rarg0, rbx);
 556   __ mov(c_rarg1, rax);
 557   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)));
 558 
 559   // De-allocate argument register save area
 560   if (frame::arg_reg_save_area_bytes != 0) {
 561     __ addptr(rsp, frame::arg_reg_save_area_bytes);
 562   }
 563 
 564   __ vzeroupper();
 565   __ pop_CPU_state();
 566   // restore sp
 567   __ mov(rsp, r13);
 568   __ bind(L);
 569 }
 570 












































































































 571 
 572 static void gen_c2i_adapter(MacroAssembler *masm,
<span class="line-modified"> 573                             int total_args_passed,</span>
<span class="line-removed"> 574                             int comp_args_on_stack,</span>
<span class="line-removed"> 575                             const BasicType *sig_bt,</span>
 576                             const VMRegPair *regs,
<span class="line-modified"> 577                             Label&amp; skip_fixup) {</span>





 578   // Before we get into the guts of the C2I adapter, see if we should be here
 579   // at all.  We&#39;ve come from compiled code and are attempting to jump to the
 580   // interpreter, which means the caller made a static call to get here
 581   // (vcalls always get a compiled target if there is one).  Check for a
 582   // compiled target.  If there is one, we need to patch the caller&#39;s call.
 583   patch_callers_callsite(masm);
 584 
 585   __ bind(skip_fixup);
 586 










































 587   // Since all args are passed on the stack, total_args_passed *
 588   // Interpreter::stackElementSize is the space we need. Plus 1 because
 589   // we also account for the return address location since
 590   // we store it first rather than hold it in rax across all the shuffling
<span class="line-modified"> 591 </span>
 592   int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;
 593 
 594   // stack is aligned, keep it that way
 595   extraspace = align_up(extraspace, 2*wordSize);
 596 
 597   // Get return address
 598   __ pop(rax);
 599 
 600   // set senderSP value
 601   __ mov(r13, rsp);
 602 
 603   __ subptr(rsp, extraspace);
 604 
 605   // Store the return address in the expected location
 606   __ movptr(Address(rsp, 0), rax);
 607 
 608   // Now write the args into the outgoing interpreter space
<span class="line-modified"> 609   for (int i = 0; i &lt; total_args_passed; i++) {</span>
<span class="line-modified"> 610     if (sig_bt[i] == T_VOID) {</span>
<span class="line-modified"> 611       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);</span>
<span class="line-modified"> 612       continue;</span>
<span class="line-modified"> 613     }</span>
<span class="line-modified"> 614 </span>
<span class="line-modified"> 615     // offset to start parameters</span>
<span class="line-modified"> 616     int st_off   = (total_args_passed - i) * Interpreter::stackElementSize;</span>
<span class="line-modified"> 617     int next_off = st_off - Interpreter::stackElementSize;</span>
<span class="line-modified"> 618 </span>
<span class="line-modified"> 619     // Say 4 args:</span>
<span class="line-modified"> 620     // i   st_off</span>
<span class="line-modified"> 621     // 0   32 T_LONG</span>
<span class="line-modified"> 622     // 1   24 T_VOID</span>
<span class="line-modified"> 623     // 2   16 T_OBJECT</span>
<span class="line-modified"> 624     // 3    8 T_BOOL</span>
<span class="line-modified"> 625     // -    0 return address</span>
<span class="line-modified"> 626     //</span>
<span class="line-modified"> 627     // However to make thing extra confusing. Because we can fit a long/double in</span>
<span class="line-removed"> 628     // a single slot on a 64 bt vm and it would be silly to break them up, the interpreter</span>
<span class="line-removed"> 629     // leaves one slot empty and only stores to a single slot. In this case the</span>
<span class="line-removed"> 630     // slot that is occupied is the T_VOID slot. See I said it was confusing.</span>
<span class="line-removed"> 631 </span>
<span class="line-removed"> 632     VMReg r_1 = regs[i].first();</span>
<span class="line-removed"> 633     VMReg r_2 = regs[i].second();</span>
<span class="line-removed"> 634     if (!r_1-&gt;is_valid()) {</span>
<span class="line-removed"> 635       assert(!r_2-&gt;is_valid(), &quot;&quot;);</span>
<span class="line-removed"> 636       continue;</span>
<span class="line-removed"> 637     }</span>
<span class="line-removed"> 638     if (r_1-&gt;is_stack()) {</span>
<span class="line-removed"> 639       // memory to memory use rax</span>
<span class="line-removed"> 640       int ld_off = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;</span>
<span class="line-removed"> 641       if (!r_2-&gt;is_valid()) {</span>
<span class="line-removed"> 642         // sign extend??</span>
<span class="line-removed"> 643         __ movl(rax, Address(rsp, ld_off));</span>
<span class="line-removed"> 644         __ movptr(Address(rsp, st_off), rax);</span>
<span class="line-removed"> 645 </span>
<span class="line-removed"> 646       } else {</span>
<span class="line-removed"> 647 </span>
<span class="line-removed"> 648         __ movq(rax, Address(rsp, ld_off));</span>
<span class="line-removed"> 649 </span>
<span class="line-removed"> 650         // Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG</span>
<span class="line-removed"> 651         // T_DOUBLE and T_LONG use two slots in the interpreter</span>
<span class="line-removed"> 652         if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {</span>
<span class="line-removed"> 653           // ld_off == LSW, ld_off+wordSize == MSW</span>
<span class="line-removed"> 654           // st_off == MSW, next_off == LSW</span>
<span class="line-removed"> 655           __ movq(Address(rsp, next_off), rax);</span>
<span class="line-removed"> 656 #ifdef ASSERT</span>
<span class="line-removed"> 657           // Overwrite the unused slot with known junk</span>
<span class="line-removed"> 658           __ mov64(rax, CONST64(0xdeadffffdeadaaaa));</span>
<span class="line-removed"> 659           __ movptr(Address(rsp, st_off), rax);</span>
<span class="line-removed"> 660 #endif /* ASSERT */</span>
<span class="line-removed"> 661         } else {</span>
<span class="line-removed"> 662           __ movq(Address(rsp, st_off), rax);</span>
 663         }
 664       }
<span class="line-modified"> 665     } else if (r_1-&gt;is_Register()) {</span>
<span class="line-modified"> 666       Register r = r_1-&gt;as_Register();</span>
<span class="line-modified"> 667       if (!r_2-&gt;is_valid()) {</span>
<span class="line-modified"> 668         // must be only an int (or less ) so move only 32bits to slot</span>
<span class="line-modified"> 669         // why not sign extend??</span>
<span class="line-modified"> 670         __ movl(Address(rsp, st_off), r);</span>
<span class="line-modified"> 671       } else {</span>
<span class="line-removed"> 672         // Two VMREgs|OptoRegs can be T_OBJECT, T_ADDRESS, T_DOUBLE, T_LONG</span>
<span class="line-removed"> 673         // T_DOUBLE and T_LONG use two slots in the interpreter</span>
<span class="line-removed"> 674         if ( sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {</span>
<span class="line-removed"> 675           // long/double in gpr</span>
<span class="line-removed"> 676 #ifdef ASSERT</span>
<span class="line-removed"> 677           // Overwrite the unused slot with known junk</span>
<span class="line-removed"> 678           __ mov64(rax, CONST64(0xdeadffffdeadaaab));</span>
<span class="line-removed"> 679           __ movptr(Address(rsp, st_off), rax);</span>
<span class="line-removed"> 680 #endif /* ASSERT */</span>
<span class="line-removed"> 681           __ movq(Address(rsp, next_off), r);</span>
<span class="line-removed"> 682         } else {</span>
<span class="line-removed"> 683           __ movptr(Address(rsp, st_off), r);</span>
<span class="line-removed"> 684         }</span>
<span class="line-removed"> 685       }</span>
<span class="line-removed"> 686     } else {</span>
<span class="line-removed"> 687       assert(r_1-&gt;is_XMMRegister(), &quot;&quot;);</span>
<span class="line-removed"> 688       if (!r_2-&gt;is_valid()) {</span>
<span class="line-removed"> 689         // only a float use just part of the slot</span>
<span class="line-removed"> 690         __ movflt(Address(rsp, st_off), r_1-&gt;as_XMMRegister());</span>
<span class="line-removed"> 691       } else {</span>
 692 #ifdef ASSERT

 693         // Overwrite the unused slot with known junk
<span class="line-modified"> 694         __ mov64(rax, CONST64(0xdeadffffdeadaaac));</span>
 695         __ movptr(Address(rsp, st_off), rax);
<span class="line-modified"> 696 #endif /* ASSERT */</span>
<span class="line-modified"> 697         __ movdbl(Address(rsp, next_off), r_1-&gt;as_XMMRegister());</span>





































 698       }
 699     }
 700   }
 701 
 702   // Schedule the branch target address early.
 703   __ movptr(rcx, Address(rbx, in_bytes(Method::interpreter_entry_offset())));
 704   __ jmp(rcx);
 705 }
 706 
 707 static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg,
 708                         address code_start, address code_end,
 709                         Label&amp; L_ok) {
 710   Label L_fail;
 711   __ lea(temp_reg, ExternalAddress(code_start));
 712   __ cmpptr(pc_reg, temp_reg);
 713   __ jcc(Assembler::belowEqual, L_fail);
 714   __ lea(temp_reg, ExternalAddress(code_end));
 715   __ cmpptr(pc_reg, temp_reg);
 716   __ jcc(Assembler::below, L_ok);
 717   __ bind(L_fail);
 718 }
 719 
 720 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
<span class="line-modified"> 721                                     int total_args_passed,</span>
<span class="line-removed"> 722                                     int comp_args_on_stack,</span>
 723                                     const BasicType *sig_bt,
 724                                     const VMRegPair *regs) {
 725 
 726   // Note: r13 contains the senderSP on entry. We must preserve it since
 727   // we may do a i2c -&gt; c2i transition if we lose a race where compiled
 728   // code goes non-entrant while we get args ready.
 729   // In addition we use r13 to locate all the interpreter args as
 730   // we must align the stack to 16 bytes on an i2c entry else we
 731   // lose alignment we expect in all compiled code and register
 732   // save code can segv when fxsave instructions find improperly
 733   // aligned stack pointer.
 734 
 735   // Adapters can be frameless because they do not require the caller
 736   // to perform additional cleanup work, such as correcting the stack pointer.
 737   // An i2c adapter is frameless because the *caller* frame, which is interpreted,
 738   // routinely repairs its own stack pointer (from interpreter_frame_last_sp),
 739   // even if a callee has modified the stack pointer.
 740   // A c2i adapter is frameless because the *callee* frame, which is interpreted,
 741   // routinely repairs its caller&#39;s stack pointer (from sender_sp, which is set
 742   // up via the senderSP register).
</pre>
<hr />
<pre>
 795     comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)&gt;&gt;LogBytesPerWord;
 796     // Round up to miminum stack alignment, in wordSize
 797     comp_words_on_stack = align_up(comp_words_on_stack, 2);
 798     __ subptr(rsp, comp_words_on_stack * wordSize);
 799   }
 800 
 801 
 802   // Ensure compiled code always sees stack at proper alignment
 803   __ andptr(rsp, -16);
 804 
 805   // push the return address and misalign the stack that youngest frame always sees
 806   // as far as the placement of the call instruction
 807   __ push(rax);
 808 
 809   // Put saved SP in another register
 810   const Register saved_sp = rax;
 811   __ movptr(saved_sp, r11);
 812 
 813   // Will jump to the compiled code just as if compiled code was doing it.
 814   // Pre-load the register-jump target early, to schedule it better.
<span class="line-modified"> 815   __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_offset())));</span>
 816 
 817 #if INCLUDE_JVMCI
 818   if (EnableJVMCI || UseAOT) {
 819     // check if this call should be routed towards a specific entry point
 820     __ cmpptr(Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), 0);
 821     Label no_alternative_target;
 822     __ jcc(Assembler::equal, no_alternative_target);
 823     __ movptr(r11, Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())));
 824     __ movptr(Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), 0);
 825     __ bind(no_alternative_target);
 826   }
 827 #endif // INCLUDE_JVMCI
 828 


 829   // Now generate the shuffle code.  Pick up all register args and move the
 830   // rest through the floating point stack top.
 831   for (int i = 0; i &lt; total_args_passed; i++) {
<span class="line-modified"> 832     if (sig_bt[i] == T_VOID) {</span>


 833       // Longs and doubles are passed in native word order, but misaligned
 834       // in the 32-bit build.
<span class="line-modified"> 835       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);</span>

 836       continue;
 837     }
 838 
 839     // Pick up 0, 1 or 2 words from SP+offset.
 840 
 841     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
 842             &quot;scrambled load targets?&quot;);
 843     // Load in argument order going down.
 844     int ld_off = (total_args_passed - i)*Interpreter::stackElementSize;
 845     // Point to interpreter value (vs. tag)
 846     int next_off = ld_off - Interpreter::stackElementSize;
 847     //
 848     //
 849     //
 850     VMReg r_1 = regs[i].first();
 851     VMReg r_2 = regs[i].second();
 852     if (!r_1-&gt;is_valid()) {
 853       assert(!r_2-&gt;is_valid(), &quot;&quot;);
 854       continue;
 855     }
</pre>
<hr />
<pre>
 857       // Convert stack slot to an SP offset (+ wordSize to account for return address )
 858       int st_off = regs[i].first()-&gt;reg2stack()*VMRegImpl::stack_slot_size + wordSize;
 859 
 860       // We can use r13 as a temp here because compiled code doesn&#39;t need r13 as an input
 861       // and if we end up going thru a c2i because of a miss a reasonable value of r13
 862       // will be generated.
 863       if (!r_2-&gt;is_valid()) {
 864         // sign extend???
 865         __ movl(r13, Address(saved_sp, ld_off));
 866         __ movptr(Address(rsp, st_off), r13);
 867       } else {
 868         //
 869         // We are using two optoregs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
 870         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
 871         // So we must adjust where to pick up the data to match the interpreter.
 872         //
 873         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
 874         // are accessed as negative so LSW is at LOW address
 875 
 876         // ld_off is MSW so get LSW
<span class="line-modified"> 877         const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?</span>
 878                            next_off : ld_off;
 879         __ movq(r13, Address(saved_sp, offset));
 880         // st_off is LSW (i.e. reg.first())
 881         __ movq(Address(rsp, st_off), r13);
 882       }
 883     } else if (r_1-&gt;is_Register()) {  // Register argument
 884       Register r = r_1-&gt;as_Register();
 885       assert(r != rax, &quot;must be different&quot;);
 886       if (r_2-&gt;is_valid()) {
 887         //
 888         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
 889         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
 890         // So we must adjust where to pick up the data to match the interpreter.
 891 
<span class="line-modified"> 892         const int offset = (sig_bt[i]==T_LONG||sig_bt[i]==T_DOUBLE)?</span>
 893                            next_off : ld_off;
 894 
 895         // this can be a misaligned move
 896         __ movq(r, Address(saved_sp, offset));
 897       } else {
 898         // sign extend and use a full word?
 899         __ movl(r, Address(saved_sp, ld_off));
 900       }
 901     } else {
 902       if (!r_2-&gt;is_valid()) {
 903         __ movflt(r_1-&gt;as_XMMRegister(), Address(saved_sp, ld_off));
 904       } else {
 905         __ movdbl(r_1-&gt;as_XMMRegister(), Address(saved_sp, next_off));
 906       }
 907     }
 908   }
 909 
 910   // 6243940 We might end up in handle_wrong_method if
 911   // the callee is deoptimized as we race thru here. If that
 912   // happens we don&#39;t want to take a safepoint because the
 913   // caller frame will look interpreted and arguments are now
 914   // &quot;compiled&quot; so it is much better to make this transition
 915   // invisible to the stack walking code. Unfortunately if
 916   // we try and find the callee by normal means a safepoint
 917   // is possible. So we stash the desired callee in the thread
 918   // and the vm will find there should this case occur.
 919 
 920   __ movptr(Address(r15_thread, JavaThread::callee_target_offset()), rbx);
 921 
 922   // put Method* where a c2i would expect should we end up there
<span class="line-modified"> 923   // only needed becaus eof c2 resolve stubs return Method* as a result in</span>
 924   // rax
 925   __ mov(rax, rbx);
 926   __ jmp(r11);
 927 }
 928 






















 929 // ---------------------------------------------------------------
 930 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
<span class="line-modified"> 931                                                             int total_args_passed,</span>
<span class="line-modified"> 932                                                             int comp_args_on_stack,</span>
<span class="line-modified"> 933                                                             const BasicType *sig_bt,</span>
<span class="line-modified"> 934                                                             const VMRegPair *regs,</span>




 935                                                             AdapterFingerPrint* fingerprint) {
 936   address i2c_entry = __ pc();
<span class="line-modified"> 937 </span>
<span class="line-removed"> 938   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);</span>
 939 
 940   // -------------------------------------------------------------------------
 941   // Generate a C2I adapter.  On entry we know rbx holds the Method* during calls
 942   // to the interpreter.  The args start out packed in the compiled layout.  They
 943   // need to be unpacked into the interpreter layout.  This will almost always
 944   // require some stack space.  We grow the current (compiled) stack, then repack
 945   // the args.  We  finally end in a jump to the generic interpreter entry point.
 946   // On exit from the interpreter, the interpreter will restore our SP (lest the
 947   // compiled code, which relys solely on SP and not RBP, get sick).
 948 
 949   address c2i_unverified_entry = __ pc();
 950   Label skip_fixup;
<span class="line-modified"> 951   Label ok;</span>
<span class="line-removed"> 952 </span>
<span class="line-removed"> 953   Register holder = rax;</span>
<span class="line-removed"> 954   Register receiver = j_rarg0;</span>
<span class="line-removed"> 955   Register temp = rbx;</span>
<span class="line-removed"> 956 </span>
<span class="line-removed"> 957   {</span>
<span class="line-removed"> 958     __ load_klass(temp, receiver, rscratch1);</span>
<span class="line-removed"> 959     __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));</span>
<span class="line-removed"> 960     __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));</span>
<span class="line-removed"> 961     __ jcc(Assembler::equal, ok);</span>
 962     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
 963 
<span class="line-modified"> 964     __ bind(ok);</span>
<span class="line-modified"> 965     // Method might have been compiled since the call site was patched to</span>
<span class="line-modified"> 966     // interpreted if that is the case treat it as a miss so we can get</span>
<span class="line-modified"> 967     // the call site corrected.</span>
<span class="line-modified"> 968     __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);</span>
<span class="line-modified"> 969     __ jcc(Assembler::equal, skip_fixup);</span>
<span class="line-modified"> 970     __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));</span>



 971   }
 972 

 973   address c2i_entry = __ pc();
 974 
 975   // Class initialization barrier for static methods
 976   address c2i_no_clinit_check_entry = NULL;
 977   if (VM_Version::supports_fast_class_init_checks()) {
 978     Label L_skip_barrier;
 979     Register method = rbx;
 980 
 981     { // Bypass the barrier for non-static methods
 982       Register flags  = rscratch1;
 983       __ movl(flags, Address(method, Method::access_flags_offset()));
 984       __ testl(flags, JVM_ACC_STATIC);
 985       __ jcc(Assembler::zero, L_skip_barrier); // non-static
 986     }
 987 
 988     Register klass = rscratch1;
 989     __ load_method_holder(klass, method);
 990     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
 991 
 992     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
 993 
 994     __ bind(L_skip_barrier);
 995     c2i_no_clinit_check_entry = __ pc();
 996   }
 997 
 998   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
 999   bs-&gt;c2i_entry_barrier(masm);
1000 
<span class="line-modified">1001   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);</span>














1002 
1003   __ flush();
<span class="line-modified">1004   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);</span>






1005 }
1006 
1007 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
1008                                          VMRegPair *regs,
1009                                          VMRegPair *regs2,
1010                                          int total_args_passed) {
1011   assert(regs2 == NULL, &quot;not needed on x86&quot;);
1012 // We return the amount of VMRegImpl stack slots we need to reserve for all
1013 // the arguments NOT counting out_preserve_stack_slots.
1014 
1015 // NOTE: These arrays will have to change when c1 is ported
1016 #ifdef _WIN64
1017     static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {
1018       c_rarg0, c_rarg1, c_rarg2, c_rarg3
1019     };
1020     static const XMMRegister FP_ArgReg[Argument::n_float_register_parameters_c] = {
1021       c_farg0, c_farg1, c_farg2, c_farg3
1022     };
1023 #else
1024     static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {
</pre>
<hr />
<pre>
1042       case T_BYTE:
1043       case T_SHORT:
1044       case T_INT:
1045         if (int_args &lt; Argument::n_int_register_parameters_c) {
1046           regs[i].set1(INT_ArgReg[int_args++]-&gt;as_VMReg());
1047 #ifdef _WIN64
1048           fp_args++;
1049           // Allocate slots for callee to stuff register args the stack.
1050           stk_args += 2;
1051 #endif
1052         } else {
1053           regs[i].set1(VMRegImpl::stack2reg(stk_args));
1054           stk_args += 2;
1055         }
1056         break;
1057       case T_LONG:
1058         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
1059         // fall through
1060       case T_OBJECT:
1061       case T_ARRAY:

1062       case T_ADDRESS:
1063       case T_METADATA:
1064         if (int_args &lt; Argument::n_int_register_parameters_c) {
1065           regs[i].set2(INT_ArgReg[int_args++]-&gt;as_VMReg());
1066 #ifdef _WIN64
1067           fp_args++;
1068           stk_args += 2;
1069 #endif
1070         } else {
1071           regs[i].set2(VMRegImpl::stack2reg(stk_args));
1072           stk_args += 2;
1073         }
1074         break;
1075       case T_FLOAT:
1076         if (fp_args &lt; Argument::n_float_register_parameters_c) {
1077           regs[i].set1(FP_ArgReg[fp_args++]-&gt;as_VMReg());
1078 #ifdef _WIN64
1079           int_args++;
1080           // Allocate slots for callee to stuff register args the stack.
1081           stk_args += 2;
</pre>
<hr />
<pre>
1392   // otherwise it should load them.
1393   int slot = arg_save_area;
1394   // Save down double word first
1395   for ( int i = 0; i &lt; total_in_args; i++) {
1396     if (in_regs[i].first()-&gt;is_XMMRegister() &amp;&amp; in_sig_bt[i] == T_DOUBLE) {
1397       int offset = slot * VMRegImpl::stack_slot_size;
1398       slot += VMRegImpl::slots_per_word;
1399       assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1400       if (map != NULL) {
1401         __ movdbl(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1402       } else {
1403         __ movdbl(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1404       }
1405     }
1406     if (in_regs[i].first()-&gt;is_Register() &amp;&amp;
1407         (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {
1408       int offset = slot * VMRegImpl::stack_slot_size;
1409       if (map != NULL) {
1410         __ movq(Address(rsp, offset), in_regs[i].first()-&gt;as_Register());
1411         if (in_sig_bt[i] == T_ARRAY) {
<span class="line-modified">1412           map-&gt;set_oop(VMRegImpl::stack2reg(slot));;</span>
1413         }
1414       } else {
1415         __ movq(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
1416       }
1417       slot += VMRegImpl::slots_per_word;
1418     }
1419   }
1420   // Save or restore single word registers
1421   for ( int i = 0; i &lt; total_in_args; i++) {
1422     if (in_regs[i].first()-&gt;is_Register()) {
1423       int offset = slot * VMRegImpl::stack_slot_size;
1424       slot++;
1425       assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1426 
1427       // Value is in an input register pass we must flush it to the stack
1428       const Register reg = in_regs[i].first()-&gt;as_Register();
1429       switch (in_sig_bt[i]) {
1430         case T_BOOLEAN:
1431         case T_CHAR:
1432         case T_BYTE:
1433         case T_SHORT:
1434         case T_INT:
1435           if (map != NULL) {
1436             __ movl(Address(rsp, offset), reg);
1437           } else {
1438             __ movl(reg, Address(rsp, offset));
1439           }
1440           break;
1441         case T_ARRAY:
1442         case T_LONG:
1443           // handled above
1444           break;
1445         case T_OBJECT:

1446         default: ShouldNotReachHere();
1447       }
1448     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1449       if (in_sig_bt[i] == T_FLOAT) {
1450         int offset = slot * VMRegImpl::stack_slot_size;
1451         slot++;
1452         assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1453         if (map != NULL) {
1454           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1455         } else {
1456           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1457         }
1458       }
1459     } else if (in_regs[i].first()-&gt;is_stack()) {
1460       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1461         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1462         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1463       }
1464     }
1465   }
</pre>
<hr />
<pre>
2339             VMRegPair result_reg;
2340             result_reg.set_ptr(rax-&gt;as_VMReg());
2341             move_ptr(masm, result_reg, in_regs[i]);
2342             if (!in_regs[i].first()-&gt;is_stack()) {
2343               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2344               move_ptr(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));
2345               pinned_slot += VMRegImpl::slots_per_word;
2346             }
2347           }
2348           unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);
2349           c_arg++;
2350 #ifdef ASSERT
2351           if (out_regs[c_arg].first()-&gt;is_Register()) {
2352             reg_destroyed[out_regs[c_arg].first()-&gt;as_Register()-&gt;encoding()] = true;
2353           } else if (out_regs[c_arg].first()-&gt;is_XMMRegister()) {
2354             freg_destroyed[out_regs[c_arg].first()-&gt;as_XMMRegister()-&gt;encoding()] = true;
2355           }
2356 #endif
2357           break;
2358         }

2359       case T_OBJECT:
2360         assert(!is_critical_native, &quot;no oop arguments&quot;);
2361         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2362                     ((i == 0) &amp;&amp; (!is_static)),
2363                     &amp;receiver_offset);
2364         break;
2365       case T_VOID:
2366         break;
2367 
2368       case T_FLOAT:
2369         float_move(masm, in_regs[i], out_regs[c_arg]);
2370           break;
2371 
2372       case T_DOUBLE:
2373         assert( i + 1 &lt; total_in_args &amp;&amp;
2374                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2375                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
2376         double_move(masm, in_regs[i], out_regs[c_arg]);
2377         break;
2378 
</pre>
<hr />
<pre>
2474     // Get the handle (the 2nd argument)
2475     __ mov(oop_handle_reg, c_rarg1);
2476 
2477     // Get address of the box
2478 
2479     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2480 
2481     // Load the oop from the handle
2482     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2483 
2484     __ resolve(IS_NOT_NULL, obj_reg);
2485     if (UseBiasedLocking) {
2486       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &amp;slow_path_lock);
2487     }
2488 
2489     // Load immediate 1 into swap_reg %rax
2490     __ movl(swap_reg, 1);
2491 
2492     // Load (object-&gt;mark() | 1) into swap_reg %rax
2493     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));




2494 
2495     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2496     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2497 
2498     // src -&gt; dest iff dest == rax else rax &lt;- dest
2499     __ lock();
2500     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2501     __ jcc(Assembler::equal, lock_done);
2502 
2503     // Hmm should this move to the slow path code area???
2504 
2505     // Test if the oopMark is an obvious stack pointer, i.e.,
2506     //  1) (mark &amp; 3) == 0, and
2507     //  2) rsp &lt;= mark &lt; mark + os::pagesize()
2508     // These 3 tests can be done by evaluating the following
2509     // expression: ((mark - rsp) &amp; (3 - os::vm_page_size())),
2510     // assuming both stack pointer and pagesize have their
2511     // least significant 2 bits clear.
2512     // NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg
2513 
</pre>
<hr />
<pre>
2535   // Now set thread in native
2536   __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);
2537 
2538   __ call(RuntimeAddress(native_func));
2539 
2540   // Verify or restore cpu control state after JNI call
2541   __ restore_cpu_control_state_after_jni();
2542 
2543   // Unpack native results.
2544   switch (ret_type) {
2545   case T_BOOLEAN: __ c2bool(rax);            break;
2546   case T_CHAR   : __ movzwl(rax, rax);      break;
2547   case T_BYTE   : __ sign_extend_byte (rax); break;
2548   case T_SHORT  : __ sign_extend_short(rax); break;
2549   case T_INT    : /* nothing to do */        break;
2550   case T_DOUBLE :
2551   case T_FLOAT  :
2552     // Result is in xmm0 we&#39;ll save as needed
2553     break;
2554   case T_ARRAY:                 // Really a handle

2555   case T_OBJECT:                // Really a handle
2556       break; // can&#39;t de-handlize until after safepoint check
2557   case T_VOID: break;
2558   case T_LONG: break;
2559   default       : ShouldNotReachHere();
2560   }
2561 
2562   // unpin pinned arguments
2563   pinned_slot = oop_handle_offset;
2564   if (pinned_args.length() &gt; 0) {
2565     // save return value that may be overwritten otherwise.
2566     save_native_result(masm, ret_type, stack_slots);
2567     for (int index = 0; index &lt; pinned_args.length(); index ++) {
2568       int i = pinned_args.at(index);
2569       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2570       if (!in_regs[i].first()-&gt;is_stack()) {
2571         int offset = pinned_slot * VMRegImpl::stack_slot_size;
2572         __ movq(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
2573         pinned_slot += VMRegImpl::slots_per_word;
2574       }
</pre>
<hr />
<pre>
4053   __ movptr(Address(r15_thread, JavaThread::exception_handler_pc_offset()), (int)NULL_WORD);
4054   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), (int)NULL_WORD);
4055 #endif
4056   // Clear the exception oop so GC no longer processes it as a root.
4057   __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()), (int)NULL_WORD);
4058 
4059   // rax: exception oop
4060   // r8:  exception handler
4061   // rdx: exception pc
4062   // Jump to handler
4063 
4064   __ jmp(r8);
4065 
4066   // Make sure all code is generated
4067   masm-&gt;flush();
4068 
4069   // Set exception blob
4070   _exception_blob =  ExceptionBlob::create(&amp;buffer, oop_maps, SimpleRuntimeFrame::framesize &gt;&gt; 1);
4071 }
4072 #endif // COMPILER2















































































































</pre>
</td>
<td>
<hr />
<pre>
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #ifndef _WINDOWS
  27 #include &quot;alloca.h&quot;
  28 #endif
  29 #include &quot;asm/macroAssembler.hpp&quot;
  30 #include &quot;asm/macroAssembler.inline.hpp&quot;
<span class="line-added">  31 #include &quot;classfile/symbolTable.hpp&quot;</span>
  32 #include &quot;code/debugInfoRec.hpp&quot;
  33 #include &quot;code/icBuffer.hpp&quot;
  34 #include &quot;code/nativeInst.hpp&quot;
  35 #include &quot;code/vtableStubs.hpp&quot;
  36 #include &quot;gc/shared/collectedHeap.hpp&quot;
  37 #include &quot;gc/shared/gcLocker.hpp&quot;
  38 #include &quot;gc/shared/barrierSet.hpp&quot;
  39 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  40 #include &quot;interpreter/interpreter.hpp&quot;
  41 #include &quot;logging/log.hpp&quot;
  42 #include &quot;memory/resourceArea.hpp&quot;
  43 #include &quot;memory/universe.hpp&quot;
  44 #include &quot;oops/compiledICHolder.hpp&quot;
  45 #include &quot;oops/klass.inline.hpp&quot;
  46 #include &quot;runtime/safepointMechanism.hpp&quot;
  47 #include &quot;runtime/sharedRuntime.hpp&quot;
  48 #include &quot;runtime/vframeArray.hpp&quot;
  49 #include &quot;runtime/vm_version.hpp&quot;
  50 #include &quot;utilities/align.hpp&quot;
  51 #include &quot;utilities/formatBuffer.hpp&quot;
</pre>
<hr />
<pre>
 477     case T_SHORT:
 478     case T_INT:
 479       if (int_args &lt; Argument::n_int_register_parameters_j) {
 480         regs[i].set1(INT_ArgReg[int_args++]-&gt;as_VMReg());
 481       } else {
 482         regs[i].set1(VMRegImpl::stack2reg(stk_args));
 483         stk_args += 2;
 484       }
 485       break;
 486     case T_VOID:
 487       // halves of T_LONG or T_DOUBLE
 488       assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);
 489       regs[i].set_bad();
 490       break;
 491     case T_LONG:
 492       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 493       // fall through
 494     case T_OBJECT:
 495     case T_ARRAY:
 496     case T_ADDRESS:
<span class="line-added"> 497     case T_INLINE_TYPE:</span>
 498       if (int_args &lt; Argument::n_int_register_parameters_j) {
 499         regs[i].set2(INT_ArgReg[int_args++]-&gt;as_VMReg());
 500       } else {
 501         regs[i].set2(VMRegImpl::stack2reg(stk_args));
 502         stk_args += 2;
 503       }
 504       break;
 505     case T_FLOAT:
 506       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 507         regs[i].set1(FP_ArgReg[fp_args++]-&gt;as_VMReg());
 508       } else {
 509         regs[i].set1(VMRegImpl::stack2reg(stk_args));
 510         stk_args += 2;
 511       }
 512       break;
 513     case T_DOUBLE:
 514       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 515       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 516         regs[i].set2(FP_ArgReg[fp_args++]-&gt;as_VMReg());
 517       } else {
 518         regs[i].set2(VMRegImpl::stack2reg(stk_args));
 519         stk_args += 2;
 520       }
 521       break;
 522     default:
 523       ShouldNotReachHere();
 524       break;
 525     }
 526   }
 527 
 528   return align_up(stk_args, 2);
 529 }
 530 
<span class="line-added"> 531 // Same as java_calling_convention() but for multiple return</span>
<span class="line-added"> 532 // values. There&#39;s no way to store them on the stack so if we don&#39;t</span>
<span class="line-added"> 533 // have enough registers, multiple values can&#39;t be returned.</span>
<span class="line-added"> 534 const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;</span>
<span class="line-added"> 535 const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;</span>
<span class="line-added"> 536 int SharedRuntime::java_return_convention(const BasicType *sig_bt,</span>
<span class="line-added"> 537                                           VMRegPair *regs,</span>
<span class="line-added"> 538                                           int total_args_passed) {</span>
<span class="line-added"> 539   // Create the mapping between argument positions and</span>
<span class="line-added"> 540   // registers.</span>
<span class="line-added"> 541   static const Register INT_ArgReg[java_return_convention_max_int] = {</span>
<span class="line-added"> 542     rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0</span>
<span class="line-added"> 543   };</span>
<span class="line-added"> 544   static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {</span>
<span class="line-added"> 545     j_farg0, j_farg1, j_farg2, j_farg3,</span>
<span class="line-added"> 546     j_farg4, j_farg5, j_farg6, j_farg7</span>
<span class="line-added"> 547   };</span>
<span class="line-added"> 548 </span>
<span class="line-added"> 549 </span>
<span class="line-added"> 550   uint int_args = 0;</span>
<span class="line-added"> 551   uint fp_args = 0;</span>
<span class="line-added"> 552 </span>
<span class="line-added"> 553   for (int i = 0; i &lt; total_args_passed; i++) {</span>
<span class="line-added"> 554     switch (sig_bt[i]) {</span>
<span class="line-added"> 555     case T_BOOLEAN:</span>
<span class="line-added"> 556     case T_CHAR:</span>
<span class="line-added"> 557     case T_BYTE:</span>
<span class="line-added"> 558     case T_SHORT:</span>
<span class="line-added"> 559     case T_INT:</span>
<span class="line-added"> 560       if (int_args &lt; Argument::n_int_register_parameters_j+1) {</span>
<span class="line-added"> 561         regs[i].set1(INT_ArgReg[int_args]-&gt;as_VMReg());</span>
<span class="line-added"> 562         int_args++;</span>
<span class="line-added"> 563       } else {</span>
<span class="line-added"> 564         return -1;</span>
<span class="line-added"> 565       }</span>
<span class="line-added"> 566       break;</span>
<span class="line-added"> 567     case T_VOID:</span>
<span class="line-added"> 568       // halves of T_LONG or T_DOUBLE</span>
<span class="line-added"> 569       assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);</span>
<span class="line-added"> 570       regs[i].set_bad();</span>
<span class="line-added"> 571       break;</span>
<span class="line-added"> 572     case T_LONG:</span>
<span class="line-added"> 573       assert(sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);</span>
<span class="line-added"> 574       // fall through</span>
<span class="line-added"> 575     case T_OBJECT:</span>
<span class="line-added"> 576     case T_INLINE_TYPE:</span>
<span class="line-added"> 577     case T_ARRAY:</span>
<span class="line-added"> 578     case T_ADDRESS:</span>
<span class="line-added"> 579     case T_METADATA:</span>
<span class="line-added"> 580       if (int_args &lt; Argument::n_int_register_parameters_j+1) {</span>
<span class="line-added"> 581         regs[i].set2(INT_ArgReg[int_args]-&gt;as_VMReg());</span>
<span class="line-added"> 582         int_args++;</span>
<span class="line-added"> 583       } else {</span>
<span class="line-added"> 584         return -1;</span>
<span class="line-added"> 585       }</span>
<span class="line-added"> 586       break;</span>
<span class="line-added"> 587     case T_FLOAT:</span>
<span class="line-added"> 588       if (fp_args &lt; Argument::n_float_register_parameters_j) {</span>
<span class="line-added"> 589         regs[i].set1(FP_ArgReg[fp_args]-&gt;as_VMReg());</span>
<span class="line-added"> 590         fp_args++;</span>
<span class="line-added"> 591       } else {</span>
<span class="line-added"> 592         return -1;</span>
<span class="line-added"> 593       }</span>
<span class="line-added"> 594       break;</span>
<span class="line-added"> 595     case T_DOUBLE:</span>
<span class="line-added"> 596       assert(sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);</span>
<span class="line-added"> 597       if (fp_args &lt; Argument::n_float_register_parameters_j) {</span>
<span class="line-added"> 598         regs[i].set2(FP_ArgReg[fp_args]-&gt;as_VMReg());</span>
<span class="line-added"> 599         fp_args++;</span>
<span class="line-added"> 600       } else {</span>
<span class="line-added"> 601         return -1;</span>
<span class="line-added"> 602       }</span>
<span class="line-added"> 603       break;</span>
<span class="line-added"> 604     default:</span>
<span class="line-added"> 605       ShouldNotReachHere();</span>
<span class="line-added"> 606       break;</span>
<span class="line-added"> 607     }</span>
<span class="line-added"> 608   }</span>
<span class="line-added"> 609 </span>
<span class="line-added"> 610   return int_args + fp_args;</span>
<span class="line-added"> 611 }</span>
<span class="line-added"> 612 </span>
 613 // Patch the callers callsite with entry to compiled code if it exists.
 614 static void patch_callers_callsite(MacroAssembler *masm) {
 615   Label L;
 616   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 617   __ jcc(Assembler::equal, L);
 618 
 619   // Save the current stack pointer
 620   __ mov(r13, rsp);
 621   // Schedule the branch target address early.
 622   // Call into the VM to patch the caller, then jump to compiled callee
 623   // rax isn&#39;t live so capture return address while we easily can
 624   __ movptr(rax, Address(rsp, 0));
 625 
 626   // align stack so push_CPU_state doesn&#39;t fault
 627   __ andptr(rsp, -(StackAlignmentInBytes));
 628   __ push_CPU_state();
 629   __ vzeroupper();
 630   // VM needs caller&#39;s callsite
 631   // VM needs target method
 632   // This needs to be a long call since we will relocate this adapter to
</pre>
<hr />
<pre>
 635   // Allocate argument register save area
 636   if (frame::arg_reg_save_area_bytes != 0) {
 637     __ subptr(rsp, frame::arg_reg_save_area_bytes);
 638   }
 639   __ mov(c_rarg0, rbx);
 640   __ mov(c_rarg1, rax);
 641   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)));
 642 
 643   // De-allocate argument register save area
 644   if (frame::arg_reg_save_area_bytes != 0) {
 645     __ addptr(rsp, frame::arg_reg_save_area_bytes);
 646   }
 647 
 648   __ vzeroupper();
 649   __ pop_CPU_state();
 650   // restore sp
 651   __ mov(rsp, r13);
 652   __ bind(L);
 653 }
 654 
<span class="line-added"> 655 // For each inline type argument, sig includes the list of fields of</span>
<span class="line-added"> 656 // the inline type. This utility function computes the number of</span>
<span class="line-added"> 657 // arguments for the call if inline types are passed by reference (the</span>
<span class="line-added"> 658 // calling convention the interpreter expects).</span>
<span class="line-added"> 659 static int compute_total_args_passed_int(const GrowableArray&lt;SigEntry&gt;* sig_extended) {</span>
<span class="line-added"> 660   int total_args_passed = 0;</span>
<span class="line-added"> 661   if (InlineTypePassFieldsAsArgs) {</span>
<span class="line-added"> 662     for (int i = 0; i &lt; sig_extended-&gt;length(); i++) {</span>
<span class="line-added"> 663       BasicType bt = sig_extended-&gt;at(i)._bt;</span>
<span class="line-added"> 664       if (SigEntry::is_reserved_entry(sig_extended, i)) {</span>
<span class="line-added"> 665         // Ignore reserved entry</span>
<span class="line-added"> 666       } else if (bt == T_INLINE_TYPE) {</span>
<span class="line-added"> 667         // In sig_extended, an inline type argument starts with:</span>
<span class="line-added"> 668         // T_INLINE_TYPE, followed by the types of the fields of the</span>
<span class="line-added"> 669         // inline type and T_VOID to mark the end of the value</span>
<span class="line-added"> 670         // type. Inline types are flattened so, for instance, in the</span>
<span class="line-added"> 671         // case of an inline type with an int field and an inline type</span>
<span class="line-added"> 672         // field that itself has 2 fields, an int and a long:</span>
<span class="line-added"> 673         // T_INLINE_TYPE T_INT T_INLINE_TYPE T_INT T_LONG T_VOID (second</span>
<span class="line-added"> 674         // slot for the T_LONG) T_VOID (inner T_INLINE_TYPE) T_VOID</span>
<span class="line-added"> 675         // (outer T_INLINE_TYPE)</span>
<span class="line-added"> 676         total_args_passed++;</span>
<span class="line-added"> 677         int vt = 1;</span>
<span class="line-added"> 678         do {</span>
<span class="line-added"> 679           i++;</span>
<span class="line-added"> 680           BasicType bt = sig_extended-&gt;at(i)._bt;</span>
<span class="line-added"> 681           BasicType prev_bt = sig_extended-&gt;at(i-1)._bt;</span>
<span class="line-added"> 682           if (bt == T_INLINE_TYPE) {</span>
<span class="line-added"> 683             vt++;</span>
<span class="line-added"> 684           } else if (bt == T_VOID &amp;&amp;</span>
<span class="line-added"> 685                      prev_bt != T_LONG &amp;&amp;</span>
<span class="line-added"> 686                      prev_bt != T_DOUBLE) {</span>
<span class="line-added"> 687             vt--;</span>
<span class="line-added"> 688           }</span>
<span class="line-added"> 689         } while (vt != 0);</span>
<span class="line-added"> 690       } else {</span>
<span class="line-added"> 691         total_args_passed++;</span>
<span class="line-added"> 692       }</span>
<span class="line-added"> 693     }</span>
<span class="line-added"> 694   } else {</span>
<span class="line-added"> 695     total_args_passed = sig_extended-&gt;length();</span>
<span class="line-added"> 696   }</span>
<span class="line-added"> 697   return total_args_passed;</span>
<span class="line-added"> 698 }</span>
<span class="line-added"> 699 </span>
<span class="line-added"> 700 </span>
<span class="line-added"> 701 static void gen_c2i_adapter_helper(MacroAssembler* masm,</span>
<span class="line-added"> 702                                    BasicType bt,</span>
<span class="line-added"> 703                                    BasicType prev_bt,</span>
<span class="line-added"> 704                                    size_t size_in_bytes,</span>
<span class="line-added"> 705                                    const VMRegPair&amp; reg_pair,</span>
<span class="line-added"> 706                                    const Address&amp; to,</span>
<span class="line-added"> 707                                    int extraspace,</span>
<span class="line-added"> 708                                    bool is_oop) {</span>
<span class="line-added"> 709   assert(bt != T_INLINE_TYPE || !InlineTypePassFieldsAsArgs, &quot;no inline type here&quot;);</span>
<span class="line-added"> 710   if (bt == T_VOID) {</span>
<span class="line-added"> 711     assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, &quot;missing half&quot;);</span>
<span class="line-added"> 712     return;</span>
<span class="line-added"> 713   }</span>
<span class="line-added"> 714 </span>
<span class="line-added"> 715   // Say 4 args:</span>
<span class="line-added"> 716   // i   st_off</span>
<span class="line-added"> 717   // 0   32 T_LONG</span>
<span class="line-added"> 718   // 1   24 T_VOID</span>
<span class="line-added"> 719   // 2   16 T_OBJECT</span>
<span class="line-added"> 720   // 3    8 T_BOOL</span>
<span class="line-added"> 721   // -    0 return address</span>
<span class="line-added"> 722   //</span>
<span class="line-added"> 723   // However to make thing extra confusing. Because we can fit a long/double in</span>
<span class="line-added"> 724   // a single slot on a 64 bt vm and it would be silly to break them up, the interpreter</span>
<span class="line-added"> 725   // leaves one slot empty and only stores to a single slot. In this case the</span>
<span class="line-added"> 726   // slot that is occupied is the T_VOID slot. See I said it was confusing.</span>
<span class="line-added"> 727 </span>
<span class="line-added"> 728   bool wide = (size_in_bytes == wordSize);</span>
<span class="line-added"> 729   VMReg r_1 = reg_pair.first();</span>
<span class="line-added"> 730   VMReg r_2 = reg_pair.second();</span>
<span class="line-added"> 731   assert(r_2-&gt;is_valid() == wide, &quot;invalid size&quot;);</span>
<span class="line-added"> 732   if (!r_1-&gt;is_valid()) {</span>
<span class="line-added"> 733     assert(!r_2-&gt;is_valid(), &quot;must be invalid&quot;);</span>
<span class="line-added"> 734     return;</span>
<span class="line-added"> 735   }</span>
<span class="line-added"> 736 </span>
<span class="line-added"> 737   if (!r_1-&gt;is_XMMRegister()) {</span>
<span class="line-added"> 738     Register val = rax;</span>
<span class="line-added"> 739     if (r_1-&gt;is_stack()) {</span>
<span class="line-added"> 740       int ld_off = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;</span>
<span class="line-added"> 741       __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, /* is_signed */ false);</span>
<span class="line-added"> 742     } else {</span>
<span class="line-added"> 743       val = r_1-&gt;as_Register();</span>
<span class="line-added"> 744     }</span>
<span class="line-added"> 745     assert_different_registers(to.base(), val, rscratch1);</span>
<span class="line-added"> 746     if (is_oop) {</span>
<span class="line-added"> 747       __ push(r13);</span>
<span class="line-added"> 748       __ push(rbx);</span>
<span class="line-added"> 749       __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);</span>
<span class="line-added"> 750       __ pop(rbx);</span>
<span class="line-added"> 751       __ pop(r13);</span>
<span class="line-added"> 752     } else {</span>
<span class="line-added"> 753       __ store_sized_value(to, val, size_in_bytes);</span>
<span class="line-added"> 754     }</span>
<span class="line-added"> 755   } else {</span>
<span class="line-added"> 756     if (wide) {</span>
<span class="line-added"> 757       __ movdbl(to, r_1-&gt;as_XMMRegister());</span>
<span class="line-added"> 758     } else {</span>
<span class="line-added"> 759       __ movflt(to, r_1-&gt;as_XMMRegister());</span>
<span class="line-added"> 760     }</span>
<span class="line-added"> 761   }</span>
<span class="line-added"> 762 }</span>
 763 
 764 static void gen_c2i_adapter(MacroAssembler *masm,
<span class="line-modified"> 765                             const GrowableArray&lt;SigEntry&gt;* sig_extended,</span>


 766                             const VMRegPair *regs,
<span class="line-modified"> 767                             Label&amp; skip_fixup,</span>
<span class="line-added"> 768                             address start,</span>
<span class="line-added"> 769                             OopMapSet* oop_maps,</span>
<span class="line-added"> 770                             int&amp; frame_complete,</span>
<span class="line-added"> 771                             int&amp; frame_size_in_words,</span>
<span class="line-added"> 772                             bool alloc_inline_receiver) {</span>
 773   // Before we get into the guts of the C2I adapter, see if we should be here
 774   // at all.  We&#39;ve come from compiled code and are attempting to jump to the
 775   // interpreter, which means the caller made a static call to get here
 776   // (vcalls always get a compiled target if there is one).  Check for a
 777   // compiled target.  If there is one, we need to patch the caller&#39;s call.
 778   patch_callers_callsite(masm);
 779 
 780   __ bind(skip_fixup);
 781 
<span class="line-added"> 782   if (InlineTypePassFieldsAsArgs) {</span>
<span class="line-added"> 783     // Is there an inline type argument?</span>
<span class="line-added"> 784     bool has_inline_argument = false;</span>
<span class="line-added"> 785     for (int i = 0; i &lt; sig_extended-&gt;length() &amp;&amp; !has_inline_argument; i++) {</span>
<span class="line-added"> 786       has_inline_argument = (sig_extended-&gt;at(i)._bt == T_INLINE_TYPE);</span>
<span class="line-added"> 787     }</span>
<span class="line-added"> 788     if (has_inline_argument) {</span>
<span class="line-added"> 789       // There is at least an inline type argument: we&#39;re coming from</span>
<span class="line-added"> 790       // compiled code so we have no buffers to back the inline types.</span>
<span class="line-added"> 791       // Allocate the buffers here with a runtime call.</span>
<span class="line-added"> 792       OopMap* map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);</span>
<span class="line-added"> 793 </span>
<span class="line-added"> 794       frame_complete = __ offset();</span>
<span class="line-added"> 795 </span>
<span class="line-added"> 796       __ set_last_Java_frame(noreg, noreg, NULL);</span>
<span class="line-added"> 797 </span>
<span class="line-added"> 798       __ mov(c_rarg0, r15_thread);</span>
<span class="line-added"> 799       __ mov(c_rarg1, rbx);</span>
<span class="line-added"> 800       __ mov64(c_rarg2, (int64_t)alloc_inline_receiver);</span>
<span class="line-added"> 801       __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_inline_types)));</span>
<span class="line-added"> 802 </span>
<span class="line-added"> 803       oop_maps-&gt;add_gc_map((int)(__ pc() - start), map);</span>
<span class="line-added"> 804       __ reset_last_Java_frame(false);</span>
<span class="line-added"> 805 </span>
<span class="line-added"> 806       RegisterSaver::restore_live_registers(masm);</span>
<span class="line-added"> 807 </span>
<span class="line-added"> 808       Label no_exception;</span>
<span class="line-added"> 809       __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);</span>
<span class="line-added"> 810       __ jcc(Assembler::equal, no_exception);</span>
<span class="line-added"> 811 </span>
<span class="line-added"> 812       __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);</span>
<span class="line-added"> 813       __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));</span>
<span class="line-added"> 814       __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));</span>
<span class="line-added"> 815 </span>
<span class="line-added"> 816       __ bind(no_exception);</span>
<span class="line-added"> 817 </span>
<span class="line-added"> 818       // We get an array of objects from the runtime call</span>
<span class="line-added"> 819       __ get_vm_result(rscratch2, r15_thread); // Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()</span>
<span class="line-added"> 820       __ get_vm_result_2(rbx, r15_thread); // TODO: required to keep the callee Method live?</span>
<span class="line-added"> 821     }</span>
<span class="line-added"> 822   }</span>
<span class="line-added"> 823 </span>
 824   // Since all args are passed on the stack, total_args_passed *
 825   // Interpreter::stackElementSize is the space we need. Plus 1 because
 826   // we also account for the return address location since
 827   // we store it first rather than hold it in rax across all the shuffling
<span class="line-modified"> 828   int total_args_passed = compute_total_args_passed_int(sig_extended);</span>
 829   int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;
 830 
 831   // stack is aligned, keep it that way
 832   extraspace = align_up(extraspace, 2*wordSize);
 833 
 834   // Get return address
 835   __ pop(rax);
 836 
 837   // set senderSP value
 838   __ mov(r13, rsp);
 839 
 840   __ subptr(rsp, extraspace);
 841 
 842   // Store the return address in the expected location
 843   __ movptr(Address(rsp, 0), rax);
 844 
 845   // Now write the args into the outgoing interpreter space
<span class="line-modified"> 846 </span>
<span class="line-modified"> 847   // next_arg_comp is the next argument from the compiler point of</span>
<span class="line-modified"> 848   // view (inline type fields are passed in registers/on the stack). In</span>
<span class="line-modified"> 849   // sig_extended, an inline type argument starts with: T_INLINE_TYPE,</span>
<span class="line-modified"> 850   // followed by the types of the fields of the inline type and T_VOID</span>
<span class="line-modified"> 851   // to mark the end of the inline type. ignored counts the number of</span>
<span class="line-modified"> 852   // T_INLINE_TYPE/T_VOID. next_vt_arg is the next inline type argument:</span>
<span class="line-modified"> 853   // used to get the buffer for that argument from the pool of buffers</span>
<span class="line-modified"> 854   // we allocated above and want to pass to the</span>
<span class="line-modified"> 855   // interpreter. next_arg_int is the next argument from the</span>
<span class="line-modified"> 856   // interpreter point of view (inline types are passed by reference).</span>
<span class="line-modified"> 857   for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;</span>
<span class="line-modified"> 858        next_arg_comp &lt; sig_extended-&gt;length(); next_arg_comp++) {</span>
<span class="line-modified"> 859     assert(ignored &lt;= next_arg_comp, &quot;shouldn&#39;t skip over more slots than there are arguments&quot;);</span>
<span class="line-modified"> 860     assert(next_arg_int &lt;= total_args_passed, &quot;more arguments for the interpreter than expected?&quot;);</span>
<span class="line-modified"> 861     BasicType bt = sig_extended-&gt;at(next_arg_comp)._bt;</span>
<span class="line-modified"> 862     int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;</span>
<span class="line-modified"> 863     if (!InlineTypePassFieldsAsArgs || bt != T_INLINE_TYPE) {</span>
<span class="line-modified"> 864       if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {</span>



































 865         continue; // Ignore reserved entry
 866       }
<span class="line-modified"> 867       int next_off = st_off - Interpreter::stackElementSize;</span>
<span class="line-modified"> 868       const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;</span>
<span class="line-modified"> 869       const VMRegPair reg_pair = regs[next_arg_comp-ignored];</span>
<span class="line-modified"> 870       size_t size_in_bytes = reg_pair.second()-&gt;is_valid() ? 8 : 4;</span>
<span class="line-modified"> 871       gen_c2i_adapter_helper(masm, bt, next_arg_comp &gt; 0 ? sig_extended-&gt;at(next_arg_comp-1)._bt : T_ILLEGAL,</span>
<span class="line-modified"> 872                              size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);</span>
<span class="line-modified"> 873       next_arg_int++;</span>




















 874 #ifdef ASSERT
<span class="line-added"> 875       if (bt == T_LONG || bt == T_DOUBLE) {</span>
 876         // Overwrite the unused slot with known junk
<span class="line-modified"> 877         __ mov64(rax, CONST64(0xdeadffffdeadaaaa));</span>
 878         __ movptr(Address(rsp, st_off), rax);
<span class="line-modified"> 879       }</span>
<span class="line-modified"> 880 #endif /* ASSERT */</span>
<span class="line-added"> 881     } else {</span>
<span class="line-added"> 882       ignored++;</span>
<span class="line-added"> 883       // get the buffer from the just allocated pool of buffers</span>
<span class="line-added"> 884       int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_INLINE_TYPE);</span>
<span class="line-added"> 885       __ load_heap_oop(r14, Address(rscratch2, index));</span>
<span class="line-added"> 886       next_vt_arg++; next_arg_int++;</span>
<span class="line-added"> 887       int vt = 1;</span>
<span class="line-added"> 888       // write fields we get from compiled code in registers/stack</span>
<span class="line-added"> 889       // slots to the buffer: we know we are done with that inline type</span>
<span class="line-added"> 890       // argument when we hit the T_VOID that acts as an end of inline</span>
<span class="line-added"> 891       // type delimiter for this inline type. Inline types are flattened</span>
<span class="line-added"> 892       // so we might encounter embedded inline types. Each entry in</span>
<span class="line-added"> 893       // sig_extended contains a field offset in the buffer.</span>
<span class="line-added"> 894       do {</span>
<span class="line-added"> 895         next_arg_comp++;</span>
<span class="line-added"> 896         BasicType bt = sig_extended-&gt;at(next_arg_comp)._bt;</span>
<span class="line-added"> 897         BasicType prev_bt = sig_extended-&gt;at(next_arg_comp-1)._bt;</span>
<span class="line-added"> 898         if (bt == T_INLINE_TYPE) {</span>
<span class="line-added"> 899           vt++;</span>
<span class="line-added"> 900           ignored++;</span>
<span class="line-added"> 901         } else if (bt == T_VOID &amp;&amp;</span>
<span class="line-added"> 902                    prev_bt != T_LONG &amp;&amp;</span>
<span class="line-added"> 903                    prev_bt != T_DOUBLE) {</span>
<span class="line-added"> 904           vt--;</span>
<span class="line-added"> 905           ignored++;</span>
<span class="line-added"> 906         } else if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {</span>
<span class="line-added"> 907           // Ignore reserved entry</span>
<span class="line-added"> 908         } else {</span>
<span class="line-added"> 909           int off = sig_extended-&gt;at(next_arg_comp)._offset;</span>
<span class="line-added"> 910           assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added"> 911           size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;</span>
<span class="line-added"> 912           bool is_oop = is_reference_type(bt);</span>
<span class="line-added"> 913           gen_c2i_adapter_helper(masm, bt, next_arg_comp &gt; 0 ? sig_extended-&gt;at(next_arg_comp-1)._bt : T_ILLEGAL,</span>
<span class="line-added"> 914                                  size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);</span>
<span class="line-added"> 915         }</span>
<span class="line-added"> 916       } while (vt != 0);</span>
<span class="line-added"> 917       // pass the buffer to the interpreter</span>
 918       __ movptr(Address(rsp, st_off), r14);
 919     }
 920   }
 921 
 922   // Schedule the branch target address early.
 923   __ movptr(rcx, Address(rbx, in_bytes(Method::interpreter_entry_offset())));
 924   __ jmp(rcx);
 925 }
 926 
 927 static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg,
 928                         address code_start, address code_end,
 929                         Label&amp; L_ok) {
 930   Label L_fail;
 931   __ lea(temp_reg, ExternalAddress(code_start));
 932   __ cmpptr(pc_reg, temp_reg);
 933   __ jcc(Assembler::belowEqual, L_fail);
 934   __ lea(temp_reg, ExternalAddress(code_end));
 935   __ cmpptr(pc_reg, temp_reg);
 936   __ jcc(Assembler::below, L_ok);
 937   __ bind(L_fail);
 938 }
 939 
 940 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
<span class="line-modified"> 941                                     int comp_args_on_stack,</span>

 942                                     const GrowableArray&lt;SigEntry&gt;* sig,
 943                                     const VMRegPair *regs) {
 944 
 945   // Note: r13 contains the senderSP on entry. We must preserve it since
 946   // we may do a i2c -&gt; c2i transition if we lose a race where compiled
 947   // code goes non-entrant while we get args ready.
 948   // In addition we use r13 to locate all the interpreter args as
 949   // we must align the stack to 16 bytes on an i2c entry else we
 950   // lose alignment we expect in all compiled code and register
 951   // save code can segv when fxsave instructions find improperly
 952   // aligned stack pointer.
 953 
 954   // Adapters can be frameless because they do not require the caller
 955   // to perform additional cleanup work, such as correcting the stack pointer.
 956   // An i2c adapter is frameless because the *caller* frame, which is interpreted,
 957   // routinely repairs its own stack pointer (from interpreter_frame_last_sp),
 958   // even if a callee has modified the stack pointer.
 959   // A c2i adapter is frameless because the *callee* frame, which is interpreted,
 960   // routinely repairs its caller&#39;s stack pointer (from sender_sp, which is set
 961   // up via the senderSP register).
</pre>
<hr />
<pre>
1014     comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)&gt;&gt;LogBytesPerWord;
1015     // Round up to miminum stack alignment, in wordSize
1016     comp_words_on_stack = align_up(comp_words_on_stack, 2);
1017     __ subptr(rsp, comp_words_on_stack * wordSize);
1018   }
1019 
1020 
1021   // Ensure compiled code always sees stack at proper alignment
1022   __ andptr(rsp, -16);
1023 
1024   // push the return address and misalign the stack that youngest frame always sees
1025   // as far as the placement of the call instruction
1026   __ push(rax);
1027 
1028   // Put saved SP in another register
1029   const Register saved_sp = rax;
1030   __ movptr(saved_sp, r11);
1031 
1032   // Will jump to the compiled code just as if compiled code was doing it.
1033   // Pre-load the register-jump target early, to schedule it better.
<span class="line-modified">1034   __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_inline_offset())));</span>
1035 
1036 #if INCLUDE_JVMCI
1037   if (EnableJVMCI || UseAOT) {
1038     // check if this call should be routed towards a specific entry point
1039     __ cmpptr(Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), 0);
1040     Label no_alternative_target;
1041     __ jcc(Assembler::equal, no_alternative_target);
1042     __ movptr(r11, Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())));
1043     __ movptr(Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), 0);
1044     __ bind(no_alternative_target);
1045   }
1046 #endif // INCLUDE_JVMCI
1047 
<span class="line-added">1048   int total_args_passed = sig-&gt;length();</span>
<span class="line-added">1049 </span>
1050   // Now generate the shuffle code.  Pick up all register args and move the
1051   // rest through the floating point stack top.
1052   for (int i = 0; i &lt; total_args_passed; i++) {
<span class="line-modified">1053     BasicType bt = sig-&gt;at(i)._bt;</span>
<span class="line-added">1054     assert(bt != T_INLINE_TYPE, &quot;i2c adapter doesn&#39;t unpack inline type args&quot;);</span>
<span class="line-added">1055     if (bt == T_VOID) {</span>
1056       // Longs and doubles are passed in native word order, but misaligned
1057       // in the 32-bit build.
<span class="line-modified">1058       BasicType prev_bt = (i &gt; 0) ? sig-&gt;at(i-1)._bt : T_ILLEGAL;</span>
<span class="line-added">1059       assert(i &gt; 0 &amp;&amp; (prev_bt == T_LONG || prev_bt == T_DOUBLE), &quot;missing half&quot;);</span>
1060       continue;
1061     }
1062 
1063     // Pick up 0, 1 or 2 words from SP+offset.
1064 
1065     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
1066             &quot;scrambled load targets?&quot;);
1067     // Load in argument order going down.
1068     int ld_off = (total_args_passed - i)*Interpreter::stackElementSize;
1069     // Point to interpreter value (vs. tag)
1070     int next_off = ld_off - Interpreter::stackElementSize;
1071     //
1072     //
1073     //
1074     VMReg r_1 = regs[i].first();
1075     VMReg r_2 = regs[i].second();
1076     if (!r_1-&gt;is_valid()) {
1077       assert(!r_2-&gt;is_valid(), &quot;&quot;);
1078       continue;
1079     }
</pre>
<hr />
<pre>
1081       // Convert stack slot to an SP offset (+ wordSize to account for return address )
1082       int st_off = regs[i].first()-&gt;reg2stack()*VMRegImpl::stack_slot_size + wordSize;
1083 
1084       // We can use r13 as a temp here because compiled code doesn&#39;t need r13 as an input
1085       // and if we end up going thru a c2i because of a miss a reasonable value of r13
1086       // will be generated.
1087       if (!r_2-&gt;is_valid()) {
1088         // sign extend???
1089         __ movl(r13, Address(saved_sp, ld_off));
1090         __ movptr(Address(rsp, st_off), r13);
1091       } else {
1092         //
1093         // We are using two optoregs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
1094         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
1095         // So we must adjust where to pick up the data to match the interpreter.
1096         //
1097         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
1098         // are accessed as negative so LSW is at LOW address
1099 
1100         // ld_off is MSW so get LSW
<span class="line-modified">1101         const int offset = (bt==T_LONG||bt==T_DOUBLE)?</span>
1102                            next_off : ld_off;
1103         __ movq(r13, Address(saved_sp, offset));
1104         // st_off is LSW (i.e. reg.first())
1105         __ movq(Address(rsp, st_off), r13);
1106       }
1107     } else if (r_1-&gt;is_Register()) {  // Register argument
1108       Register r = r_1-&gt;as_Register();
1109       assert(r != rax, &quot;must be different&quot;);
1110       if (r_2-&gt;is_valid()) {
1111         //
1112         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
1113         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
1114         // So we must adjust where to pick up the data to match the interpreter.
1115 
<span class="line-modified">1116         const int offset = (bt==T_LONG||bt==T_DOUBLE)?</span>
1117                            next_off : ld_off;
1118 
1119         // this can be a misaligned move
1120         __ movq(r, Address(saved_sp, offset));
1121       } else {
1122         // sign extend and use a full word?
1123         __ movl(r, Address(saved_sp, ld_off));
1124       }
1125     } else {
1126       if (!r_2-&gt;is_valid()) {
1127         __ movflt(r_1-&gt;as_XMMRegister(), Address(saved_sp, ld_off));
1128       } else {
1129         __ movdbl(r_1-&gt;as_XMMRegister(), Address(saved_sp, next_off));
1130       }
1131     }
1132   }
1133 
1134   // 6243940 We might end up in handle_wrong_method if
1135   // the callee is deoptimized as we race thru here. If that
1136   // happens we don&#39;t want to take a safepoint because the
1137   // caller frame will look interpreted and arguments are now
1138   // &quot;compiled&quot; so it is much better to make this transition
1139   // invisible to the stack walking code. Unfortunately if
1140   // we try and find the callee by normal means a safepoint
1141   // is possible. So we stash the desired callee in the thread
1142   // and the vm will find there should this case occur.
1143 
1144   __ movptr(Address(r15_thread, JavaThread::callee_target_offset()), rbx);
1145 
1146   // put Method* where a c2i would expect should we end up there
<span class="line-modified">1147   // only needed because of c2 resolve stubs return Method* as a result in</span>
1148   // rax
1149   __ mov(rax, rbx);
1150   __ jmp(r11);
1151 }
1152 
<span class="line-added">1153 static void gen_inline_cache_check(MacroAssembler *masm, Label&amp; skip_fixup) {</span>
<span class="line-added">1154   Label ok;</span>
<span class="line-added">1155 </span>
<span class="line-added">1156   Register holder = rax;</span>
<span class="line-added">1157   Register receiver = j_rarg0;</span>
<span class="line-added">1158   Register temp = rbx;</span>
<span class="line-added">1159 </span>
<span class="line-added">1160   __ load_klass(temp, receiver, rscratch1);</span>
<span class="line-added">1161   __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));</span>
<span class="line-added">1162   __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));</span>
<span class="line-added">1163   __ jcc(Assembler::equal, ok);</span>
<span class="line-added">1164   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));</span>
<span class="line-added">1165 </span>
<span class="line-added">1166   __ bind(ok);</span>
<span class="line-added">1167   // Method might have been compiled since the call site was patched to</span>
<span class="line-added">1168   // interpreted if that is the case treat it as a miss so we can get</span>
<span class="line-added">1169   // the call site corrected.</span>
<span class="line-added">1170   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);</span>
<span class="line-added">1171   __ jcc(Assembler::equal, skip_fixup);</span>
<span class="line-added">1172   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));</span>
<span class="line-added">1173 }</span>
<span class="line-added">1174 </span>
1175 // ---------------------------------------------------------------
1176 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
<span class="line-modified">1177                                                             int comp_args_on_stack,</span>
<span class="line-modified">1178                                                             const GrowableArray&lt;SigEntry&gt;* sig,</span>
<span class="line-modified">1179                                                             const VMRegPair* regs,</span>
<span class="line-modified">1180                                                             const GrowableArray&lt;SigEntry&gt;* sig_cc,</span>
<span class="line-added">1181                                                             const VMRegPair* regs_cc,</span>
<span class="line-added">1182                                                             const GrowableArray&lt;SigEntry&gt;* sig_cc_ro,</span>
<span class="line-added">1183                                                             const VMRegPair* regs_cc_ro,</span>
<span class="line-added">1184                                                             AdapterFingerPrint* fingerprint,</span>
1185                                                             AdapterBlob*&amp; new_adapter) {
1186   address i2c_entry = __ pc();
<span class="line-modified">1187   gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);</span>

1188 
1189   // -------------------------------------------------------------------------
1190   // Generate a C2I adapter.  On entry we know rbx holds the Method* during calls
1191   // to the interpreter.  The args start out packed in the compiled layout.  They
1192   // need to be unpacked into the interpreter layout.  This will almost always
1193   // require some stack space.  We grow the current (compiled) stack, then repack
1194   // the args.  We  finally end in a jump to the generic interpreter entry point.
1195   // On exit from the interpreter, the interpreter will restore our SP (lest the
1196   // compiled code, which relys solely on SP and not RBP, get sick).
1197 
1198   address c2i_unverified_entry = __ pc();
1199   Label skip_fixup;
<span class="line-modified">1200 </span>










1201   gen_inline_cache_check(masm, skip_fixup);
1202 
<span class="line-modified">1203   OopMapSet* oop_maps = new OopMapSet();</span>
<span class="line-modified">1204   int frame_complete = CodeOffsets::frame_never_safe;</span>
<span class="line-modified">1205   int frame_size_in_words = 0;</span>
<span class="line-modified">1206 </span>
<span class="line-modified">1207   // Scalarized c2i adapter with non-scalarized receiver (i.e., don&#39;t pack receiver)</span>
<span class="line-modified">1208   address c2i_inline_ro_entry = __ pc();</span>
<span class="line-modified">1209   if (regs_cc != regs_cc_ro) {</span>
<span class="line-added">1210     Label unused;</span>
<span class="line-added">1211     gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);</span>
<span class="line-added">1212     skip_fixup = unused;</span>
1213   }
1214 
<span class="line-added">1215   // Scalarized c2i adapter</span>
1216   address c2i_entry = __ pc();
1217 
1218   // Class initialization barrier for static methods
1219   address c2i_no_clinit_check_entry = NULL;
1220   if (VM_Version::supports_fast_class_init_checks()) {
1221     Label L_skip_barrier;
1222     Register method = rbx;
1223 
1224     { // Bypass the barrier for non-static methods
1225       Register flags  = rscratch1;
1226       __ movl(flags, Address(method, Method::access_flags_offset()));
1227       __ testl(flags, JVM_ACC_STATIC);
1228       __ jcc(Assembler::zero, L_skip_barrier); // non-static
1229     }
1230 
1231     Register klass = rscratch1;
1232     __ load_method_holder(klass, method);
1233     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
1234 
1235     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
1236 
1237     __ bind(L_skip_barrier);
1238     c2i_no_clinit_check_entry = __ pc();
1239   }
1240 
1241   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1242   bs-&gt;c2i_entry_barrier(masm);
1243 
<span class="line-modified">1244   gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);</span>
<span class="line-added">1245 </span>
<span class="line-added">1246   address c2i_unverified_inline_entry = c2i_unverified_entry;</span>
<span class="line-added">1247 </span>
<span class="line-added">1248   // Non-scalarized c2i adapter</span>
<span class="line-added">1249   address c2i_inline_entry = c2i_entry;</span>
<span class="line-added">1250   if (regs != regs_cc) {</span>
<span class="line-added">1251     Label inline_entry_skip_fixup;</span>
<span class="line-added">1252     c2i_unverified_inline_entry = __ pc();</span>
<span class="line-added">1253     gen_inline_cache_check(masm, inline_entry_skip_fixup);</span>
<span class="line-added">1254 </span>
<span class="line-added">1255     c2i_inline_entry = __ pc();</span>
<span class="line-added">1256     Label unused;</span>
<span class="line-added">1257     gen_c2i_adapter(masm, sig, regs, inline_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);</span>
<span class="line-added">1258   }</span>
1259 
1260   __ flush();
<span class="line-modified">1261 </span>
<span class="line-added">1262   // The c2i adapters might safepoint and trigger a GC. The caller must make sure that</span>
<span class="line-added">1263   // the GC knows about the location of oop argument locations passed to the c2i adapter.</span>
<span class="line-added">1264   bool caller_must_gc_arguments = (regs != regs_cc);</span>
<span class="line-added">1265   new_adapter = AdapterBlob::create(masm-&gt;code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);</span>
<span class="line-added">1266 </span>
<span class="line-added">1267   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_inline_entry, c2i_inline_ro_entry, c2i_unverified_entry, c2i_unverified_inline_entry, c2i_no_clinit_check_entry);</span>
1268 }
1269 
1270 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
1271                                          VMRegPair *regs,
1272                                          VMRegPair *regs2,
1273                                          int total_args_passed) {
1274   assert(regs2 == NULL, &quot;not needed on x86&quot;);
1275 // We return the amount of VMRegImpl stack slots we need to reserve for all
1276 // the arguments NOT counting out_preserve_stack_slots.
1277 
1278 // NOTE: These arrays will have to change when c1 is ported
1279 #ifdef _WIN64
1280     static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {
1281       c_rarg0, c_rarg1, c_rarg2, c_rarg3
1282     };
1283     static const XMMRegister FP_ArgReg[Argument::n_float_register_parameters_c] = {
1284       c_farg0, c_farg1, c_farg2, c_farg3
1285     };
1286 #else
1287     static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {
</pre>
<hr />
<pre>
1305       case T_BYTE:
1306       case T_SHORT:
1307       case T_INT:
1308         if (int_args &lt; Argument::n_int_register_parameters_c) {
1309           regs[i].set1(INT_ArgReg[int_args++]-&gt;as_VMReg());
1310 #ifdef _WIN64
1311           fp_args++;
1312           // Allocate slots for callee to stuff register args the stack.
1313           stk_args += 2;
1314 #endif
1315         } else {
1316           regs[i].set1(VMRegImpl::stack2reg(stk_args));
1317           stk_args += 2;
1318         }
1319         break;
1320       case T_LONG:
1321         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
1322         // fall through
1323       case T_OBJECT:
1324       case T_ARRAY:
<span class="line-added">1325       case T_INLINE_TYPE:</span>
1326       case T_ADDRESS:
1327       case T_METADATA:
1328         if (int_args &lt; Argument::n_int_register_parameters_c) {
1329           regs[i].set2(INT_ArgReg[int_args++]-&gt;as_VMReg());
1330 #ifdef _WIN64
1331           fp_args++;
1332           stk_args += 2;
1333 #endif
1334         } else {
1335           regs[i].set2(VMRegImpl::stack2reg(stk_args));
1336           stk_args += 2;
1337         }
1338         break;
1339       case T_FLOAT:
1340         if (fp_args &lt; Argument::n_float_register_parameters_c) {
1341           regs[i].set1(FP_ArgReg[fp_args++]-&gt;as_VMReg());
1342 #ifdef _WIN64
1343           int_args++;
1344           // Allocate slots for callee to stuff register args the stack.
1345           stk_args += 2;
</pre>
<hr />
<pre>
1656   // otherwise it should load them.
1657   int slot = arg_save_area;
1658   // Save down double word first
1659   for ( int i = 0; i &lt; total_in_args; i++) {
1660     if (in_regs[i].first()-&gt;is_XMMRegister() &amp;&amp; in_sig_bt[i] == T_DOUBLE) {
1661       int offset = slot * VMRegImpl::stack_slot_size;
1662       slot += VMRegImpl::slots_per_word;
1663       assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1664       if (map != NULL) {
1665         __ movdbl(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1666       } else {
1667         __ movdbl(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1668       }
1669     }
1670     if (in_regs[i].first()-&gt;is_Register() &amp;&amp;
1671         (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {
1672       int offset = slot * VMRegImpl::stack_slot_size;
1673       if (map != NULL) {
1674         __ movq(Address(rsp, offset), in_regs[i].first()-&gt;as_Register());
1675         if (in_sig_bt[i] == T_ARRAY) {
<span class="line-modified">1676           map-&gt;set_oop(VMRegImpl::stack2reg(slot));</span>
1677         }
1678       } else {
1679         __ movq(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
1680       }
1681       slot += VMRegImpl::slots_per_word;
1682     }
1683   }
1684   // Save or restore single word registers
1685   for ( int i = 0; i &lt; total_in_args; i++) {
1686     if (in_regs[i].first()-&gt;is_Register()) {
1687       int offset = slot * VMRegImpl::stack_slot_size;
1688       slot++;
1689       assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1690 
1691       // Value is in an input register pass we must flush it to the stack
1692       const Register reg = in_regs[i].first()-&gt;as_Register();
1693       switch (in_sig_bt[i]) {
1694         case T_BOOLEAN:
1695         case T_CHAR:
1696         case T_BYTE:
1697         case T_SHORT:
1698         case T_INT:
1699           if (map != NULL) {
1700             __ movl(Address(rsp, offset), reg);
1701           } else {
1702             __ movl(reg, Address(rsp, offset));
1703           }
1704           break;
1705         case T_ARRAY:
1706         case T_LONG:
1707           // handled above
1708           break;
1709         case T_OBJECT:
<span class="line-added">1710         case T_INLINE_TYPE:</span>
1711         default: ShouldNotReachHere();
1712       }
1713     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1714       if (in_sig_bt[i] == T_FLOAT) {
1715         int offset = slot * VMRegImpl::stack_slot_size;
1716         slot++;
1717         assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1718         if (map != NULL) {
1719           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1720         } else {
1721           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1722         }
1723       }
1724     } else if (in_regs[i].first()-&gt;is_stack()) {
1725       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1726         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1727         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1728       }
1729     }
1730   }
</pre>
<hr />
<pre>
2604             VMRegPair result_reg;
2605             result_reg.set_ptr(rax-&gt;as_VMReg());
2606             move_ptr(masm, result_reg, in_regs[i]);
2607             if (!in_regs[i].first()-&gt;is_stack()) {
2608               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2609               move_ptr(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));
2610               pinned_slot += VMRegImpl::slots_per_word;
2611             }
2612           }
2613           unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);
2614           c_arg++;
2615 #ifdef ASSERT
2616           if (out_regs[c_arg].first()-&gt;is_Register()) {
2617             reg_destroyed[out_regs[c_arg].first()-&gt;as_Register()-&gt;encoding()] = true;
2618           } else if (out_regs[c_arg].first()-&gt;is_XMMRegister()) {
2619             freg_destroyed[out_regs[c_arg].first()-&gt;as_XMMRegister()-&gt;encoding()] = true;
2620           }
2621 #endif
2622           break;
2623         }
<span class="line-added">2624       case T_INLINE_TYPE:</span>
2625       case T_OBJECT:
2626         assert(!is_critical_native, &quot;no oop arguments&quot;);
2627         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2628                     ((i == 0) &amp;&amp; (!is_static)),
2629                     &amp;receiver_offset);
2630         break;
2631       case T_VOID:
2632         break;
2633 
2634       case T_FLOAT:
2635         float_move(masm, in_regs[i], out_regs[c_arg]);
2636           break;
2637 
2638       case T_DOUBLE:
2639         assert( i + 1 &lt; total_in_args &amp;&amp;
2640                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2641                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
2642         double_move(masm, in_regs[i], out_regs[c_arg]);
2643         break;
2644 
</pre>
<hr />
<pre>
2740     // Get the handle (the 2nd argument)
2741     __ mov(oop_handle_reg, c_rarg1);
2742 
2743     // Get address of the box
2744 
2745     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2746 
2747     // Load the oop from the handle
2748     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2749 
2750     __ resolve(IS_NOT_NULL, obj_reg);
2751     if (UseBiasedLocking) {
2752       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, rscratch2, false, lock_done, &amp;slow_path_lock);
2753     }
2754 
2755     // Load immediate 1 into swap_reg %rax
2756     __ movl(swap_reg, 1);
2757 
2758     // Load (object-&gt;mark() | 1) into swap_reg %rax
2759     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
<span class="line-added">2760     if (EnableValhalla &amp;&amp; !UseBiasedLocking) {</span>
<span class="line-added">2761       // For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking</span>
<span class="line-added">2762       __ andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));</span>
<span class="line-added">2763     }</span>
2764 
2765     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2766     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2767 
2768     // src -&gt; dest iff dest == rax else rax &lt;- dest
2769     __ lock();
2770     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2771     __ jcc(Assembler::equal, lock_done);
2772 
2773     // Hmm should this move to the slow path code area???
2774 
2775     // Test if the oopMark is an obvious stack pointer, i.e.,
2776     //  1) (mark &amp; 3) == 0, and
2777     //  2) rsp &lt;= mark &lt; mark + os::pagesize()
2778     // These 3 tests can be done by evaluating the following
2779     // expression: ((mark - rsp) &amp; (3 - os::vm_page_size())),
2780     // assuming both stack pointer and pagesize have their
2781     // least significant 2 bits clear.
2782     // NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg
2783 
</pre>
<hr />
<pre>
2805   // Now set thread in native
2806   __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);
2807 
2808   __ call(RuntimeAddress(native_func));
2809 
2810   // Verify or restore cpu control state after JNI call
2811   __ restore_cpu_control_state_after_jni();
2812 
2813   // Unpack native results.
2814   switch (ret_type) {
2815   case T_BOOLEAN: __ c2bool(rax);            break;
2816   case T_CHAR   : __ movzwl(rax, rax);      break;
2817   case T_BYTE   : __ sign_extend_byte (rax); break;
2818   case T_SHORT  : __ sign_extend_short(rax); break;
2819   case T_INT    : /* nothing to do */        break;
2820   case T_DOUBLE :
2821   case T_FLOAT  :
2822     // Result is in xmm0 we&#39;ll save as needed
2823     break;
2824   case T_ARRAY:                 // Really a handle
<span class="line-added">2825   case T_INLINE_TYPE:           // Really a handle</span>
2826   case T_OBJECT:                // Really a handle
2827       break; // can&#39;t de-handlize until after safepoint check
2828   case T_VOID: break;
2829   case T_LONG: break;
2830   default       : ShouldNotReachHere();
2831   }
2832 
2833   // unpin pinned arguments
2834   pinned_slot = oop_handle_offset;
2835   if (pinned_args.length() &gt; 0) {
2836     // save return value that may be overwritten otherwise.
2837     save_native_result(masm, ret_type, stack_slots);
2838     for (int index = 0; index &lt; pinned_args.length(); index ++) {
2839       int i = pinned_args.at(index);
2840       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2841       if (!in_regs[i].first()-&gt;is_stack()) {
2842         int offset = pinned_slot * VMRegImpl::stack_slot_size;
2843         __ movq(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
2844         pinned_slot += VMRegImpl::slots_per_word;
2845       }
</pre>
<hr />
<pre>
4324   __ movptr(Address(r15_thread, JavaThread::exception_handler_pc_offset()), (int)NULL_WORD);
4325   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), (int)NULL_WORD);
4326 #endif
4327   // Clear the exception oop so GC no longer processes it as a root.
4328   __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()), (int)NULL_WORD);
4329 
4330   // rax: exception oop
4331   // r8:  exception handler
4332   // rdx: exception pc
4333   // Jump to handler
4334 
4335   __ jmp(r8);
4336 
4337   // Make sure all code is generated
4338   masm-&gt;flush();
4339 
4340   // Set exception blob
4341   _exception_blob =  ExceptionBlob::create(&amp;buffer, oop_maps, SimpleRuntimeFrame::framesize &gt;&gt; 1);
4342 }
4343 #endif // COMPILER2
<span class="line-added">4344 </span>
<span class="line-added">4345 BufferedInlineTypeBlob* SharedRuntime::generate_buffered_inline_type_adapter(const InlineKlass* vk) {</span>
<span class="line-added">4346   BufferBlob* buf = BufferBlob::create(&quot;inline types pack/unpack&quot;, 16 * K);</span>
<span class="line-added">4347   CodeBuffer buffer(buf);</span>
<span class="line-added">4348   short buffer_locs[20];</span>
<span class="line-added">4349   buffer.insts()-&gt;initialize_shared_locs((relocInfo*)buffer_locs,</span>
<span class="line-added">4350                                          sizeof(buffer_locs)/sizeof(relocInfo));</span>
<span class="line-added">4351 </span>
<span class="line-added">4352   MacroAssembler* masm = new MacroAssembler(&amp;buffer);</span>
<span class="line-added">4353 </span>
<span class="line-added">4354   const Array&lt;SigEntry&gt;* sig_vk = vk-&gt;extended_sig();</span>
<span class="line-added">4355   const Array&lt;VMRegPair&gt;* regs = vk-&gt;return_regs();</span>
<span class="line-added">4356 </span>
<span class="line-added">4357   int pack_fields_jobject_off = __ offset();</span>
<span class="line-added">4358   // Resolve pre-allocated buffer from JNI handle.</span>
<span class="line-added">4359   // We cannot do this in generate_call_stub() because it requires GC code to be initialized.</span>
<span class="line-added">4360   __ movptr(rax, Address(r13, 0));</span>
<span class="line-added">4361   __ resolve_jobject(rax /* value */,</span>
<span class="line-added">4362                      r15_thread /* thread */,</span>
<span class="line-added">4363                      r12 /* tmp */);</span>
<span class="line-added">4364   __ movptr(Address(r13, 0), rax);</span>
<span class="line-added">4365 </span>
<span class="line-added">4366   int pack_fields_off = __ offset();</span>
<span class="line-added">4367 </span>
<span class="line-added">4368   int j = 1;</span>
<span class="line-added">4369   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {</span>
<span class="line-added">4370     BasicType bt = sig_vk-&gt;at(i)._bt;</span>
<span class="line-added">4371     if (bt == T_INLINE_TYPE) {</span>
<span class="line-added">4372       continue;</span>
<span class="line-added">4373     }</span>
<span class="line-added">4374     if (bt == T_VOID) {</span>
<span class="line-added">4375       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||</span>
<span class="line-added">4376           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {</span>
<span class="line-added">4377         j++;</span>
<span class="line-added">4378       }</span>
<span class="line-added">4379       continue;</span>
<span class="line-added">4380     }</span>
<span class="line-added">4381     int off = sig_vk-&gt;at(i)._offset;</span>
<span class="line-added">4382     assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">4383     VMRegPair pair = regs-&gt;at(j);</span>
<span class="line-added">4384     VMReg r_1 = pair.first();</span>
<span class="line-added">4385     VMReg r_2 = pair.second();</span>
<span class="line-added">4386     Address to(rax, off);</span>
<span class="line-added">4387     if (bt == T_FLOAT) {</span>
<span class="line-added">4388       __ movflt(to, r_1-&gt;as_XMMRegister());</span>
<span class="line-added">4389     } else if (bt == T_DOUBLE) {</span>
<span class="line-added">4390       __ movdbl(to, r_1-&gt;as_XMMRegister());</span>
<span class="line-added">4391     } else {</span>
<span class="line-added">4392       Register val = r_1-&gt;as_Register();</span>
<span class="line-added">4393       assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);</span>
<span class="line-added">4394       if (is_reference_type(bt)) {</span>
<span class="line-added">4395         __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);</span>
<span class="line-added">4396       } else {</span>
<span class="line-added">4397         __ store_sized_value(to, r_1-&gt;as_Register(), type2aelembytes(bt));</span>
<span class="line-added">4398       }</span>
<span class="line-added">4399     }</span>
<span class="line-added">4400     j++;</span>
<span class="line-added">4401   }</span>
<span class="line-added">4402   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);</span>
<span class="line-added">4403 </span>
<span class="line-added">4404   __ ret(0);</span>
<span class="line-added">4405 </span>
<span class="line-added">4406   int unpack_fields_off = __ offset();</span>
<span class="line-added">4407 </span>
<span class="line-added">4408   j = 1;</span>
<span class="line-added">4409   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {</span>
<span class="line-added">4410     BasicType bt = sig_vk-&gt;at(i)._bt;</span>
<span class="line-added">4411     if (bt == T_INLINE_TYPE) {</span>
<span class="line-added">4412       continue;</span>
<span class="line-added">4413     }</span>
<span class="line-added">4414     if (bt == T_VOID) {</span>
<span class="line-added">4415       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||</span>
<span class="line-added">4416           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {</span>
<span class="line-added">4417         j++;</span>
<span class="line-added">4418       }</span>
<span class="line-added">4419       continue;</span>
<span class="line-added">4420     }</span>
<span class="line-added">4421     int off = sig_vk-&gt;at(i)._offset;</span>
<span class="line-added">4422     assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">4423     VMRegPair pair = regs-&gt;at(j);</span>
<span class="line-added">4424     VMReg r_1 = pair.first();</span>
<span class="line-added">4425     VMReg r_2 = pair.second();</span>
<span class="line-added">4426     Address from(rax, off);</span>
<span class="line-added">4427     if (bt == T_FLOAT) {</span>
<span class="line-added">4428       __ movflt(r_1-&gt;as_XMMRegister(), from);</span>
<span class="line-added">4429     } else if (bt == T_DOUBLE) {</span>
<span class="line-added">4430       __ movdbl(r_1-&gt;as_XMMRegister(), from);</span>
<span class="line-added">4431     } else if (bt == T_OBJECT || bt == T_ARRAY) {</span>
<span class="line-added">4432       assert_different_registers(rax, r_1-&gt;as_Register());</span>
<span class="line-added">4433       __ load_heap_oop(r_1-&gt;as_Register(), from);</span>
<span class="line-added">4434     } else {</span>
<span class="line-added">4435       assert(is_java_primitive(bt), &quot;unexpected basic type&quot;);</span>
<span class="line-added">4436       assert_different_registers(rax, r_1-&gt;as_Register());</span>
<span class="line-added">4437       size_t size_in_bytes = type2aelembytes(bt);</span>
<span class="line-added">4438       __ load_sized_value(r_1-&gt;as_Register(), from, size_in_bytes, bt != T_CHAR &amp;&amp; bt != T_BOOLEAN);</span>
<span class="line-added">4439     }</span>
<span class="line-added">4440     j++;</span>
<span class="line-added">4441   }</span>
<span class="line-added">4442   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);</span>
<span class="line-added">4443 </span>
<span class="line-added">4444   if (StressInlineTypeReturnedAsFields) {</span>
<span class="line-added">4445     __ load_klass(rax, rax, rscratch1);</span>
<span class="line-added">4446     __ orptr(rax, 1);</span>
<span class="line-added">4447   }</span>
<span class="line-added">4448 </span>
<span class="line-added">4449   __ ret(0);</span>
<span class="line-added">4450 </span>
<span class="line-added">4451   __ flush();</span>
<span class="line-added">4452 </span>
<span class="line-added">4453   return BufferedInlineTypeBlob::create(&amp;buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);</span>
<span class="line-added">4454 }</span>
</pre>
</td>
</tr>
</table>
<center><a href="../s390/interp_masm_s390.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stubGenerator_x86_64.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
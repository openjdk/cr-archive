<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/cpu/x86/stubGenerator_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
<a name="1" id="anc1"></a><span class="line-modified">   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.</span>
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;ci/ciUtilities.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  31 #include &quot;gc/shared/barrierSetNMethod.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;memory/universe.hpp&quot;
  34 #include &quot;nativeInst_x86.hpp&quot;
  35 #include &quot;oops/instanceOop.hpp&quot;
  36 #include &quot;oops/method.hpp&quot;
  37 #include &quot;oops/objArrayKlass.hpp&quot;
  38 #include &quot;oops/oop.inline.hpp&quot;
  39 #include &quot;prims/methodHandles.hpp&quot;
  40 #include &quot;runtime/frame.inline.hpp&quot;
  41 #include &quot;runtime/handles.inline.hpp&quot;
  42 #include &quot;runtime/sharedRuntime.hpp&quot;
  43 #include &quot;runtime/stubCodeGenerator.hpp&quot;
  44 #include &quot;runtime/stubRoutines.hpp&quot;
  45 #include &quot;runtime/thread.inline.hpp&quot;
  46 #ifdef COMPILER2
  47 #include &quot;opto/runtime.hpp&quot;
  48 #endif
  49 #if INCLUDE_ZGC
  50 #include &quot;gc/z/zThreadLocalData.hpp&quot;
  51 #endif
  52 
  53 // Declaration and definition of StubGenerator (no .hpp file).
  54 // For a more detailed description of the stub routine structure
  55 // see the comment in stubRoutines.hpp
  56 
  57 #define __ _masm-&gt;
  58 #define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)
  59 #define a__ ((Assembler*)_masm)-&gt;
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) __ block_comment(str)
  65 #endif
  66 
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 const int MXCSR_MASK = 0xFFC0;  // Mask out any pending exceptions
  69 
  70 // Stub Code definitions
  71 
  72 class StubGenerator: public StubCodeGenerator {
  73  private:
  74 
  75 #ifdef PRODUCT
  76 #define inc_counter_np(counter) ((void)0)
  77 #else
  78   void inc_counter_np_(int&amp; counter) {
  79     // This can destroy rscratch1 if counter is far from the code cache
  80     __ incrementl(ExternalAddress((address)&amp;counter));
  81   }
  82 #define inc_counter_np(counter) \
  83   BLOCK_COMMENT(&quot;inc_counter &quot; #counter); \
  84   inc_counter_np_(counter);
  85 #endif
  86 
  87   // Call stubs are used to call Java from C
  88   //
  89   // Linux Arguments:
  90   //    c_rarg0:   call wrapper address                   address
  91   //    c_rarg1:   result                                 address
  92   //    c_rarg2:   result type                            BasicType
  93   //    c_rarg3:   method                                 Method*
  94   //    c_rarg4:   (interpreter) entry point              address
  95   //    c_rarg5:   parameters                             intptr_t*
  96   //    16(rbp): parameter size (in words)              int
  97   //    24(rbp): thread                                 Thread*
  98   //
  99   //     [ return_from_Java     ] &lt;--- rsp
 100   //     [ argument word n      ]
 101   //      ...
 102   // -12 [ argument word 1      ]
 103   // -11 [ saved r15            ] &lt;--- rsp_after_call
 104   // -10 [ saved r14            ]
 105   //  -9 [ saved r13            ]
 106   //  -8 [ saved r12            ]
 107   //  -7 [ saved rbx            ]
 108   //  -6 [ call wrapper         ]
 109   //  -5 [ result               ]
 110   //  -4 [ result type          ]
 111   //  -3 [ method               ]
 112   //  -2 [ entry point          ]
 113   //  -1 [ parameters           ]
 114   //   0 [ saved rbp            ] &lt;--- rbp
 115   //   1 [ return address       ]
 116   //   2 [ parameter size       ]
 117   //   3 [ thread               ]
 118   //
 119   // Windows Arguments:
 120   //    c_rarg0:   call wrapper address                   address
 121   //    c_rarg1:   result                                 address
 122   //    c_rarg2:   result type                            BasicType
 123   //    c_rarg3:   method                                 Method*
 124   //    48(rbp): (interpreter) entry point              address
 125   //    56(rbp): parameters                             intptr_t*
 126   //    64(rbp): parameter size (in words)              int
 127   //    72(rbp): thread                                 Thread*
 128   //
 129   //     [ return_from_Java     ] &lt;--- rsp
 130   //     [ argument word n      ]
 131   //      ...
 132   // -60 [ argument word 1      ]
 133   // -59 [ saved xmm31          ] &lt;--- rsp after_call
 134   //     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)
 135   // -27 [ saved xmm15          ]
 136   //     [ saved xmm7-xmm14     ]
 137   //  -9 [ saved xmm6           ] (each xmm register takes 2 slots)
 138   //  -7 [ saved r15            ]
 139   //  -6 [ saved r14            ]
 140   //  -5 [ saved r13            ]
 141   //  -4 [ saved r12            ]
 142   //  -3 [ saved rdi            ]
 143   //  -2 [ saved rsi            ]
 144   //  -1 [ saved rbx            ]
 145   //   0 [ saved rbp            ] &lt;--- rbp
 146   //   1 [ return address       ]
 147   //   2 [ call wrapper         ]
 148   //   3 [ result               ]
 149   //   4 [ result type          ]
 150   //   5 [ method               ]
 151   //   6 [ entry point          ]
 152   //   7 [ parameters           ]
 153   //   8 [ parameter size       ]
 154   //   9 [ thread               ]
 155   //
 156   //    Windows reserves the callers stack space for arguments 1-4.
 157   //    We spill c_rarg0-c_rarg3 to this space.
 158 
 159   // Call stub stack layout word offsets from rbp
 160   enum call_stub_layout {
 161 #ifdef _WIN64
 162     xmm_save_first     = 6,  // save from xmm6
 163     xmm_save_last      = 31, // to xmm31
 164     xmm_save_base      = -9,
 165     rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), // -27
 166     r15_off            = -7,
 167     r14_off            = -6,
 168     r13_off            = -5,
 169     r12_off            = -4,
 170     rdi_off            = -3,
 171     rsi_off            = -2,
 172     rbx_off            = -1,
 173     rbp_off            =  0,
 174     retaddr_off        =  1,
 175     call_wrapper_off   =  2,
 176     result_off         =  3,
 177     result_type_off    =  4,
 178     method_off         =  5,
 179     entry_point_off    =  6,
 180     parameters_off     =  7,
 181     parameter_size_off =  8,
 182     thread_off         =  9
 183 #else
 184     rsp_after_call_off = -12,
 185     mxcsr_off          = rsp_after_call_off,
 186     r15_off            = -11,
 187     r14_off            = -10,
 188     r13_off            = -9,
 189     r12_off            = -8,
 190     rbx_off            = -7,
 191     call_wrapper_off   = -6,
 192     result_off         = -5,
 193     result_type_off    = -4,
 194     method_off         = -3,
 195     entry_point_off    = -2,
 196     parameters_off     = -1,
 197     rbp_off            =  0,
 198     retaddr_off        =  1,
 199     parameter_size_off =  2,
 200     thread_off         =  3
 201 #endif
 202   };
 203 
 204 #ifdef _WIN64
 205   Address xmm_save(int reg) {
 206     assert(reg &gt;= xmm_save_first &amp;&amp; reg &lt;= xmm_save_last, &quot;XMM register number out of range&quot;);
 207     return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);
 208   }
 209 #endif
 210 
 211   address generate_call_stub(address&amp; return_address) {
 212     assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &amp;&amp;
 213            (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,
 214            &quot;adjust this code&quot;);
 215     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;call_stub&quot;);
 216     address start = __ pc();
 217 
 218     // same as in generate_catch_exception()!
 219     const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);
 220 
 221     const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);
 222     const Address result        (rbp, result_off         * wordSize);
 223     const Address result_type   (rbp, result_type_off    * wordSize);
 224     const Address method        (rbp, method_off         * wordSize);
 225     const Address entry_point   (rbp, entry_point_off    * wordSize);
 226     const Address parameters    (rbp, parameters_off     * wordSize);
 227     const Address parameter_size(rbp, parameter_size_off * wordSize);
 228 
 229     // same as in generate_catch_exception()!
 230     const Address thread        (rbp, thread_off         * wordSize);
 231 
 232     const Address r15_save(rbp, r15_off * wordSize);
 233     const Address r14_save(rbp, r14_off * wordSize);
 234     const Address r13_save(rbp, r13_off * wordSize);
 235     const Address r12_save(rbp, r12_off * wordSize);
 236     const Address rbx_save(rbp, rbx_off * wordSize);
 237 
 238     // stub code
 239     __ enter();
 240     __ subptr(rsp, -rsp_after_call_off * wordSize);
 241 
 242     // save register parameters
 243 #ifndef _WIN64
 244     __ movptr(parameters,   c_rarg5); // parameters
 245     __ movptr(entry_point,  c_rarg4); // entry_point
 246 #endif
 247 
 248     __ movptr(method,       c_rarg3); // method
 249     __ movl(result_type,  c_rarg2);   // result type
 250     __ movptr(result,       c_rarg1); // result
 251     __ movptr(call_wrapper, c_rarg0); // call wrapper
 252 
 253     // save regs belonging to calling function
 254     __ movptr(rbx_save, rbx);
 255     __ movptr(r12_save, r12);
 256     __ movptr(r13_save, r13);
 257     __ movptr(r14_save, r14);
 258     __ movptr(r15_save, r15);
 259 
 260 #ifdef _WIN64
 261     int last_reg = 15;
 262     if (UseAVX &gt; 2) {
 263       last_reg = 31;
 264     }
 265     if (VM_Version::supports_evex()) {
 266       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 267         __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);
 268       }
 269     } else {
 270       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 271         __ movdqu(xmm_save(i), as_XMMRegister(i));
 272       }
 273     }
 274 
 275     const Address rdi_save(rbp, rdi_off * wordSize);
 276     const Address rsi_save(rbp, rsi_off * wordSize);
 277 
 278     __ movptr(rsi_save, rsi);
 279     __ movptr(rdi_save, rdi);
 280 #else
 281     const Address mxcsr_save(rbp, mxcsr_off * wordSize);
 282     {
 283       Label skip_ldmx;
 284       __ stmxcsr(mxcsr_save);
 285       __ movl(rax, mxcsr_save);
 286       __ andl(rax, MXCSR_MASK);    // Only check control and mask bits
 287       ExternalAddress mxcsr_std(StubRoutines::addr_mxcsr_std());
 288       __ cmp32(rax, mxcsr_std);
 289       __ jcc(Assembler::equal, skip_ldmx);
 290       __ ldmxcsr(mxcsr_std);
 291       __ bind(skip_ldmx);
 292     }
 293 #endif
 294 
 295     // Load up thread register
 296     __ movptr(r15_thread, thread);
 297     __ reinit_heapbase();
 298 
 299 #ifdef ASSERT
 300     // make sure we have no pending exceptions
 301     {
 302       Label L;
 303       __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
 304       __ jcc(Assembler::equal, L);
 305       __ stop(&quot;StubRoutines::call_stub: entered with pending exception&quot;);
 306       __ bind(L);
 307     }
 308 #endif
 309 
 310     // pass parameters if any
 311     BLOCK_COMMENT(&quot;pass parameters if any&quot;);
 312     Label parameters_done;
 313     __ movl(c_rarg3, parameter_size);
 314     __ testl(c_rarg3, c_rarg3);
 315     __ jcc(Assembler::zero, parameters_done);
 316 
 317     Label loop;
 318     __ movptr(c_rarg2, parameters);       // parameter pointer
 319     __ movl(c_rarg1, c_rarg3);            // parameter counter is in c_rarg1
 320     __ BIND(loop);
 321     __ movptr(rax, Address(c_rarg2, 0));// get parameter
 322     __ addptr(c_rarg2, wordSize);       // advance to next parameter
 323     __ decrementl(c_rarg1);             // decrement counter
 324     __ push(rax);                       // pass parameter
 325     __ jcc(Assembler::notZero, loop);
 326 
 327     // call Java function
 328     __ BIND(parameters_done);
 329     __ movptr(rbx, method);             // get Method*
 330     __ movptr(c_rarg1, entry_point);    // get entry_point
 331     __ mov(r13, rsp);                   // set sender sp
 332     BLOCK_COMMENT(&quot;call Java function&quot;);
 333     __ call(c_rarg1);
 334 
 335     BLOCK_COMMENT(&quot;call_stub_return_address:&quot;);
 336     return_address = __ pc();
 337 
 338     // store result depending on type (everything that is not
<a name="2" id="anc2"></a><span class="line-modified"> 339     // T_OBJECT, T_INLINE_TYPE, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)</span>
<span class="line-modified"> 340     __ movptr(r13, result);</span>
<span class="line-modified"> 341     Label is_long, is_float, is_double, is_value, exit;</span>
<span class="line-modified"> 342     __ movl(rbx, result_type);</span>
<span class="line-modified"> 343     __ cmpl(rbx, T_OBJECT);</span>
 344     __ jcc(Assembler::equal, is_long);
<a name="3" id="anc3"></a><span class="line-modified"> 345     __ cmpl(rbx, T_INLINE_TYPE);</span>
<span class="line-added"> 346     __ jcc(Assembler::equal, is_value);</span>
<span class="line-added"> 347     __ cmpl(rbx, T_LONG);</span>
 348     __ jcc(Assembler::equal, is_long);
<a name="4" id="anc4"></a><span class="line-modified"> 349     __ cmpl(rbx, T_FLOAT);</span>
 350     __ jcc(Assembler::equal, is_float);
<a name="5" id="anc5"></a><span class="line-modified"> 351     __ cmpl(rbx, T_DOUBLE);</span>
 352     __ jcc(Assembler::equal, is_double);
 353 
 354     // handle T_INT case
<a name="6" id="anc6"></a><span class="line-modified"> 355     __ movl(Address(r13, 0), rax);</span>
 356 
 357     __ BIND(exit);
 358 
 359     // pop parameters
 360     __ lea(rsp, rsp_after_call);
 361 
 362 #ifdef ASSERT
 363     // verify that threads correspond
 364     {
 365      Label L1, L2, L3;
 366       __ cmpptr(r15_thread, thread);
 367       __ jcc(Assembler::equal, L1);
 368       __ stop(&quot;StubRoutines::call_stub: r15_thread is corrupted&quot;);
 369       __ bind(L1);
 370       __ get_thread(rbx);
 371       __ cmpptr(r15_thread, thread);
 372       __ jcc(Assembler::equal, L2);
 373       __ stop(&quot;StubRoutines::call_stub: r15_thread is modified by call&quot;);
 374       __ bind(L2);
 375       __ cmpptr(r15_thread, rbx);
 376       __ jcc(Assembler::equal, L3);
 377       __ stop(&quot;StubRoutines::call_stub: threads must correspond&quot;);
 378       __ bind(L3);
 379     }
 380 #endif
 381 
 382     // restore regs belonging to calling function
 383 #ifdef _WIN64
 384     // emit the restores for xmm regs
 385     if (VM_Version::supports_evex()) {
 386       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 387         __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);
 388       }
 389     } else {
 390       for (int i = xmm_save_first; i &lt;= last_reg; i++) {
 391         __ movdqu(as_XMMRegister(i), xmm_save(i));
 392       }
 393     }
 394 #endif
 395     __ movptr(r15, r15_save);
 396     __ movptr(r14, r14_save);
 397     __ movptr(r13, r13_save);
 398     __ movptr(r12, r12_save);
 399     __ movptr(rbx, rbx_save);
 400 
 401 #ifdef _WIN64
 402     __ movptr(rdi, rdi_save);
 403     __ movptr(rsi, rsi_save);
 404 #else
 405     __ ldmxcsr(mxcsr_save);
 406 #endif
 407 
 408     // restore rsp
 409     __ addptr(rsp, -rsp_after_call_off * wordSize);
 410 
 411     // return
 412     __ vzeroupper();
 413     __ pop(rbp);
 414     __ ret(0);
 415 
 416     // handle return types different from T_INT
<a name="7" id="anc7"></a><span class="line-added"> 417     __ BIND(is_value);</span>
<span class="line-added"> 418     if (InlineTypeReturnedAsFields) {</span>
<span class="line-added"> 419       // Check for flattened return value</span>
<span class="line-added"> 420       __ testptr(rax, 1);</span>
<span class="line-added"> 421       __ jcc(Assembler::zero, is_long);</span>
<span class="line-added"> 422       // Load pack handler address</span>
<span class="line-added"> 423       __ andptr(rax, -2);</span>
<span class="line-added"> 424       __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));</span>
<span class="line-added"> 425       __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));</span>
<span class="line-added"> 426       // Call pack handler to initialize the buffer</span>
<span class="line-added"> 427       __ call(rbx);</span>
<span class="line-added"> 428       __ jmp(exit);</span>
<span class="line-added"> 429     }</span>
 430     __ BIND(is_long);
<a name="8" id="anc8"></a><span class="line-modified"> 431     __ movq(Address(r13, 0), rax);</span>
 432     __ jmp(exit);
 433 
 434     __ BIND(is_float);
<a name="9" id="anc9"></a><span class="line-modified"> 435     __ movflt(Address(r13, 0), xmm0);</span>
 436     __ jmp(exit);
 437 
 438     __ BIND(is_double);
<a name="10" id="anc10"></a><span class="line-modified"> 439     __ movdbl(Address(r13, 0), xmm0);</span>
 440     __ jmp(exit);
 441 
 442     return start;
 443   }
 444 
 445   // Return point for a Java call if there&#39;s an exception thrown in
 446   // Java code.  The exception is caught and transformed into a
 447   // pending exception stored in JavaThread that can be tested from
 448   // within the VM.
 449   //
 450   // Note: Usually the parameters are removed by the callee. In case
 451   // of an exception crossing an activation frame boundary, that is
 452   // not the case if the callee is compiled code =&gt; need to setup the
 453   // rsp.
 454   //
 455   // rax: exception oop
 456 
 457   address generate_catch_exception() {
 458     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;catch_exception&quot;);
 459     address start = __ pc();
 460 
 461     // same as in generate_call_stub():
 462     const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);
 463     const Address thread        (rbp, thread_off         * wordSize);
 464 
 465 #ifdef ASSERT
 466     // verify that threads correspond
 467     {
 468       Label L1, L2, L3;
 469       __ cmpptr(r15_thread, thread);
 470       __ jcc(Assembler::equal, L1);
 471       __ stop(&quot;StubRoutines::catch_exception: r15_thread is corrupted&quot;);
 472       __ bind(L1);
 473       __ get_thread(rbx);
 474       __ cmpptr(r15_thread, thread);
 475       __ jcc(Assembler::equal, L2);
 476       __ stop(&quot;StubRoutines::catch_exception: r15_thread is modified by call&quot;);
 477       __ bind(L2);
 478       __ cmpptr(r15_thread, rbx);
 479       __ jcc(Assembler::equal, L3);
 480       __ stop(&quot;StubRoutines::catch_exception: threads must correspond&quot;);
 481       __ bind(L3);
 482     }
 483 #endif
 484 
 485     // set pending exception
 486     __ verify_oop(rax);
 487 
 488     __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);
 489     __ lea(rscratch1, ExternalAddress((address)__FILE__));
 490     __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);
 491     __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);
 492 
 493     // complete return to VM
 494     assert(StubRoutines::_call_stub_return_address != NULL,
 495            &quot;_call_stub_return_address must have been generated before&quot;);
 496     __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));
 497 
 498     return start;
 499   }
 500 
 501   // Continuation point for runtime calls returning with a pending
 502   // exception.  The pending exception check happened in the runtime
 503   // or native call stub.  The pending exception in Thread is
 504   // converted into a Java-level exception.
 505   //
 506   // Contract with Java-level exception handlers:
 507   // rax: exception
 508   // rdx: throwing pc
 509   //
 510   // NOTE: At entry of this stub, exception-pc must be on stack !!
 511 
 512   address generate_forward_exception() {
 513     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;forward exception&quot;);
 514     address start = __ pc();
 515 
 516     // Upon entry, the sp points to the return address returning into
 517     // Java (interpreted or compiled) code; i.e., the return address
 518     // becomes the throwing pc.
 519     //
 520     // Arguments pushed before the runtime call are still on the stack
 521     // but the exception handler will reset the stack pointer -&gt;
 522     // ignore them.  A potential result in registers can be ignored as
 523     // well.
 524 
 525 #ifdef ASSERT
 526     // make sure this code is only executed if there is a pending exception
 527     {
 528       Label L;
 529       __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t) NULL);
 530       __ jcc(Assembler::notEqual, L);
 531       __ stop(&quot;StubRoutines::forward exception: no pending exception (1)&quot;);
 532       __ bind(L);
 533     }
 534 #endif
 535 
 536     // compute exception handler into rbx
 537     __ movptr(c_rarg0, Address(rsp, 0));
 538     BLOCK_COMMENT(&quot;call exception_handler_for_return_address&quot;);
 539     __ call_VM_leaf(CAST_FROM_FN_PTR(address,
 540                          SharedRuntime::exception_handler_for_return_address),
 541                     r15_thread, c_rarg0);
 542     __ mov(rbx, rax);
 543 
 544     // setup rax &amp; rdx, remove return address &amp; clear pending exception
 545     __ pop(rdx);
 546     __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
 547     __ movptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
 548 
 549 #ifdef ASSERT
 550     // make sure exception is set
 551     {
 552       Label L;
 553       __ testptr(rax, rax);
 554       __ jcc(Assembler::notEqual, L);
 555       __ stop(&quot;StubRoutines::forward exception: no pending exception (2)&quot;);
 556       __ bind(L);
 557     }
 558 #endif
 559 
 560     // continue at exception handler (return address removed)
 561     // rax: exception
 562     // rbx: exception handler
 563     // rdx: throwing pc
 564     __ verify_oop(rax);
 565     __ jmp(rbx);
 566 
 567     return start;
 568   }
 569 
 570   // Implementation of jint atomic_xchg(jint add_value, volatile jint* dest)
 571   // used by Atomic::xchg(volatile jint* dest, jint exchange_value)
 572   //
 573   // Arguments :
 574   //    c_rarg0: exchange_value
 575   //    c_rarg0: dest
 576   //
 577   // Result:
 578   //    *dest &lt;- ex, return (orig *dest)
 579   address generate_atomic_xchg() {
 580     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_xchg&quot;);
 581     address start = __ pc();
 582 
 583     __ movl(rax, c_rarg0); // Copy to eax we need a return value anyhow
 584     __ xchgl(rax, Address(c_rarg1, 0)); // automatic LOCK
 585     __ ret(0);
 586 
 587     return start;
 588   }
 589 
 590   // Implementation of intptr_t atomic_xchg(jlong add_value, volatile jlong* dest)
 591   // used by Atomic::xchg(volatile jlong* dest, jlong exchange_value)
 592   //
 593   // Arguments :
 594   //    c_rarg0: exchange_value
 595   //    c_rarg1: dest
 596   //
 597   // Result:
 598   //    *dest &lt;- ex, return (orig *dest)
 599   address generate_atomic_xchg_long() {
 600     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_xchg_long&quot;);
 601     address start = __ pc();
 602 
 603     __ movptr(rax, c_rarg0); // Copy to eax we need a return value anyhow
 604     __ xchgptr(rax, Address(c_rarg1, 0)); // automatic LOCK
 605     __ ret(0);
 606 
 607     return start;
 608   }
 609 
 610   // Support for jint atomic::atomic_cmpxchg(jint exchange_value, volatile jint* dest,
 611   //                                         jint compare_value)
 612   //
 613   // Arguments :
 614   //    c_rarg0: exchange_value
 615   //    c_rarg1: dest
 616   //    c_rarg2: compare_value
 617   //
 618   // Result:
 619   //    if ( compare_value == *dest ) {
 620   //       *dest = exchange_value
 621   //       return compare_value;
 622   //    else
 623   //       return *dest;
 624   address generate_atomic_cmpxchg() {
 625     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg&quot;);
 626     address start = __ pc();
 627 
 628     __ movl(rax, c_rarg2);
 629     __ lock();
 630     __ cmpxchgl(c_rarg0, Address(c_rarg1, 0));
 631     __ ret(0);
 632 
 633     return start;
 634   }
 635 
 636   // Support for int8_t atomic::atomic_cmpxchg(int8_t exchange_value, volatile int8_t* dest,
 637   //                                           int8_t compare_value)
 638   //
 639   // Arguments :
 640   //    c_rarg0: exchange_value
 641   //    c_rarg1: dest
 642   //    c_rarg2: compare_value
 643   //
 644   // Result:
 645   //    if ( compare_value == *dest ) {
 646   //       *dest = exchange_value
 647   //       return compare_value;
 648   //    else
 649   //       return *dest;
 650   address generate_atomic_cmpxchg_byte() {
 651     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg_byte&quot;);
 652     address start = __ pc();
 653 
 654     __ movsbq(rax, c_rarg2);
 655     __ lock();
 656     __ cmpxchgb(c_rarg0, Address(c_rarg1, 0));
 657     __ ret(0);
 658 
 659     return start;
 660   }
 661 
 662   // Support for int64_t atomic::atomic_cmpxchg(int64_t exchange_value,
 663   //                                            volatile int64_t* dest,
 664   //                                            int64_t compare_value)
 665   // Arguments :
 666   //    c_rarg0: exchange_value
 667   //    c_rarg1: dest
 668   //    c_rarg2: compare_value
 669   //
 670   // Result:
 671   //    if ( compare_value == *dest ) {
 672   //       *dest = exchange_value
 673   //       return compare_value;
 674   //    else
 675   //       return *dest;
 676   address generate_atomic_cmpxchg_long() {
 677     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_cmpxchg_long&quot;);
 678     address start = __ pc();
 679 
 680     __ movq(rax, c_rarg2);
 681     __ lock();
 682     __ cmpxchgq(c_rarg0, Address(c_rarg1, 0));
 683     __ ret(0);
 684 
 685     return start;
 686   }
 687 
 688   // Implementation of jint atomic_add(jint add_value, volatile jint* dest)
 689   // used by Atomic::add(volatile jint* dest, jint add_value)
 690   //
 691   // Arguments :
 692   //    c_rarg0: add_value
 693   //    c_rarg1: dest
 694   //
 695   // Result:
 696   //    *dest += add_value
 697   //    return *dest;
 698   address generate_atomic_add() {
 699     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_add&quot;);
 700     address start = __ pc();
 701 
 702     __ movl(rax, c_rarg0);
 703     __ lock();
 704     __ xaddl(Address(c_rarg1, 0), c_rarg0);
 705     __ addl(rax, c_rarg0);
 706     __ ret(0);
 707 
 708     return start;
 709   }
 710 
 711   // Implementation of intptr_t atomic_add(intptr_t add_value, volatile intptr_t* dest)
 712   // used by Atomic::add(volatile intptr_t* dest, intptr_t add_value)
 713   //
 714   // Arguments :
 715   //    c_rarg0: add_value
 716   //    c_rarg1: dest
 717   //
 718   // Result:
 719   //    *dest += add_value
 720   //    return *dest;
 721   address generate_atomic_add_long() {
 722     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;atomic_add_long&quot;);
 723     address start = __ pc();
 724 
 725     __ movptr(rax, c_rarg0); // Copy to eax we need a return value anyhow
 726     __ lock();
 727     __ xaddptr(Address(c_rarg1, 0), c_rarg0);
 728     __ addptr(rax, c_rarg0);
 729     __ ret(0);
 730 
 731     return start;
 732   }
 733 
 734   // Support for intptr_t OrderAccess::fence()
 735   //
 736   // Arguments :
 737   //
 738   // Result:
 739   address generate_orderaccess_fence() {
 740     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;orderaccess_fence&quot;);
 741     address start = __ pc();
 742     __ membar(Assembler::StoreLoad);
 743     __ ret(0);
 744 
 745     return start;
 746   }
 747 
 748   // Support for intptr_t get_previous_fp()
 749   //
 750   // This routine is used to find the previous frame pointer for the
 751   // caller (current_frame_guess). This is used as part of debugging
 752   // ps() is seemingly lost trying to find frames.
 753   // This code assumes that caller current_frame_guess) has a frame.
 754   address generate_get_previous_fp() {
 755     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;get_previous_fp&quot;);
 756     const Address old_fp(rbp, 0);
 757     const Address older_fp(rax, 0);
 758     address start = __ pc();
 759 
 760     __ enter();
 761     __ movptr(rax, old_fp); // callers fp
 762     __ movptr(rax, older_fp); // the frame for ps()
 763     __ pop(rbp);
 764     __ ret(0);
 765 
 766     return start;
 767   }
 768 
 769   // Support for intptr_t get_previous_sp()
 770   //
 771   // This routine is used to find the previous stack pointer for the
 772   // caller.
 773   address generate_get_previous_sp() {
 774     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;get_previous_sp&quot;);
 775     address start = __ pc();
 776 
 777     __ movptr(rax, rsp);
 778     __ addptr(rax, 8); // return address is at the top of the stack.
 779     __ ret(0);
 780 
 781     return start;
 782   }
 783 
 784   //----------------------------------------------------------------------------------------------------
 785   // Support for void verify_mxcsr()
 786   //
 787   // This routine is used with -Xcheck:jni to verify that native
 788   // JNI code does not return to Java code without restoring the
 789   // MXCSR register to our expected state.
 790 
 791   address generate_verify_mxcsr() {
 792     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_mxcsr&quot;);
 793     address start = __ pc();
 794 
 795     const Address mxcsr_save(rsp, 0);
 796 
 797     if (CheckJNICalls) {
 798       Label ok_ret;
 799       ExternalAddress mxcsr_std(StubRoutines::addr_mxcsr_std());
 800       __ push(rax);
 801       __ subptr(rsp, wordSize);      // allocate a temp location
 802       __ stmxcsr(mxcsr_save);
 803       __ movl(rax, mxcsr_save);
 804       __ andl(rax, MXCSR_MASK);    // Only check control and mask bits
 805       __ cmp32(rax, mxcsr_std);
 806       __ jcc(Assembler::equal, ok_ret);
 807 
 808       __ warn(&quot;MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall&quot;);
 809 
 810       __ ldmxcsr(mxcsr_std);
 811 
 812       __ bind(ok_ret);
 813       __ addptr(rsp, wordSize);
 814       __ pop(rax);
 815     }
 816 
 817     __ ret(0);
 818 
 819     return start;
 820   }
 821 
 822   address generate_f2i_fixup() {
 823     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;f2i_fixup&quot;);
 824     Address inout(rsp, 5 * wordSize); // return address + 4 saves
 825 
 826     address start = __ pc();
 827 
 828     Label L;
 829 
 830     __ push(rax);
 831     __ push(c_rarg3);
 832     __ push(c_rarg2);
 833     __ push(c_rarg1);
 834 
 835     __ movl(rax, 0x7f800000);
 836     __ xorl(c_rarg3, c_rarg3);
 837     __ movl(c_rarg2, inout);
 838     __ movl(c_rarg1, c_rarg2);
 839     __ andl(c_rarg1, 0x7fffffff);
 840     __ cmpl(rax, c_rarg1); // NaN? -&gt; 0
 841     __ jcc(Assembler::negative, L);
 842     __ testl(c_rarg2, c_rarg2); // signed ? min_jint : max_jint
 843     __ movl(c_rarg3, 0x80000000);
 844     __ movl(rax, 0x7fffffff);
 845     __ cmovl(Assembler::positive, c_rarg3, rax);
 846 
 847     __ bind(L);
 848     __ movptr(inout, c_rarg3);
 849 
 850     __ pop(c_rarg1);
 851     __ pop(c_rarg2);
 852     __ pop(c_rarg3);
 853     __ pop(rax);
 854 
 855     __ ret(0);
 856 
 857     return start;
 858   }
 859 
 860   address generate_f2l_fixup() {
 861     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;f2l_fixup&quot;);
 862     Address inout(rsp, 5 * wordSize); // return address + 4 saves
 863     address start = __ pc();
 864 
 865     Label L;
 866 
 867     __ push(rax);
 868     __ push(c_rarg3);
 869     __ push(c_rarg2);
 870     __ push(c_rarg1);
 871 
 872     __ movl(rax, 0x7f800000);
 873     __ xorl(c_rarg3, c_rarg3);
 874     __ movl(c_rarg2, inout);
 875     __ movl(c_rarg1, c_rarg2);
 876     __ andl(c_rarg1, 0x7fffffff);
 877     __ cmpl(rax, c_rarg1); // NaN? -&gt; 0
 878     __ jcc(Assembler::negative, L);
 879     __ testl(c_rarg2, c_rarg2); // signed ? min_jlong : max_jlong
 880     __ mov64(c_rarg3, 0x8000000000000000);
 881     __ mov64(rax, 0x7fffffffffffffff);
 882     __ cmov(Assembler::positive, c_rarg3, rax);
 883 
 884     __ bind(L);
 885     __ movptr(inout, c_rarg3);
 886 
 887     __ pop(c_rarg1);
 888     __ pop(c_rarg2);
 889     __ pop(c_rarg3);
 890     __ pop(rax);
 891 
 892     __ ret(0);
 893 
 894     return start;
 895   }
 896 
 897   address generate_d2i_fixup() {
 898     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;d2i_fixup&quot;);
 899     Address inout(rsp, 6 * wordSize); // return address + 5 saves
 900 
 901     address start = __ pc();
 902 
 903     Label L;
 904 
 905     __ push(rax);
 906     __ push(c_rarg3);
 907     __ push(c_rarg2);
 908     __ push(c_rarg1);
 909     __ push(c_rarg0);
 910 
 911     __ movl(rax, 0x7ff00000);
 912     __ movq(c_rarg2, inout);
 913     __ movl(c_rarg3, c_rarg2);
 914     __ mov(c_rarg1, c_rarg2);
 915     __ mov(c_rarg0, c_rarg2);
 916     __ negl(c_rarg3);
 917     __ shrptr(c_rarg1, 0x20);
 918     __ orl(c_rarg3, c_rarg2);
 919     __ andl(c_rarg1, 0x7fffffff);
 920     __ xorl(c_rarg2, c_rarg2);
 921     __ shrl(c_rarg3, 0x1f);
 922     __ orl(c_rarg1, c_rarg3);
 923     __ cmpl(rax, c_rarg1);
 924     __ jcc(Assembler::negative, L); // NaN -&gt; 0
 925     __ testptr(c_rarg0, c_rarg0); // signed ? min_jint : max_jint
 926     __ movl(c_rarg2, 0x80000000);
 927     __ movl(rax, 0x7fffffff);
 928     __ cmov(Assembler::positive, c_rarg2, rax);
 929 
 930     __ bind(L);
 931     __ movptr(inout, c_rarg2);
 932 
 933     __ pop(c_rarg0);
 934     __ pop(c_rarg1);
 935     __ pop(c_rarg2);
 936     __ pop(c_rarg3);
 937     __ pop(rax);
 938 
 939     __ ret(0);
 940 
 941     return start;
 942   }
 943 
 944   address generate_d2l_fixup() {
 945     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;d2l_fixup&quot;);
 946     Address inout(rsp, 6 * wordSize); // return address + 5 saves
 947 
 948     address start = __ pc();
 949 
 950     Label L;
 951 
 952     __ push(rax);
 953     __ push(c_rarg3);
 954     __ push(c_rarg2);
 955     __ push(c_rarg1);
 956     __ push(c_rarg0);
 957 
 958     __ movl(rax, 0x7ff00000);
 959     __ movq(c_rarg2, inout);
 960     __ movl(c_rarg3, c_rarg2);
 961     __ mov(c_rarg1, c_rarg2);
 962     __ mov(c_rarg0, c_rarg2);
 963     __ negl(c_rarg3);
 964     __ shrptr(c_rarg1, 0x20);
 965     __ orl(c_rarg3, c_rarg2);
 966     __ andl(c_rarg1, 0x7fffffff);
 967     __ xorl(c_rarg2, c_rarg2);
 968     __ shrl(c_rarg3, 0x1f);
 969     __ orl(c_rarg1, c_rarg3);
 970     __ cmpl(rax, c_rarg1);
 971     __ jcc(Assembler::negative, L); // NaN -&gt; 0
 972     __ testq(c_rarg0, c_rarg0); // signed ? min_jlong : max_jlong
 973     __ mov64(c_rarg2, 0x8000000000000000);
 974     __ mov64(rax, 0x7fffffffffffffff);
 975     __ cmovq(Assembler::positive, c_rarg2, rax);
 976 
 977     __ bind(L);
 978     __ movq(inout, c_rarg2);
 979 
 980     __ pop(c_rarg0);
 981     __ pop(c_rarg1);
 982     __ pop(c_rarg2);
 983     __ pop(c_rarg3);
 984     __ pop(rax);
 985 
 986     __ ret(0);
 987 
 988     return start;
 989   }
 990 
 991   address generate_fp_mask(const char *stub_name, int64_t mask) {
 992     __ align(CodeEntryAlignment);
 993     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
 994     address start = __ pc();
 995 
 996     __ emit_data64( mask, relocInfo::none );
 997     __ emit_data64( mask, relocInfo::none );
 998 
 999     return start;
1000   }
1001 
1002   address generate_vector_mask(const char *stub_name, int64_t mask) {
1003     __ align(CodeEntryAlignment);
1004     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
1005     address start = __ pc();
1006 
1007     __ emit_data64(mask, relocInfo::none);
1008     __ emit_data64(mask, relocInfo::none);
1009     __ emit_data64(mask, relocInfo::none);
1010     __ emit_data64(mask, relocInfo::none);
1011     __ emit_data64(mask, relocInfo::none);
1012     __ emit_data64(mask, relocInfo::none);
1013     __ emit_data64(mask, relocInfo::none);
1014     __ emit_data64(mask, relocInfo::none);
1015 
1016     return start;
1017   }
1018 
1019   address generate_vector_byte_perm_mask(const char *stub_name) {
1020     __ align(CodeEntryAlignment);
1021     StubCodeMark mark(this, &quot;StubRoutines&quot;, stub_name);
1022     address start = __ pc();
1023 
1024     __ emit_data64(0x0000000000000001, relocInfo::none);
1025     __ emit_data64(0x0000000000000003, relocInfo::none);
1026     __ emit_data64(0x0000000000000005, relocInfo::none);
1027     __ emit_data64(0x0000000000000007, relocInfo::none);
1028     __ emit_data64(0x0000000000000000, relocInfo::none);
1029     __ emit_data64(0x0000000000000002, relocInfo::none);
1030     __ emit_data64(0x0000000000000004, relocInfo::none);
1031     __ emit_data64(0x0000000000000006, relocInfo::none);
1032 
1033     return start;
1034   }
1035 
1036   // Non-destructive plausibility checks for oops
1037   //
1038   // Arguments:
1039   //    all args on stack!
1040   //
1041   // Stack after saving c_rarg3:
1042   //    [tos + 0]: saved c_rarg3
1043   //    [tos + 1]: saved c_rarg2
1044   //    [tos + 2]: saved r12 (several TemplateTable methods use it)
1045   //    [tos + 3]: saved flags
1046   //    [tos + 4]: return address
1047   //  * [tos + 5]: error message (char*)
1048   //  * [tos + 6]: object to verify (oop)
1049   //  * [tos + 7]: saved rax - saved by caller and bashed
1050   //  * [tos + 8]: saved r10 (rscratch1) - saved by caller
1051   //  * = popped on exit
1052   address generate_verify_oop() {
1053     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;verify_oop&quot;);
1054     address start = __ pc();
1055 
1056     Label exit, error;
1057 
1058     __ pushf();
1059     __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()));
1060 
1061     __ push(r12);
1062 
1063     // save c_rarg2 and c_rarg3
1064     __ push(c_rarg2);
1065     __ push(c_rarg3);
1066 
1067     enum {
1068            // After previous pushes.
1069            oop_to_verify = 6 * wordSize,
1070            saved_rax     = 7 * wordSize,
1071            saved_r10     = 8 * wordSize,
1072 
1073            // Before the call to MacroAssembler::debug(), see below.
1074            return_addr   = 16 * wordSize,
1075            error_msg     = 17 * wordSize
1076     };
1077 
1078     // get object
1079     __ movptr(rax, Address(rsp, oop_to_verify));
1080 
1081     // make sure object is &#39;reasonable&#39;
1082     __ testptr(rax, rax);
1083     __ jcc(Assembler::zero, exit); // if obj is NULL it is OK
1084 
1085 #if INCLUDE_ZGC
1086     if (UseZGC) {
1087       // Check if metadata bits indicate a bad oop
1088       __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));
1089       __ jcc(Assembler::notZero, error);
1090     }
1091 #endif
1092 
1093     // Check if the oop is in the right area of memory
1094     __ movptr(c_rarg2, rax);
1095     __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());
1096     __ andptr(c_rarg2, c_rarg3);
1097     __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());
1098     __ cmpptr(c_rarg2, c_rarg3);
1099     __ jcc(Assembler::notZero, error);
1100 
1101     // make sure klass is &#39;reasonable&#39;, which is not zero.
1102     __ load_klass(rax, rax, rscratch1);  // get klass
1103     __ testptr(rax, rax);
1104     __ jcc(Assembler::zero, error); // if klass is NULL it is broken
1105 
1106     // return if everything seems ok
1107     __ bind(exit);
1108     __ movptr(rax, Address(rsp, saved_rax));     // get saved rax back
1109     __ movptr(rscratch1, Address(rsp, saved_r10)); // get saved r10 back
1110     __ pop(c_rarg3);                             // restore c_rarg3
1111     __ pop(c_rarg2);                             // restore c_rarg2
1112     __ pop(r12);                                 // restore r12
1113     __ popf();                                   // restore flags
1114     __ ret(4 * wordSize);                        // pop caller saved stuff
1115 
1116     // handle errors
1117     __ bind(error);
1118     __ movptr(rax, Address(rsp, saved_rax));     // get saved rax back
1119     __ movptr(rscratch1, Address(rsp, saved_r10)); // get saved r10 back
1120     __ pop(c_rarg3);                             // get saved c_rarg3 back
1121     __ pop(c_rarg2);                             // get saved c_rarg2 back
1122     __ pop(r12);                                 // get saved r12 back
1123     __ popf();                                   // get saved flags off stack --
1124                                                  // will be ignored
1125 
1126     __ pusha();                                  // push registers
1127                                                  // (rip is already
1128                                                  // already pushed)
1129     // debug(char* msg, int64_t pc, int64_t regs[])
1130     // We&#39;ve popped the registers we&#39;d saved (c_rarg3, c_rarg2 and flags), and
1131     // pushed all the registers, so now the stack looks like:
1132     //     [tos +  0] 16 saved registers
1133     //     [tos + 16] return address
1134     //   * [tos + 17] error message (char*)
1135     //   * [tos + 18] object to verify (oop)
1136     //   * [tos + 19] saved rax - saved by caller and bashed
1137     //   * [tos + 20] saved r10 (rscratch1) - saved by caller
1138     //   * = popped on exit
1139 
1140     __ movptr(c_rarg0, Address(rsp, error_msg));    // pass address of error message
1141     __ movptr(c_rarg1, Address(rsp, return_addr));  // pass return address
1142     __ movq(c_rarg2, rsp);                          // pass address of regs on stack
1143     __ mov(r12, rsp);                               // remember rsp
1144     __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
1145     __ andptr(rsp, -16);                            // align stack as required by ABI
1146     BLOCK_COMMENT(&quot;call MacroAssembler::debug&quot;);
1147     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));
1148     __ hlt();
1149     return start;
1150   }
1151 
1152   //
1153   // Verify that a register contains clean 32-bits positive value
1154   // (high 32-bits are 0) so it could be used in 64-bits shifts.
1155   //
1156   //  Input:
1157   //    Rint  -  32-bits value
1158   //    Rtmp  -  scratch
1159   //
1160   void assert_clean_int(Register Rint, Register Rtmp) {
1161 #ifdef ASSERT
1162     Label L;
1163     assert_different_registers(Rtmp, Rint);
1164     __ movslq(Rtmp, Rint);
1165     __ cmpq(Rtmp, Rint);
1166     __ jcc(Assembler::equal, L);
1167     __ stop(&quot;high 32-bits of int value are not 0&quot;);
1168     __ bind(L);
1169 #endif
1170   }
1171 
1172   //  Generate overlap test for array copy stubs
1173   //
1174   //  Input:
1175   //     c_rarg0 - from
1176   //     c_rarg1 - to
1177   //     c_rarg2 - element count
1178   //
1179   //  Output:
1180   //     rax   - &amp;from[element count - 1]
1181   //
1182   void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {
1183     assert(no_overlap_target != NULL, &quot;must be generated&quot;);
1184     array_overlap_test(no_overlap_target, NULL, sf);
1185   }
1186   void array_overlap_test(Label&amp; L_no_overlap, Address::ScaleFactor sf) {
1187     array_overlap_test(NULL, &amp;L_no_overlap, sf);
1188   }
1189   void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {
1190     const Register from     = c_rarg0;
1191     const Register to       = c_rarg1;
1192     const Register count    = c_rarg2;
1193     const Register end_from = rax;
1194 
1195     __ cmpptr(to, from);
1196     __ lea(end_from, Address(from, count, sf, 0));
1197     if (NOLp == NULL) {
1198       ExternalAddress no_overlap(no_overlap_target);
1199       __ jump_cc(Assembler::belowEqual, no_overlap);
1200       __ cmpptr(to, end_from);
1201       __ jump_cc(Assembler::aboveEqual, no_overlap);
1202     } else {
1203       __ jcc(Assembler::belowEqual, (*NOLp));
1204       __ cmpptr(to, end_from);
1205       __ jcc(Assembler::aboveEqual, (*NOLp));
1206     }
1207   }
1208 
1209   // Shuffle first three arg regs on Windows into Linux/Solaris locations.
1210   //
1211   // Outputs:
1212   //    rdi - rcx
1213   //    rsi - rdx
1214   //    rdx - r8
1215   //    rcx - r9
1216   //
1217   // Registers r9 and r10 are used to save rdi and rsi on Windows, which latter
1218   // are non-volatile.  r9 and r10 should not be used by the caller.
1219   //
1220   DEBUG_ONLY(bool regs_in_thread;)
1221 
1222   void setup_arg_regs(int nargs = 3) {
1223     const Register saved_rdi = r9;
1224     const Register saved_rsi = r10;
1225     assert(nargs == 3 || nargs == 4, &quot;else fix&quot;);
1226 #ifdef _WIN64
1227     assert(c_rarg0 == rcx &amp;&amp; c_rarg1 == rdx &amp;&amp; c_rarg2 == r8 &amp;&amp; c_rarg3 == r9,
1228            &quot;unexpected argument registers&quot;);
1229     if (nargs &gt;= 4)
1230       __ mov(rax, r9);  // r9 is also saved_rdi
1231     __ movptr(saved_rdi, rdi);
1232     __ movptr(saved_rsi, rsi);
1233     __ mov(rdi, rcx); // c_rarg0
1234     __ mov(rsi, rdx); // c_rarg1
1235     __ mov(rdx, r8);  // c_rarg2
1236     if (nargs &gt;= 4)
1237       __ mov(rcx, rax); // c_rarg3 (via rax)
1238 #else
1239     assert(c_rarg0 == rdi &amp;&amp; c_rarg1 == rsi &amp;&amp; c_rarg2 == rdx &amp;&amp; c_rarg3 == rcx,
1240            &quot;unexpected argument registers&quot;);
1241 #endif
1242     DEBUG_ONLY(regs_in_thread = false;)
1243   }
1244 
1245   void restore_arg_regs() {
1246     assert(!regs_in_thread, &quot;wrong call to restore_arg_regs&quot;);
1247     const Register saved_rdi = r9;
1248     const Register saved_rsi = r10;
1249 #ifdef _WIN64
1250     __ movptr(rdi, saved_rdi);
1251     __ movptr(rsi, saved_rsi);
1252 #endif
1253   }
1254 
1255   // This is used in places where r10 is a scratch register, and can
1256   // be adapted if r9 is needed also.
1257   void setup_arg_regs_using_thread() {
1258     const Register saved_r15 = r9;
1259 #ifdef _WIN64
1260     __ mov(saved_r15, r15);  // r15 is callee saved and needs to be restored
1261     __ get_thread(r15_thread);
1262     assert(c_rarg0 == rcx &amp;&amp; c_rarg1 == rdx &amp;&amp; c_rarg2 == r8 &amp;&amp; c_rarg3 == r9,
1263            &quot;unexpected argument registers&quot;);
1264     __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);
1265     __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);
1266 
1267     __ mov(rdi, rcx); // c_rarg0
1268     __ mov(rsi, rdx); // c_rarg1
1269     __ mov(rdx, r8);  // c_rarg2
1270 #else
1271     assert(c_rarg0 == rdi &amp;&amp; c_rarg1 == rsi &amp;&amp; c_rarg2 == rdx &amp;&amp; c_rarg3 == rcx,
1272            &quot;unexpected argument registers&quot;);
1273 #endif
1274     DEBUG_ONLY(regs_in_thread = true;)
1275   }
1276 
1277   void restore_arg_regs_using_thread() {
1278     assert(regs_in_thread, &quot;wrong call to restore_arg_regs&quot;);
1279     const Register saved_r15 = r9;
1280 #ifdef _WIN64
1281     __ get_thread(r15_thread);
1282     __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));
1283     __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));
1284     __ mov(r15, saved_r15);  // r15 is callee saved and needs to be restored
1285 #endif
1286   }
1287 
1288   // Copy big chunks forward
1289   //
1290   // Inputs:
1291   //   end_from     - source arrays end address
1292   //   end_to       - destination array end address
1293   //   qword_count  - 64-bits element count, negative
1294   //   to           - scratch
1295   //   L_copy_bytes - entry label
1296   //   L_copy_8_bytes  - exit  label
1297   //
1298   void copy_bytes_forward(Register end_from, Register end_to,
1299                              Register qword_count, Register to,
1300                              Label&amp; L_copy_bytes, Label&amp; L_copy_8_bytes) {
1301     DEBUG_ONLY(__ stop(&quot;enter at entry label, not here&quot;));
1302     Label L_loop;
1303     __ align(OptoLoopAlignment);
1304     if (UseUnalignedLoadStores) {
1305       Label L_end;
1306       // Copy 64-bytes per iteration
1307       if (UseAVX &gt; 2) {
1308         Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;
1309 
1310         __ BIND(L_copy_bytes);
1311         __ cmpptr(qword_count, (-1 * AVX3Threshold / 8));
1312         __ jccb(Assembler::less, L_above_threshold);
1313         __ jmpb(L_below_threshold);
1314 
1315         __ bind(L_loop_avx512);
1316         __ evmovdqul(xmm0, Address(end_from, qword_count, Address::times_8, -56), Assembler::AVX_512bit);
1317         __ evmovdqul(Address(end_to, qword_count, Address::times_8, -56), xmm0, Assembler::AVX_512bit);
1318         __ bind(L_above_threshold);
1319         __ addptr(qword_count, 8);
1320         __ jcc(Assembler::lessEqual, L_loop_avx512);
1321         __ jmpb(L_32_byte_head);
1322 
1323         __ bind(L_loop_avx2);
1324         __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));
1325         __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);
1326         __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));
1327         __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);
1328         __ bind(L_below_threshold);
1329         __ addptr(qword_count, 8);
1330         __ jcc(Assembler::lessEqual, L_loop_avx2);
1331 
1332         __ bind(L_32_byte_head);
1333         __ subptr(qword_count, 4);  // sub(8) and add(4)
1334         __ jccb(Assembler::greater, L_end);
1335       } else {
1336         __ BIND(L_loop);
1337         if (UseAVX == 2) {
1338           __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));
1339           __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);
1340           __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));
1341           __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);
1342         } else {
1343           __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));
1344           __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);
1345           __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));
1346           __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);
1347           __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));
1348           __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);
1349           __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));
1350           __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);
1351         }
1352 
1353         __ BIND(L_copy_bytes);
1354         __ addptr(qword_count, 8);
1355         __ jcc(Assembler::lessEqual, L_loop);
1356         __ subptr(qword_count, 4);  // sub(8) and add(4)
1357         __ jccb(Assembler::greater, L_end);
1358       }
1359       // Copy trailing 32 bytes
1360       if (UseAVX &gt;= 2) {
1361         __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));
1362         __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);
1363       } else {
1364         __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));
1365         __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);
1366         __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));
1367         __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);
1368       }
1369       __ addptr(qword_count, 4);
1370       __ BIND(L_end);
1371       if (UseAVX &gt;= 2) {
1372         // clean upper bits of YMM registers
1373         __ vpxor(xmm0, xmm0);
1374         __ vpxor(xmm1, xmm1);
1375       }
1376     } else {
1377       // Copy 32-bytes per iteration
1378       __ BIND(L_loop);
1379       __ movq(to, Address(end_from, qword_count, Address::times_8, -24));
1380       __ movq(Address(end_to, qword_count, Address::times_8, -24), to);
1381       __ movq(to, Address(end_from, qword_count, Address::times_8, -16));
1382       __ movq(Address(end_to, qword_count, Address::times_8, -16), to);
1383       __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));
1384       __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);
1385       __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));
1386       __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);
1387 
1388       __ BIND(L_copy_bytes);
1389       __ addptr(qword_count, 4);
1390       __ jcc(Assembler::lessEqual, L_loop);
1391     }
1392     __ subptr(qword_count, 4);
1393     __ jcc(Assembler::less, L_copy_8_bytes); // Copy trailing qwords
1394   }
1395 
1396   // Copy big chunks backward
1397   //
1398   // Inputs:
1399   //   from         - source arrays address
1400   //   dest         - destination array address
1401   //   qword_count  - 64-bits element count
1402   //   to           - scratch
1403   //   L_copy_bytes - entry label
1404   //   L_copy_8_bytes  - exit  label
1405   //
1406   void copy_bytes_backward(Register from, Register dest,
1407                               Register qword_count, Register to,
1408                               Label&amp; L_copy_bytes, Label&amp; L_copy_8_bytes) {
1409     DEBUG_ONLY(__ stop(&quot;enter at entry label, not here&quot;));
1410     Label L_loop;
1411     __ align(OptoLoopAlignment);
1412     if (UseUnalignedLoadStores) {
1413       Label L_end;
1414       // Copy 64-bytes per iteration
1415       if (UseAVX &gt; 2) {
1416         Label L_loop_avx512, L_loop_avx2, L_32_byte_head, L_above_threshold, L_below_threshold;
1417 
1418         __ BIND(L_copy_bytes);
1419         __ cmpptr(qword_count, (AVX3Threshold / 8));
1420         __ jccb(Assembler::greater, L_above_threshold);
1421         __ jmpb(L_below_threshold);
1422 
1423         __ BIND(L_loop_avx512);
1424         __ evmovdqul(xmm0, Address(from, qword_count, Address::times_8, 0), Assembler::AVX_512bit);
1425         __ evmovdqul(Address(dest, qword_count, Address::times_8, 0), xmm0, Assembler::AVX_512bit);
1426         __ bind(L_above_threshold);
1427         __ subptr(qword_count, 8);
1428         __ jcc(Assembler::greaterEqual, L_loop_avx512);
1429         __ jmpb(L_32_byte_head);
1430 
1431         __ bind(L_loop_avx2);
1432         __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));
1433         __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);
1434         __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8, 0));
1435         __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm1);
1436         __ bind(L_below_threshold);
1437         __ subptr(qword_count, 8);
1438         __ jcc(Assembler::greaterEqual, L_loop_avx2);
1439 
1440         __ bind(L_32_byte_head);
1441         __ addptr(qword_count, 4);  // add(8) and sub(4)
1442         __ jccb(Assembler::less, L_end);
1443       } else {
1444         __ BIND(L_loop);
1445         if (UseAVX == 2) {
1446           __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));
1447           __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);
1448           __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));
1449           __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);
1450         } else {
1451           __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));
1452           __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);
1453           __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));
1454           __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);
1455           __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));
1456           __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);
1457           __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));
1458           __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);
1459         }
1460 
1461         __ BIND(L_copy_bytes);
1462         __ subptr(qword_count, 8);
1463         __ jcc(Assembler::greaterEqual, L_loop);
1464 
1465         __ addptr(qword_count, 4);  // add(8) and sub(4)
1466         __ jccb(Assembler::less, L_end);
1467       }
1468       // Copy trailing 32 bytes
1469       if (UseAVX &gt;= 2) {
1470         __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));
1471         __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);
1472       } else {
1473         __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));
1474         __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);
1475         __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));
1476         __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);
1477       }
1478       __ subptr(qword_count, 4);
1479       __ BIND(L_end);
1480       if (UseAVX &gt;= 2) {
1481         // clean upper bits of YMM registers
1482         __ vpxor(xmm0, xmm0);
1483         __ vpxor(xmm1, xmm1);
1484       }
1485     } else {
1486       // Copy 32-bytes per iteration
1487       __ BIND(L_loop);
1488       __ movq(to, Address(from, qword_count, Address::times_8, 24));
1489       __ movq(Address(dest, qword_count, Address::times_8, 24), to);
1490       __ movq(to, Address(from, qword_count, Address::times_8, 16));
1491       __ movq(Address(dest, qword_count, Address::times_8, 16), to);
1492       __ movq(to, Address(from, qword_count, Address::times_8,  8));
1493       __ movq(Address(dest, qword_count, Address::times_8,  8), to);
1494       __ movq(to, Address(from, qword_count, Address::times_8,  0));
1495       __ movq(Address(dest, qword_count, Address::times_8,  0), to);
1496 
1497       __ BIND(L_copy_bytes);
1498       __ subptr(qword_count, 4);
1499       __ jcc(Assembler::greaterEqual, L_loop);
1500     }
1501     __ addptr(qword_count, 4);
1502     __ jcc(Assembler::greater, L_copy_8_bytes); // Copy trailing qwords
1503   }
1504 
1505   // Arguments:
1506   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1507   //             ignored
1508   //   name    - stub name string
1509   //
1510   // Inputs:
1511   //   c_rarg0   - source array address
1512   //   c_rarg1   - destination array address
1513   //   c_rarg2   - element count, treated as ssize_t, can be zero
1514   //
1515   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1516   // we let the hardware handle it.  The one to eight bytes within words,
1517   // dwords or qwords that span cache line boundaries will still be loaded
1518   // and stored atomically.
1519   //
1520   // Side Effects:
1521   //   disjoint_byte_copy_entry is set to the no-overlap entry point
1522   //   used by generate_conjoint_byte_copy().
1523   //
1524   address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {
1525     __ align(CodeEntryAlignment);
1526     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1527     address start = __ pc();
1528 
1529     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;
1530     Label L_copy_byte, L_exit;
1531     const Register from        = rdi;  // source array address
1532     const Register to          = rsi;  // destination array address
1533     const Register count       = rdx;  // elements count
1534     const Register byte_count  = rcx;
1535     const Register qword_count = count;
1536     const Register end_from    = from; // source array end address
1537     const Register end_to      = to;   // destination array end address
1538     // End pointers are inclusive, and if count is not zero they point
1539     // to the last unit copied:  end_to[0] := end_from[0]
1540 
1541     __ enter(); // required for proper stackwalking of RuntimeStub frame
1542     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1543 
1544     if (entry != NULL) {
1545       *entry = __ pc();
1546        // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1547       BLOCK_COMMENT(&quot;Entry:&quot;);
1548     }
1549 
1550     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1551                       // r9 and r10 may be used to save non-volatile registers
1552 
1553     {
1554       // UnsafeCopyMemory page error: continue after ucm
1555       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1556       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1557       __ movptr(byte_count, count);
1558       __ shrptr(count, 3); // count =&gt; qword_count
1559 
1560       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
1561       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
1562       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
1563       __ negptr(qword_count); // make the count negative
1564       __ jmp(L_copy_bytes);
1565 
1566       // Copy trailing qwords
1567     __ BIND(L_copy_8_bytes);
1568       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
1569       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
1570       __ increment(qword_count);
1571       __ jcc(Assembler::notZero, L_copy_8_bytes);
1572 
1573       // Check for and copy trailing dword
1574     __ BIND(L_copy_4_bytes);
1575       __ testl(byte_count, 4);
1576       __ jccb(Assembler::zero, L_copy_2_bytes);
1577       __ movl(rax, Address(end_from, 8));
1578       __ movl(Address(end_to, 8), rax);
1579 
1580       __ addptr(end_from, 4);
1581       __ addptr(end_to, 4);
1582 
1583       // Check for and copy trailing word
1584     __ BIND(L_copy_2_bytes);
1585       __ testl(byte_count, 2);
1586       __ jccb(Assembler::zero, L_copy_byte);
1587       __ movw(rax, Address(end_from, 8));
1588       __ movw(Address(end_to, 8), rax);
1589 
1590       __ addptr(end_from, 2);
1591       __ addptr(end_to, 2);
1592 
1593       // Check for and copy trailing byte
1594     __ BIND(L_copy_byte);
1595       __ testl(byte_count, 1);
1596       __ jccb(Assembler::zero, L_exit);
1597       __ movb(rax, Address(end_from, 8));
1598       __ movb(Address(end_to, 8), rax);
1599     }
1600   __ BIND(L_exit);
1601     address ucme_exit_pc = __ pc();
1602     restore_arg_regs();
1603     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); // Update counter after rscratch1 is free
1604     __ xorptr(rax, rax); // return 0
1605     __ vzeroupper();
1606     __ leave(); // required for proper stackwalking of RuntimeStub frame
1607     __ ret(0);
1608 
1609     {
1610       UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);
1611       // Copy in multi-bytes chunks
1612       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1613       __ jmp(L_copy_4_bytes);
1614     }
1615     return start;
1616   }
1617 
1618   // Arguments:
1619   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1620   //             ignored
1621   //   name    - stub name string
1622   //
1623   // Inputs:
1624   //   c_rarg0   - source array address
1625   //   c_rarg1   - destination array address
1626   //   c_rarg2   - element count, treated as ssize_t, can be zero
1627   //
1628   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-, 2-, or 1-byte boundaries,
1629   // we let the hardware handle it.  The one to eight bytes within words,
1630   // dwords or qwords that span cache line boundaries will still be loaded
1631   // and stored atomically.
1632   //
1633   address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,
1634                                       address* entry, const char *name) {
1635     __ align(CodeEntryAlignment);
1636     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1637     address start = __ pc();
1638 
1639     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;
1640     const Register from        = rdi;  // source array address
1641     const Register to          = rsi;  // destination array address
1642     const Register count       = rdx;  // elements count
1643     const Register byte_count  = rcx;
1644     const Register qword_count = count;
1645 
1646     __ enter(); // required for proper stackwalking of RuntimeStub frame
1647     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1648 
1649     if (entry != NULL) {
1650       *entry = __ pc();
1651       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1652       BLOCK_COMMENT(&quot;Entry:&quot;);
1653     }
1654 
1655     array_overlap_test(nooverlap_target, Address::times_1);
1656     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1657                       // r9 and r10 may be used to save non-volatile registers
1658 
1659     {
1660       // UnsafeCopyMemory page error: continue after ucm
1661       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1662       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1663       __ movptr(byte_count, count);
1664       __ shrptr(count, 3);   // count =&gt; qword_count
1665 
1666       // Copy from high to low addresses.
1667 
1668       // Check for and copy trailing byte
1669       __ testl(byte_count, 1);
1670       __ jcc(Assembler::zero, L_copy_2_bytes);
1671       __ movb(rax, Address(from, byte_count, Address::times_1, -1));
1672       __ movb(Address(to, byte_count, Address::times_1, -1), rax);
1673       __ decrement(byte_count); // Adjust for possible trailing word
1674 
1675       // Check for and copy trailing word
1676     __ BIND(L_copy_2_bytes);
1677       __ testl(byte_count, 2);
1678       __ jcc(Assembler::zero, L_copy_4_bytes);
1679       __ movw(rax, Address(from, byte_count, Address::times_1, -2));
1680       __ movw(Address(to, byte_count, Address::times_1, -2), rax);
1681 
1682       // Check for and copy trailing dword
1683     __ BIND(L_copy_4_bytes);
1684       __ testl(byte_count, 4);
1685       __ jcc(Assembler::zero, L_copy_bytes);
1686       __ movl(rax, Address(from, qword_count, Address::times_8));
1687       __ movl(Address(to, qword_count, Address::times_8), rax);
1688       __ jmp(L_copy_bytes);
1689 
1690       // Copy trailing qwords
1691     __ BIND(L_copy_8_bytes);
1692       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
1693       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
1694       __ decrement(qword_count);
1695       __ jcc(Assembler::notZero, L_copy_8_bytes);
1696     }
1697     restore_arg_regs();
1698     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); // Update counter after rscratch1 is free
1699     __ xorptr(rax, rax); // return 0
1700     __ vzeroupper();
1701     __ leave(); // required for proper stackwalking of RuntimeStub frame
1702     __ ret(0);
1703 
1704     {
1705       // UnsafeCopyMemory page error: continue after ucm
1706       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1707       // Copy in multi-bytes chunks
1708       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1709     }
1710     restore_arg_regs();
1711     inc_counter_np(SharedRuntime::_jbyte_array_copy_ctr); // Update counter after rscratch1 is free
1712     __ xorptr(rax, rax); // return 0
1713     __ vzeroupper();
1714     __ leave(); // required for proper stackwalking of RuntimeStub frame
1715     __ ret(0);
1716 
1717     return start;
1718   }
1719 
1720   // Arguments:
1721   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1722   //             ignored
1723   //   name    - stub name string
1724   //
1725   // Inputs:
1726   //   c_rarg0   - source array address
1727   //   c_rarg1   - destination array address
1728   //   c_rarg2   - element count, treated as ssize_t, can be zero
1729   //
1730   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1731   // let the hardware handle it.  The two or four words within dwords
1732   // or qwords that span cache line boundaries will still be loaded
1733   // and stored atomically.
1734   //
1735   // Side Effects:
1736   //   disjoint_short_copy_entry is set to the no-overlap entry point
1737   //   used by generate_conjoint_short_copy().
1738   //
1739   address generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {
1740     __ align(CodeEntryAlignment);
1741     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1742     address start = __ pc();
1743 
1744     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;
1745     const Register from        = rdi;  // source array address
1746     const Register to          = rsi;  // destination array address
1747     const Register count       = rdx;  // elements count
1748     const Register word_count  = rcx;
1749     const Register qword_count = count;
1750     const Register end_from    = from; // source array end address
1751     const Register end_to      = to;   // destination array end address
1752     // End pointers are inclusive, and if count is not zero they point
1753     // to the last unit copied:  end_to[0] := end_from[0]
1754 
1755     __ enter(); // required for proper stackwalking of RuntimeStub frame
1756     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1757 
1758     if (entry != NULL) {
1759       *entry = __ pc();
1760       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1761       BLOCK_COMMENT(&quot;Entry:&quot;);
1762     }
1763 
1764     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1765                       // r9 and r10 may be used to save non-volatile registers
1766 
1767     {
1768       // UnsafeCopyMemory page error: continue after ucm
1769       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1770       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1771       __ movptr(word_count, count);
1772       __ shrptr(count, 2); // count =&gt; qword_count
1773 
1774       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
1775       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
1776       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
1777       __ negptr(qword_count);
1778       __ jmp(L_copy_bytes);
1779 
1780       // Copy trailing qwords
1781     __ BIND(L_copy_8_bytes);
1782       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
1783       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
1784       __ increment(qword_count);
1785       __ jcc(Assembler::notZero, L_copy_8_bytes);
1786 
1787       // Original &#39;dest&#39; is trashed, so we can&#39;t use it as a
1788       // base register for a possible trailing word copy
1789 
1790       // Check for and copy trailing dword
1791     __ BIND(L_copy_4_bytes);
1792       __ testl(word_count, 2);
1793       __ jccb(Assembler::zero, L_copy_2_bytes);
1794       __ movl(rax, Address(end_from, 8));
1795       __ movl(Address(end_to, 8), rax);
1796 
1797       __ addptr(end_from, 4);
1798       __ addptr(end_to, 4);
1799 
1800       // Check for and copy trailing word
1801     __ BIND(L_copy_2_bytes);
1802       __ testl(word_count, 1);
1803       __ jccb(Assembler::zero, L_exit);
1804       __ movw(rax, Address(end_from, 8));
1805       __ movw(Address(end_to, 8), rax);
1806     }
1807   __ BIND(L_exit);
1808     address ucme_exit_pc = __ pc();
1809     restore_arg_regs();
1810     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); // Update counter after rscratch1 is free
1811     __ xorptr(rax, rax); // return 0
1812     __ vzeroupper();
1813     __ leave(); // required for proper stackwalking of RuntimeStub frame
1814     __ ret(0);
1815 
1816     {
1817       UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);
1818       // Copy in multi-bytes chunks
1819       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1820       __ jmp(L_copy_4_bytes);
1821     }
1822 
1823     return start;
1824   }
1825 
1826   address generate_fill(BasicType t, bool aligned, const char *name) {
1827     __ align(CodeEntryAlignment);
1828     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1829     address start = __ pc();
1830 
1831     BLOCK_COMMENT(&quot;Entry:&quot;);
1832 
1833     const Register to       = c_rarg0;  // source array address
1834     const Register value    = c_rarg1;  // value
1835     const Register count    = c_rarg2;  // elements count
1836 
1837     __ enter(); // required for proper stackwalking of RuntimeStub frame
1838 
1839     __ generate_fill(t, aligned, to, value, count, rax, xmm0);
1840 
1841     __ vzeroupper();
1842     __ leave(); // required for proper stackwalking of RuntimeStub frame
1843     __ ret(0);
1844     return start;
1845   }
1846 
1847   // Arguments:
1848   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1849   //             ignored
1850   //   name    - stub name string
1851   //
1852   // Inputs:
1853   //   c_rarg0   - source array address
1854   //   c_rarg1   - destination array address
1855   //   c_rarg2   - element count, treated as ssize_t, can be zero
1856   //
1857   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4- or 2-byte boundaries, we
1858   // let the hardware handle it.  The two or four words within dwords
1859   // or qwords that span cache line boundaries will still be loaded
1860   // and stored atomically.
1861   //
1862   address generate_conjoint_short_copy(bool aligned, address nooverlap_target,
1863                                        address *entry, const char *name) {
1864     __ align(CodeEntryAlignment);
1865     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1866     address start = __ pc();
1867 
1868     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;
1869     const Register from        = rdi;  // source array address
1870     const Register to          = rsi;  // destination array address
1871     const Register count       = rdx;  // elements count
1872     const Register word_count  = rcx;
1873     const Register qword_count = count;
1874 
1875     __ enter(); // required for proper stackwalking of RuntimeStub frame
1876     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1877 
1878     if (entry != NULL) {
1879       *entry = __ pc();
1880       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1881       BLOCK_COMMENT(&quot;Entry:&quot;);
1882     }
1883 
1884     array_overlap_test(nooverlap_target, Address::times_2);
1885     setup_arg_regs(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1886                       // r9 and r10 may be used to save non-volatile registers
1887 
1888     {
1889       // UnsafeCopyMemory page error: continue after ucm
1890       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1891       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
1892       __ movptr(word_count, count);
1893       __ shrptr(count, 2); // count =&gt; qword_count
1894 
1895       // Copy from high to low addresses.  Use &#39;to&#39; as scratch.
1896 
1897       // Check for and copy trailing word
1898       __ testl(word_count, 1);
1899       __ jccb(Assembler::zero, L_copy_4_bytes);
1900       __ movw(rax, Address(from, word_count, Address::times_2, -2));
1901       __ movw(Address(to, word_count, Address::times_2, -2), rax);
1902 
1903      // Check for and copy trailing dword
1904     __ BIND(L_copy_4_bytes);
1905       __ testl(word_count, 2);
1906       __ jcc(Assembler::zero, L_copy_bytes);
1907       __ movl(rax, Address(from, qword_count, Address::times_8));
1908       __ movl(Address(to, qword_count, Address::times_8), rax);
1909       __ jmp(L_copy_bytes);
1910 
1911       // Copy trailing qwords
1912     __ BIND(L_copy_8_bytes);
1913       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
1914       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
1915       __ decrement(qword_count);
1916       __ jcc(Assembler::notZero, L_copy_8_bytes);
1917     }
1918     restore_arg_regs();
1919     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); // Update counter after rscratch1 is free
1920     __ xorptr(rax, rax); // return 0
1921     __ vzeroupper();
1922     __ leave(); // required for proper stackwalking of RuntimeStub frame
1923     __ ret(0);
1924 
1925     {
1926       // UnsafeCopyMemory page error: continue after ucm
1927       UnsafeCopyMemoryMark ucmm(this, !aligned, true);
1928       // Copy in multi-bytes chunks
1929       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
1930     }
1931     restore_arg_regs();
1932     inc_counter_np(SharedRuntime::_jshort_array_copy_ctr); // Update counter after rscratch1 is free
1933     __ xorptr(rax, rax); // return 0
1934     __ vzeroupper();
1935     __ leave(); // required for proper stackwalking of RuntimeStub frame
1936     __ ret(0);
1937 
1938     return start;
1939   }
1940 
1941   // Arguments:
1942   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
1943   //             ignored
1944   //   is_oop  - true =&gt; oop array, so generate store check code
1945   //   name    - stub name string
1946   //
1947   // Inputs:
1948   //   c_rarg0   - source array address
1949   //   c_rarg1   - destination array address
1950   //   c_rarg2   - element count, treated as ssize_t, can be zero
1951   //
1952   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
1953   // the hardware handle it.  The two dwords within qwords that span
1954   // cache line boundaries will still be loaded and stored atomicly.
1955   //
1956   // Side Effects:
1957   //   disjoint_int_copy_entry is set to the no-overlap entry point
1958   //   used by generate_conjoint_int_oop_copy().
1959   //
1960   address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,
1961                                          const char *name, bool dest_uninitialized = false) {
1962     __ align(CodeEntryAlignment);
1963     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
1964     address start = __ pc();
1965 
1966     Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;
1967     const Register from        = rdi;  // source array address
1968     const Register to          = rsi;  // destination array address
1969     const Register count       = rdx;  // elements count
1970     const Register dword_count = rcx;
1971     const Register qword_count = count;
1972     const Register end_from    = from; // source array end address
1973     const Register end_to      = to;   // destination array end address
1974     // End pointers are inclusive, and if count is not zero they point
1975     // to the last unit copied:  end_to[0] := end_from[0]
1976 
1977     __ enter(); // required for proper stackwalking of RuntimeStub frame
1978     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
1979 
1980     if (entry != NULL) {
1981       *entry = __ pc();
1982       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
1983       BLOCK_COMMENT(&quot;Entry:&quot;);
1984     }
1985 
1986     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
1987                                    // r9 is used to save r15_thread
1988 
1989     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
1990     if (dest_uninitialized) {
1991       decorators |= IS_DEST_UNINITIALIZED;
1992     }
1993     if (aligned) {
1994       decorators |= ARRAYCOPY_ALIGNED;
1995     }
1996 
1997     BasicType type = is_oop ? T_OBJECT : T_INT;
1998     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1999     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, count);
2000 
2001     {
2002       // UnsafeCopyMemory page error: continue after ucm
2003       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2004       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
2005       __ movptr(dword_count, count);
2006       __ shrptr(count, 1); // count =&gt; qword_count
2007 
2008       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
2009       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
2010       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
2011       __ negptr(qword_count);
2012       __ jmp(L_copy_bytes);
2013 
2014       // Copy trailing qwords
2015     __ BIND(L_copy_8_bytes);
2016       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
2017       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
2018       __ increment(qword_count);
2019       __ jcc(Assembler::notZero, L_copy_8_bytes);
2020 
2021       // Check for and copy trailing dword
2022     __ BIND(L_copy_4_bytes);
2023       __ testl(dword_count, 1); // Only byte test since the value is 0 or 1
2024       __ jccb(Assembler::zero, L_exit);
2025       __ movl(rax, Address(end_from, 8));
2026       __ movl(Address(end_to, 8), rax);
2027     }
2028   __ BIND(L_exit);
2029     address ucme_exit_pc = __ pc();
2030     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);
2031     restore_arg_regs_using_thread();
2032     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); // Update counter after rscratch1 is free
2033     __ vzeroupper();
2034     __ xorptr(rax, rax); // return 0
2035     __ leave(); // required for proper stackwalking of RuntimeStub frame
2036     __ ret(0);
2037 
2038     {
2039       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, false, ucme_exit_pc);
2040       // Copy in multi-bytes chunks
2041       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2042       __ jmp(L_copy_4_bytes);
2043     }
2044 
2045     return start;
2046   }
2047 
2048   // Arguments:
2049   //   aligned - true =&gt; Input and output aligned on a HeapWord == 8-byte boundary
2050   //             ignored
2051   //   is_oop  - true =&gt; oop array, so generate store check code
2052   //   name    - stub name string
2053   //
2054   // Inputs:
2055   //   c_rarg0   - source array address
2056   //   c_rarg1   - destination array address
2057   //   c_rarg2   - element count, treated as ssize_t, can be zero
2058   //
2059   // If &#39;from&#39; and/or &#39;to&#39; are aligned on 4-byte boundaries, we let
2060   // the hardware handle it.  The two dwords within qwords that span
2061   // cache line boundaries will still be loaded and stored atomicly.
2062   //
2063   address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,
2064                                          address *entry, const char *name,
2065                                          bool dest_uninitialized = false) {
2066     __ align(CodeEntryAlignment);
2067     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2068     address start = __ pc();
2069 
2070     Label L_copy_bytes, L_copy_8_bytes, L_exit;
2071     const Register from        = rdi;  // source array address
2072     const Register to          = rsi;  // destination array address
2073     const Register count       = rdx;  // elements count
2074     const Register dword_count = rcx;
2075     const Register qword_count = count;
2076 
2077     __ enter(); // required for proper stackwalking of RuntimeStub frame
2078     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
2079 
2080     if (entry != NULL) {
2081       *entry = __ pc();
2082        // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2083       BLOCK_COMMENT(&quot;Entry:&quot;);
2084     }
2085 
2086     array_overlap_test(nooverlap_target, Address::times_4);
2087     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
2088                                    // r9 is used to save r15_thread
2089 
2090     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
2091     if (dest_uninitialized) {
2092       decorators |= IS_DEST_UNINITIALIZED;
2093     }
2094     if (aligned) {
2095       decorators |= ARRAYCOPY_ALIGNED;
2096     }
2097 
2098     BasicType type = is_oop ? T_OBJECT : T_INT;
2099     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2100     // no registers are destroyed by this call
2101     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, count);
2102 
2103     assert_clean_int(count, rax); // Make sure &#39;count&#39; is clean int.
2104     {
2105       // UnsafeCopyMemory page error: continue after ucm
2106       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2107       // &#39;from&#39;, &#39;to&#39; and &#39;count&#39; are now valid
2108       __ movptr(dword_count, count);
2109       __ shrptr(count, 1); // count =&gt; qword_count
2110 
2111       // Copy from high to low addresses.  Use &#39;to&#39; as scratch.
2112 
2113       // Check for and copy trailing dword
2114       __ testl(dword_count, 1);
2115       __ jcc(Assembler::zero, L_copy_bytes);
2116       __ movl(rax, Address(from, dword_count, Address::times_4, -4));
2117       __ movl(Address(to, dword_count, Address::times_4, -4), rax);
2118       __ jmp(L_copy_bytes);
2119 
2120       // Copy trailing qwords
2121     __ BIND(L_copy_8_bytes);
2122       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
2123       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
2124       __ decrement(qword_count);
2125       __ jcc(Assembler::notZero, L_copy_8_bytes);
2126     }
2127     if (is_oop) {
2128       __ jmp(L_exit);
2129     }
2130     restore_arg_regs_using_thread();
2131     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); // Update counter after rscratch1 is free
2132     __ xorptr(rax, rax); // return 0
2133     __ vzeroupper();
2134     __ leave(); // required for proper stackwalking of RuntimeStub frame
2135     __ ret(0);
2136 
2137     {
2138       // UnsafeCopyMemory page error: continue after ucm
2139       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2140       // Copy in multi-bytes chunks
2141       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2142     }
2143 
2144   __ BIND(L_exit);
2145     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);
2146     restore_arg_regs_using_thread();
2147     inc_counter_np(SharedRuntime::_jint_array_copy_ctr); // Update counter after rscratch1 is free
2148     __ xorptr(rax, rax); // return 0
2149     __ vzeroupper();
2150     __ leave(); // required for proper stackwalking of RuntimeStub frame
2151     __ ret(0);
2152 
2153     return start;
2154   }
2155 
2156   // Arguments:
2157   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
2158   //             ignored
2159   //   is_oop  - true =&gt; oop array, so generate store check code
2160   //   name    - stub name string
2161   //
2162   // Inputs:
2163   //   c_rarg0   - source array address
2164   //   c_rarg1   - destination array address
2165   //   c_rarg2   - element count, treated as ssize_t, can be zero
2166   //
2167  // Side Effects:
2168   //   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the
2169   //   no-overlap entry point used by generate_conjoint_long_oop_copy().
2170   //
2171   address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,
2172                                           const char *name, bool dest_uninitialized = false) {
2173     __ align(CodeEntryAlignment);
2174     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2175     address start = __ pc();
2176 
2177     Label L_copy_bytes, L_copy_8_bytes, L_exit;
2178     const Register from        = rdi;  // source array address
2179     const Register to          = rsi;  // destination array address
2180     const Register qword_count = rdx;  // elements count
2181     const Register end_from    = from; // source array end address
2182     const Register end_to      = rcx;  // destination array end address
2183     const Register saved_count = r11;
2184     // End pointers are inclusive, and if count is not zero they point
2185     // to the last unit copied:  end_to[0] := end_from[0]
2186 
2187     __ enter(); // required for proper stackwalking of RuntimeStub frame
2188     // Save no-overlap entry point for generate_conjoint_long_oop_copy()
2189     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
2190 
2191     if (entry != NULL) {
2192       *entry = __ pc();
2193       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2194       BLOCK_COMMENT(&quot;Entry:&quot;);
2195     }
2196 
2197     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
2198                                      // r9 is used to save r15_thread
2199     // &#39;from&#39;, &#39;to&#39; and &#39;qword_count&#39; are now valid
2200 
2201     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;
2202     if (dest_uninitialized) {
2203       decorators |= IS_DEST_UNINITIALIZED;
2204     }
2205     if (aligned) {
2206       decorators |= ARRAYCOPY_ALIGNED;
2207     }
2208 
2209     BasicType type = is_oop ? T_OBJECT : T_LONG;
2210     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2211     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, qword_count);
2212     {
2213       // UnsafeCopyMemory page error: continue after ucm
2214       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2215 
2216       // Copy from low to high addresses.  Use &#39;to&#39; as scratch.
2217       __ lea(end_from, Address(from, qword_count, Address::times_8, -8));
2218       __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));
2219       __ negptr(qword_count);
2220       __ jmp(L_copy_bytes);
2221 
2222       // Copy trailing qwords
2223     __ BIND(L_copy_8_bytes);
2224       __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));
2225       __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);
2226       __ increment(qword_count);
2227       __ jcc(Assembler::notZero, L_copy_8_bytes);
2228     }
2229     if (is_oop) {
2230       __ jmp(L_exit);
2231     } else {
2232       restore_arg_regs_using_thread();
2233       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2234       __ xorptr(rax, rax); // return 0
2235       __ vzeroupper();
2236       __ leave(); // required for proper stackwalking of RuntimeStub frame
2237       __ ret(0);
2238     }
2239 
2240     {
2241       // UnsafeCopyMemory page error: continue after ucm
2242       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2243       // Copy in multi-bytes chunks
2244       copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2245     }
2246 
2247     __ BIND(L_exit);
2248     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);
2249     restore_arg_regs_using_thread();
2250     if (is_oop) {
2251       inc_counter_np(SharedRuntime::_oop_array_copy_ctr); // Update counter after rscratch1 is free
2252     } else {
2253       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2254     }
2255     __ vzeroupper();
2256     __ xorptr(rax, rax); // return 0
2257     __ leave(); // required for proper stackwalking of RuntimeStub frame
2258     __ ret(0);
2259 
2260     return start;
2261   }
2262 
2263   // Arguments:
2264   //   aligned - true =&gt; Input and output aligned on a HeapWord boundary == 8 bytes
2265   //             ignored
2266   //   is_oop  - true =&gt; oop array, so generate store check code
2267   //   name    - stub name string
2268   //
2269   // Inputs:
2270   //   c_rarg0   - source array address
2271   //   c_rarg1   - destination array address
2272   //   c_rarg2   - element count, treated as ssize_t, can be zero
2273   //
2274   address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,
2275                                           address nooverlap_target, address *entry,
2276                                           const char *name, bool dest_uninitialized = false) {
2277     __ align(CodeEntryAlignment);
2278     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2279     address start = __ pc();
2280 
2281     Label L_copy_bytes, L_copy_8_bytes, L_exit;
2282     const Register from        = rdi;  // source array address
2283     const Register to          = rsi;  // destination array address
2284     const Register qword_count = rdx;  // elements count
2285     const Register saved_count = rcx;
2286 
2287     __ enter(); // required for proper stackwalking of RuntimeStub frame
2288     assert_clean_int(c_rarg2, rax);    // Make sure &#39;count&#39; is clean int.
2289 
2290     if (entry != NULL) {
2291       *entry = __ pc();
2292       // caller can pass a 64-bit byte count here (from Unsafe.copyMemory)
2293       BLOCK_COMMENT(&quot;Entry:&quot;);
2294     }
2295 
2296     array_overlap_test(nooverlap_target, Address::times_8);
2297     setup_arg_regs_using_thread(); // from =&gt; rdi, to =&gt; rsi, count =&gt; rdx
2298                                    // r9 is used to save r15_thread
2299     // &#39;from&#39;, &#39;to&#39; and &#39;qword_count&#39; are now valid
2300 
2301     DecoratorSet decorators = IN_HEAP | IS_ARRAY;
2302     if (dest_uninitialized) {
2303       decorators |= IS_DEST_UNINITIALIZED;
2304     }
2305     if (aligned) {
2306       decorators |= ARRAYCOPY_ALIGNED;
2307     }
2308 
2309     BasicType type = is_oop ? T_OBJECT : T_LONG;
2310     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2311     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, qword_count);
2312     {
2313       // UnsafeCopyMemory page error: continue after ucm
2314       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2315 
2316       __ jmp(L_copy_bytes);
2317 
2318       // Copy trailing qwords
2319     __ BIND(L_copy_8_bytes);
2320       __ movq(rax, Address(from, qword_count, Address::times_8, -8));
2321       __ movq(Address(to, qword_count, Address::times_8, -8), rax);
2322       __ decrement(qword_count);
2323       __ jcc(Assembler::notZero, L_copy_8_bytes);
2324     }
2325     if (is_oop) {
2326       __ jmp(L_exit);
2327     } else {
2328       restore_arg_regs_using_thread();
2329       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2330       __ xorptr(rax, rax); // return 0
2331       __ vzeroupper();
2332       __ leave(); // required for proper stackwalking of RuntimeStub frame
2333       __ ret(0);
2334     }
2335     {
2336       // UnsafeCopyMemory page error: continue after ucm
2337       UnsafeCopyMemoryMark ucmm(this, !is_oop &amp;&amp; !aligned, true);
2338 
2339       // Copy in multi-bytes chunks
2340       copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);
2341     }
2342     __ BIND(L_exit);
2343     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);
2344     restore_arg_regs_using_thread();
2345     if (is_oop) {
2346       inc_counter_np(SharedRuntime::_oop_array_copy_ctr); // Update counter after rscratch1 is free
2347     } else {
2348       inc_counter_np(SharedRuntime::_jlong_array_copy_ctr); // Update counter after rscratch1 is free
2349     }
2350     __ vzeroupper();
2351     __ xorptr(rax, rax); // return 0
2352     __ leave(); // required for proper stackwalking of RuntimeStub frame
2353     __ ret(0);
2354 
2355     return start;
2356   }
2357 
2358 
2359   // Helper for generating a dynamic type check.
2360   // Smashes no registers.
2361   void generate_type_check(Register sub_klass,
2362                            Register super_check_offset,
2363                            Register super_klass,
2364                            Label&amp; L_success) {
2365     assert_different_registers(sub_klass, super_check_offset, super_klass);
2366 
2367     BLOCK_COMMENT(&quot;type_check:&quot;);
2368 
2369     Label L_miss;
2370 
2371     __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &amp;L_success, &amp;L_miss, NULL,
2372                                      super_check_offset);
2373     __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &amp;L_success, NULL);
2374 
2375     // Fall through on failure!
2376     __ BIND(L_miss);
2377   }
2378 
2379   //
2380   //  Generate checkcasting array copy stub
2381   //
2382   //  Input:
2383   //    c_rarg0   - source array address
2384   //    c_rarg1   - destination array address
2385   //    c_rarg2   - element count, treated as ssize_t, can be zero
2386   //    c_rarg3   - size_t ckoff (super_check_offset)
2387   // not Win64
2388   //    c_rarg4   - oop ckval (super_klass)
2389   // Win64
2390   //    rsp+40    - oop ckval (super_klass)
2391   //
2392   //  Output:
2393   //    rax ==  0  -  success
2394   //    rax == -1^K - failure, where K is partial transfer count
2395   //
2396   address generate_checkcast_copy(const char *name, address *entry,
2397                                   bool dest_uninitialized = false) {
2398 
2399     Label L_load_element, L_store_element, L_do_card_marks, L_done;
2400 
2401     // Input registers (after setup_arg_regs)
2402     const Register from        = rdi;   // source array address
2403     const Register to          = rsi;   // destination array address
2404     const Register length      = rdx;   // elements count
2405     const Register ckoff       = rcx;   // super_check_offset
2406     const Register ckval       = r8;    // super_klass
2407 
2408     // Registers used as temps (r13, r14 are save-on-entry)
2409     const Register end_from    = from;  // source array end address
2410     const Register end_to      = r13;   // destination array end address
2411     const Register count       = rdx;   // -(count_remaining)
2412     const Register r14_length  = r14;   // saved copy of length
2413     // End pointers are inclusive, and if length is not zero they point
2414     // to the last unit copied:  end_to[0] := end_from[0]
2415 
2416     const Register rax_oop    = rax;    // actual oop copied
2417     const Register r11_klass  = r11;    // oop._klass
2418 
2419     //---------------------------------------------------------------
2420     // Assembler stub will be used for this call to arraycopy
2421     // if the two arrays are subtypes of Object[] but the
2422     // destination array type is not equal to or a supertype
2423     // of the source type.  Each element must be separately
2424     // checked.
2425 
2426     __ align(CodeEntryAlignment);
2427     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2428     address start = __ pc();
2429 
2430     __ enter(); // required for proper stackwalking of RuntimeStub frame
2431 
2432 #ifdef ASSERT
2433     // caller guarantees that the arrays really are different
2434     // otherwise, we would have to make conjoint checks
2435     { Label L;
2436       array_overlap_test(L, TIMES_OOP);
2437       __ stop(&quot;checkcast_copy within a single array&quot;);
2438       __ bind(L);
2439     }
2440 #endif //ASSERT
2441 
2442     setup_arg_regs(4); // from =&gt; rdi, to =&gt; rsi, length =&gt; rdx
2443                        // ckoff =&gt; rcx, ckval =&gt; r8
2444                        // r9 and r10 may be used to save non-volatile registers
2445 #ifdef _WIN64
2446     // last argument (#4) is on stack on Win64
2447     __ movptr(ckval, Address(rsp, 6 * wordSize));
2448 #endif
2449 
2450     // Caller of this entry point must set up the argument registers.
2451     if (entry != NULL) {
2452       *entry = __ pc();
2453       BLOCK_COMMENT(&quot;Entry:&quot;);
2454     }
2455 
2456     // allocate spill slots for r13, r14
2457     enum {
2458       saved_r13_offset,
2459       saved_r14_offset,
2460       saved_r10_offset,
2461       saved_rbp_offset
2462     };
2463     __ subptr(rsp, saved_rbp_offset * wordSize);
2464     __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);
2465     __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);
2466     __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);
2467 
2468 #ifdef ASSERT
2469       Label L2;
2470       __ get_thread(r14);
2471       __ cmpptr(r15_thread, r14);
2472       __ jcc(Assembler::equal, L2);
2473       __ stop(&quot;StubRoutines::call_stub: r15_thread is modified by call&quot;);
2474       __ bind(L2);
2475 #endif // ASSERT
2476 
2477     // check that int operands are properly extended to size_t
2478     assert_clean_int(length, rax);
2479     assert_clean_int(ckoff, rax);
2480 
2481 #ifdef ASSERT
2482     BLOCK_COMMENT(&quot;assert consistent ckoff/ckval&quot;);
2483     // The ckoff and ckval must be mutually consistent,
2484     // even though caller generates both.
2485     { Label L;
2486       int sco_offset = in_bytes(Klass::super_check_offset_offset());
2487       __ cmpl(ckoff, Address(ckval, sco_offset));
2488       __ jcc(Assembler::equal, L);
2489       __ stop(&quot;super_check_offset inconsistent&quot;);
2490       __ bind(L);
2491     }
2492 #endif //ASSERT
2493 
2494     // Loop-invariant addresses.  They are exclusive end pointers.
2495     Address end_from_addr(from, length, TIMES_OOP, 0);
2496     Address   end_to_addr(to,   length, TIMES_OOP, 0);
2497     // Loop-variant addresses.  They assume post-incremented count &lt; 0.
2498     Address from_element_addr(end_from, count, TIMES_OOP, 0);
2499     Address   to_element_addr(end_to,   count, TIMES_OOP, 0);
2500 
2501     DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;
2502     if (dest_uninitialized) {
2503       decorators |= IS_DEST_UNINITIALIZED;
2504     }
2505 
2506     BasicType type = T_OBJECT;
2507     BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2508     bs-&gt;arraycopy_prologue(_masm, decorators, type, from, to, count);
2509 
2510     // Copy from low to high addresses, indexed from the end of each array.
2511     __ lea(end_from, end_from_addr);
2512     __ lea(end_to,   end_to_addr);
2513     __ movptr(r14_length, length);        // save a copy of the length
2514     assert(length == count, &quot;&quot;);          // else fix next line:
2515     __ negptr(count);                     // negate and test the length
2516     __ jcc(Assembler::notZero, L_load_element);
2517 
2518     // Empty array:  Nothing to do.
2519     __ xorptr(rax, rax);                  // return 0 on (trivial) success
2520     __ jmp(L_done);
2521 
2522     // ======== begin loop ========
2523     // (Loop is rotated; its entry is L_load_element.)
2524     // Loop control:
2525     //   for (count = -count; count != 0; count++)
2526     // Base pointers src, dst are biased by 8*(count-1),to last element.
2527     __ align(OptoLoopAlignment);
2528 
2529     __ BIND(L_store_element);
<a name="11" id="anc11"></a><span class="line-modified">2530     __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  // store the oop</span>
2531     __ increment(count);               // increment the count toward zero
2532     __ jcc(Assembler::zero, L_do_card_marks);
2533 
2534     // ======== loop entry is here ========
2535     __ BIND(L_load_element);
2536     __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); // load the oop
2537     __ testptr(rax_oop, rax_oop);
2538     __ jcc(Assembler::zero, L_store_element);
2539 
2540     __ load_klass(r11_klass, rax_oop, rscratch1);// query the object klass
2541     generate_type_check(r11_klass, ckoff, ckval, L_store_element);
2542     // ======== end loop ========
2543 
2544     // It was a real error; we must depend on the caller to finish the job.
2545     // Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.
2546     // Emit GC store barriers for the oops we have copied (r14 + rdx),
2547     // and report their number to the caller.
2548     assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);
2549     Label L_post_barrier;
2550     __ addptr(r14_length, count);     // K = (original - remaining) oops
2551     __ movptr(rax, r14_length);       // save the value
2552     __ notptr(rax);                   // report (-1^K) to caller (does not affect flags)
2553     __ jccb(Assembler::notZero, L_post_barrier);
2554     __ jmp(L_done); // K == 0, nothing was copied, skip post barrier
2555 
2556     // Come here on success only.
2557     __ BIND(L_do_card_marks);
2558     __ xorptr(rax, rax);              // return 0 on success
2559 
2560     __ BIND(L_post_barrier);
2561     bs-&gt;arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);
2562 
2563     // Common exit point (success or failure).
2564     __ BIND(L_done);
2565     __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));
2566     __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));
2567     __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));
2568     restore_arg_regs();
2569     inc_counter_np(SharedRuntime::_checkcast_array_copy_ctr); // Update counter after rscratch1 is free
2570     __ leave(); // required for proper stackwalking of RuntimeStub frame
2571     __ ret(0);
2572 
2573     return start;
2574   }
2575 
2576   //
2577   //  Generate &#39;unsafe&#39; array copy stub
2578   //  Though just as safe as the other stubs, it takes an unscaled
2579   //  size_t argument instead of an element count.
2580   //
2581   //  Input:
2582   //    c_rarg0   - source array address
2583   //    c_rarg1   - destination array address
2584   //    c_rarg2   - byte count, treated as ssize_t, can be zero
2585   //
2586   // Examines the alignment of the operands and dispatches
2587   // to a long, int, short, or byte copy loop.
2588   //
2589   address generate_unsafe_copy(const char *name,
2590                                address byte_copy_entry, address short_copy_entry,
2591                                address int_copy_entry, address long_copy_entry) {
2592 
2593     Label L_long_aligned, L_int_aligned, L_short_aligned;
2594 
2595     // Input registers (before setup_arg_regs)
2596     const Register from        = c_rarg0;  // source array address
2597     const Register to          = c_rarg1;  // destination array address
2598     const Register size        = c_rarg2;  // byte count (size_t)
2599 
2600     // Register used as a temp
2601     const Register bits        = rax;      // test copy of low bits
2602 
2603     __ align(CodeEntryAlignment);
2604     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2605     address start = __ pc();
2606 
2607     __ enter(); // required for proper stackwalking of RuntimeStub frame
2608 
2609     // bump this on entry, not on exit:
2610     inc_counter_np(SharedRuntime::_unsafe_array_copy_ctr);
2611 
2612     __ mov(bits, from);
2613     __ orptr(bits, to);
2614     __ orptr(bits, size);
2615 
2616     __ testb(bits, BytesPerLong-1);
2617     __ jccb(Assembler::zero, L_long_aligned);
2618 
2619     __ testb(bits, BytesPerInt-1);
2620     __ jccb(Assembler::zero, L_int_aligned);
2621 
2622     __ testb(bits, BytesPerShort-1);
2623     __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));
2624 
2625     __ BIND(L_short_aligned);
2626     __ shrptr(size, LogBytesPerShort); // size =&gt; short_count
2627     __ jump(RuntimeAddress(short_copy_entry));
2628 
2629     __ BIND(L_int_aligned);
2630     __ shrptr(size, LogBytesPerInt); // size =&gt; int_count
2631     __ jump(RuntimeAddress(int_copy_entry));
2632 
2633     __ BIND(L_long_aligned);
2634     __ shrptr(size, LogBytesPerLong); // size =&gt; qword_count
2635     __ jump(RuntimeAddress(long_copy_entry));
2636 
2637     return start;
2638   }
2639 
2640   // Perform range checks on the proposed arraycopy.
2641   // Kills temp, but nothing else.
2642   // Also, clean the sign bits of src_pos and dst_pos.
2643   void arraycopy_range_checks(Register src,     // source array oop (c_rarg0)
2644                               Register src_pos, // source position (c_rarg1)
2645                               Register dst,     // destination array oo (c_rarg2)
2646                               Register dst_pos, // destination position (c_rarg3)
2647                               Register length,
2648                               Register temp,
2649                               Label&amp; L_failed) {
2650     BLOCK_COMMENT(&quot;arraycopy_range_checks:&quot;);
2651 
2652     //  if (src_pos + length &gt; arrayOop(src)-&gt;length())  FAIL;
2653     __ movl(temp, length);
2654     __ addl(temp, src_pos);             // src_pos + length
2655     __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));
2656     __ jcc(Assembler::above, L_failed);
2657 
2658     //  if (dst_pos + length &gt; arrayOop(dst)-&gt;length())  FAIL;
2659     __ movl(temp, length);
2660     __ addl(temp, dst_pos);             // dst_pos + length
2661     __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));
2662     __ jcc(Assembler::above, L_failed);
2663 
2664     // Have to clean up high 32-bits of &#39;src_pos&#39; and &#39;dst_pos&#39;.
2665     // Move with sign extension can be used since they are positive.
2666     __ movslq(src_pos, src_pos);
2667     __ movslq(dst_pos, dst_pos);
2668 
2669     BLOCK_COMMENT(&quot;arraycopy_range_checks done&quot;);
2670   }
2671 
2672   //
2673   //  Generate generic array copy stubs
2674   //
2675   //  Input:
2676   //    c_rarg0    -  src oop
2677   //    c_rarg1    -  src_pos (32-bits)
2678   //    c_rarg2    -  dst oop
2679   //    c_rarg3    -  dst_pos (32-bits)
2680   // not Win64
2681   //    c_rarg4    -  element count (32-bits)
2682   // Win64
2683   //    rsp+40     -  element count (32-bits)
2684   //
2685   //  Output:
2686   //    rax ==  0  -  success
2687   //    rax == -1^K - failure, where K is partial transfer count
2688   //
2689   address generate_generic_copy(const char *name,
2690                                 address byte_copy_entry, address short_copy_entry,
2691                                 address int_copy_entry, address oop_copy_entry,
2692                                 address long_copy_entry, address checkcast_copy_entry) {
2693 
2694     Label L_failed, L_failed_0, L_objArray;
2695     Label L_copy_bytes, L_copy_shorts, L_copy_ints, L_copy_longs;
2696 
2697     // Input registers
2698     const Register src        = c_rarg0;  // source array oop
2699     const Register src_pos    = c_rarg1;  // source position
2700     const Register dst        = c_rarg2;  // destination array oop
2701     const Register dst_pos    = c_rarg3;  // destination position
2702 #ifndef _WIN64
2703     const Register length     = c_rarg4;
2704     const Register rklass_tmp = r9;  // load_klass
2705 #else
2706     const Address  length(rsp, 6 * wordSize);  // elements count is on stack on Win64
2707     const Register rklass_tmp = rdi;  // load_klass
2708 #endif
2709 
2710     { int modulus = CodeEntryAlignment;
2711       int target  = modulus - 5; // 5 = sizeof jmp(L_failed)
2712       int advance = target - (__ offset() % modulus);
2713       if (advance &lt; 0)  advance += modulus;
2714       if (advance &gt; 0)  __ nop(advance);
2715     }
2716     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
2717 
2718     // Short-hop target to L_failed.  Makes for denser prologue code.
2719     __ BIND(L_failed_0);
2720     __ jmp(L_failed);
2721     assert(__ offset() % CodeEntryAlignment == 0, &quot;no further alignment needed&quot;);
2722 
2723     __ align(CodeEntryAlignment);
2724     address start = __ pc();
2725 
2726     __ enter(); // required for proper stackwalking of RuntimeStub frame
2727 
2728     // bump this on entry, not on exit:
2729     inc_counter_np(SharedRuntime::_generic_array_copy_ctr);
2730 
2731     //-----------------------------------------------------------------------
2732     // Assembler stub will be used for this call to arraycopy
2733     // if the following conditions are met:
2734     //
2735     // (1) src and dst must not be null.
2736     // (2) src_pos must not be negative.
2737     // (3) dst_pos must not be negative.
2738     // (4) length  must not be negative.
2739     // (5) src klass and dst klass should be the same and not NULL.
2740     // (6) src and dst should be arrays.
2741     // (7) src_pos + length must not exceed length of src.
2742     // (8) dst_pos + length must not exceed length of dst.
2743     //
2744 
2745     //  if (src == NULL) return -1;
2746     __ testptr(src, src);         // src oop
2747     size_t j1off = __ offset();
2748     __ jccb(Assembler::zero, L_failed_0);
2749 
2750     //  if (src_pos &lt; 0) return -1;
2751     __ testl(src_pos, src_pos); // src_pos (32-bits)
2752     __ jccb(Assembler::negative, L_failed_0);
2753 
2754     //  if (dst == NULL) return -1;
2755     __ testptr(dst, dst);         // dst oop
2756     __ jccb(Assembler::zero, L_failed_0);
2757 
2758     //  if (dst_pos &lt; 0) return -1;
2759     __ testl(dst_pos, dst_pos); // dst_pos (32-bits)
2760     size_t j4off = __ offset();
2761     __ jccb(Assembler::negative, L_failed_0);
2762 
2763     // The first four tests are very dense code,
2764     // but not quite dense enough to put four
2765     // jumps in a 16-byte instruction fetch buffer.
2766     // That&#39;s good, because some branch predicters
2767     // do not like jumps so close together.
2768     // Make sure of this.
2769     guarantee(((j1off ^ j4off) &amp; ~15) != 0, &quot;I$ line of 1st &amp; 4th jumps&quot;);
2770 
2771     // registers used as temp
2772     const Register r11_length    = r11; // elements count to copy
2773     const Register r10_src_klass = r10; // array klass
2774 
2775     //  if (length &lt; 0) return -1;
2776     __ movl(r11_length, length);        // length (elements count, 32-bits value)
2777     __ testl(r11_length, r11_length);
2778     __ jccb(Assembler::negative, L_failed_0);
2779 
2780     __ load_klass(r10_src_klass, src, rklass_tmp);
2781 #ifdef ASSERT
2782     //  assert(src-&gt;klass() != NULL);
2783     {
2784       BLOCK_COMMENT(&quot;assert klasses not null {&quot;);
2785       Label L1, L2;
2786       __ testptr(r10_src_klass, r10_src_klass);
2787       __ jcc(Assembler::notZero, L2);   // it is broken if klass is NULL
2788       __ bind(L1);
2789       __ stop(&quot;broken null klass&quot;);
2790       __ bind(L2);
2791       __ load_klass(rax, dst, rklass_tmp);
2792       __ cmpq(rax, 0);
2793       __ jcc(Assembler::equal, L1);     // this would be broken also
2794       BLOCK_COMMENT(&quot;} assert klasses not null done&quot;);
2795     }
2796 #endif
2797 
2798     // Load layout helper (32-bits)
2799     //
2800     //  |array_tag|     | header_size | element_type |     |log2_element_size|
2801     // 32        30    24            16              8     2                 0
2802     //
2803     //   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0
2804     //
2805 
2806     const int lh_offset = in_bytes(Klass::layout_helper_offset());
2807 
2808     // Handle objArrays completely differently...
2809     const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);
2810     __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);
2811     __ jcc(Assembler::equal, L_objArray);
2812 
2813     //  if (src-&gt;klass() != dst-&gt;klass()) return -1;
2814     __ load_klass(rax, dst, rklass_tmp);
2815     __ cmpq(r10_src_klass, rax);
2816     __ jcc(Assembler::notEqual, L_failed);
2817 
2818     const Register rax_lh = rax;  // layout helper
2819     __ movl(rax_lh, Address(r10_src_klass, lh_offset));
2820 
2821     //  if (!src-&gt;is_Array()) return -1;
2822     __ cmpl(rax_lh, Klass::_lh_neutral_value);
2823     __ jcc(Assembler::greaterEqual, L_failed);
2824 
2825     // At this point, it is known to be a typeArray (array_tag 0x3).
2826 #ifdef ASSERT
2827     {
2828       BLOCK_COMMENT(&quot;assert primitive array {&quot;);
2829       Label L;
2830       __ cmpl(rax_lh, (Klass::_lh_array_tag_type_value &lt;&lt; Klass::_lh_array_tag_shift));
2831       __ jcc(Assembler::greaterEqual, L);
2832       __ stop(&quot;must be a primitive array&quot;);
2833       __ bind(L);
2834       BLOCK_COMMENT(&quot;} assert primitive array done&quot;);
2835     }
2836 #endif
2837 
2838     arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
2839                            r10, L_failed);
2840 
2841     // TypeArrayKlass
2842     //
2843     // src_addr = (src + array_header_in_bytes()) + (src_pos &lt;&lt; log2elemsize);
2844     // dst_addr = (dst + array_header_in_bytes()) + (dst_pos &lt;&lt; log2elemsize);
2845     //
2846 
2847     const Register r10_offset = r10;    // array offset
2848     const Register rax_elsize = rax_lh; // element size
2849 
2850     __ movl(r10_offset, rax_lh);
2851     __ shrl(r10_offset, Klass::_lh_header_size_shift);
2852     __ andptr(r10_offset, Klass::_lh_header_size_mask);   // array_offset
2853     __ addptr(src, r10_offset);           // src array offset
2854     __ addptr(dst, r10_offset);           // dst array offset
2855     BLOCK_COMMENT(&quot;choose copy loop based on element size&quot;);
2856     __ andl(rax_lh, Klass::_lh_log2_element_size_mask); // rax_lh -&gt; rax_elsize
2857 
2858     // next registers should be set before the jump to corresponding stub
2859     const Register from     = c_rarg0;  // source array address
2860     const Register to       = c_rarg1;  // destination array address
2861     const Register count    = c_rarg2;  // elements count
2862 
2863     // &#39;from&#39;, &#39;to&#39;, &#39;count&#39; registers should be set in such order
2864     // since they are the same as &#39;src&#39;, &#39;src_pos&#39;, &#39;dst&#39;.
2865 
2866   __ BIND(L_copy_bytes);
2867     __ cmpl(rax_elsize, 0);
2868     __ jccb(Assembler::notEqual, L_copy_shorts);
2869     __ lea(from, Address(src, src_pos, Address::times_1, 0));// src_addr
2870     __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));// dst_addr
2871     __ movl2ptr(count, r11_length); // length
2872     __ jump(RuntimeAddress(byte_copy_entry));
2873 
2874   __ BIND(L_copy_shorts);
2875     __ cmpl(rax_elsize, LogBytesPerShort);
2876     __ jccb(Assembler::notEqual, L_copy_ints);
2877     __ lea(from, Address(src, src_pos, Address::times_2, 0));// src_addr
2878     __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));// dst_addr
2879     __ movl2ptr(count, r11_length); // length
2880     __ jump(RuntimeAddress(short_copy_entry));
2881 
2882   __ BIND(L_copy_ints);
2883     __ cmpl(rax_elsize, LogBytesPerInt);
2884     __ jccb(Assembler::notEqual, L_copy_longs);
2885     __ lea(from, Address(src, src_pos, Address::times_4, 0));// src_addr
2886     __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));// dst_addr
2887     __ movl2ptr(count, r11_length); // length
2888     __ jump(RuntimeAddress(int_copy_entry));
2889 
2890   __ BIND(L_copy_longs);
2891 #ifdef ASSERT
2892     {
2893       BLOCK_COMMENT(&quot;assert long copy {&quot;);
2894       Label L;
2895       __ cmpl(rax_elsize, LogBytesPerLong);
2896       __ jcc(Assembler::equal, L);
2897       __ stop(&quot;must be long copy, but elsize is wrong&quot;);
2898       __ bind(L);
2899       BLOCK_COMMENT(&quot;} assert long copy done&quot;);
2900     }
2901 #endif
2902     __ lea(from, Address(src, src_pos, Address::times_8, 0));// src_addr
2903     __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));// dst_addr
2904     __ movl2ptr(count, r11_length); // length
2905     __ jump(RuntimeAddress(long_copy_entry));
2906 
2907     // ObjArrayKlass
2908   __ BIND(L_objArray);
2909     // live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]
2910 
2911     Label L_plain_copy, L_checkcast_copy;
2912     //  test array classes for subtyping
2913     __ load_klass(rax, dst, rklass_tmp);
2914     __ cmpq(r10_src_klass, rax); // usual case is exact equality
2915     __ jcc(Assembler::notEqual, L_checkcast_copy);
2916 
2917     // Identically typed arrays can be copied without element-wise checks.
2918     arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
2919                            r10, L_failed);
2920 
2921     __ lea(from, Address(src, src_pos, TIMES_OOP,
2922                  arrayOopDesc::base_offset_in_bytes(T_OBJECT))); // src_addr
2923     __ lea(to,   Address(dst, dst_pos, TIMES_OOP,
2924                  arrayOopDesc::base_offset_in_bytes(T_OBJECT))); // dst_addr
2925     __ movl2ptr(count, r11_length); // length
2926   __ BIND(L_plain_copy);
2927     __ jump(RuntimeAddress(oop_copy_entry));
2928 
2929   __ BIND(L_checkcast_copy);
2930     // live at this point:  r10_src_klass, r11_length, rax (dst_klass)
2931     {
2932       // Before looking at dst.length, make sure dst is also an objArray.
2933       __ cmpl(Address(rax, lh_offset), objArray_lh);
2934       __ jcc(Assembler::notEqual, L_failed);
2935 
2936       // It is safe to examine both src.length and dst.length.
2937       arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,
2938                              rax, L_failed);
2939 
2940       const Register r11_dst_klass = r11;
2941       __ load_klass(r11_dst_klass, dst, rklass_tmp); // reload
2942 
2943       // Marshal the base address arguments now, freeing registers.
2944       __ lea(from, Address(src, src_pos, TIMES_OOP,
2945                    arrayOopDesc::base_offset_in_bytes(T_OBJECT)));
2946       __ lea(to,   Address(dst, dst_pos, TIMES_OOP,
2947                    arrayOopDesc::base_offset_in_bytes(T_OBJECT)));
2948       __ movl(count, length);           // length (reloaded)
2949       Register sco_temp = c_rarg3;      // this register is free now
2950       assert_different_registers(from, to, count, sco_temp,
2951                                  r11_dst_klass, r10_src_klass);
2952       assert_clean_int(count, sco_temp);
2953 
2954       // Generate the type check.
2955       const int sco_offset = in_bytes(Klass::super_check_offset_offset());
2956       __ movl(sco_temp, Address(r11_dst_klass, sco_offset));
2957       assert_clean_int(sco_temp, rax);
2958       generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);
2959 
2960       // Fetch destination element klass from the ObjArrayKlass header.
2961       int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());
2962       __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));
2963       __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));
2964       assert_clean_int(sco_temp, rax);
2965 
2966       // the checkcast_copy loop needs two extra arguments:
2967       assert(c_rarg3 == sco_temp, &quot;#3 already in place&quot;);
2968       // Set up arguments for checkcast_copy_entry.
2969       setup_arg_regs(4);
2970       __ movptr(r8, r11_dst_klass);  // dst.klass.element_klass, r8 is c_rarg4 on Linux/Solaris
2971       __ jump(RuntimeAddress(checkcast_copy_entry));
2972     }
2973 
2974   __ BIND(L_failed);
2975     __ xorptr(rax, rax);
2976     __ notptr(rax); // return -1
2977     __ leave();   // required for proper stackwalking of RuntimeStub frame
2978     __ ret(0);
2979 
2980     return start;
2981   }
2982 
2983   address generate_data_cache_writeback() {
2984     const Register src        = c_rarg0;  // source address
2985 
2986     __ align(CodeEntryAlignment);
2987 
2988     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback&quot;);
2989 
2990     address start = __ pc();
2991     __ enter();
2992     __ cache_wb(Address(src, 0));
2993     __ leave();
2994     __ ret(0);
2995 
2996     return start;
2997   }
2998 
2999   address generate_data_cache_writeback_sync() {
3000     const Register is_pre    = c_rarg0;  // pre or post sync
3001 
3002     __ align(CodeEntryAlignment);
3003 
3004     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_data_cache_writeback_sync&quot;);
3005 
3006     // pre wbsync is a no-op
3007     // post wbsync translates to an sfence
3008 
3009     Label skip;
3010     address start = __ pc();
3011     __ enter();
3012     __ cmpl(is_pre, 0);
3013     __ jcc(Assembler::notEqual, skip);
3014     __ cache_wbsync(false);
3015     __ bind(skip);
3016     __ leave();
3017     __ ret(0);
3018 
3019     return start;
3020   }
3021 
3022   void generate_arraycopy_stubs() {
3023     address entry;
3024     address entry_jbyte_arraycopy;
3025     address entry_jshort_arraycopy;
3026     address entry_jint_arraycopy;
3027     address entry_oop_arraycopy;
3028     address entry_jlong_arraycopy;
3029     address entry_checkcast_arraycopy;
3030 
3031     StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &amp;entry,
3032                                                                            &quot;jbyte_disjoint_arraycopy&quot;);
3033     StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &amp;entry_jbyte_arraycopy,
3034                                                                            &quot;jbyte_arraycopy&quot;);
3035 
3036     StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &amp;entry,
3037                                                                             &quot;jshort_disjoint_arraycopy&quot;);
3038     StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &amp;entry_jshort_arraycopy,
3039                                                                             &quot;jshort_arraycopy&quot;);
3040 
3041     StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &amp;entry,
3042                                                                               &quot;jint_disjoint_arraycopy&quot;);
3043     StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,
3044                                                                               &amp;entry_jint_arraycopy, &quot;jint_arraycopy&quot;);
3045 
3046     StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &amp;entry,
3047                                                                                &quot;jlong_disjoint_arraycopy&quot;);
3048     StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,
3049                                                                                &amp;entry_jlong_arraycopy, &quot;jlong_arraycopy&quot;);
3050 
3051 
3052     if (UseCompressedOops) {
3053       StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &amp;entry,
3054                                                                               &quot;oop_disjoint_arraycopy&quot;);
3055       StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,
3056                                                                               &amp;entry_oop_arraycopy, &quot;oop_arraycopy&quot;);
3057       StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &amp;entry,
3058                                                                                      &quot;oop_disjoint_arraycopy_uninit&quot;,
3059                                                                                      /*dest_uninitialized*/true);
3060       StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,
3061                                                                                      NULL, &quot;oop_arraycopy_uninit&quot;,
3062                                                                                      /*dest_uninitialized*/true);
3063     } else {
3064       StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &amp;entry,
3065                                                                                &quot;oop_disjoint_arraycopy&quot;);
3066       StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,
3067                                                                                &amp;entry_oop_arraycopy, &quot;oop_arraycopy&quot;);
3068       StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &amp;entry,
3069                                                                                       &quot;oop_disjoint_arraycopy_uninit&quot;,
3070                                                                                       /*dest_uninitialized*/true);
3071       StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,
3072                                                                                       NULL, &quot;oop_arraycopy_uninit&quot;,
3073                                                                                       /*dest_uninitialized*/true);
3074     }
3075 
3076     StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(&quot;checkcast_arraycopy&quot;, &amp;entry_checkcast_arraycopy);
3077     StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(&quot;checkcast_arraycopy_uninit&quot;, NULL,
3078                                                                         /*dest_uninitialized*/true);
3079 
3080     StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(&quot;unsafe_arraycopy&quot;,
3081                                                               entry_jbyte_arraycopy,
3082                                                               entry_jshort_arraycopy,
3083                                                               entry_jint_arraycopy,
3084                                                               entry_jlong_arraycopy);
3085     StubRoutines::_generic_arraycopy   = generate_generic_copy(&quot;generic_arraycopy&quot;,
3086                                                                entry_jbyte_arraycopy,
3087                                                                entry_jshort_arraycopy,
3088                                                                entry_jint_arraycopy,
3089                                                                entry_oop_arraycopy,
3090                                                                entry_jlong_arraycopy,
3091                                                                entry_checkcast_arraycopy);
3092 
3093     StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, &quot;jbyte_fill&quot;);
3094     StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, &quot;jshort_fill&quot;);
3095     StubRoutines::_jint_fill = generate_fill(T_INT, false, &quot;jint_fill&quot;);
3096     StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, &quot;arrayof_jbyte_fill&quot;);
3097     StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, &quot;arrayof_jshort_fill&quot;);
3098     StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, &quot;arrayof_jint_fill&quot;);
3099 
3100     // We don&#39;t generate specialized code for HeapWord-aligned source
3101     // arrays, so just use the code we&#39;ve already generated
3102     StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;
3103     StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;
3104 
3105     StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;
3106     StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;
3107 
3108     StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;
3109     StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;
3110 
3111     StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;
3112     StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;
3113 
3114     StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;
3115     StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;
3116 
3117     StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;
3118     StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;
3119   }
3120 
3121   // AES intrinsic stubs
3122   enum {AESBlockSize = 16};
3123 
3124   address generate_key_shuffle_mask() {
3125     __ align(16);
3126     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;key_shuffle_mask&quot;);
3127     address start = __ pc();
3128     __ emit_data64( 0x0405060700010203, relocInfo::none );
3129     __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );
3130     return start;
3131   }
3132 
3133   address generate_counter_shuffle_mask() {
3134     __ align(16);
3135     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counter_shuffle_mask&quot;);
3136     address start = __ pc();
3137     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
3138     __ emit_data64(0x0001020304050607, relocInfo::none);
3139     return start;
3140   }
3141 
3142   // Utility routine for loading a 128-bit key word in little endian format
3143   // can optionally specify that the shuffle mask is already in an xmmregister
3144   void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask=NULL) {
3145     __ movdqu(xmmdst, Address(key, offset));
3146     if (xmm_shuf_mask != NULL) {
3147       __ pshufb(xmmdst, xmm_shuf_mask);
3148     } else {
3149       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3150     }
3151   }
3152 
3153   // Utility routine for increase 128bit counter (iv in CTR mode)
3154   void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label&amp; next_block) {
3155     __ pextrq(reg, xmmdst, 0x0);
3156     __ addq(reg, inc_delta);
3157     __ pinsrq(xmmdst, reg, 0x0);
3158     __ jcc(Assembler::carryClear, next_block); // jump if no carry
3159     __ pextrq(reg, xmmdst, 0x01); // Carry
3160     __ addq(reg, 0x01);
3161     __ pinsrq(xmmdst, reg, 0x01); //Carry end
3162     __ BIND(next_block);          // next instruction
3163   }
3164 
3165   // Arguments:
3166   //
3167   // Inputs:
3168   //   c_rarg0   - source byte array address
3169   //   c_rarg1   - destination byte array address
3170   //   c_rarg2   - K (key) in little endian int array
3171   //
3172   address generate_aescrypt_encryptBlock() {
3173     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3174     __ align(CodeEntryAlignment);
3175     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_encryptBlock&quot;);
3176     Label L_doLast;
3177     address start = __ pc();
3178 
3179     const Register from        = c_rarg0;  // source array address
3180     const Register to          = c_rarg1;  // destination array address
3181     const Register key         = c_rarg2;  // key array address
3182     const Register keylen      = rax;
3183 
3184     const XMMRegister xmm_result = xmm0;
3185     const XMMRegister xmm_key_shuf_mask = xmm1;
3186     // On win64 xmm6-xmm15 must be preserved so don&#39;t use them.
3187     const XMMRegister xmm_temp1  = xmm2;
3188     const XMMRegister xmm_temp2  = xmm3;
3189     const XMMRegister xmm_temp3  = xmm4;
3190     const XMMRegister xmm_temp4  = xmm5;
3191 
3192     __ enter(); // required for proper stackwalking of RuntimeStub frame
3193 
3194     // keylen could be only {11, 13, 15} * 4 = {44, 52, 60}
3195     __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3196 
3197     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3198     __ movdqu(xmm_result, Address(from, 0));  // get 16 bytes of input
3199 
3200     // For encryption, the java expanded key ordering is just what we need
3201     // we don&#39;t know if the key is aligned, hence not using load-execute form
3202 
3203     load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);
3204     __ pxor(xmm_result, xmm_temp1);
3205 
3206     load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);
3207     load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);
3208     load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);
3209     load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);
3210 
3211     __ aesenc(xmm_result, xmm_temp1);
3212     __ aesenc(xmm_result, xmm_temp2);
3213     __ aesenc(xmm_result, xmm_temp3);
3214     __ aesenc(xmm_result, xmm_temp4);
3215 
3216     load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);
3217     load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);
3218     load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);
3219     load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);
3220 
3221     __ aesenc(xmm_result, xmm_temp1);
3222     __ aesenc(xmm_result, xmm_temp2);
3223     __ aesenc(xmm_result, xmm_temp3);
3224     __ aesenc(xmm_result, xmm_temp4);
3225 
3226     load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);
3227     load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);
3228 
3229     __ cmpl(keylen, 44);
3230     __ jccb(Assembler::equal, L_doLast);
3231 
3232     __ aesenc(xmm_result, xmm_temp1);
3233     __ aesenc(xmm_result, xmm_temp2);
3234 
3235     load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);
3236     load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);
3237 
3238     __ cmpl(keylen, 52);
3239     __ jccb(Assembler::equal, L_doLast);
3240 
3241     __ aesenc(xmm_result, xmm_temp1);
3242     __ aesenc(xmm_result, xmm_temp2);
3243 
3244     load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);
3245     load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);
3246 
3247     __ BIND(L_doLast);
3248     __ aesenc(xmm_result, xmm_temp1);
3249     __ aesenclast(xmm_result, xmm_temp2);
3250     __ movdqu(Address(to, 0), xmm_result);        // store the result
3251     __ xorptr(rax, rax); // return 0
3252     __ leave(); // required for proper stackwalking of RuntimeStub frame
3253     __ ret(0);
3254 
3255     return start;
3256   }
3257 
3258 
3259   // Arguments:
3260   //
3261   // Inputs:
3262   //   c_rarg0   - source byte array address
3263   //   c_rarg1   - destination byte array address
3264   //   c_rarg2   - K (key) in little endian int array
3265   //
3266   address generate_aescrypt_decryptBlock() {
3267     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3268     __ align(CodeEntryAlignment);
3269     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;aescrypt_decryptBlock&quot;);
3270     Label L_doLast;
3271     address start = __ pc();
3272 
3273     const Register from        = c_rarg0;  // source array address
3274     const Register to          = c_rarg1;  // destination array address
3275     const Register key         = c_rarg2;  // key array address
3276     const Register keylen      = rax;
3277 
3278     const XMMRegister xmm_result = xmm0;
3279     const XMMRegister xmm_key_shuf_mask = xmm1;
3280     // On win64 xmm6-xmm15 must be preserved so don&#39;t use them.
3281     const XMMRegister xmm_temp1  = xmm2;
3282     const XMMRegister xmm_temp2  = xmm3;
3283     const XMMRegister xmm_temp3  = xmm4;
3284     const XMMRegister xmm_temp4  = xmm5;
3285 
3286     __ enter(); // required for proper stackwalking of RuntimeStub frame
3287 
3288     // keylen could be only {11, 13, 15} * 4 = {44, 52, 60}
3289     __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3290 
3291     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3292     __ movdqu(xmm_result, Address(from, 0));
3293 
3294     // for decryption java expanded key ordering is rotated one position from what we want
3295     // so we start from 0x10 here and hit 0x00 last
3296     // we don&#39;t know if the key is aligned, hence not using load-execute form
3297     load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);
3298     load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);
3299     load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);
3300     load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);
3301 
3302     __ pxor  (xmm_result, xmm_temp1);
3303     __ aesdec(xmm_result, xmm_temp2);
3304     __ aesdec(xmm_result, xmm_temp3);
3305     __ aesdec(xmm_result, xmm_temp4);
3306 
3307     load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);
3308     load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);
3309     load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);
3310     load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);
3311 
3312     __ aesdec(xmm_result, xmm_temp1);
3313     __ aesdec(xmm_result, xmm_temp2);
3314     __ aesdec(xmm_result, xmm_temp3);
3315     __ aesdec(xmm_result, xmm_temp4);
3316 
3317     load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);
3318     load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);
3319     load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);
3320 
3321     __ cmpl(keylen, 44);
3322     __ jccb(Assembler::equal, L_doLast);
3323 
3324     __ aesdec(xmm_result, xmm_temp1);
3325     __ aesdec(xmm_result, xmm_temp2);
3326 
3327     load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);
3328     load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);
3329 
3330     __ cmpl(keylen, 52);
3331     __ jccb(Assembler::equal, L_doLast);
3332 
3333     __ aesdec(xmm_result, xmm_temp1);
3334     __ aesdec(xmm_result, xmm_temp2);
3335 
3336     load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);
3337     load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);
3338 
3339     __ BIND(L_doLast);
3340     __ aesdec(xmm_result, xmm_temp1);
3341     __ aesdec(xmm_result, xmm_temp2);
3342 
3343     // for decryption the aesdeclast operation is always on key+0x00
3344     __ aesdeclast(xmm_result, xmm_temp3);
3345     __ movdqu(Address(to, 0), xmm_result);  // store the result
3346     __ xorptr(rax, rax); // return 0
3347     __ leave(); // required for proper stackwalking of RuntimeStub frame
3348     __ ret(0);
3349 
3350     return start;
3351   }
3352 
3353 
3354   // Arguments:
3355   //
3356   // Inputs:
3357   //   c_rarg0   - source byte array address
3358   //   c_rarg1   - destination byte array address
3359   //   c_rarg2   - K (key) in little endian int array
3360   //   c_rarg3   - r vector byte array address
3361   //   c_rarg4   - input length
3362   //
3363   // Output:
3364   //   rax       - input length
3365   //
3366   address generate_cipherBlockChaining_encryptAESCrypt() {
3367     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3368     __ align(CodeEntryAlignment);
3369     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_encryptAESCrypt&quot;);
3370     address start = __ pc();
3371 
3372     Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;
3373     const Register from        = c_rarg0;  // source array address
3374     const Register to          = c_rarg1;  // destination array address
3375     const Register key         = c_rarg2;  // key array address
3376     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
3377                                            // and left with the results of the last encryption block
3378 #ifndef _WIN64
3379     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
3380 #else
3381     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
3382     const Register len_reg     = r11;      // pick the volatile windows register
3383 #endif
3384     const Register pos         = rax;
3385 
3386     // xmm register assignments for the loops below
3387     const XMMRegister xmm_result = xmm0;
3388     const XMMRegister xmm_temp   = xmm1;
3389     // keys 0-10 preloaded into xmm2-xmm12
3390     const int XMM_REG_NUM_KEY_FIRST = 2;
3391     const int XMM_REG_NUM_KEY_LAST  = 15;
3392     const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);
3393     const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);
3394     const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);
3395     const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);
3396     const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);
3397 
3398     __ enter(); // required for proper stackwalking of RuntimeStub frame
3399 
3400 #ifdef _WIN64
3401     // on win64, fill len_reg from stack position
3402     __ movl(len_reg, len_mem);
3403 #else
3404     __ push(len_reg); // Save
3405 #endif
3406 
3407     const XMMRegister xmm_key_shuf_mask = xmm_temp;  // used temporarily to swap key bytes up front
3408     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3409     // load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0
3410     for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum &lt;= XMM_REG_NUM_KEY_FIRST+10; rnum++) {
3411       load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);
3412       offset += 0x10;
3413     }
3414     __ movdqu(xmm_result, Address(rvec, 0x00));   // initialize xmm_result with r vec
3415 
3416     // now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))
3417     __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3418     __ cmpl(rax, 44);
3419     __ jcc(Assembler::notEqual, L_key_192_256);
3420 
3421     // 128 bit code follows here
3422     __ movptr(pos, 0);
3423     __ align(OptoLoopAlignment);
3424 
3425     __ BIND(L_loopTop_128);
3426     __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   // get next 16 bytes of input
3427     __ pxor  (xmm_result, xmm_temp);               // xor with the current r vector
3428     __ pxor  (xmm_result, xmm_key0);               // do the aes rounds
3429     for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum &lt;= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {
3430       __ aesenc(xmm_result, as_XMMRegister(rnum));
3431     }
3432     __ aesenclast(xmm_result, xmm_key10);
3433     __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     // store into the next 16 bytes of output
3434     // no need to store r to memory until we exit
3435     __ addptr(pos, AESBlockSize);
3436     __ subptr(len_reg, AESBlockSize);
3437     __ jcc(Assembler::notEqual, L_loopTop_128);
3438 
3439     __ BIND(L_exit);
3440     __ movdqu(Address(rvec, 0), xmm_result);     // final value of r stored in rvec of CipherBlockChaining object
3441 
3442 #ifdef _WIN64
3443     __ movl(rax, len_mem);
3444 #else
3445     __ pop(rax); // return length
3446 #endif
3447     __ leave(); // required for proper stackwalking of RuntimeStub frame
3448     __ ret(0);
3449 
3450     __ BIND(L_key_192_256);
3451     // here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)
3452     load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);
3453     load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);
3454     __ cmpl(rax, 52);
3455     __ jcc(Assembler::notEqual, L_key_256);
3456 
3457     // 192-bit code follows here (could be changed to use more xmm registers)
3458     __ movptr(pos, 0);
3459     __ align(OptoLoopAlignment);
3460 
3461     __ BIND(L_loopTop_192);
3462     __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   // get next 16 bytes of input
3463     __ pxor  (xmm_result, xmm_temp);               // xor with the current r vector
3464     __ pxor  (xmm_result, xmm_key0);               // do the aes rounds
3465     for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  &lt;= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {
3466       __ aesenc(xmm_result, as_XMMRegister(rnum));
3467     }
3468     __ aesenclast(xmm_result, xmm_key12);
3469     __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     // store into the next 16 bytes of output
3470     // no need to store r to memory until we exit
3471     __ addptr(pos, AESBlockSize);
3472     __ subptr(len_reg, AESBlockSize);
3473     __ jcc(Assembler::notEqual, L_loopTop_192);
3474     __ jmp(L_exit);
3475 
3476     __ BIND(L_key_256);
3477     // 256-bit code follows here (could be changed to use more xmm registers)
3478     load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);
3479     __ movptr(pos, 0);
3480     __ align(OptoLoopAlignment);
3481 
3482     __ BIND(L_loopTop_256);
3483     __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   // get next 16 bytes of input
3484     __ pxor  (xmm_result, xmm_temp);               // xor with the current r vector
3485     __ pxor  (xmm_result, xmm_key0);               // do the aes rounds
3486     for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  &lt;= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {
3487       __ aesenc(xmm_result, as_XMMRegister(rnum));
3488     }
3489     load_key(xmm_temp, key, 0xe0);
3490     __ aesenclast(xmm_result, xmm_temp);
3491     __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     // store into the next 16 bytes of output
3492     // no need to store r to memory until we exit
3493     __ addptr(pos, AESBlockSize);
3494     __ subptr(len_reg, AESBlockSize);
3495     __ jcc(Assembler::notEqual, L_loopTop_256);
3496     __ jmp(L_exit);
3497 
3498     return start;
3499   }
3500 
3501   // Safefetch stubs.
3502   void generate_safefetch(const char* name, int size, address* entry,
3503                           address* fault_pc, address* continuation_pc) {
3504     // safefetch signatures:
3505     //   int      SafeFetch32(int*      adr, int      errValue);
3506     //   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);
3507     //
3508     // arguments:
3509     //   c_rarg0 = adr
3510     //   c_rarg1 = errValue
3511     //
3512     // result:
3513     //   PPC_RET  = *adr or errValue
3514 
3515     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3516 
3517     // Entry point, pc or function descriptor.
3518     *entry = __ pc();
3519 
3520     // Load *adr into c_rarg1, may fault.
3521     *fault_pc = __ pc();
3522     switch (size) {
3523       case 4:
3524         // int32_t
3525         __ movl(c_rarg1, Address(c_rarg0, 0));
3526         break;
3527       case 8:
3528         // int64_t
3529         __ movq(c_rarg1, Address(c_rarg0, 0));
3530         break;
3531       default:
3532         ShouldNotReachHere();
3533     }
3534 
3535     // return errValue or *adr
3536     *continuation_pc = __ pc();
3537     __ movq(rax, c_rarg1);
3538     __ ret(0);
3539   }
3540 
3541   // This is a version of CBC/AES Decrypt which does 4 blocks in a loop at a time
3542   // to hide instruction latency
3543   //
3544   // Arguments:
3545   //
3546   // Inputs:
3547   //   c_rarg0   - source byte array address
3548   //   c_rarg1   - destination byte array address
3549   //   c_rarg2   - K (key) in little endian int array
3550   //   c_rarg3   - r vector byte array address
3551   //   c_rarg4   - input length
3552   //
3553   // Output:
3554   //   rax       - input length
3555   //
3556   address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {
3557     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
3558     __ align(CodeEntryAlignment);
3559     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
3560     address start = __ pc();
3561 
3562     const Register from        = c_rarg0;  // source array address
3563     const Register to          = c_rarg1;  // destination array address
3564     const Register key         = c_rarg2;  // key array address
3565     const Register rvec        = c_rarg3;  // r byte array initialized from initvector array address
3566                                            // and left with the results of the last encryption block
3567 #ifndef _WIN64
3568     const Register len_reg     = c_rarg4;  // src len (must be multiple of blocksize 16)
3569 #else
3570     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
3571     const Register len_reg     = r11;      // pick the volatile windows register
3572 #endif
3573     const Register pos         = rax;
3574 
3575     const int PARALLEL_FACTOR = 4;
3576     const int ROUNDS[3] = { 10, 12, 14 }; // aes rounds for key128, key192, key256
3577 
3578     Label L_exit;
3579     Label L_singleBlock_loopTopHead[3]; // 128, 192, 256
3580     Label L_singleBlock_loopTopHead2[3]; // 128, 192, 256
3581     Label L_singleBlock_loopTop[3]; // 128, 192, 256
3582     Label L_multiBlock_loopTopHead[3]; // 128, 192, 256
3583     Label L_multiBlock_loopTop[3]; // 128, 192, 256
3584 
3585     // keys 0-10 preloaded into xmm5-xmm15
3586     const int XMM_REG_NUM_KEY_FIRST = 5;
3587     const int XMM_REG_NUM_KEY_LAST  = 15;
3588     const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);
3589     const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);
3590 
3591     __ enter(); // required for proper stackwalking of RuntimeStub frame
3592 
3593 #ifdef _WIN64
3594     // on win64, fill len_reg from stack position
3595     __ movl(len_reg, len_mem);
3596 #else
3597     __ push(len_reg); // Save
3598 #endif
3599     __ push(rbx);
3600     // the java expanded key ordering is rotated one position from what we want
3601     // so we start from 0x10 here and hit 0x00 last
3602     const XMMRegister xmm_key_shuf_mask = xmm1;  // used temporarily to swap key bytes up front
3603     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
3604     // load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00
3605     for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum &lt; XMM_REG_NUM_KEY_LAST; rnum++) {
3606       load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);
3607       offset += 0x10;
3608     }
3609     load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);
3610 
3611     const XMMRegister xmm_prev_block_cipher = xmm1;  // holds cipher of previous block
3612 
3613     // registers holding the four results in the parallelized loop
3614     const XMMRegister xmm_result0 = xmm0;
3615     const XMMRegister xmm_result1 = xmm2;
3616     const XMMRegister xmm_result2 = xmm3;
3617     const XMMRegister xmm_result3 = xmm4;
3618 
3619     __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   // initialize with initial rvec
3620 
3621     __ xorptr(pos, pos);
3622 
3623     // now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))
3624     __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
3625     __ cmpl(rbx, 52);
3626     __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);
3627     __ cmpl(rbx, 60);
3628     __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);
3629 
3630 #define DoFour(opc, src_reg)           \
3631   __ opc(xmm_result0, src_reg);         \
3632   __ opc(xmm_result1, src_reg);         \
3633   __ opc(xmm_result2, src_reg);         \
3634   __ opc(xmm_result3, src_reg);         \
3635 
3636     for (int k = 0; k &lt; 3; ++k) {
3637       __ BIND(L_multiBlock_loopTopHead[k]);
3638       if (k != 0) {
3639         __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // see if at least 4 blocks left
3640         __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);
3641       }
3642       if (k == 1) {
3643         __ subptr(rsp, 6 * wordSize);
3644         __ movdqu(Address(rsp, 0), xmm15); //save last_key from xmm15
3645         load_key(xmm15, key, 0xb0); // 0xb0; 192-bit key goes up to 0xc0
3646         __ movdqu(Address(rsp, 2 * wordSize), xmm15);
3647         load_key(xmm1, key, 0xc0);  // 0xc0;
3648         __ movdqu(Address(rsp, 4 * wordSize), xmm1);
3649       } else if (k == 2) {
3650         __ subptr(rsp, 10 * wordSize);
3651         __ movdqu(Address(rsp, 0), xmm15); //save last_key from xmm15
3652         load_key(xmm15, key, 0xd0); // 0xd0; 256-bit key goes upto 0xe0
3653         __ movdqu(Address(rsp, 6 * wordSize), xmm15);
3654         load_key(xmm1, key, 0xe0);  // 0xe0;
3655         __ movdqu(Address(rsp, 8 * wordSize), xmm1);
3656         load_key(xmm15, key, 0xb0); // 0xb0;
3657         __ movdqu(Address(rsp, 2 * wordSize), xmm15);
3658         load_key(xmm1, key, 0xc0);  // 0xc0;
3659         __ movdqu(Address(rsp, 4 * wordSize), xmm1);
3660       }
3661       __ align(OptoLoopAlignment);
3662       __ BIND(L_multiBlock_loopTop[k]);
3663       __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // see if at least 4 blocks left
3664       __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);
3665 
3666       if  (k != 0) {
3667         __ movdqu(xmm15, Address(rsp, 2 * wordSize));
3668         __ movdqu(xmm1, Address(rsp, 4 * wordSize));
3669       }
3670 
3671       __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); // get next 4 blocks into xmmresult registers
3672       __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));
3673       __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));
3674       __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));
3675 
3676       DoFour(pxor, xmm_key_first);
3677       if (k == 0) {
3678         for (int rnum = 1; rnum &lt; ROUNDS[k]; rnum++) {
3679           DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3680         }
3681         DoFour(aesdeclast, xmm_key_last);
3682       } else if (k == 1) {
3683         for (int rnum = 1; rnum &lt;= ROUNDS[k]-2; rnum++) {
3684           DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3685         }
3686         __ movdqu(xmm_key_last, Address(rsp, 0)); // xmm15 needs to be loaded again.
3687         DoFour(aesdec, xmm1);  // key : 0xc0
3688         __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  // xmm1 needs to be loaded again
3689         DoFour(aesdeclast, xmm_key_last);
3690       } else if (k == 2) {
3691         for (int rnum = 1; rnum &lt;= ROUNDS[k] - 4; rnum++) {
3692           DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3693         }
3694         DoFour(aesdec, xmm1);  // key : 0xc0
3695         __ movdqu(xmm15, Address(rsp, 6 * wordSize));
3696         __ movdqu(xmm1, Address(rsp, 8 * wordSize));
3697         DoFour(aesdec, xmm15);  // key : 0xd0
3698         __ movdqu(xmm_key_last, Address(rsp, 0)); // xmm15 needs to be loaded again.
3699         DoFour(aesdec, xmm1);  // key : 0xe0
3700         __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  // xmm1 needs to be loaded again
3701         DoFour(aesdeclast, xmm_key_last);
3702       }
3703 
3704       // for each result, xor with the r vector of previous cipher block
3705       __ pxor(xmm_result0, xmm_prev_block_cipher);
3706       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));
3707       __ pxor(xmm_result1, xmm_prev_block_cipher);
3708       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));
3709       __ pxor(xmm_result2, xmm_prev_block_cipher);
3710       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));
3711       __ pxor(xmm_result3, xmm_prev_block_cipher);
3712       __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   // this will carry over to next set of blocks
3713       if (k != 0) {
3714         __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);
3715       }
3716 
3717       __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     // store 4 results into the next 64 bytes of output
3718       __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);
3719       __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);
3720       __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);
3721 
3722       __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);
3723       __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);
3724       __ jmp(L_multiBlock_loopTop[k]);
3725 
3726       // registers used in the non-parallelized loops
3727       // xmm register assignments for the loops below
3728       const XMMRegister xmm_result = xmm0;
3729       const XMMRegister xmm_prev_block_cipher_save = xmm2;
3730       const XMMRegister xmm_key11 = xmm3;
3731       const XMMRegister xmm_key12 = xmm4;
3732       const XMMRegister key_tmp = xmm4;
3733 
3734       __ BIND(L_singleBlock_loopTopHead[k]);
3735       if (k == 1) {
3736         __ addptr(rsp, 6 * wordSize);
3737       } else if (k == 2) {
3738         __ addptr(rsp, 10 * wordSize);
3739       }
3740       __ cmpptr(len_reg, 0); // any blocks left??
3741       __ jcc(Assembler::equal, L_exit);
3742       __ BIND(L_singleBlock_loopTopHead2[k]);
3743       if (k == 1) {
3744         load_key(xmm_key11, key, 0xb0); // 0xb0; 192-bit key goes upto 0xc0
3745         load_key(xmm_key12, key, 0xc0); // 0xc0; 192-bit key goes upto 0xc0
3746       }
3747       if (k == 2) {
3748         load_key(xmm_key11, key, 0xb0); // 0xb0; 256-bit key goes upto 0xe0
3749       }
3750       __ align(OptoLoopAlignment);
3751       __ BIND(L_singleBlock_loopTop[k]);
3752       __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); // get next 16 bytes of cipher input
3753       __ movdqa(xmm_prev_block_cipher_save, xmm_result); // save for next r vector
3754       __ pxor(xmm_result, xmm_key_first); // do the aes dec rounds
3755       for (int rnum = 1; rnum &lt;= 9 ; rnum++) {
3756           __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));
3757       }
3758       if (k == 1) {
3759         __ aesdec(xmm_result, xmm_key11);
3760         __ aesdec(xmm_result, xmm_key12);
3761       }
3762       if (k == 2) {
3763         __ aesdec(xmm_result, xmm_key11);
3764         load_key(key_tmp, key, 0xc0);
3765         __ aesdec(xmm_result, key_tmp);
3766         load_key(key_tmp, key, 0xd0);
3767         __ aesdec(xmm_result, key_tmp);
3768         load_key(key_tmp, key, 0xe0);
3769         __ aesdec(xmm_result, key_tmp);
3770       }
3771 
3772       __ aesdeclast(xmm_result, xmm_key_last); // xmm15 always came from key+0
3773       __ pxor(xmm_result, xmm_prev_block_cipher); // xor with the current r vector
3774       __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); // store into the next 16 bytes of output
3775       // no need to store r to memory until we exit
3776       __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); // set up next r vector with cipher input from this block
3777       __ addptr(pos, AESBlockSize);
3778       __ subptr(len_reg, AESBlockSize);
3779       __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);
3780       if (k != 2) {
3781         __ jmp(L_exit);
3782       }
3783     } //for 128/192/256
3784 
3785     __ BIND(L_exit);
3786     __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     // final value of r stored in rvec of CipherBlockChaining object
3787     __ pop(rbx);
3788 #ifdef _WIN64
3789     __ movl(rax, len_mem);
3790 #else
3791     __ pop(rax); // return length
3792 #endif
3793     __ leave(); // required for proper stackwalking of RuntimeStub frame
3794     __ ret(0);
3795     return start;
3796 }
3797 
3798   address generate_electronicCodeBook_encryptAESCrypt() {
3799     __ align(CodeEntryAlignment);
3800     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;electronicCodeBook_encryptAESCrypt&quot;);
3801     address start = __ pc();
3802     const Register from = c_rarg0;  // source array address
3803     const Register to = c_rarg1;  // destination array address
3804     const Register key = c_rarg2;  // key array address
3805     const Register len = c_rarg3;  // src len (must be multiple of blocksize 16)
3806     __ enter(); // required for proper stackwalking of RuntimeStub frame
3807     __ aesecb_encrypt(from, to, key, len);
3808     __ leave(); // required for proper stackwalking of RuntimeStub frame
3809     __ ret(0);
3810     return start;
3811  }
3812 
3813   address generate_electronicCodeBook_decryptAESCrypt() {
3814     __ align(CodeEntryAlignment);
3815     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;electronicCodeBook_decryptAESCrypt&quot;);
3816     address start = __ pc();
3817     const Register from = c_rarg0;  // source array address
3818     const Register to = c_rarg1;  // destination array address
3819     const Register key = c_rarg2;  // key array address
3820     const Register len = c_rarg3;  // src len (must be multiple of blocksize 16)
3821     __ enter(); // required for proper stackwalking of RuntimeStub frame
3822     __ aesecb_decrypt(from, to, key, len);
3823     __ leave(); // required for proper stackwalking of RuntimeStub frame
3824     __ ret(0);
3825     return start;
3826   }
3827 
3828   address generate_upper_word_mask() {
3829     __ align(64);
3830     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;upper_word_mask&quot;);
3831     address start = __ pc();
3832     __ emit_data64(0x0000000000000000, relocInfo::none);
3833     __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);
3834     return start;
3835   }
3836 
3837   address generate_shuffle_byte_flip_mask() {
3838     __ align(64);
3839     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;shuffle_byte_flip_mask&quot;);
3840     address start = __ pc();
3841     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
3842     __ emit_data64(0x0001020304050607, relocInfo::none);
3843     return start;
3844   }
3845 
3846   // ofs and limit are use for multi-block byte array.
3847   // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
3848   address generate_sha1_implCompress(bool multi_block, const char *name) {
3849     __ align(CodeEntryAlignment);
3850     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3851     address start = __ pc();
3852 
3853     Register buf = c_rarg0;
3854     Register state = c_rarg1;
3855     Register ofs = c_rarg2;
3856     Register limit = c_rarg3;
3857 
3858     const XMMRegister abcd = xmm0;
3859     const XMMRegister e0 = xmm1;
3860     const XMMRegister e1 = xmm2;
3861     const XMMRegister msg0 = xmm3;
3862 
3863     const XMMRegister msg1 = xmm4;
3864     const XMMRegister msg2 = xmm5;
3865     const XMMRegister msg3 = xmm6;
3866     const XMMRegister shuf_mask = xmm7;
3867 
3868     __ enter();
3869 
3870     __ subptr(rsp, 4 * wordSize);
3871 
3872     __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,
3873       buf, state, ofs, limit, rsp, multi_block);
3874 
3875     __ addptr(rsp, 4 * wordSize);
3876 
3877     __ leave();
3878     __ ret(0);
3879     return start;
3880   }
3881 
3882   address generate_pshuffle_byte_flip_mask() {
3883     __ align(64);
3884     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;pshuffle_byte_flip_mask&quot;);
3885     address start = __ pc();
3886     __ emit_data64(0x0405060700010203, relocInfo::none);
3887     __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);
3888 
3889     if (VM_Version::supports_avx2()) {
3890       __ emit_data64(0x0405060700010203, relocInfo::none); // second copy
3891       __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);
3892       // _SHUF_00BA
3893       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3894       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3895       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3896       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3897       // _SHUF_DC00
3898       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3899       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3900       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3901       __ emit_data64(0x0b0a090803020100, relocInfo::none);
3902     }
3903 
3904     return start;
3905   }
3906 
3907   //Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.
3908   address generate_pshuffle_byte_flip_mask_sha512() {
3909     __ align(32);
3910     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;pshuffle_byte_flip_mask_sha512&quot;);
3911     address start = __ pc();
3912     if (VM_Version::supports_avx2()) {
3913       __ emit_data64(0x0001020304050607, relocInfo::none); // PSHUFFLE_BYTE_FLIP_MASK
3914       __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
3915       __ emit_data64(0x1011121314151617, relocInfo::none);
3916       __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);
3917       __ emit_data64(0x0000000000000000, relocInfo::none); //MASK_YMM_LO
3918       __ emit_data64(0x0000000000000000, relocInfo::none);
3919       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3920       __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);
3921     }
3922 
3923     return start;
3924   }
3925 
3926 // ofs and limit are use for multi-block byte array.
3927 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
3928   address generate_sha256_implCompress(bool multi_block, const char *name) {
3929     assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), &quot;&quot;);
3930     __ align(CodeEntryAlignment);
3931     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3932     address start = __ pc();
3933 
3934     Register buf = c_rarg0;
3935     Register state = c_rarg1;
3936     Register ofs = c_rarg2;
3937     Register limit = c_rarg3;
3938 
3939     const XMMRegister msg = xmm0;
3940     const XMMRegister state0 = xmm1;
3941     const XMMRegister state1 = xmm2;
3942     const XMMRegister msgtmp0 = xmm3;
3943 
3944     const XMMRegister msgtmp1 = xmm4;
3945     const XMMRegister msgtmp2 = xmm5;
3946     const XMMRegister msgtmp3 = xmm6;
3947     const XMMRegister msgtmp4 = xmm7;
3948 
3949     const XMMRegister shuf_mask = xmm8;
3950 
3951     __ enter();
3952 
3953     __ subptr(rsp, 4 * wordSize);
3954 
3955     if (VM_Version::supports_sha()) {
3956       __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,
3957         buf, state, ofs, limit, rsp, multi_block, shuf_mask);
3958     } else if (VM_Version::supports_avx2()) {
3959       __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,
3960         buf, state, ofs, limit, rsp, multi_block, shuf_mask);
3961     }
3962     __ addptr(rsp, 4 * wordSize);
3963     __ vzeroupper();
3964     __ leave();
3965     __ ret(0);
3966     return start;
3967   }
3968 
3969   address generate_sha512_implCompress(bool multi_block, const char *name) {
3970     assert(VM_Version::supports_avx2(), &quot;&quot;);
3971     assert(VM_Version::supports_bmi2(), &quot;&quot;);
3972     __ align(CodeEntryAlignment);
3973     StubCodeMark mark(this, &quot;StubRoutines&quot;, name);
3974     address start = __ pc();
3975 
3976     Register buf = c_rarg0;
3977     Register state = c_rarg1;
3978     Register ofs = c_rarg2;
3979     Register limit = c_rarg3;
3980 
3981     const XMMRegister msg = xmm0;
3982     const XMMRegister state0 = xmm1;
3983     const XMMRegister state1 = xmm2;
3984     const XMMRegister msgtmp0 = xmm3;
3985     const XMMRegister msgtmp1 = xmm4;
3986     const XMMRegister msgtmp2 = xmm5;
3987     const XMMRegister msgtmp3 = xmm6;
3988     const XMMRegister msgtmp4 = xmm7;
3989 
3990     const XMMRegister shuf_mask = xmm8;
3991 
3992     __ enter();
3993 
3994     __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,
3995     buf, state, ofs, limit, rsp, multi_block, shuf_mask);
3996 
3997     __ vzeroupper();
3998     __ leave();
3999     __ ret(0);
4000     return start;
4001   }
4002 
4003   // This mask is used for incrementing counter value(linc0, linc4, etc.)
4004   address counter_mask_addr() {
4005     __ align(64);
4006     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counter_mask_addr&quot;);
4007     address start = __ pc();
4008     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);//lbswapmask
4009     __ emit_data64(0x0001020304050607, relocInfo::none);
4010     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
4011     __ emit_data64(0x0001020304050607, relocInfo::none);
4012     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
4013     __ emit_data64(0x0001020304050607, relocInfo::none);
4014     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);
4015     __ emit_data64(0x0001020304050607, relocInfo::none);
4016     __ emit_data64(0x0000000000000000, relocInfo::none);//linc0 = counter_mask_addr+64
4017     __ emit_data64(0x0000000000000000, relocInfo::none);
4018     __ emit_data64(0x0000000000000001, relocInfo::none);//counter_mask_addr() + 80
4019     __ emit_data64(0x0000000000000000, relocInfo::none);
4020     __ emit_data64(0x0000000000000002, relocInfo::none);
4021     __ emit_data64(0x0000000000000000, relocInfo::none);
4022     __ emit_data64(0x0000000000000003, relocInfo::none);
4023     __ emit_data64(0x0000000000000000, relocInfo::none);
4024     __ emit_data64(0x0000000000000004, relocInfo::none);//linc4 = counter_mask_addr() + 128
4025     __ emit_data64(0x0000000000000000, relocInfo::none);
4026     __ emit_data64(0x0000000000000004, relocInfo::none);
4027     __ emit_data64(0x0000000000000000, relocInfo::none);
4028     __ emit_data64(0x0000000000000004, relocInfo::none);
4029     __ emit_data64(0x0000000000000000, relocInfo::none);
4030     __ emit_data64(0x0000000000000004, relocInfo::none);
4031     __ emit_data64(0x0000000000000000, relocInfo::none);
4032     __ emit_data64(0x0000000000000008, relocInfo::none);//linc8 = counter_mask_addr() + 192
4033     __ emit_data64(0x0000000000000000, relocInfo::none);
4034     __ emit_data64(0x0000000000000008, relocInfo::none);
4035     __ emit_data64(0x0000000000000000, relocInfo::none);
4036     __ emit_data64(0x0000000000000008, relocInfo::none);
4037     __ emit_data64(0x0000000000000000, relocInfo::none);
4038     __ emit_data64(0x0000000000000008, relocInfo::none);
4039     __ emit_data64(0x0000000000000000, relocInfo::none);
4040     __ emit_data64(0x0000000000000020, relocInfo::none);//linc32 = counter_mask_addr() + 256
4041     __ emit_data64(0x0000000000000000, relocInfo::none);
4042     __ emit_data64(0x0000000000000020, relocInfo::none);
4043     __ emit_data64(0x0000000000000000, relocInfo::none);
4044     __ emit_data64(0x0000000000000020, relocInfo::none);
4045     __ emit_data64(0x0000000000000000, relocInfo::none);
4046     __ emit_data64(0x0000000000000020, relocInfo::none);
4047     __ emit_data64(0x0000000000000000, relocInfo::none);
4048     __ emit_data64(0x0000000000000010, relocInfo::none);//linc16 = counter_mask_addr() + 320
4049     __ emit_data64(0x0000000000000000, relocInfo::none);
4050     __ emit_data64(0x0000000000000010, relocInfo::none);
4051     __ emit_data64(0x0000000000000000, relocInfo::none);
4052     __ emit_data64(0x0000000000000010, relocInfo::none);
4053     __ emit_data64(0x0000000000000000, relocInfo::none);
4054     __ emit_data64(0x0000000000000010, relocInfo::none);
4055     __ emit_data64(0x0000000000000000, relocInfo::none);
4056     return start;
4057   }
4058 
4059  // Vector AES Counter implementation
4060   address generate_counterMode_VectorAESCrypt()  {
4061     __ align(CodeEntryAlignment);
4062     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counterMode_AESCrypt&quot;);
4063     address start = __ pc();
4064     const Register from = c_rarg0; // source array address
4065     const Register to = c_rarg1; // destination array address
4066     const Register key = c_rarg2; // key array address r8
4067     const Register counter = c_rarg3; // counter byte array initialized from counter array address
4068     // and updated with the incremented counter in the end
4069 #ifndef _WIN64
4070     const Register len_reg = c_rarg4;
4071     const Register saved_encCounter_start = c_rarg5;
4072     const Register used_addr = r10;
4073     const Address  used_mem(rbp, 2 * wordSize);
4074     const Register used = r11;
4075 #else
4076     const Address len_mem(rbp, 6 * wordSize); // length is on stack on Win64
4077     const Address saved_encCounter_mem(rbp, 7 * wordSize); // saved encrypted counter is on stack on Win64
4078     const Address used_mem(rbp, 8 * wordSize); // used length is on stack on Win64
4079     const Register len_reg = r10; // pick the first volatile windows register
4080     const Register saved_encCounter_start = r11;
4081     const Register used_addr = r13;
4082     const Register used = r14;
4083 #endif
4084     __ enter();
4085    // Save state before entering routine
4086     __ push(r12);
4087     __ push(r13);
4088     __ push(r14);
4089     __ push(r15);
4090 #ifdef _WIN64
4091     // on win64, fill len_reg from stack position
4092     __ movl(len_reg, len_mem);
4093     __ movptr(saved_encCounter_start, saved_encCounter_mem);
4094     __ movptr(used_addr, used_mem);
4095     __ movl(used, Address(used_addr, 0));
4096 #else
4097     __ push(len_reg); // Save
4098     __ movptr(used_addr, used_mem);
4099     __ movl(used, Address(used_addr, 0));
4100 #endif
4101     __ push(rbx);
4102     __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);
4103     // Restore state before leaving routine
4104     __ pop(rbx);
4105 #ifdef _WIN64
4106     __ movl(rax, len_mem); // return length
4107 #else
4108     __ pop(rax); // return length
4109 #endif
4110     __ pop(r15);
4111     __ pop(r14);
4112     __ pop(r13);
4113     __ pop(r12);
4114 
4115     __ leave(); // required for proper stackwalking of RuntimeStub frame
4116     __ ret(0);
4117     return start;
4118   }
4119 
4120   // This is a version of CTR/AES crypt which does 6 blocks in a loop at a time
4121   // to hide instruction latency
4122   //
4123   // Arguments:
4124   //
4125   // Inputs:
4126   //   c_rarg0   - source byte array address
4127   //   c_rarg1   - destination byte array address
4128   //   c_rarg2   - K (key) in little endian int array
4129   //   c_rarg3   - counter vector byte array address
4130   //   Linux
4131   //     c_rarg4   -          input length
4132   //     c_rarg5   -          saved encryptedCounter start
4133   //     rbp + 6 * wordSize - saved used length
4134   //   Windows
4135   //     rbp + 6 * wordSize - input length
4136   //     rbp + 7 * wordSize - saved encryptedCounter start
4137   //     rbp + 8 * wordSize - saved used length
4138   //
4139   // Output:
4140   //   rax       - input length
4141   //
4142   address generate_counterMode_AESCrypt_Parallel() {
4143     assert(UseAES, &quot;need AES instructions and misaligned SSE support&quot;);
4144     __ align(CodeEntryAlignment);
4145     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;counterMode_AESCrypt&quot;);
4146     address start = __ pc();
4147     const Register from = c_rarg0; // source array address
4148     const Register to = c_rarg1; // destination array address
4149     const Register key = c_rarg2; // key array address
4150     const Register counter = c_rarg3; // counter byte array initialized from counter array address
4151                                       // and updated with the incremented counter in the end
4152 #ifndef _WIN64
4153     const Register len_reg = c_rarg4;
4154     const Register saved_encCounter_start = c_rarg5;
4155     const Register used_addr = r10;
4156     const Address  used_mem(rbp, 2 * wordSize);
4157     const Register used = r11;
4158 #else
4159     const Address len_mem(rbp, 6 * wordSize); // length is on stack on Win64
4160     const Address saved_encCounter_mem(rbp, 7 * wordSize); // length is on stack on Win64
4161     const Address used_mem(rbp, 8 * wordSize); // length is on stack on Win64
4162     const Register len_reg = r10; // pick the first volatile windows register
4163     const Register saved_encCounter_start = r11;
4164     const Register used_addr = r13;
4165     const Register used = r14;
4166 #endif
4167     const Register pos = rax;
4168 
4169     const int PARALLEL_FACTOR = 6;
4170     const XMMRegister xmm_counter_shuf_mask = xmm0;
4171     const XMMRegister xmm_key_shuf_mask = xmm1; // used temporarily to swap key bytes up front
4172     const XMMRegister xmm_curr_counter = xmm2;
4173 
4174     const XMMRegister xmm_key_tmp0 = xmm3;
4175     const XMMRegister xmm_key_tmp1 = xmm4;
4176 
4177     // registers holding the four results in the parallelized loop
4178     const XMMRegister xmm_result0 = xmm5;
4179     const XMMRegister xmm_result1 = xmm6;
4180     const XMMRegister xmm_result2 = xmm7;
4181     const XMMRegister xmm_result3 = xmm8;
4182     const XMMRegister xmm_result4 = xmm9;
4183     const XMMRegister xmm_result5 = xmm10;
4184 
4185     const XMMRegister xmm_from0 = xmm11;
4186     const XMMRegister xmm_from1 = xmm12;
4187     const XMMRegister xmm_from2 = xmm13;
4188     const XMMRegister xmm_from3 = xmm14; //the last one is xmm14. we have to preserve it on WIN64.
4189     const XMMRegister xmm_from4 = xmm3; //reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text
4190     const XMMRegister xmm_from5 = xmm4;
4191 
4192     //for key_128, key_192, key_256
4193     const int rounds[3] = {10, 12, 14};
4194     Label L_exit_preLoop, L_preLoop_start;
4195     Label L_multiBlock_loopTop[3];
4196     Label L_singleBlockLoopTop[3];
4197     Label L__incCounter[3][6]; //for 6 blocks
4198     Label L__incCounter_single[3]; //for single block, key128, key192, key256
4199     Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];
4200     Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];
4201 
4202     Label L_exit;
4203 
4204     __ enter(); // required for proper stackwalking of RuntimeStub frame
4205 
4206 #ifdef _WIN64
4207     // allocate spill slots for r13, r14
4208     enum {
4209         saved_r13_offset,
4210         saved_r14_offset
4211     };
4212     __ subptr(rsp, 2 * wordSize);
4213     __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);
4214     __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);
4215 
4216     // on win64, fill len_reg from stack position
4217     __ movl(len_reg, len_mem);
4218     __ movptr(saved_encCounter_start, saved_encCounter_mem);
4219     __ movptr(used_addr, used_mem);
4220     __ movl(used, Address(used_addr, 0));
4221 #else
4222     __ push(len_reg); // Save
4223     __ movptr(used_addr, used_mem);
4224     __ movl(used, Address(used_addr, 0));
4225 #endif
4226 
4227     __ push(rbx); // Save RBX
4228     __ movdqu(xmm_curr_counter, Address(counter, 0x00)); // initialize counter with initial counter
4229     __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); // pos as scratch
4230     __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); //counter is shuffled
4231     __ movptr(pos, 0);
4232 
4233     // Use the partially used encrpyted counter from last invocation
4234     __ BIND(L_preLoop_start);
4235     __ cmpptr(used, 16);
4236     __ jcc(Assembler::aboveEqual, L_exit_preLoop);
4237       __ cmpptr(len_reg, 0);
4238       __ jcc(Assembler::lessEqual, L_exit_preLoop);
4239       __ movb(rbx, Address(saved_encCounter_start, used));
4240       __ xorb(rbx, Address(from, pos));
4241       __ movb(Address(to, pos), rbx);
4242       __ addptr(pos, 1);
4243       __ addptr(used, 1);
4244       __ subptr(len_reg, 1);
4245 
4246     __ jmp(L_preLoop_start);
4247 
4248     __ BIND(L_exit_preLoop);
4249     __ movl(Address(used_addr, 0), used);
4250 
4251     // key length could be only {11, 13, 15} * 4 = {44, 52, 60}
4252     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); // rbx as scratch
4253     __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
4254     __ cmpl(rbx, 52);
4255     __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);
4256     __ cmpl(rbx, 60);
4257     __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);
4258 
4259 #define CTR_DoSix(opc, src_reg)                \
4260     __ opc(xmm_result0, src_reg);              \
4261     __ opc(xmm_result1, src_reg);              \
4262     __ opc(xmm_result2, src_reg);              \
4263     __ opc(xmm_result3, src_reg);              \
4264     __ opc(xmm_result4, src_reg);              \
4265     __ opc(xmm_result5, src_reg);
4266 
4267     // k == 0 :  generate code for key_128
4268     // k == 1 :  generate code for key_192
4269     // k == 2 :  generate code for key_256
4270     for (int k = 0; k &lt; 3; ++k) {
4271       //multi blocks starts here
4272       __ align(OptoLoopAlignment);
4273       __ BIND(L_multiBlock_loopTop[k]);
4274       __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // see if at least PARALLEL_FACTOR blocks left
4275       __ jcc(Assembler::less, L_singleBlockLoopTop[k]);
4276       load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);
4277 
4278       //load, then increase counters
4279       CTR_DoSix(movdqa, xmm_curr_counter);
4280       inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);
4281       inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);
4282       inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);
4283       inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);
4284       inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);
4285       inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);
4286       CTR_DoSix(pshufb, xmm_counter_shuf_mask); // after increased, shuffled counters back for PXOR
4287       CTR_DoSix(pxor, xmm_key_tmp0);   //PXOR with Round 0 key
4288 
4289       //load two ROUND_KEYs at a time
4290       for (int i = 1; i &lt; rounds[k]; ) {
4291         load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);
4292         load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);
4293         CTR_DoSix(aesenc, xmm_key_tmp1);
4294         i++;
4295         if (i != rounds[k]) {
4296           CTR_DoSix(aesenc, xmm_key_tmp0);
4297         } else {
4298           CTR_DoSix(aesenclast, xmm_key_tmp0);
4299         }
4300         i++;
4301       }
4302 
4303       // get next PARALLEL_FACTOR blocks into xmm_result registers
4304       __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));
4305       __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));
4306       __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));
4307       __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));
4308       __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));
4309       __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));
4310 
4311       __ pxor(xmm_result0, xmm_from0);
4312       __ pxor(xmm_result1, xmm_from1);
4313       __ pxor(xmm_result2, xmm_from2);
4314       __ pxor(xmm_result3, xmm_from3);
4315       __ pxor(xmm_result4, xmm_from4);
4316       __ pxor(xmm_result5, xmm_from5);
4317 
4318       // store 6 results into the next 64 bytes of output
4319       __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);
4320       __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);
4321       __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);
4322       __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);
4323       __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);
4324       __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);
4325 
4326       __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); // increase the length of crypt text
4327       __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); // decrease the remaining length
4328       __ jmp(L_multiBlock_loopTop[k]);
4329 
4330       // singleBlock starts here
4331       __ align(OptoLoopAlignment);
4332       __ BIND(L_singleBlockLoopTop[k]);
4333       __ cmpptr(len_reg, 0);
4334       __ jcc(Assembler::lessEqual, L_exit);
4335       load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);
4336       __ movdqa(xmm_result0, xmm_curr_counter);
4337       inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);
4338       __ pshufb(xmm_result0, xmm_counter_shuf_mask);
4339       __ pxor(xmm_result0, xmm_key_tmp0);
4340       for (int i = 1; i &lt; rounds[k]; i++) {
4341         load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);
4342         __ aesenc(xmm_result0, xmm_key_tmp0);
4343       }
4344       load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);
4345       __ aesenclast(xmm_result0, xmm_key_tmp0);
4346       __ cmpptr(len_reg, AESBlockSize);
4347       __ jcc(Assembler::less, L_processTail_insr[k]);
4348         __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));
4349         __ pxor(xmm_result0, xmm_from0);
4350         __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);
4351         __ addptr(pos, AESBlockSize);
4352         __ subptr(len_reg, AESBlockSize);
4353         __ jmp(L_singleBlockLoopTop[k]);
4354       __ BIND(L_processTail_insr[k]);                               // Process the tail part of the input array
4355         __ addptr(pos, len_reg);                                    // 1. Insert bytes from src array into xmm_from0 register
4356         __ testptr(len_reg, 8);
4357         __ jcc(Assembler::zero, L_processTail_4_insr[k]);
4358           __ subptr(pos,8);
4359           __ pinsrq(xmm_from0, Address(from, pos), 0);
4360         __ BIND(L_processTail_4_insr[k]);
4361         __ testptr(len_reg, 4);
4362         __ jcc(Assembler::zero, L_processTail_2_insr[k]);
4363           __ subptr(pos,4);
4364           __ pslldq(xmm_from0, 4);
4365           __ pinsrd(xmm_from0, Address(from, pos), 0);
4366         __ BIND(L_processTail_2_insr[k]);
4367         __ testptr(len_reg, 2);
4368         __ jcc(Assembler::zero, L_processTail_1_insr[k]);
4369           __ subptr(pos, 2);
4370           __ pslldq(xmm_from0, 2);
4371           __ pinsrw(xmm_from0, Address(from, pos), 0);
4372         __ BIND(L_processTail_1_insr[k]);
4373         __ testptr(len_reg, 1);
4374         __ jcc(Assembler::zero, L_processTail_exit_insr[k]);
4375           __ subptr(pos, 1);
4376           __ pslldq(xmm_from0, 1);
4377           __ pinsrb(xmm_from0, Address(from, pos), 0);
4378         __ BIND(L_processTail_exit_insr[k]);
4379 
4380         __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  // 2. Perform pxor of the encrypted counter and plaintext Bytes.
4381         __ pxor(xmm_result0, xmm_from0);                             //    Also the encrypted counter is saved for next invocation.
4382 
4383         __ testptr(len_reg, 8);
4384         __ jcc(Assembler::zero, L_processTail_4_extr[k]);            // 3. Extract bytes from xmm_result0 into the dest. array
4385           __ pextrq(Address(to, pos), xmm_result0, 0);
4386           __ psrldq(xmm_result0, 8);
4387           __ addptr(pos, 8);
4388         __ BIND(L_processTail_4_extr[k]);
4389         __ testptr(len_reg, 4);
4390         __ jcc(Assembler::zero, L_processTail_2_extr[k]);
4391           __ pextrd(Address(to, pos), xmm_result0, 0);
4392           __ psrldq(xmm_result0, 4);
4393           __ addptr(pos, 4);
4394         __ BIND(L_processTail_2_extr[k]);
4395         __ testptr(len_reg, 2);
4396         __ jcc(Assembler::zero, L_processTail_1_extr[k]);
4397           __ pextrw(Address(to, pos), xmm_result0, 0);
4398           __ psrldq(xmm_result0, 2);
4399           __ addptr(pos, 2);
4400         __ BIND(L_processTail_1_extr[k]);
4401         __ testptr(len_reg, 1);
4402         __ jcc(Assembler::zero, L_processTail_exit_extr[k]);
4403           __ pextrb(Address(to, pos), xmm_result0, 0);
4404 
4405         __ BIND(L_processTail_exit_extr[k]);
4406         __ movl(Address(used_addr, 0), len_reg);
4407         __ jmp(L_exit);
4408 
4409     }
4410 
4411     __ BIND(L_exit);
4412     __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); //counter is shuffled back.
4413     __ movdqu(Address(counter, 0), xmm_curr_counter); //save counter back
4414     __ pop(rbx); // pop the saved RBX.
4415 #ifdef _WIN64
4416     __ movl(rax, len_mem);
4417     __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));
4418     __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));
4419     __ addptr(rsp, 2 * wordSize);
4420 #else
4421     __ pop(rax); // return &#39;len&#39;
4422 #endif
4423     __ leave(); // required for proper stackwalking of RuntimeStub frame
4424     __ ret(0);
4425     return start;
4426   }
4427 
4428 void roundDec(XMMRegister xmm_reg) {
4429   __ vaesdec(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);
4430   __ vaesdec(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);
4431   __ vaesdec(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4432   __ vaesdec(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4433   __ vaesdec(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4434   __ vaesdec(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4435   __ vaesdec(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4436   __ vaesdec(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4437 }
4438 
4439 void roundDeclast(XMMRegister xmm_reg) {
4440   __ vaesdeclast(xmm1, xmm1, xmm_reg, Assembler::AVX_512bit);
4441   __ vaesdeclast(xmm2, xmm2, xmm_reg, Assembler::AVX_512bit);
4442   __ vaesdeclast(xmm3, xmm3, xmm_reg, Assembler::AVX_512bit);
4443   __ vaesdeclast(xmm4, xmm4, xmm_reg, Assembler::AVX_512bit);
4444   __ vaesdeclast(xmm5, xmm5, xmm_reg, Assembler::AVX_512bit);
4445   __ vaesdeclast(xmm6, xmm6, xmm_reg, Assembler::AVX_512bit);
4446   __ vaesdeclast(xmm7, xmm7, xmm_reg, Assembler::AVX_512bit);
4447   __ vaesdeclast(xmm8, xmm8, xmm_reg, Assembler::AVX_512bit);
4448 }
4449 
4450   void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = NULL) {
4451     __ movdqu(xmmdst, Address(key, offset));
4452     if (xmm_shuf_mask != NULL) {
4453       __ pshufb(xmmdst, xmm_shuf_mask);
4454     } else {
4455       __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4456     }
4457     __ evshufi64x2(xmmdst, xmmdst, xmmdst, 0x0, Assembler::AVX_512bit);
4458 
4459   }
4460 
4461 address generate_cipherBlockChaining_decryptVectorAESCrypt() {
4462     assert(VM_Version::supports_avx512_vaes(), &quot;need AES instructions and misaligned SSE support&quot;);
4463     __ align(CodeEntryAlignment);
4464     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;cipherBlockChaining_decryptAESCrypt&quot;);
4465     address start = __ pc();
4466 
4467     const Register from = c_rarg0;  // source array address
4468     const Register to = c_rarg1;  // destination array address
4469     const Register key = c_rarg2;  // key array address
4470     const Register rvec = c_rarg3;  // r byte array initialized from initvector array address
4471     // and left with the results of the last encryption block
4472 #ifndef _WIN64
4473     const Register len_reg = c_rarg4;  // src len (must be multiple of blocksize 16)
4474 #else
4475     const Address  len_mem(rbp, 6 * wordSize);  // length is on stack on Win64
4476     const Register len_reg = r11;      // pick the volatile windows register
4477 #endif
4478 
4479     Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,
4480           Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;
4481 
4482     __ enter();
4483 
4484 #ifdef _WIN64
4485   // on win64, fill len_reg from stack position
4486     __ movl(len_reg, len_mem);
4487 #else
4488     __ push(len_reg); // Save
4489 #endif
4490     __ push(rbx);
4491     __ vzeroupper();
4492 
4493     // Temporary variable declaration for swapping key bytes
4494     const XMMRegister xmm_key_shuf_mask = xmm1;
4495     __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));
4496 
4497     // Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds
4498     const Register rounds = rbx;
4499     __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));
4500 
4501     const XMMRegister IV = xmm0;
4502     // Load IV and broadcast value to 512-bits
4503     __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);
4504 
4505     // Temporary variables for storing round keys
4506     const XMMRegister RK0 = xmm30;
4507     const XMMRegister RK1 = xmm9;
4508     const XMMRegister RK2 = xmm18;
4509     const XMMRegister RK3 = xmm19;
4510     const XMMRegister RK4 = xmm20;
4511     const XMMRegister RK5 = xmm21;
4512     const XMMRegister RK6 = xmm22;
4513     const XMMRegister RK7 = xmm23;
4514     const XMMRegister RK8 = xmm24;
4515     const XMMRegister RK9 = xmm25;
4516     const XMMRegister RK10 = xmm26;
4517 
4518      // Load and shuffle key
4519     // the java expanded key ordering is rotated one position from what we want
4520     // so we start from 1*16 here and hit 0*16 last
4521     ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);
4522     ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);
4523     ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);
4524     ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);
4525     ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);
4526     ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);
4527     ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);
4528     ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);
4529     ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);
4530     ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);
4531     ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);
4532 
4533     // Variables for storing source cipher text
4534     const XMMRegister S0 = xmm10;
4535     const XMMRegister S1 = xmm11;
4536     const XMMRegister S2 = xmm12;
4537     const XMMRegister S3 = xmm13;
4538     const XMMRegister S4 = xmm14;
4539     const XMMRegister S5 = xmm15;
4540     const XMMRegister S6 = xmm16;
4541     const XMMRegister S7 = xmm17;
4542 
4543     // Variables for storing decrypted text
4544     const XMMRegister B0 = xmm1;
4545     const XMMRegister B1 = xmm2;
4546     const XMMRegister B2 = xmm3;
4547     const XMMRegister B3 = xmm4;
4548     const XMMRegister B4 = xmm5;
4549     const XMMRegister B5 = xmm6;
4550     const XMMRegister B6 = xmm7;
4551     const XMMRegister B7 = xmm8;
4552 
4553     __ cmpl(rounds, 44);
4554     __ jcc(Assembler::greater, KEY_192);
4555     __ jmp(Loop);
4556 
4557     __ BIND(KEY_192);
4558     const XMMRegister RK11 = xmm27;
4559     const XMMRegister RK12 = xmm28;
4560     ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);
4561     ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);
4562 
4563     __ cmpl(rounds, 52);
4564     __ jcc(Assembler::greater, KEY_256);
4565     __ jmp(Loop);
4566 
4567     __ BIND(KEY_256);
4568     const XMMRegister RK13 = xmm29;
4569     const XMMRegister RK14 = xmm31;
4570     ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);
4571     ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);
4572 
4573     __ BIND(Loop);
4574     __ cmpl(len_reg, 512);
4575     __ jcc(Assembler::below, Lcbc_dec_rem);
4576     __ BIND(Loop1);
4577     __ subl(len_reg, 512);
4578     __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);
4579     __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);
4580     __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);
4581     __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);
4582     __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);
4583     __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);
4584     __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);
4585     __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);
4586     __ leaq(from, Address(from, 8 * 64));
4587 
4588     __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);
4589     __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);
4590     __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);
4591     __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);
4592     __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);
4593     __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);
4594     __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);
4595     __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);
4596 
4597     __ evalignq(IV, S0, IV, 0x06);
4598     __ evalignq(S0, S1, S0, 0x06);
4599     __ evalignq(S1, S2, S1, 0x06);
4600     __ evalignq(S2, S3, S2, 0x06);
4601     __ evalignq(S3, S4, S3, 0x06);
4602     __ evalignq(S4, S5, S4, 0x06);
4603     __ evalignq(S5, S6, S5, 0x06);
4604     __ evalignq(S6, S7, S6, 0x06);
4605 
4606     roundDec(RK2);
4607     roundDec(RK3);
4608     roundDec(RK4);
4609     roundDec(RK5);
4610     roundDec(RK6);
4611     roundDec(RK7);
4612     roundDec(RK8);
4613     roundDec(RK9);
4614     roundDec(RK10);
4615 
4616     __ cmpl(rounds, 44);
4617     __ jcc(Assembler::belowEqual, L_128);
4618     roundDec(RK11);
4619     roundDec(RK12);
4620 
4621     __ cmpl(rounds, 52);
4622     __ jcc(Assembler::belowEqual, L_192);
4623     roundDec(RK13);
4624     roundDec(RK14);
4625 
4626     __ BIND(L_256);
4627     roundDeclast(RK0);
4628     __ jmp(Loop2);
4629 
4630     __ BIND(L_128);
4631     roundDeclast(RK0);
4632     __ jmp(Loop2);
4633 
4634     __ BIND(L_192);
4635     roundDeclast(RK0);
4636 
4637     __ BIND(Loop2);
4638     __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);
4639     __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);
4640     __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);
4641     __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);
4642     __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);
4643     __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);
4644     __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);
4645     __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);
4646     __ evmovdquq(IV, S7, Assembler::AVX_512bit);
4647 
4648     __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);
4649     __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);
4650     __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);
4651     __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);
4652     __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);
4653     __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);
4654     __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);
4655     __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);
4656     __ leaq(to, Address(to, 8 * 64));
4657     __ jmp(Loop);
4658 
4659     __ BIND(Lcbc_dec_rem);
4660     __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);
4661 
4662     __ BIND(Lcbc_dec_rem_loop);
4663     __ subl(len_reg, 16);
4664     __ jcc(Assembler::carrySet, Lcbc_dec_ret);
4665 
4666     __ movdqu(S0, Address(from, 0));
4667     __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);
4668     __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);
4669     __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);
4670     __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);
4671     __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);
4672     __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);
4673     __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);
4674     __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);
4675     __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);
4676     __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);
4677     __ cmpl(rounds, 44);
4678     __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);
4679 
4680     __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);
4681     __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);
4682     __ cmpl(rounds, 52);
4683     __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);
4684 
4685     __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);
4686     __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);
4687 
4688     __ BIND(Lcbc_dec_rem_last);
4689     __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);
4690 
4691     __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);
4692     __ evmovdquq(IV, S0, Assembler::AVX_512bit);
4693     __ movdqu(Address(to, 0), B0);
4694     __ leaq(from, Address(from, 16));
4695     __ leaq(to, Address(to, 16));
4696     __ jmp(Lcbc_dec_rem_loop);
4697 
4698     __ BIND(Lcbc_dec_ret);
4699     __ movdqu(Address(rvec, 0), IV);
4700 
4701     // Zero out the round keys
4702     __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);
4703     __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);
4704     __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);
4705     __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);
4706     __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);
4707     __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);
4708     __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);
4709     __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);
4710     __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);
4711     __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);
4712     __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);
4713     __ cmpl(rounds, 44);
4714     __ jcc(Assembler::belowEqual, Lcbc_exit);
4715     __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);
4716     __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);
4717     __ cmpl(rounds, 52);
4718     __ jcc(Assembler::belowEqual, Lcbc_exit);
4719     __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);
4720     __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);
4721 
4722     __ BIND(Lcbc_exit);
4723     __ pop(rbx);
4724 #ifdef _WIN64
4725     __ movl(rax, len_mem);
4726 #else
4727     __ pop(rax); // return length
4728 #endif
4729     __ leave(); // required for proper stackwalking of RuntimeStub frame
4730     __ ret(0);
4731     return start;
4732 }
4733 
4734 // Polynomial x^128+x^127+x^126+x^121+1
4735 address ghash_polynomial_addr() {
4736     __ align(CodeEntryAlignment);
4737     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_ghash_poly_addr&quot;);
4738     address start = __ pc();
4739     __ emit_data64(0x0000000000000001, relocInfo::none);
4740     __ emit_data64(0xc200000000000000, relocInfo::none);
4741     return start;
4742 }
4743 
4744 address ghash_shufflemask_addr() {
4745     __ align(CodeEntryAlignment);
4746     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;_ghash_shuffmask_addr&quot;);
4747     address start = __ pc();
4748     __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);
4749     __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);
4750     return start;
4751 }
4752 
4753 // Ghash single and multi block operations using AVX instructions
4754 address generate_avx_ghash_processBlocks() {
4755     __ align(CodeEntryAlignment);
4756 
4757     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4758     address start = __ pc();
4759 
4760     // arguments
4761     const Register state = c_rarg0;
4762     const Register htbl = c_rarg1;
4763     const Register data = c_rarg2;
4764     const Register blocks = c_rarg3;
4765     __ enter();
4766    // Save state before entering routine
4767     __ avx_ghash(state, htbl, data, blocks);
4768     __ leave(); // required for proper stackwalking of RuntimeStub frame
4769     __ ret(0);
4770     return start;
4771 }
4772 
4773   // byte swap x86 long
4774   address generate_ghash_long_swap_mask() {
4775     __ align(CodeEntryAlignment);
4776     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_long_swap_mask&quot;);
4777     address start = __ pc();
4778     __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );
4779     __ emit_data64(0x0706050403020100, relocInfo::none );
4780   return start;
4781   }
4782 
4783   // byte swap x86 byte array
4784   address generate_ghash_byte_swap_mask() {
4785     __ align(CodeEntryAlignment);
4786     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_byte_swap_mask&quot;);
4787     address start = __ pc();
4788     __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );
4789     __ emit_data64(0x0001020304050607, relocInfo::none );
4790   return start;
4791   }
4792 
4793   /* Single and multi-block ghash operations */
4794   address generate_ghash_processBlocks() {
4795     __ align(CodeEntryAlignment);
4796     Label L_ghash_loop, L_exit;
4797     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;ghash_processBlocks&quot;);
4798     address start = __ pc();
4799 
4800     const Register state        = c_rarg0;
4801     const Register subkeyH      = c_rarg1;
4802     const Register data         = c_rarg2;
4803     const Register blocks       = c_rarg3;
4804 
4805     const XMMRegister xmm_temp0 = xmm0;
4806     const XMMRegister xmm_temp1 = xmm1;
4807     const XMMRegister xmm_temp2 = xmm2;
4808     const XMMRegister xmm_temp3 = xmm3;
4809     const XMMRegister xmm_temp4 = xmm4;
4810     const XMMRegister xmm_temp5 = xmm5;
4811     const XMMRegister xmm_temp6 = xmm6;
4812     const XMMRegister xmm_temp7 = xmm7;
4813     const XMMRegister xmm_temp8 = xmm8;
4814     const XMMRegister xmm_temp9 = xmm9;
4815     const XMMRegister xmm_temp10 = xmm10;
4816 
4817     __ enter();
4818 
4819     __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));
4820 
4821     __ movdqu(xmm_temp0, Address(state, 0));
4822     __ pshufb(xmm_temp0, xmm_temp10);
4823 
4824 
4825     __ BIND(L_ghash_loop);
4826     __ movdqu(xmm_temp2, Address(data, 0));
4827     __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));
4828 
4829     __ movdqu(xmm_temp1, Address(subkeyH, 0));
4830     __ pshufb(xmm_temp1, xmm_temp10);
4831 
4832     __ pxor(xmm_temp0, xmm_temp2);
4833 
4834     //
4835     // Multiply with the hash key
4836     //
4837     __ movdqu(xmm_temp3, xmm_temp0);
4838     __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      // xmm3 holds a0*b0
4839     __ movdqu(xmm_temp4, xmm_temp0);
4840     __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     // xmm4 holds a0*b1
4841 
4842     __ movdqu(xmm_temp5, xmm_temp0);
4843     __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      // xmm5 holds a1*b0
4844     __ movdqu(xmm_temp6, xmm_temp0);
4845     __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     // xmm6 holds a1*b1
4846 
4847     __ pxor(xmm_temp4, xmm_temp5);      // xmm4 holds a0*b1 + a1*b0
4848 
4849     __ movdqu(xmm_temp5, xmm_temp4);    // move the contents of xmm4 to xmm5
4850     __ psrldq(xmm_temp4, 8);    // shift by xmm4 64 bits to the right
4851     __ pslldq(xmm_temp5, 8);    // shift by xmm5 64 bits to the left
4852     __ pxor(xmm_temp3, xmm_temp5);
4853     __ pxor(xmm_temp6, xmm_temp4);      // Register pair &lt;xmm6:xmm3&gt; holds the result
4854                                         // of the carry-less multiplication of
4855                                         // xmm0 by xmm1.
4856 
4857     // We shift the result of the multiplication by one bit position
4858     // to the left to cope for the fact that the bits are reversed.
4859     __ movdqu(xmm_temp7, xmm_temp3);
4860     __ movdqu(xmm_temp8, xmm_temp6);
4861     __ pslld(xmm_temp3, 1);
4862     __ pslld(xmm_temp6, 1);
4863     __ psrld(xmm_temp7, 31);
4864     __ psrld(xmm_temp8, 31);
4865     __ movdqu(xmm_temp9, xmm_temp7);
4866     __ pslldq(xmm_temp8, 4);
4867     __ pslldq(xmm_temp7, 4);
4868     __ psrldq(xmm_temp9, 12);
4869     __ por(xmm_temp3, xmm_temp7);
4870     __ por(xmm_temp6, xmm_temp8);
4871     __ por(xmm_temp6, xmm_temp9);
4872 
4873     //
4874     // First phase of the reduction
4875     //
4876     // Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts
4877     // independently.
4878     __ movdqu(xmm_temp7, xmm_temp3);
4879     __ movdqu(xmm_temp8, xmm_temp3);
4880     __ movdqu(xmm_temp9, xmm_temp3);
4881     __ pslld(xmm_temp7, 31);    // packed right shift shifting &lt;&lt; 31
4882     __ pslld(xmm_temp8, 30);    // packed right shift shifting &lt;&lt; 30
4883     __ pslld(xmm_temp9, 25);    // packed right shift shifting &lt;&lt; 25
4884     __ pxor(xmm_temp7, xmm_temp8);      // xor the shifted versions
4885     __ pxor(xmm_temp7, xmm_temp9);
4886     __ movdqu(xmm_temp8, xmm_temp7);
4887     __ pslldq(xmm_temp7, 12);
4888     __ psrldq(xmm_temp8, 4);
4889     __ pxor(xmm_temp3, xmm_temp7);      // first phase of the reduction complete
4890 
4891     //
4892     // Second phase of the reduction
4893     //
4894     // Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these
4895     // shift operations.
4896     __ movdqu(xmm_temp2, xmm_temp3);
4897     __ movdqu(xmm_temp4, xmm_temp3);
4898     __ movdqu(xmm_temp5, xmm_temp3);
4899     __ psrld(xmm_temp2, 1);     // packed left shifting &gt;&gt; 1
4900     __ psrld(xmm_temp4, 2);     // packed left shifting &gt;&gt; 2
4901     __ psrld(xmm_temp5, 7);     // packed left shifting &gt;&gt; 7
4902     __ pxor(xmm_temp2, xmm_temp4);      // xor the shifted versions
4903     __ pxor(xmm_temp2, xmm_temp5);
4904     __ pxor(xmm_temp2, xmm_temp8);
4905     __ pxor(xmm_temp3, xmm_temp2);
4906     __ pxor(xmm_temp6, xmm_temp3);      // the result is in xmm6
4907 
4908     __ decrement(blocks);
4909     __ jcc(Assembler::zero, L_exit);
4910     __ movdqu(xmm_temp0, xmm_temp6);
4911     __ addptr(data, 16);
4912     __ jmp(L_ghash_loop);
4913 
4914     __ BIND(L_exit);
4915     __ pshufb(xmm_temp6, xmm_temp10);          // Byte swap 16-byte result
4916     __ movdqu(Address(state, 0), xmm_temp6);   // store the result
4917     __ leave();
4918     __ ret(0);
4919     return start;
4920   }
4921 
4922   //base64 character set
4923   address base64_charset_addr() {
4924     __ align(CodeEntryAlignment);
4925     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;base64_charset&quot;);
4926     address start = __ pc();
4927     __ emit_data64(0x0000004200000041, relocInfo::none);
4928     __ emit_data64(0x0000004400000043, relocInfo::none);
4929     __ emit_data64(0x0000004600000045, relocInfo::none);
4930     __ emit_data64(0x0000004800000047, relocInfo::none);
4931     __ emit_data64(0x0000004a00000049, relocInfo::none);
4932     __ emit_data64(0x0000004c0000004b, relocInfo::none);
4933     __ emit_data64(0x0000004e0000004d, relocInfo::none);
4934     __ emit_data64(0x000000500000004f, relocInfo::none);
4935     __ emit_data64(0x0000005200000051, relocInfo::none);
4936     __ emit_data64(0x0000005400000053, relocInfo::none);
4937     __ emit_data64(0x0000005600000055, relocInfo::none);
4938     __ emit_data64(0x0000005800000057, relocInfo::none);
4939     __ emit_data64(0x0000005a00000059, relocInfo::none);
4940     __ emit_data64(0x0000006200000061, relocInfo::none);
4941     __ emit_data64(0x0000006400000063, relocInfo::none);
4942     __ emit_data64(0x0000006600000065, relocInfo::none);
4943     __ emit_data64(0x0000006800000067, relocInfo::none);
4944     __ emit_data64(0x0000006a00000069, relocInfo::none);
4945     __ emit_data64(0x0000006c0000006b, relocInfo::none);
4946     __ emit_data64(0x0000006e0000006d, relocInfo::none);
4947     __ emit_data64(0x000000700000006f, relocInfo::none);
4948     __ emit_data64(0x0000007200000071, relocInfo::none);
4949     __ emit_data64(0x0000007400000073, relocInfo::none);
4950     __ emit_data64(0x0000007600000075, relocInfo::none);
4951     __ emit_data64(0x0000007800000077, relocInfo::none);
4952     __ emit_data64(0x0000007a00000079, relocInfo::none);
4953     __ emit_data64(0x0000003100000030, relocInfo::none);
4954     __ emit_data64(0x0000003300000032, relocInfo::none);
4955     __ emit_data64(0x0000003500000034, relocInfo::none);
4956     __ emit_data64(0x0000003700000036, relocInfo::none);
4957     __ emit_data64(0x0000003900000038, relocInfo::none);
4958     __ emit_data64(0x0000002f0000002b, relocInfo::none);
4959     return start;
4960   }
4961 
4962   //base64 url character set
4963   address base64url_charset_addr() {
4964     __ align(CodeEntryAlignment);
4965     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;base64url_charset&quot;);
4966     address start = __ pc();
4967     __ emit_data64(0x0000004200000041, relocInfo::none);
4968     __ emit_data64(0x0000004400000043, relocInfo::none);
4969     __ emit_data64(0x0000004600000045, relocInfo::none);
4970     __ emit_data64(0x0000004800000047, relocInfo::none);
4971     __ emit_data64(0x0000004a00000049, relocInfo::none);
4972     __ emit_data64(0x0000004c0000004b, relocInfo::none);
4973     __ emit_data64(0x0000004e0000004d, relocInfo::none);
4974     __ emit_data64(0x000000500000004f, relocInfo::none);
4975     __ emit_data64(0x0000005200000051, relocInfo::none);
4976     __ emit_data64(0x0000005400000053, relocInfo::none);
4977     __ emit_data64(0x0000005600000055, relocInfo::none);
4978     __ emit_data64(0x0000005800000057, relocInfo::none);
4979     __ emit_data64(0x0000005a00000059, relocInfo::none);
4980     __ emit_data64(0x0000006200000061, relocInfo::none);
4981     __ emit_data64(0x0000006400000063, relocInfo::none);
4982     __ emit_data64(0x0000006600000065, relocInfo::none);
4983     __ emit_data64(0x0000006800000067, relocInfo::none);
4984     __ emit_data64(0x0000006a00000069, relocInfo::none);
4985     __ emit_data64(0x0000006c0000006b, relocInfo::none);
4986     __ emit_data64(0x0000006e0000006d, relocInfo::none);
4987     __ emit_data64(0x000000700000006f, relocInfo::none);
4988     __ emit_data64(0x0000007200000071, relocInfo::none);
4989     __ emit_data64(0x0000007400000073, relocInfo::none);
4990     __ emit_data64(0x0000007600000075, relocInfo::none);
4991     __ emit_data64(0x0000007800000077, relocInfo::none);
4992     __ emit_data64(0x0000007a00000079, relocInfo::none);
4993     __ emit_data64(0x0000003100000030, relocInfo::none);
4994     __ emit_data64(0x0000003300000032, relocInfo::none);
4995     __ emit_data64(0x0000003500000034, relocInfo::none);
4996     __ emit_data64(0x0000003700000036, relocInfo::none);
4997     __ emit_data64(0x0000003900000038, relocInfo::none);
4998     __ emit_data64(0x0000005f0000002d, relocInfo::none);
4999 
5000     return start;
5001   }
5002 
5003   address base64_bswap_mask_addr() {
5004     __ align(CodeEntryAlignment);
5005     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;bswap_mask_base64&quot;);
5006     address start = __ pc();
5007     __ emit_data64(0x0504038002010080, relocInfo::none);
5008     __ emit_data64(0x0b0a098008070680, relocInfo::none);
5009     __ emit_data64(0x0908078006050480, relocInfo::none);
5010     __ emit_data64(0x0f0e0d800c0b0a80, relocInfo::none);
5011     __ emit_data64(0x0605048003020180, relocInfo::none);
5012     __ emit_data64(0x0c0b0a8009080780, relocInfo::none);
5013     __ emit_data64(0x0504038002010080, relocInfo::none);
5014     __ emit_data64(0x0b0a098008070680, relocInfo::none);
5015 
5016     return start;
5017   }
5018 
5019   address base64_right_shift_mask_addr() {
5020     __ align(CodeEntryAlignment);
5021     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;right_shift_mask&quot;);
5022     address start = __ pc();
5023     __ emit_data64(0x0006000400020000, relocInfo::none);
5024     __ emit_data64(0x0006000400020000, relocInfo::none);
5025     __ emit_data64(0x0006000400020000, relocInfo::none);
5026     __ emit_data64(0x0006000400020000, relocInfo::none);
5027     __ emit_data64(0x0006000400020000, relocInfo::none);
5028     __ emit_data64(0x0006000400020000, relocInfo::none);
5029     __ emit_data64(0x0006000400020000, relocInfo::none);
5030     __ emit_data64(0x0006000400020000, relocInfo::none);
5031 
5032     return start;
5033   }
5034 
5035   address base64_left_shift_mask_addr() {
5036     __ align(CodeEntryAlignment);
5037     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;left_shift_mask&quot;);
5038     address start = __ pc();
5039     __ emit_data64(0x0000000200040000, relocInfo::none);
5040     __ emit_data64(0x0000000200040000, relocInfo::none);
5041     __ emit_data64(0x0000000200040000, relocInfo::none);
5042     __ emit_data64(0x0000000200040000, relocInfo::none);
5043     __ emit_data64(0x0000000200040000, relocInfo::none);
5044     __ emit_data64(0x0000000200040000, relocInfo::none);
5045     __ emit_data64(0x0000000200040000, relocInfo::none);
5046     __ emit_data64(0x0000000200040000, relocInfo::none);
5047 
5048     return start;
5049   }
5050 
5051   address base64_and_mask_addr() {
5052     __ align(CodeEntryAlignment);
5053     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;and_mask&quot;);
5054     address start = __ pc();
5055     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5056     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5057     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5058     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5059     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5060     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5061     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5062     __ emit_data64(0x3f003f003f000000, relocInfo::none);
5063     return start;
5064   }
5065 
5066   address base64_gather_mask_addr() {
5067     __ align(CodeEntryAlignment);
5068     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;gather_mask&quot;);
5069     address start = __ pc();
5070     __ emit_data64(0xffffffffffffffff, relocInfo::none);
5071     return start;
5072   }
5073 
5074 // Code for generating Base64 encoding.
5075 // Intrinsic function prototype in Base64.java:
5076 // private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL) {
5077   address generate_base64_encodeBlock() {
5078     __ align(CodeEntryAlignment);
5079     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;implEncode&quot;);
5080     address start = __ pc();
5081     __ enter();
5082 
5083     // Save callee-saved registers before using them
5084     __ push(r12);
5085     __ push(r13);
5086     __ push(r14);
5087     __ push(r15);
5088 
5089     // arguments
5090     const Register source = c_rarg0; // Source Array
5091     const Register start_offset = c_rarg1; // start offset
5092     const Register end_offset = c_rarg2; // end offset
5093     const Register dest = c_rarg3; // destination array
5094 
5095 #ifndef _WIN64
5096     const Register dp = c_rarg4;  // Position for writing to dest array
5097     const Register isURL = c_rarg5;// Base64 or URL character set
5098 #else
5099     const Address  dp_mem(rbp, 6 * wordSize);  // length is on stack on Win64
5100     const Address isURL_mem(rbp, 7 * wordSize);
5101     const Register isURL = r10;      // pick the volatile windows register
5102     const Register dp = r12;
5103     __ movl(dp, dp_mem);
5104     __ movl(isURL, isURL_mem);
5105 #endif
5106 
5107     const Register length = r14;
5108     Label L_process80, L_process32, L_process3, L_exit, L_processdata;
5109 
5110     // calculate length from offsets
5111     __ movl(length, end_offset);
5112     __ subl(length, start_offset);
5113     __ cmpl(length, 0);
5114     __ jcc(Assembler::lessEqual, L_exit);
5115 
5116     __ lea(r11, ExternalAddress(StubRoutines::x86::base64_charset_addr()));
5117     // check if base64 charset(isURL=0) or base64 url charset(isURL=1) needs to be loaded
5118     __ cmpl(isURL, 0);
5119     __ jcc(Assembler::equal, L_processdata);
5120     __ lea(r11, ExternalAddress(StubRoutines::x86::base64url_charset_addr()));
5121 
5122     // load masks required for encoding data
5123     __ BIND(L_processdata);
5124     __ movdqu(xmm16, ExternalAddress(StubRoutines::x86::base64_gather_mask_addr()));
5125     // Set 64 bits of K register.
5126     __ evpcmpeqb(k3, xmm16, xmm16, Assembler::AVX_512bit);
5127     __ evmovdquq(xmm12, ExternalAddress(StubRoutines::x86::base64_bswap_mask_addr()), Assembler::AVX_256bit, r13);
5128     __ evmovdquq(xmm13, ExternalAddress(StubRoutines::x86::base64_right_shift_mask_addr()), Assembler::AVX_512bit, r13);
5129     __ evmovdquq(xmm14, ExternalAddress(StubRoutines::x86::base64_left_shift_mask_addr()), Assembler::AVX_512bit, r13);
5130     __ evmovdquq(xmm15, ExternalAddress(StubRoutines::x86::base64_and_mask_addr()), Assembler::AVX_512bit, r13);
5131 
5132     // Vector Base64 implementation, producing 96 bytes of encoded data
5133     __ BIND(L_process80);
5134     __ cmpl(length, 80);
5135     __ jcc(Assembler::below, L_process32);
5136     __ evmovdquq(xmm0, Address(source, start_offset, Address::times_1, 0), Assembler::AVX_256bit);
5137     __ evmovdquq(xmm1, Address(source, start_offset, Address::times_1, 24), Assembler::AVX_256bit);
5138     __ evmovdquq(xmm2, Address(source, start_offset, Address::times_1, 48), Assembler::AVX_256bit);
5139 
5140     //permute the input data in such a manner that we have continuity of the source
5141     __ vpermq(xmm3, xmm0, 148, Assembler::AVX_256bit);
5142     __ vpermq(xmm4, xmm1, 148, Assembler::AVX_256bit);
5143     __ vpermq(xmm5, xmm2, 148, Assembler::AVX_256bit);
5144 
5145     //shuffle input and group 3 bytes of data and to it add 0 as the 4th byte.
5146     //we can deal with 12 bytes at a time in a 128 bit register
5147     __ vpshufb(xmm3, xmm3, xmm12, Assembler::AVX_256bit);
5148     __ vpshufb(xmm4, xmm4, xmm12, Assembler::AVX_256bit);
5149     __ vpshufb(xmm5, xmm5, xmm12, Assembler::AVX_256bit);
5150 
5151     //convert byte to word. Each 128 bit register will have 6 bytes for processing
5152     __ vpmovzxbw(xmm3, xmm3, Assembler::AVX_512bit);
5153     __ vpmovzxbw(xmm4, xmm4, Assembler::AVX_512bit);
5154     __ vpmovzxbw(xmm5, xmm5, Assembler::AVX_512bit);
5155 
5156     // Extract bits in the following pattern 6, 4+2, 2+4, 6 to convert 3, 8 bit numbers to 4, 6 bit numbers
5157     __ evpsrlvw(xmm0, xmm3, xmm13,  Assembler::AVX_512bit);
5158     __ evpsrlvw(xmm1, xmm4, xmm13, Assembler::AVX_512bit);
5159     __ evpsrlvw(xmm2, xmm5, xmm13, Assembler::AVX_512bit);
5160 
5161     __ evpsllvw(xmm3, xmm3, xmm14, Assembler::AVX_512bit);
5162     __ evpsllvw(xmm4, xmm4, xmm14, Assembler::AVX_512bit);
5163     __ evpsllvw(xmm5, xmm5, xmm14, Assembler::AVX_512bit);
5164 
5165     __ vpsrlq(xmm0, xmm0, 8, Assembler::AVX_512bit);
5166     __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);
5167     __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);
5168 
5169     __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);
5170     __ vpsllq(xmm4, xmm4, 8, Assembler::AVX_512bit);
5171     __ vpsllq(xmm5, xmm5, 8, Assembler::AVX_512bit);
5172 
5173     __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);
5174     __ vpandq(xmm4, xmm4, xmm15, Assembler::AVX_512bit);
5175     __ vpandq(xmm5, xmm5, xmm15, Assembler::AVX_512bit);
5176 
5177     // Get the final 4*6 bits base64 encoding
5178     __ vporq(xmm3, xmm3, xmm0, Assembler::AVX_512bit);
5179     __ vporq(xmm4, xmm4, xmm1, Assembler::AVX_512bit);
5180     __ vporq(xmm5, xmm5, xmm2, Assembler::AVX_512bit);
5181 
5182     // Shift
5183     __ vpsrlq(xmm3, xmm3, 8, Assembler::AVX_512bit);
5184     __ vpsrlq(xmm4, xmm4, 8, Assembler::AVX_512bit);
5185     __ vpsrlq(xmm5, xmm5, 8, Assembler::AVX_512bit);
5186 
5187     // look up 6 bits in the base64 character set to fetch the encoding
5188     // we are converting word to dword as gather instructions need dword indices for looking up encoding
5189     __ vextracti64x4(xmm6, xmm3, 0);
5190     __ vpmovzxwd(xmm0, xmm6, Assembler::AVX_512bit);
5191     __ vextracti64x4(xmm6, xmm3, 1);
5192     __ vpmovzxwd(xmm1, xmm6, Assembler::AVX_512bit);
5193 
5194     __ vextracti64x4(xmm6, xmm4, 0);
5195     __ vpmovzxwd(xmm2, xmm6, Assembler::AVX_512bit);
5196     __ vextracti64x4(xmm6, xmm4, 1);
5197     __ vpmovzxwd(xmm3, xmm6, Assembler::AVX_512bit);
5198 
5199     __ vextracti64x4(xmm4, xmm5, 0);
5200     __ vpmovzxwd(xmm6, xmm4, Assembler::AVX_512bit);
5201 
5202     __ vextracti64x4(xmm4, xmm5, 1);
5203     __ vpmovzxwd(xmm7, xmm4, Assembler::AVX_512bit);
5204 
5205     __ kmovql(k2, k3);
5206     __ evpgatherdd(xmm4, k2, Address(r11, xmm0, Address::times_4, 0), Assembler::AVX_512bit);
5207     __ kmovql(k2, k3);
5208     __ evpgatherdd(xmm5, k2, Address(r11, xmm1, Address::times_4, 0), Assembler::AVX_512bit);
5209     __ kmovql(k2, k3);
5210     __ evpgatherdd(xmm8, k2, Address(r11, xmm2, Address::times_4, 0), Assembler::AVX_512bit);
5211     __ kmovql(k2, k3);
5212     __ evpgatherdd(xmm9, k2, Address(r11, xmm3, Address::times_4, 0), Assembler::AVX_512bit);
5213     __ kmovql(k2, k3);
5214     __ evpgatherdd(xmm10, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);
5215     __ kmovql(k2, k3);
5216     __ evpgatherdd(xmm11, k2, Address(r11, xmm7, Address::times_4, 0), Assembler::AVX_512bit);
5217 
5218     //Down convert dword to byte. Final output is 16*6 = 96 bytes long
5219     __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm4, Assembler::AVX_512bit);
5220     __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm5, Assembler::AVX_512bit);
5221     __ evpmovdb(Address(dest, dp, Address::times_1, 32), xmm8, Assembler::AVX_512bit);
5222     __ evpmovdb(Address(dest, dp, Address::times_1, 48), xmm9, Assembler::AVX_512bit);
5223     __ evpmovdb(Address(dest, dp, Address::times_1, 64), xmm10, Assembler::AVX_512bit);
5224     __ evpmovdb(Address(dest, dp, Address::times_1, 80), xmm11, Assembler::AVX_512bit);
5225 
5226     __ addq(dest, 96);
5227     __ addq(source, 72);
5228     __ subq(length, 72);
5229     __ jmp(L_process80);
5230 
5231     // Vector Base64 implementation generating 32 bytes of encoded data
5232     __ BIND(L_process32);
5233     __ cmpl(length, 32);
5234     __ jcc(Assembler::below, L_process3);
5235     __ evmovdquq(xmm0, Address(source, start_offset), Assembler::AVX_256bit);
5236     __ vpermq(xmm0, xmm0, 148, Assembler::AVX_256bit);
5237     __ vpshufb(xmm6, xmm0, xmm12, Assembler::AVX_256bit);
5238     __ vpmovzxbw(xmm6, xmm6, Assembler::AVX_512bit);
5239     __ evpsrlvw(xmm2, xmm6, xmm13, Assembler::AVX_512bit);
5240     __ evpsllvw(xmm3, xmm6, xmm14, Assembler::AVX_512bit);
5241 
5242     __ vpsrlq(xmm2, xmm2, 8, Assembler::AVX_512bit);
5243     __ vpsllq(xmm3, xmm3, 8, Assembler::AVX_512bit);
5244     __ vpandq(xmm3, xmm3, xmm15, Assembler::AVX_512bit);
5245     __ vporq(xmm1, xmm2, xmm3, Assembler::AVX_512bit);
5246     __ vpsrlq(xmm1, xmm1, 8, Assembler::AVX_512bit);
5247     __ vextracti64x4(xmm9, xmm1, 0);
5248     __ vpmovzxwd(xmm6, xmm9, Assembler::AVX_512bit);
5249     __ vextracti64x4(xmm9, xmm1, 1);
5250     __ vpmovzxwd(xmm5, xmm9,  Assembler::AVX_512bit);
5251     __ kmovql(k2, k3);
5252     __ evpgatherdd(xmm8, k2, Address(r11, xmm6, Address::times_4, 0), Assembler::AVX_512bit);
5253     __ kmovql(k2, k3);
5254     __ evpgatherdd(xmm10, k2, Address(r11, xmm5, Address::times_4, 0), Assembler::AVX_512bit);
5255     __ evpmovdb(Address(dest, dp, Address::times_1, 0), xmm8, Assembler::AVX_512bit);
5256     __ evpmovdb(Address(dest, dp, Address::times_1, 16), xmm10, Assembler::AVX_512bit);
5257     __ subq(length, 24);
5258     __ addq(dest, 32);
5259     __ addq(source, 24);
5260     __ jmp(L_process32);
5261 
5262     // Scalar data processing takes 3 bytes at a time and produces 4 bytes of encoded data
5263     /* This code corresponds to the scalar version of the following snippet in Base64.java
5264     ** int bits = (src[sp0++] &amp; 0xff) &lt;&lt; 16 |(src[sp0++] &amp; 0xff) &lt;&lt; 8 |(src[sp0++] &amp; 0xff);
5265     ** dst[dp0++] = (byte)base64[(bits &gt;&gt; &gt; 18) &amp; 0x3f];
5266     ** dst[dp0++] = (byte)base64[(bits &gt;&gt; &gt; 12) &amp; 0x3f];
5267     ** dst[dp0++] = (byte)base64[(bits &gt;&gt; &gt; 6) &amp; 0x3f];
5268     ** dst[dp0++] = (byte)base64[bits &amp; 0x3f];*/
5269     __ BIND(L_process3);
5270     __ cmpl(length, 3);
5271     __ jcc(Assembler::below, L_exit);
5272     // Read 1 byte at a time
5273     __ movzbl(rax, Address(source, start_offset));
5274     __ shll(rax, 0x10);
5275     __ movl(r15, rax);
5276     __ movzbl(rax, Address(source, start_offset, Address::times_1, 1));
5277     __ shll(rax, 0x8);
5278     __ movzwl(rax, rax);
5279     __ orl(r15, rax);
5280     __ movzbl(rax, Address(source, start_offset, Address::times_1, 2));
5281     __ orl(rax, r15);
5282     // Save 3 bytes read in r15
5283     __ movl(r15, rax);
5284     __ shrl(rax, 0x12);
5285     __ andl(rax, 0x3f);
5286     // rax contains the index, r11 contains base64 lookup table
5287     __ movb(rax, Address(r11, rax, Address::times_4));
5288     // Write the encoded byte to destination
5289     __ movb(Address(dest, dp, Address::times_1, 0), rax);
5290     __ movl(rax, r15);
5291     __ shrl(rax, 0xc);
5292     __ andl(rax, 0x3f);
5293     __ movb(rax, Address(r11, rax, Address::times_4));
5294     __ movb(Address(dest, dp, Address::times_1, 1), rax);
5295     __ movl(rax, r15);
5296     __ shrl(rax, 0x6);
5297     __ andl(rax, 0x3f);
5298     __ movb(rax, Address(r11, rax, Address::times_4));
5299     __ movb(Address(dest, dp, Address::times_1, 2), rax);
5300     __ movl(rax, r15);
5301     __ andl(rax, 0x3f);
5302     __ movb(rax, Address(r11, rax, Address::times_4));
5303     __ movb(Address(dest, dp, Address::times_1, 3), rax);
5304     __ subl(length, 3);
5305     __ addq(dest, 4);
5306     __ addq(source, 3);
5307     __ jmp(L_process3);
5308     __ BIND(L_exit);
5309     __ pop(r15);
5310     __ pop(r14);
5311     __ pop(r13);
5312     __ pop(r12);
5313     __ leave();
5314     __ ret(0);
5315     return start;
5316   }
5317 
5318   /**
5319    *  Arguments:
5320    *
5321    * Inputs:
5322    *   c_rarg0   - int crc
5323    *   c_rarg1   - byte* buf
5324    *   c_rarg2   - int length
5325    *
5326    * Ouput:
5327    *       rax   - int crc result
5328    */
5329   address generate_updateBytesCRC32() {
5330     assert(UseCRC32Intrinsics, &quot;need AVX and CLMUL instructions&quot;);
5331 
5332     __ align(CodeEntryAlignment);
5333     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32&quot;);
5334 
5335     address start = __ pc();
5336     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5337     // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5338     // rscratch1: r10
5339     const Register crc   = c_rarg0;  // crc
5340     const Register buf   = c_rarg1;  // source java byte array address
5341     const Register len   = c_rarg2;  // length
5342     const Register table = c_rarg3;  // crc_table address (reuse register)
5343     const Register tmp1   = r11;
5344     const Register tmp2   = r10;
5345     assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);
5346 
5347     BLOCK_COMMENT(&quot;Entry:&quot;);
5348     __ enter(); // required for proper stackwalking of RuntimeStub frame
5349 
5350     if (VM_Version::supports_sse4_1() &amp;&amp; VM_Version::supports_avx512_vpclmulqdq() &amp;&amp;
5351         VM_Version::supports_avx512bw() &amp;&amp;
5352         VM_Version::supports_avx512vl()) {
5353       __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);
5354     } else {
5355       __ kernel_crc32(crc, buf, len, table, tmp1);
5356     }
5357 
5358     __ movl(rax, crc);
5359     __ vzeroupper();
5360     __ leave(); // required for proper stackwalking of RuntimeStub frame
5361     __ ret(0);
5362 
5363     return start;
5364   }
5365 
5366   /**
5367   *  Arguments:
5368   *
5369   * Inputs:
5370   *   c_rarg0   - int crc
5371   *   c_rarg1   - byte* buf
5372   *   c_rarg2   - long length
5373   *   c_rarg3   - table_start - optional (present only when doing a library_call,
5374   *              not used by x86 algorithm)
5375   *
5376   * Ouput:
5377   *       rax   - int crc result
5378   */
5379   address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {
5380       assert(UseCRC32CIntrinsics, &quot;need SSE4_2&quot;);
5381       __ align(CodeEntryAlignment);
5382       StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;updateBytesCRC32C&quot;);
5383       address start = __ pc();
5384       //reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs
5385       //Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3
5386       //Lin / Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7
5387       const Register crc = c_rarg0;  // crc
5388       const Register buf = c_rarg1;  // source java byte array address
5389       const Register len = c_rarg2;  // length
5390       const Register a = rax;
5391       const Register j = r9;
5392       const Register k = r10;
5393       const Register l = r11;
5394 #ifdef _WIN64
5395       const Register y = rdi;
5396       const Register z = rsi;
5397 #else
5398       const Register y = rcx;
5399       const Register z = r8;
5400 #endif
5401       assert_different_registers(crc, buf, len, a, j, k, l, y, z);
5402 
5403       BLOCK_COMMENT(&quot;Entry:&quot;);
5404       __ enter(); // required for proper stackwalking of RuntimeStub frame
5405 #ifdef _WIN64
5406       __ push(y);
5407       __ push(z);
5408 #endif
5409       __ crc32c_ipl_alg2_alt2(crc, buf, len,
5410                               a, j, k,
5411                               l, y, z,
5412                               c_farg0, c_farg1, c_farg2,
5413                               is_pclmulqdq_supported);
5414       __ movl(rax, crc);
5415 #ifdef _WIN64
5416       __ pop(z);
5417       __ pop(y);
5418 #endif
5419       __ vzeroupper();
5420       __ leave(); // required for proper stackwalking of RuntimeStub frame
5421       __ ret(0);
5422 
5423       return start;
5424   }
5425 
5426   /**
5427    *  Arguments:
5428    *
5429    *  Input:
5430    *    c_rarg0   - x address
5431    *    c_rarg1   - x length
5432    *    c_rarg2   - y address
5433    *    c_rarg3   - y length
5434    * not Win64
5435    *    c_rarg4   - z address
5436    *    c_rarg5   - z length
5437    * Win64
5438    *    rsp+40    - z address
5439    *    rsp+48    - z length
5440    */
5441   address generate_multiplyToLen() {
5442     __ align(CodeEntryAlignment);
5443     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;multiplyToLen&quot;);
5444 
5445     address start = __ pc();
5446     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5447     // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5448     const Register x     = rdi;
5449     const Register xlen  = rax;
5450     const Register y     = rsi;
5451     const Register ylen  = rcx;
5452     const Register z     = r8;
5453     const Register zlen  = r11;
5454 
5455     // Next registers will be saved on stack in multiply_to_len().
5456     const Register tmp1  = r12;
5457     const Register tmp2  = r13;
5458     const Register tmp3  = r14;
5459     const Register tmp4  = r15;
5460     const Register tmp5  = rbx;
5461 
5462     BLOCK_COMMENT(&quot;Entry:&quot;);
5463     __ enter(); // required for proper stackwalking of RuntimeStub frame
5464 
5465 #ifndef _WIN64
5466     __ movptr(zlen, r9); // Save r9 in r11 - zlen
5467 #endif
5468     setup_arg_regs(4); // x =&gt; rdi, xlen =&gt; rsi, y =&gt; rdx
5469                        // ylen =&gt; rcx, z =&gt; r8, zlen =&gt; r11
5470                        // r9 and r10 may be used to save non-volatile registers
5471 #ifdef _WIN64
5472     // last 2 arguments (#4, #5) are on stack on Win64
5473     __ movptr(z, Address(rsp, 6 * wordSize));
5474     __ movptr(zlen, Address(rsp, 7 * wordSize));
5475 #endif
5476 
5477     __ movptr(xlen, rsi);
5478     __ movptr(y,    rdx);
5479     __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);
5480 
5481     restore_arg_regs();
5482 
5483     __ leave(); // required for proper stackwalking of RuntimeStub frame
5484     __ ret(0);
5485 
5486     return start;
5487   }
5488 
5489   /**
5490   *  Arguments:
5491   *
5492   *  Input:
5493   *    c_rarg0   - obja     address
5494   *    c_rarg1   - objb     address
5495   *    c_rarg3   - length   length
5496   *    c_rarg4   - scale    log2_array_indxscale
5497   *
5498   *  Output:
5499   *        rax   - int &gt;= mismatched index, &lt; 0 bitwise complement of tail
5500   */
5501   address generate_vectorizedMismatch() {
5502     __ align(CodeEntryAlignment);
5503     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;vectorizedMismatch&quot;);
5504     address start = __ pc();
5505 
5506     BLOCK_COMMENT(&quot;Entry:&quot;);
5507     __ enter();
5508 
5509 #ifdef _WIN64  // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5510     const Register scale = c_rarg0;  //rcx, will exchange with r9
5511     const Register objb = c_rarg1;   //rdx
5512     const Register length = c_rarg2; //r8
5513     const Register obja = c_rarg3;   //r9
5514     __ xchgq(obja, scale);  //now obja and scale contains the correct contents
5515 
5516     const Register tmp1 = r10;
5517     const Register tmp2 = r11;
5518 #endif
5519 #ifndef _WIN64 // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5520     const Register obja = c_rarg0;   //U:rdi
5521     const Register objb = c_rarg1;   //U:rsi
5522     const Register length = c_rarg2; //U:rdx
5523     const Register scale = c_rarg3;  //U:rcx
5524     const Register tmp1 = r8;
5525     const Register tmp2 = r9;
5526 #endif
5527     const Register result = rax; //return value
5528     const XMMRegister vec0 = xmm0;
5529     const XMMRegister vec1 = xmm1;
5530     const XMMRegister vec2 = xmm2;
5531 
5532     __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);
5533 
5534     __ vzeroupper();
5535     __ leave();
5536     __ ret(0);
5537 
5538     return start;
5539   }
5540 
5541 /**
5542    *  Arguments:
5543    *
5544   //  Input:
5545   //    c_rarg0   - x address
5546   //    c_rarg1   - x length
5547   //    c_rarg2   - z address
5548   //    c_rarg3   - z lenth
5549    *
5550    */
5551   address generate_squareToLen() {
5552 
5553     __ align(CodeEntryAlignment);
5554     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;squareToLen&quot;);
5555 
5556     address start = __ pc();
5557     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5558     // Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)
5559     const Register x      = rdi;
5560     const Register len    = rsi;
5561     const Register z      = r8;
5562     const Register zlen   = rcx;
5563 
5564    const Register tmp1      = r12;
5565    const Register tmp2      = r13;
5566    const Register tmp3      = r14;
5567    const Register tmp4      = r15;
5568    const Register tmp5      = rbx;
5569 
5570     BLOCK_COMMENT(&quot;Entry:&quot;);
5571     __ enter(); // required for proper stackwalking of RuntimeStub frame
5572 
5573     setup_arg_regs(4); // x =&gt; rdi, len =&gt; rsi, z =&gt; rdx
5574                        // zlen =&gt; rcx
5575                        // r9 and r10 may be used to save non-volatile registers
5576     __ movptr(r8, rdx);
5577     __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);
5578 
5579     restore_arg_regs();
5580 
5581     __ leave(); // required for proper stackwalking of RuntimeStub frame
5582     __ ret(0);
5583 
5584     return start;
5585   }
5586 
5587   address generate_method_entry_barrier() {
5588     __ align(CodeEntryAlignment);
5589     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;nmethod_entry_barrier&quot;);
5590 
5591     Label deoptimize_label;
5592 
5593     address start = __ pc();
5594 
5595     __ push(-1); // cookie, this is used for writing the new rsp when deoptimizing
5596 
5597     BLOCK_COMMENT(&quot;Entry:&quot;);
5598     __ enter(); // save rbp
5599 
5600     // save c_rarg0, because we want to use that value.
5601     // We could do without it but then we depend on the number of slots used by pusha
5602     __ push(c_rarg0);
5603 
5604     __ lea(c_rarg0, Address(rsp, wordSize * 3)); // 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address
5605 
5606     __ pusha();
5607 
5608     // The method may have floats as arguments, and we must spill them before calling
5609     // the VM runtime.
5610     assert(Argument::n_float_register_parameters_j == 8, &quot;Assumption&quot;);
5611     const int xmm_size = wordSize * 2;
5612     const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;
5613     __ subptr(rsp, xmm_spill_size);
5614     __ movdqu(Address(rsp, xmm_size * 7), xmm7);
5615     __ movdqu(Address(rsp, xmm_size * 6), xmm6);
5616     __ movdqu(Address(rsp, xmm_size * 5), xmm5);
5617     __ movdqu(Address(rsp, xmm_size * 4), xmm4);
5618     __ movdqu(Address(rsp, xmm_size * 3), xmm3);
5619     __ movdqu(Address(rsp, xmm_size * 2), xmm2);
5620     __ movdqu(Address(rsp, xmm_size * 1), xmm1);
5621     __ movdqu(Address(rsp, xmm_size * 0), xmm0);
5622 
5623     __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast&lt;int (*)(address*)&gt;(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);
5624 
5625     __ movdqu(xmm0, Address(rsp, xmm_size * 0));
5626     __ movdqu(xmm1, Address(rsp, xmm_size * 1));
5627     __ movdqu(xmm2, Address(rsp, xmm_size * 2));
5628     __ movdqu(xmm3, Address(rsp, xmm_size * 3));
5629     __ movdqu(xmm4, Address(rsp, xmm_size * 4));
5630     __ movdqu(xmm5, Address(rsp, xmm_size * 5));
5631     __ movdqu(xmm6, Address(rsp, xmm_size * 6));
5632     __ movdqu(xmm7, Address(rsp, xmm_size * 7));
5633     __ addptr(rsp, xmm_spill_size);
5634 
5635     __ cmpl(rax, 1); // 1 means deoptimize
5636     __ jcc(Assembler::equal, deoptimize_label);
5637 
5638     __ popa();
5639     __ pop(c_rarg0);
5640 
5641     __ leave();
5642 
5643     __ addptr(rsp, 1 * wordSize); // cookie
5644     __ ret(0);
5645 
5646 
5647     __ BIND(deoptimize_label);
5648 
5649     __ popa();
5650     __ pop(c_rarg0);
5651 
5652     __ leave();
5653 
5654     // this can be taken out, but is good for verification purposes. getting a SIGSEGV
5655     // here while still having a correct stack is valuable
5656     __ testptr(rsp, Address(rsp, 0));
5657 
5658     __ movptr(rsp, Address(rsp, 0)); // new rsp was written in the barrier
5659     __ jmp(Address(rsp, -1 * wordSize)); // jmp target should be callers verified_entry_point
5660 
5661     return start;
5662   }
5663 
5664    /**
5665    *  Arguments:
5666    *
5667    *  Input:
5668    *    c_rarg0   - out address
5669    *    c_rarg1   - in address
5670    *    c_rarg2   - offset
5671    *    c_rarg3   - len
5672    * not Win64
5673    *    c_rarg4   - k
5674    * Win64
5675    *    rsp+40    - k
5676    */
5677   address generate_mulAdd() {
5678     __ align(CodeEntryAlignment);
5679     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;mulAdd&quot;);
5680 
5681     address start = __ pc();
5682     // Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)
5683     // Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)
5684     const Register out     = rdi;
5685     const Register in      = rsi;
5686     const Register offset  = r11;
5687     const Register len     = rcx;
5688     const Register k       = r8;
5689 
5690     // Next registers will be saved on stack in mul_add().
5691     const Register tmp1  = r12;
5692     const Register tmp2  = r13;
5693     const Register tmp3  = r14;
5694     const Register tmp4  = r15;
5695     const Register tmp5  = rbx;
5696 
5697     BLOCK_COMMENT(&quot;Entry:&quot;);
5698     __ enter(); // required for proper stackwalking of RuntimeStub frame
5699 
5700     setup_arg_regs(4); // out =&gt; rdi, in =&gt; rsi, offset =&gt; rdx
5701                        // len =&gt; rcx, k =&gt; r8
5702                        // r9 and r10 may be used to save non-volatile registers
5703 #ifdef _WIN64
5704     // last argument is on stack on Win64
5705     __ movl(k, Address(rsp, 6 * wordSize));
5706 #endif
5707     __ movptr(r11, rdx);  // move offset in rdx to offset(r11)
5708     __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);
5709 
5710     restore_arg_regs();
5711 
5712     __ leave(); // required for proper stackwalking of RuntimeStub frame
5713     __ ret(0);
5714 
5715     return start;
5716   }
5717 
5718   address generate_bigIntegerRightShift() {
5719     __ align(CodeEntryAlignment);
5720     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;bigIntegerRightShiftWorker&quot;);
5721 
5722     address start = __ pc();
5723     Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;
5724     // For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.
5725     const Register newArr = rdi;
5726     const Register oldArr = rsi;
5727     const Register newIdx = rdx;
5728     const Register shiftCount = rcx;  // It was intentional to have shiftCount in rcx since it is used implicitly for shift.
5729     const Register totalNumIter = r8;
5730 
5731     // For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.
5732     // For everything else, we prefer using r9 and r10 since we do not have to save them before use.
5733     const Register tmp1 = r11;                    // Caller save.
5734     const Register tmp2 = rax;                    // Caller save.
5735     const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   // Windows: Callee save. Linux: Caller save.
5736     const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  // Windows: Callee save. Linux: Caller save.
5737     const Register tmp5 = r14;                    // Callee save.
5738     const Register tmp6 = r15;
5739 
5740     const XMMRegister x0 = xmm0;
5741     const XMMRegister x1 = xmm1;
5742     const XMMRegister x2 = xmm2;
5743 
5744     BLOCK_COMMENT(&quot;Entry:&quot;);
5745     __ enter(); // required for proper stackwalking of RuntimeStub frame
5746 
5747 #ifdef _WINDOWS
5748     setup_arg_regs(4);
5749     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5750     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5751     // Save callee save registers.
5752     __ push(tmp3);
5753     __ push(tmp4);
5754 #endif
5755     __ push(tmp5);
5756 
5757     // Rename temps used throughout the code.
5758     const Register idx = tmp1;
5759     const Register nIdx = tmp2;
5760 
5761     __ xorl(idx, idx);
5762 
5763     // Start right shift from end of the array.
5764     // For example, if #iteration = 4 and newIdx = 1
5765     // then dest[4] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5766     // if #iteration = 4 and newIdx = 0
5767     // then dest[3] = src[4] &gt;&gt; shiftCount  | src[3] &lt;&lt;&lt; (shiftCount - 32)
5768     __ movl(idx, totalNumIter);
5769     __ movl(nIdx, idx);
5770     __ addl(nIdx, newIdx);
5771 
5772     // If vectorization is enabled, check if the number of iterations is at least 64
5773     // If not, then go to ShifTwo processing 2 iterations
5774     if (VM_Version::supports_avx512_vbmi2()) {
5775       __ cmpptr(totalNumIter, (AVX3Threshold/64));
5776       __ jcc(Assembler::less, ShiftTwo);
5777 
5778       if (AVX3Threshold &lt; 16 * 64) {
5779         __ cmpl(totalNumIter, 16);
5780         __ jcc(Assembler::less, ShiftTwo);
5781       }
5782       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5783       __ subl(idx, 16);
5784       __ subl(nIdx, 16);
5785       __ BIND(Shift512Loop);
5786       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);
5787       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5788       __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);
5789       __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);
5790       __ subl(nIdx, 16);
5791       __ subl(idx, 16);
5792       __ jcc(Assembler::greaterEqual, Shift512Loop);
5793       __ addl(idx, 16);
5794       __ addl(nIdx, 16);
5795     }
5796     __ BIND(ShiftTwo);
5797     __ cmpl(idx, 2);
5798     __ jcc(Assembler::less, ShiftOne);
5799     __ subl(idx, 2);
5800     __ subl(nIdx, 2);
5801     __ BIND(ShiftTwoLoop);
5802     __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));
5803     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));
5804     __ movl(tmp3, Address(oldArr, idx, Address::times_4));
5805     __ shrdl(tmp5, tmp4);
5806     __ shrdl(tmp4, tmp3);
5807     __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);
5808     __ movl(Address(newArr, nIdx, Address::times_4), tmp4);
5809     __ subl(nIdx, 2);
5810     __ subl(idx, 2);
5811     __ jcc(Assembler::greaterEqual, ShiftTwoLoop);
5812     __ addl(idx, 2);
5813     __ addl(nIdx, 2);
5814 
5815     // Do the last iteration
5816     __ BIND(ShiftOne);
5817     __ cmpl(idx, 1);
5818     __ jcc(Assembler::less, Exit);
5819     __ subl(idx, 1);
5820     __ subl(nIdx, 1);
5821     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));
5822     __ movl(tmp3, Address(oldArr, idx, Address::times_4));
5823     __ shrdl(tmp4, tmp3);
5824     __ movl(Address(newArr, nIdx, Address::times_4), tmp4);
5825     __ BIND(Exit);
5826     // Restore callee save registers.
5827     __ pop(tmp5);
5828 #ifdef _WINDOWS
5829     __ pop(tmp4);
5830     __ pop(tmp3);
5831     restore_arg_regs();
5832 #endif
5833     __ leave(); // required for proper stackwalking of RuntimeStub frame
5834     __ ret(0);
5835     return start;
5836   }
5837 
5838    /**
5839    *  Arguments:
5840    *
5841    *  Input:
5842    *    c_rarg0   - newArr address
5843    *    c_rarg1   - oldArr address
5844    *    c_rarg2   - newIdx
5845    *    c_rarg3   - shiftCount
5846    * not Win64
5847    *    c_rarg4   - numIter
5848    * Win64
5849    *    rsp40    - numIter
5850    */
5851   address generate_bigIntegerLeftShift() {
5852     __ align(CodeEntryAlignment);
5853     StubCodeMark mark(this,  &quot;StubRoutines&quot;, &quot;bigIntegerLeftShiftWorker&quot;);
5854     address start = __ pc();
5855     Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;
5856     // For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.
5857     const Register newArr = rdi;
5858     const Register oldArr = rsi;
5859     const Register newIdx = rdx;
5860     const Register shiftCount = rcx;  // It was intentional to have shiftCount in rcx since it is used implicitly for shift.
5861     const Register totalNumIter = r8;
5862     // For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.
5863     // For everything else, we prefer using r9 and r10 since we do not have to save them before use.
5864     const Register tmp1 = r11;                    // Caller save.
5865     const Register tmp2 = rax;                    // Caller save.
5866     const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   // Windows: Callee save. Linux: Caller save.
5867     const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  // Windows: Callee save. Linux: Caller save.
5868     const Register tmp5 = r14;                    // Callee save.
5869 
5870     const XMMRegister x0 = xmm0;
5871     const XMMRegister x1 = xmm1;
5872     const XMMRegister x2 = xmm2;
5873     BLOCK_COMMENT(&quot;Entry:&quot;);
5874     __ enter(); // required for proper stackwalking of RuntimeStub frame
5875 
5876 #ifdef _WINDOWS
5877     setup_arg_regs(4);
5878     // For windows, since last argument is on stack, we need to move it to the appropriate register.
5879     __ movl(totalNumIter, Address(rsp, 6 * wordSize));
5880     // Save callee save registers.
5881     __ push(tmp3);
5882     __ push(tmp4);
5883 #endif
5884     __ push(tmp5);
5885 
5886     // Rename temps used throughout the code
5887     const Register idx = tmp1;
5888     const Register numIterTmp = tmp2;
5889 
5890     // Start idx from zero.
5891     __ xorl(idx, idx);
5892     // Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.
5893     __ lea(newArr, Address(newArr, newIdx, Address::times_4));
5894     __ movl(numIterTmp, totalNumIter);
5895 
5896     // If vectorization is enabled, check if the number of iterations is at least 64
5897     // If not, then go to ShiftTwo shifting two numbers at a time
5898     if (VM_Version::supports_avx512_vbmi2()) {
5899       __ cmpl(totalNumIter, (AVX3Threshold/64));
5900       __ jcc(Assembler::less, ShiftTwo);
5901 
5902       if (AVX3Threshold &lt; 16 * 64) {
5903         __ cmpl(totalNumIter, 16);
5904         __ jcc(Assembler::less, ShiftTwo);
5905       }
5906       __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);
5907       __ subl(numIterTmp, 16);
5908       __ BIND(Shift512Loop);
5909       __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);
5910       __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);
5911       __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);
5912       __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);
5913       __ addl(idx, 16);
5914       __ subl(numIterTmp, 16);
5915       __ jcc(Assembler::greaterEqual, Shift512Loop);
5916       __ addl(numIterTmp, 16);
5917     }
5918     __ BIND(ShiftTwo);
5919     __ cmpl(totalNumIter, 1);
5920     __ jcc(Assembler::less, Exit);
5921     __ movl(tmp3, Address(oldArr, idx, Address::times_4));
5922     __ subl(numIterTmp, 2);
5923     __ jcc(Assembler::less, ShiftOne);
5924 
5925     __ BIND(ShiftTwoLoop);
5926     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));
5927     __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));
5928     __ shldl(tmp3, tmp4);
5929     __ shldl(tmp4, tmp5);
5930     __ movl(Address(newArr, idx, Address::times_4), tmp3);
5931     __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);
5932     __ movl(tmp3, tmp5);
5933     __ addl(idx, 2);
5934     __ subl(numIterTmp, 2);
5935     __ jcc(Assembler::greaterEqual, ShiftTwoLoop);
5936 
5937     // Do the last iteration
5938     __ BIND(ShiftOne);
5939     __ addl(numIterTmp, 2);
5940     __ cmpl(numIterTmp, 1);
5941     __ jcc(Assembler::less, Exit);
5942     __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));
5943     __ shldl(tmp3, tmp4);
5944     __ movl(Address(newArr, idx, Address::times_4), tmp3);
5945 
5946     __ BIND(Exit);
5947     // Restore callee save registers.
5948     __ pop(tmp5);
5949 #ifdef _WINDOWS
5950     __ pop(tmp4);
5951     __ pop(tmp3);
5952     restore_arg_regs();
5953 #endif
5954     __ leave(); // required for proper stackwalking of RuntimeStub frame
5955     __ ret(0);
5956     return start;
5957   }
5958 
5959   address generate_libmExp() {
5960     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmExp&quot;);
5961 
5962     address start = __ pc();
5963 
5964     const XMMRegister x0  = xmm0;
5965     const XMMRegister x1  = xmm1;
5966     const XMMRegister x2  = xmm2;
5967     const XMMRegister x3  = xmm3;
5968 
5969     const XMMRegister x4  = xmm4;
5970     const XMMRegister x5  = xmm5;
5971     const XMMRegister x6  = xmm6;
5972     const XMMRegister x7  = xmm7;
5973 
5974     const Register tmp   = r11;
5975 
5976     BLOCK_COMMENT(&quot;Entry:&quot;);
5977     __ enter(); // required for proper stackwalking of RuntimeStub frame
5978 
5979     __ fast_exp(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);
5980 
5981     __ leave(); // required for proper stackwalking of RuntimeStub frame
5982     __ ret(0);
5983 
5984     return start;
5985 
5986   }
5987 
5988   address generate_libmLog() {
5989     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmLog&quot;);
5990 
5991     address start = __ pc();
5992 
5993     const XMMRegister x0 = xmm0;
5994     const XMMRegister x1 = xmm1;
5995     const XMMRegister x2 = xmm2;
5996     const XMMRegister x3 = xmm3;
5997 
5998     const XMMRegister x4 = xmm4;
5999     const XMMRegister x5 = xmm5;
6000     const XMMRegister x6 = xmm6;
6001     const XMMRegister x7 = xmm7;
6002 
6003     const Register tmp1 = r11;
6004     const Register tmp2 = r8;
6005 
6006     BLOCK_COMMENT(&quot;Entry:&quot;);
6007     __ enter(); // required for proper stackwalking of RuntimeStub frame
6008 
6009     __ fast_log(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2);
6010 
6011     __ leave(); // required for proper stackwalking of RuntimeStub frame
6012     __ ret(0);
6013 
6014     return start;
6015 
6016   }
6017 
6018   address generate_libmLog10() {
6019     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmLog10&quot;);
6020 
6021     address start = __ pc();
6022 
6023     const XMMRegister x0 = xmm0;
6024     const XMMRegister x1 = xmm1;
6025     const XMMRegister x2 = xmm2;
6026     const XMMRegister x3 = xmm3;
6027 
6028     const XMMRegister x4 = xmm4;
6029     const XMMRegister x5 = xmm5;
6030     const XMMRegister x6 = xmm6;
6031     const XMMRegister x7 = xmm7;
6032 
6033     const Register tmp = r11;
6034 
6035     BLOCK_COMMENT(&quot;Entry:&quot;);
6036     __ enter(); // required for proper stackwalking of RuntimeStub frame
6037 
6038     __ fast_log10(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp);
6039 
6040     __ leave(); // required for proper stackwalking of RuntimeStub frame
6041     __ ret(0);
6042 
6043     return start;
6044 
6045   }
6046 
6047   address generate_libmPow() {
6048     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmPow&quot;);
6049 
6050     address start = __ pc();
6051 
6052     const XMMRegister x0 = xmm0;
6053     const XMMRegister x1 = xmm1;
6054     const XMMRegister x2 = xmm2;
6055     const XMMRegister x3 = xmm3;
6056 
6057     const XMMRegister x4 = xmm4;
6058     const XMMRegister x5 = xmm5;
6059     const XMMRegister x6 = xmm6;
6060     const XMMRegister x7 = xmm7;
6061 
6062     const Register tmp1 = r8;
6063     const Register tmp2 = r9;
6064     const Register tmp3 = r10;
6065     const Register tmp4 = r11;
6066 
6067     BLOCK_COMMENT(&quot;Entry:&quot;);
6068     __ enter(); // required for proper stackwalking of RuntimeStub frame
6069 
6070     __ fast_pow(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6071 
6072     __ leave(); // required for proper stackwalking of RuntimeStub frame
6073     __ ret(0);
6074 
6075     return start;
6076 
6077   }
6078 
6079   address generate_libmSin() {
6080     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmSin&quot;);
6081 
6082     address start = __ pc();
6083 
6084     const XMMRegister x0 = xmm0;
6085     const XMMRegister x1 = xmm1;
6086     const XMMRegister x2 = xmm2;
6087     const XMMRegister x3 = xmm3;
6088 
6089     const XMMRegister x4 = xmm4;
6090     const XMMRegister x5 = xmm5;
6091     const XMMRegister x6 = xmm6;
6092     const XMMRegister x7 = xmm7;
6093 
6094     const Register tmp1 = r8;
6095     const Register tmp2 = r9;
6096     const Register tmp3 = r10;
6097     const Register tmp4 = r11;
6098 
6099     BLOCK_COMMENT(&quot;Entry:&quot;);
6100     __ enter(); // required for proper stackwalking of RuntimeStub frame
6101 
6102 #ifdef _WIN64
6103     __ push(rsi);
6104     __ push(rdi);
6105 #endif
6106     __ fast_sin(x0, x1, x2, x3, x4, x5, x6, x7, rax, rbx, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6107 
6108 #ifdef _WIN64
6109     __ pop(rdi);
6110     __ pop(rsi);
6111 #endif
6112 
6113     __ leave(); // required for proper stackwalking of RuntimeStub frame
6114     __ ret(0);
6115 
6116     return start;
6117 
6118   }
6119 
6120   address generate_libmCos() {
6121     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmCos&quot;);
6122 
6123     address start = __ pc();
6124 
6125     const XMMRegister x0 = xmm0;
6126     const XMMRegister x1 = xmm1;
6127     const XMMRegister x2 = xmm2;
6128     const XMMRegister x3 = xmm3;
6129 
6130     const XMMRegister x4 = xmm4;
6131     const XMMRegister x5 = xmm5;
6132     const XMMRegister x6 = xmm6;
6133     const XMMRegister x7 = xmm7;
6134 
6135     const Register tmp1 = r8;
6136     const Register tmp2 = r9;
6137     const Register tmp3 = r10;
6138     const Register tmp4 = r11;
6139 
6140     BLOCK_COMMENT(&quot;Entry:&quot;);
6141     __ enter(); // required for proper stackwalking of RuntimeStub frame
6142 
6143 #ifdef _WIN64
6144     __ push(rsi);
6145     __ push(rdi);
6146 #endif
6147     __ fast_cos(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6148 
6149 #ifdef _WIN64
6150     __ pop(rdi);
6151     __ pop(rsi);
6152 #endif
6153 
6154     __ leave(); // required for proper stackwalking of RuntimeStub frame
6155     __ ret(0);
6156 
6157     return start;
6158 
6159   }
6160 
6161   address generate_libmTan() {
6162     StubCodeMark mark(this, &quot;StubRoutines&quot;, &quot;libmTan&quot;);
6163 
6164     address start = __ pc();
6165 
6166     const XMMRegister x0 = xmm0;
6167     const XMMRegister x1 = xmm1;
6168     const XMMRegister x2 = xmm2;
6169     const XMMRegister x3 = xmm3;
6170 
6171     const XMMRegister x4 = xmm4;
6172     const XMMRegister x5 = xmm5;
6173     const XMMRegister x6 = xmm6;
6174     const XMMRegister x7 = xmm7;
6175 
6176     const Register tmp1 = r8;
6177     const Register tmp2 = r9;
6178     const Register tmp3 = r10;
6179     const Register tmp4 = r11;
6180 
6181     BLOCK_COMMENT(&quot;Entry:&quot;);
6182     __ enter(); // required for proper stackwalking of RuntimeStub frame
6183 
6184 #ifdef _WIN64
6185     __ push(rsi);
6186     __ push(rdi);
6187 #endif
6188     __ fast_tan(x0, x1, x2, x3, x4, x5, x6, x7, rax, rcx, rdx, tmp1, tmp2, tmp3, tmp4);
6189 
6190 #ifdef _WIN64
6191     __ pop(rdi);
6192     __ pop(rsi);
6193 #endif
6194 
6195     __ leave(); // required for proper stackwalking of RuntimeStub frame
6196     __ ret(0);
6197 
6198     return start;
6199 
6200   }
6201 
6202 #undef __
6203 #define __ masm-&gt;
6204 
6205   // Continuation point for throwing of implicit exceptions that are
6206   // not handled in the current activation. Fabricates an exception
6207   // oop and initiates normal exception dispatching in this
6208   // frame. Since we need to preserve callee-saved values (currently
6209   // only for C2, but done for C1 as well) we need a callee-saved oop
6210   // map and therefore have to make these stubs into RuntimeStubs
6211   // rather than BufferBlobs.  If the compiler needs all registers to
6212   // be preserved between the fault point and the exception handler
6213   // then it must assume responsibility for that in
6214   // AbstractCompiler::continuation_for_implicit_null_exception or
6215   // continuation_for_implicit_division_by_zero_exception. All other
6216   // implicit exceptions (e.g., NullPointerException or
6217   // AbstractMethodError on entry) are either at call sites or
6218   // otherwise assume that stack unwinding will be initiated, so
6219   // caller saved registers were assumed volatile in the compiler.
6220   address generate_throw_exception(const char* name,
6221                                    address runtime_entry,
6222                                    Register arg1 = noreg,
6223                                    Register arg2 = noreg) {
6224     // Information about frame layout at time of blocking runtime call.
6225     // Note that we only have to preserve callee-saved registers since
6226     // the compilers are responsible for supplying a continuation point
6227     // if they expect all registers to be preserved.
6228     enum layout {
6229       rbp_off = frame::arg_reg_save_area_bytes/BytesPerInt,
6230       rbp_off2,
6231       return_off,
6232       return_off2,
6233       framesize // inclusive of return address
6234     };
6235 
6236     int insts_size = 512;
6237     int locs_size  = 64;
6238 
6239     CodeBuffer code(name, insts_size, locs_size);
6240     OopMapSet* oop_maps  = new OopMapSet();
6241     MacroAssembler* masm = new MacroAssembler(&amp;code);
6242 
6243     address start = __ pc();
6244 
6245     // This is an inlined and slightly modified version of call_VM
6246     // which has the ability to fetch the return PC out of
6247     // thread-local storage and also sets up last_Java_sp slightly
6248     // differently than the real call_VM
6249 
6250     __ enter(); // required for proper stackwalking of RuntimeStub frame
6251 
6252     assert(is_even(framesize/2), &quot;sp not 16-byte aligned&quot;);
6253 
6254     // return address and rbp are already in place
6255     __ subptr(rsp, (framesize-4) &lt;&lt; LogBytesPerInt); // prolog
6256 
6257     int frame_complete = __ pc() - start;
6258 
6259     // Set up last_Java_sp and last_Java_fp
6260     address the_pc = __ pc();
6261     __ set_last_Java_frame(rsp, rbp, the_pc);
6262     __ andptr(rsp, -(StackAlignmentInBytes));    // Align stack
6263 
6264     // Call runtime
6265     if (arg1 != noreg) {
6266       assert(arg2 != c_rarg1, &quot;clobbered&quot;);
6267       __ movptr(c_rarg1, arg1);
6268     }
6269     if (arg2 != noreg) {
6270       __ movptr(c_rarg2, arg2);
6271     }
6272     __ movptr(c_rarg0, r15_thread);
6273     BLOCK_COMMENT(&quot;call runtime_entry&quot;);
6274     __ call(RuntimeAddress(runtime_entry));
6275 
6276     // Generate oop map
6277     OopMap* map = new OopMap(framesize, 0);
6278 
6279     oop_maps-&gt;add_gc_map(the_pc - start, map);
6280 
6281     __ reset_last_Java_frame(true);
6282 
6283     __ leave(); // required for proper stackwalking of RuntimeStub frame
6284 
6285     // check for pending exceptions
6286 #ifdef ASSERT
6287     Label L;
6288     __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()),
6289             (int32_t) NULL_WORD);
6290     __ jcc(Assembler::notEqual, L);
6291     __ should_not_reach_here();
6292     __ bind(L);
6293 #endif // ASSERT
6294     __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
6295 
6296 
6297     // codeBlob framesize is in words (not VMRegImpl::slot_size)
6298     RuntimeStub* stub =
6299       RuntimeStub::new_runtime_stub(name,
6300                                     &amp;code,
6301                                     frame_complete,
6302                                     (framesize &gt;&gt; (LogBytesPerWord - LogBytesPerInt)),
6303                                     oop_maps, false);
6304     return stub-&gt;entry_point();
6305   }
6306 
6307   void create_control_words() {
6308     // Round to nearest, 53-bit mode, exceptions masked
6309     StubRoutines::_fpu_cntrl_wrd_std   = 0x027F;
6310     // Round to zero, 53-bit mode, exception mased
6311     StubRoutines::_fpu_cntrl_wrd_trunc = 0x0D7F;
6312     // Round to nearest, 24-bit mode, exceptions masked
6313     StubRoutines::_fpu_cntrl_wrd_24    = 0x007F;
6314     // Round to nearest, 64-bit mode, exceptions masked
6315     StubRoutines::_mxcsr_std           = 0x1F80;
6316     // Note: the following two constants are 80-bit values
6317     //       layout is critical for correct loading by FPU.
6318     // Bias for strict fp multiply/divide
6319     StubRoutines::_fpu_subnormal_bias1[0]= 0x00000000; // 2^(-15360) == 0x03ff 8000 0000 0000 0000
6320     StubRoutines::_fpu_subnormal_bias1[1]= 0x80000000;
6321     StubRoutines::_fpu_subnormal_bias1[2]= 0x03ff;
6322     // Un-Bias for strict fp multiply/divide
6323     StubRoutines::_fpu_subnormal_bias2[0]= 0x00000000; // 2^(+15360) == 0x7bff 8000 0000 0000 0000
6324     StubRoutines::_fpu_subnormal_bias2[1]= 0x80000000;
6325     StubRoutines::_fpu_subnormal_bias2[2]= 0x7bff;
6326   }
6327 
<a name="12" id="anc12"></a><span class="line-added">6328   // Call here from the interpreter or compiled code to either load</span>
<span class="line-added">6329   // multiple returned values from the inline type instance being</span>
<span class="line-added">6330   // returned to registers or to store returned values to a newly</span>
<span class="line-added">6331   // allocated inline type instance.</span>
<span class="line-added">6332   address generate_return_value_stub(address destination, const char* name, bool has_res) {</span>
<span class="line-added">6333     // We need to save all registers the calling convention may use so</span>
<span class="line-added">6334     // the runtime calls read or update those registers. This needs to</span>
<span class="line-added">6335     // be in sync with SharedRuntime::java_return_convention().</span>
<span class="line-added">6336     enum layout {</span>
<span class="line-added">6337       pad_off = frame::arg_reg_save_area_bytes/BytesPerInt, pad_off_2,</span>
<span class="line-added">6338       rax_off, rax_off_2,</span>
<span class="line-added">6339       j_rarg5_off, j_rarg5_2,</span>
<span class="line-added">6340       j_rarg4_off, j_rarg4_2,</span>
<span class="line-added">6341       j_rarg3_off, j_rarg3_2,</span>
<span class="line-added">6342       j_rarg2_off, j_rarg2_2,</span>
<span class="line-added">6343       j_rarg1_off, j_rarg1_2,</span>
<span class="line-added">6344       j_rarg0_off, j_rarg0_2,</span>
<span class="line-added">6345       j_farg0_off, j_farg0_2,</span>
<span class="line-added">6346       j_farg1_off, j_farg1_2,</span>
<span class="line-added">6347       j_farg2_off, j_farg2_2,</span>
<span class="line-added">6348       j_farg3_off, j_farg3_2,</span>
<span class="line-added">6349       j_farg4_off, j_farg4_2,</span>
<span class="line-added">6350       j_farg5_off, j_farg5_2,</span>
<span class="line-added">6351       j_farg6_off, j_farg6_2,</span>
<span class="line-added">6352       j_farg7_off, j_farg7_2,</span>
<span class="line-added">6353       rbp_off, rbp_off_2,</span>
<span class="line-added">6354       return_off, return_off_2,</span>
<span class="line-added">6355 </span>
<span class="line-added">6356       framesize</span>
<span class="line-added">6357     };</span>
<span class="line-added">6358 </span>
<span class="line-added">6359     CodeBuffer buffer(name, 1000, 512);</span>
<span class="line-added">6360     MacroAssembler* masm = new MacroAssembler(&amp;buffer);</span>
<span class="line-added">6361 </span>
<span class="line-added">6362     int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);</span>
<span class="line-added">6363     assert(frame_size_in_bytes == framesize*BytesPerInt, &quot;misaligned&quot;);</span>
<span class="line-added">6364     int frame_size_in_slots = frame_size_in_bytes / BytesPerInt;</span>
<span class="line-added">6365     int frame_size_in_words = frame_size_in_bytes / wordSize;</span>
<span class="line-added">6366 </span>
<span class="line-added">6367     OopMapSet *oop_maps = new OopMapSet();</span>
<span class="line-added">6368     OopMap* map = new OopMap(frame_size_in_slots, 0);</span>
<span class="line-added">6369 </span>
<span class="line-added">6370     map-&gt;set_callee_saved(VMRegImpl::stack2reg(rax_off), rax-&gt;as_VMReg());</span>
<span class="line-added">6371     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5-&gt;as_VMReg());</span>
<span class="line-added">6372     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4-&gt;as_VMReg());</span>
<span class="line-added">6373     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3-&gt;as_VMReg());</span>
<span class="line-added">6374     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2-&gt;as_VMReg());</span>
<span class="line-added">6375     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1-&gt;as_VMReg());</span>
<span class="line-added">6376     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0-&gt;as_VMReg());</span>
<span class="line-added">6377     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0-&gt;as_VMReg());</span>
<span class="line-added">6378     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1-&gt;as_VMReg());</span>
<span class="line-added">6379     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2-&gt;as_VMReg());</span>
<span class="line-added">6380     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3-&gt;as_VMReg());</span>
<span class="line-added">6381     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4-&gt;as_VMReg());</span>
<span class="line-added">6382     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5-&gt;as_VMReg());</span>
<span class="line-added">6383     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6-&gt;as_VMReg());</span>
<span class="line-added">6384     map-&gt;set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7-&gt;as_VMReg());</span>
<span class="line-added">6385 </span>
<span class="line-added">6386     int start = __ offset();</span>
<span class="line-added">6387 </span>
<span class="line-added">6388     __ subptr(rsp, frame_size_in_bytes - 8 /* return address*/);</span>
<span class="line-added">6389 </span>
<span class="line-added">6390     __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);</span>
<span class="line-added">6391     __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);</span>
<span class="line-added">6392     __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);</span>
<span class="line-added">6393     __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);</span>
<span class="line-added">6394     __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);</span>
<span class="line-added">6395     __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);</span>
<span class="line-added">6396     __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);</span>
<span class="line-added">6397     __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);</span>
<span class="line-added">6398     __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);</span>
<span class="line-added">6399 </span>
<span class="line-added">6400     __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);</span>
<span class="line-added">6401     __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);</span>
<span class="line-added">6402     __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);</span>
<span class="line-added">6403     __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);</span>
<span class="line-added">6404     __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);</span>
<span class="line-added">6405     __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);</span>
<span class="line-added">6406     __ movptr(Address(rsp, rax_off * BytesPerInt), rax);</span>
<span class="line-added">6407 </span>
<span class="line-added">6408     int frame_complete = __ offset();</span>
<span class="line-added">6409 </span>
<span class="line-added">6410     __ set_last_Java_frame(noreg, noreg, NULL);</span>
<span class="line-added">6411 </span>
<span class="line-added">6412     __ mov(c_rarg0, r15_thread);</span>
<span class="line-added">6413     __ mov(c_rarg1, rax);</span>
<span class="line-added">6414 </span>
<span class="line-added">6415     __ call(RuntimeAddress(destination));</span>
<span class="line-added">6416 </span>
<span class="line-added">6417     // Set an oopmap for the call site.</span>
<span class="line-added">6418 </span>
<span class="line-added">6419     oop_maps-&gt;add_gc_map( __ offset() - start, map);</span>
<span class="line-added">6420 </span>
<span class="line-added">6421     // clear last_Java_sp</span>
<span class="line-added">6422     __ reset_last_Java_frame(false);</span>
<span class="line-added">6423 </span>
<span class="line-added">6424     __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));</span>
<span class="line-added">6425     __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));</span>
<span class="line-added">6426     __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));</span>
<span class="line-added">6427     __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));</span>
<span class="line-added">6428     __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));</span>
<span class="line-added">6429     __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));</span>
<span class="line-added">6430     __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));</span>
<span class="line-added">6431     __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));</span>
<span class="line-added">6432     __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));</span>
<span class="line-added">6433 </span>
<span class="line-added">6434     __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));</span>
<span class="line-added">6435     __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));</span>
<span class="line-added">6436     __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));</span>
<span class="line-added">6437     __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));</span>
<span class="line-added">6438     __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));</span>
<span class="line-added">6439     __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));</span>
<span class="line-added">6440     __ movptr(rax, Address(rsp, rax_off * BytesPerInt));</span>
<span class="line-added">6441 </span>
<span class="line-added">6442     __ addptr(rsp, frame_size_in_bytes-8);</span>
<span class="line-added">6443 </span>
<span class="line-added">6444     // check for pending exceptions</span>
<span class="line-added">6445     Label pending;</span>
<span class="line-added">6446     __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);</span>
<span class="line-added">6447     __ jcc(Assembler::notEqual, pending);</span>
<span class="line-added">6448 </span>
<span class="line-added">6449     if (has_res) {</span>
<span class="line-added">6450       __ get_vm_result(rax, r15_thread);</span>
<span class="line-added">6451     }</span>
<span class="line-added">6452 </span>
<span class="line-added">6453     __ ret(0);</span>
<span class="line-added">6454 </span>
<span class="line-added">6455     __ bind(pending);</span>
<span class="line-added">6456 </span>
<span class="line-added">6457     __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));</span>
<span class="line-added">6458     __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));</span>
<span class="line-added">6459 </span>
<span class="line-added">6460     // -------------</span>
<span class="line-added">6461     // make sure all code is generated</span>
<span class="line-added">6462     masm-&gt;flush();</span>
<span class="line-added">6463 </span>
<span class="line-added">6464     RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_in_words, oop_maps, false);</span>
<span class="line-added">6465     return stub-&gt;entry_point();</span>
<span class="line-added">6466   }</span>
<span class="line-added">6467 </span>
6468   // Initialization
6469   void generate_initial() {
6470     // Generates all stubs and initializes the entry points
6471 
6472     // This platform-specific settings are needed by generate_call_stub()
6473     create_control_words();
6474 
6475     // entry points that exist in all platforms Note: This is code
6476     // that could be shared among different platforms - however the
6477     // benefit seems to be smaller than the disadvantage of having a
6478     // much more complicated generator structure. See also comment in
6479     // stubRoutines.hpp.
6480 
6481     StubRoutines::_forward_exception_entry = generate_forward_exception();
6482 
<a name="13" id="anc13"></a><span class="line-modified">6483     // Generate these first because they are called from other stubs</span>
<span class="line-modified">6484     StubRoutines::_load_inline_type_fields_in_regs = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), &quot;load_inline_type_fields_in_regs&quot;, false);</span>
<span class="line-added">6485     StubRoutines::_store_inline_type_fields_to_buf = generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), &quot;store_inline_type_fields_to_buf&quot;, true);</span>
<span class="line-added">6486 </span>
<span class="line-added">6487     StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);</span>
6488 
6489     // is referenced by megamorphic call
6490     StubRoutines::_catch_exception_entry = generate_catch_exception();
6491 
6492     // atomic calls
6493     StubRoutines::_atomic_xchg_entry          = generate_atomic_xchg();
6494     StubRoutines::_atomic_xchg_long_entry     = generate_atomic_xchg_long();
6495     StubRoutines::_atomic_cmpxchg_entry       = generate_atomic_cmpxchg();
6496     StubRoutines::_atomic_cmpxchg_byte_entry  = generate_atomic_cmpxchg_byte();
6497     StubRoutines::_atomic_cmpxchg_long_entry  = generate_atomic_cmpxchg_long();
6498     StubRoutines::_atomic_add_entry           = generate_atomic_add();
6499     StubRoutines::_atomic_add_long_entry      = generate_atomic_add_long();
6500     StubRoutines::_fence_entry                = generate_orderaccess_fence();
6501 
6502     // platform dependent
6503     StubRoutines::x86::_get_previous_fp_entry = generate_get_previous_fp();
6504     StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();
6505 
6506     StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();
6507 
6508     StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();
6509     StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();
6510     StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();
6511     StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();
6512 
6513     StubRoutines::x86::_float_sign_mask       = generate_fp_mask(&quot;float_sign_mask&quot;,  0x7FFFFFFF7FFFFFFF);
6514     StubRoutines::x86::_float_sign_flip       = generate_fp_mask(&quot;float_sign_flip&quot;,  0x8000000080000000);
6515     StubRoutines::x86::_double_sign_mask      = generate_fp_mask(&quot;double_sign_mask&quot;, 0x7FFFFFFFFFFFFFFF);
6516     StubRoutines::x86::_double_sign_flip      = generate_fp_mask(&quot;double_sign_flip&quot;, 0x8000000000000000);
6517 
6518     // Build this early so it&#39;s available for the interpreter.
6519     StubRoutines::_throw_StackOverflowError_entry =
6520       generate_throw_exception(&quot;StackOverflowError throw_exception&quot;,
6521                                CAST_FROM_FN_PTR(address,
6522                                                 SharedRuntime::
6523                                                 throw_StackOverflowError));
6524     StubRoutines::_throw_delayed_StackOverflowError_entry =
6525       generate_throw_exception(&quot;delayed StackOverflowError throw_exception&quot;,
6526                                CAST_FROM_FN_PTR(address,
6527                                                 SharedRuntime::
6528                                                 throw_delayed_StackOverflowError));
6529     if (UseCRC32Intrinsics) {
6530       // set table address before stub generation which use it
6531       StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;
6532       StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();
6533     }
6534 
6535     if (UseCRC32CIntrinsics) {
6536       bool supports_clmul = VM_Version::supports_clmul();
6537       StubRoutines::x86::generate_CRC32C_table(supports_clmul);
6538       StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;
6539       StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);
6540     }
6541     if (UseLibmIntrinsic &amp;&amp; InlineIntrinsics) {
6542       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin) ||
6543           vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos) ||
6544           vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {
6545         StubRoutines::x86::_ONEHALF_adr = (address)StubRoutines::x86::_ONEHALF;
6546         StubRoutines::x86::_P_2_adr = (address)StubRoutines::x86::_P_2;
6547         StubRoutines::x86::_SC_4_adr = (address)StubRoutines::x86::_SC_4;
6548         StubRoutines::x86::_Ctable_adr = (address)StubRoutines::x86::_Ctable;
6549         StubRoutines::x86::_SC_2_adr = (address)StubRoutines::x86::_SC_2;
6550         StubRoutines::x86::_SC_3_adr = (address)StubRoutines::x86::_SC_3;
6551         StubRoutines::x86::_SC_1_adr = (address)StubRoutines::x86::_SC_1;
6552         StubRoutines::x86::_PI_INV_TABLE_adr = (address)StubRoutines::x86::_PI_INV_TABLE;
6553         StubRoutines::x86::_PI_4_adr = (address)StubRoutines::x86::_PI_4;
6554         StubRoutines::x86::_PI32INV_adr = (address)StubRoutines::x86::_PI32INV;
6555         StubRoutines::x86::_SIGN_MASK_adr = (address)StubRoutines::x86::_SIGN_MASK;
6556         StubRoutines::x86::_P_1_adr = (address)StubRoutines::x86::_P_1;
6557         StubRoutines::x86::_P_3_adr = (address)StubRoutines::x86::_P_3;
6558         StubRoutines::x86::_NEG_ZERO_adr = (address)StubRoutines::x86::_NEG_ZERO;
6559       }
6560       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {
6561         StubRoutines::_dexp = generate_libmExp();
6562       }
6563       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {
6564         StubRoutines::_dlog = generate_libmLog();
6565       }
6566       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {
6567         StubRoutines::_dlog10 = generate_libmLog10();
6568       }
6569       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {
6570         StubRoutines::_dpow = generate_libmPow();
6571       }
6572       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {
6573         StubRoutines::_dsin = generate_libmSin();
6574       }
6575       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {
6576         StubRoutines::_dcos = generate_libmCos();
6577       }
6578       if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {
6579         StubRoutines::_dtan = generate_libmTan();
6580       }
6581     }
6582 
6583     // Safefetch stubs.
6584     generate_safefetch(&quot;SafeFetch32&quot;, sizeof(int),     &amp;StubRoutines::_safefetch32_entry,
6585                                                        &amp;StubRoutines::_safefetch32_fault_pc,
6586                                                        &amp;StubRoutines::_safefetch32_continuation_pc);
6587     generate_safefetch(&quot;SafeFetchN&quot;, sizeof(intptr_t), &amp;StubRoutines::_safefetchN_entry,
6588                                                        &amp;StubRoutines::_safefetchN_fault_pc,
6589                                                        &amp;StubRoutines::_safefetchN_continuation_pc);
6590   }
6591 
6592   void generate_all() {
6593     // Generates all stubs and initializes the entry points
6594 
6595     // These entry points require SharedInfo::stack0 to be set up in
6596     // non-core builds and need to be relocatable, so they each
6597     // fabricate a RuntimeStub internally.
6598     StubRoutines::_throw_AbstractMethodError_entry =
6599       generate_throw_exception(&quot;AbstractMethodError throw_exception&quot;,
6600                                CAST_FROM_FN_PTR(address,
6601                                                 SharedRuntime::
6602                                                 throw_AbstractMethodError));
6603 
6604     StubRoutines::_throw_IncompatibleClassChangeError_entry =
6605       generate_throw_exception(&quot;IncompatibleClassChangeError throw_exception&quot;,
6606                                CAST_FROM_FN_PTR(address,
6607                                                 SharedRuntime::
6608                                                 throw_IncompatibleClassChangeError));
6609 
6610     StubRoutines::_throw_NullPointerException_at_call_entry =
6611       generate_throw_exception(&quot;NullPointerException at call throw_exception&quot;,
6612                                CAST_FROM_FN_PTR(address,
6613                                                 SharedRuntime::
6614                                                 throw_NullPointerException_at_call));
6615 
6616     // entry points that are platform specific
6617     StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(&quot;vector_float_sign_mask&quot;, 0x7FFFFFFF7FFFFFFF);
6618     StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(&quot;vector_float_sign_flip&quot;, 0x8000000080000000);
6619     StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(&quot;vector_double_sign_mask&quot;, 0x7FFFFFFFFFFFFFFF);
6620     StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(&quot;vector_double_sign_flip&quot;, 0x8000000000000000);
6621     StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(&quot;vector_short_to_byte_mask&quot;, 0x00ff00ff00ff00ff);
6622     StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(&quot;vector_byte_perm_mask&quot;);
6623     StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(&quot;vector_long_sign_mask&quot;, 0x8000000000000000);
6624 
6625     // support for verify_oop (must happen after universe_init)
6626     StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();
6627 
6628     // data cache line writeback
6629     StubRoutines::_data_cache_writeback = generate_data_cache_writeback();
6630     StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();
6631 
6632     // arraycopy stubs used by compilers
6633     generate_arraycopy_stubs();
6634 
6635     // don&#39;t bother generating these AES intrinsic stubs unless global flag is set
6636     if (UseAESIntrinsics) {
6637       StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  // needed by the others
6638       StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();
6639       StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();
6640       StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();
6641       if (VM_Version::supports_avx512_vaes() &amp;&amp;  VM_Version::supports_avx512vl() &amp;&amp; VM_Version::supports_avx512dq() ) {
6642         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();
6643         StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();
6644         StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();
6645       } else {
6646         StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();
6647       }
6648     }
6649     if (UseAESCTRIntrinsics) {
6650       if (VM_Version::supports_avx512_vaes() &amp;&amp; VM_Version::supports_avx512bw() &amp;&amp; VM_Version::supports_avx512vl()) {
6651         StubRoutines::x86::_counter_mask_addr = counter_mask_addr();
6652         StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();
6653       } else {
6654         StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();
6655         StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();
6656       }
6657     }
6658 
6659     if (UseSHA1Intrinsics) {
6660       StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();
6661       StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();
6662       StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, &quot;sha1_implCompress&quot;);
6663       StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, &quot;sha1_implCompressMB&quot;);
6664     }
6665     if (UseSHA256Intrinsics) {
6666       StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;
6667       char* dst = (char*)StubRoutines::x86::_k256_W;
6668       char* src = (char*)StubRoutines::x86::_k256;
6669       for (int ii = 0; ii &lt; 16; ++ii) {
6670         memcpy(dst + 32 * ii,      src + 16 * ii, 16);
6671         memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);
6672       }
6673       StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;
6674       StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();
6675       StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, &quot;sha256_implCompress&quot;);
6676       StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, &quot;sha256_implCompressMB&quot;);
6677     }
6678     if (UseSHA512Intrinsics) {
6679       StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;
6680       StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();
6681       StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, &quot;sha512_implCompress&quot;);
6682       StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, &quot;sha512_implCompressMB&quot;);
6683     }
6684 
6685     // Generate GHASH intrinsics code
6686     if (UseGHASHIntrinsics) {
6687     StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();
6688     StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();
6689       if (VM_Version::supports_avx()) {
6690         StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();
6691         StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();
6692         StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();
6693       } else {
6694         StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();
6695       }
6696     }
6697 
6698     if (UseBASE64Intrinsics) {
6699       StubRoutines::x86::_and_mask = base64_and_mask_addr();
6700       StubRoutines::x86::_bswap_mask = base64_bswap_mask_addr();
6701       StubRoutines::x86::_base64_charset = base64_charset_addr();
6702       StubRoutines::x86::_url_charset = base64url_charset_addr();
6703       StubRoutines::x86::_gather_mask = base64_gather_mask_addr();
6704       StubRoutines::x86::_left_shift_mask = base64_left_shift_mask_addr();
6705       StubRoutines::x86::_right_shift_mask = base64_right_shift_mask_addr();
6706       StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();
6707     }
6708 
6709     BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()-&gt;barrier_set_nmethod();
6710     if (bs_nm != NULL) {
6711       StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();
6712     }
6713 #ifdef COMPILER2
6714     if (UseMultiplyToLenIntrinsic) {
6715       StubRoutines::_multiplyToLen = generate_multiplyToLen();
6716     }
6717     if (UseSquareToLenIntrinsic) {
6718       StubRoutines::_squareToLen = generate_squareToLen();
6719     }
6720     if (UseMulAddIntrinsic) {
6721       StubRoutines::_mulAdd = generate_mulAdd();
6722     }
6723     if (VM_Version::supports_avx512_vbmi2()) {
6724       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
6725       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
6726     }
6727     if (UseMontgomeryMultiplyIntrinsic) {
6728       StubRoutines::_montgomeryMultiply
6729         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
6730     }
6731     if (UseMontgomerySquareIntrinsic) {
6732       StubRoutines::_montgomerySquare
6733         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
6734     }
6735 #endif // COMPILER2
6736 
6737     if (UseVectorizedMismatchIntrinsic) {
6738       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
6739     }
6740   }
6741 
6742  public:
6743   StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {
6744     if (all) {
6745       generate_all();
6746     } else {
6747       generate_initial();
6748     }
6749   }
6750 }; // end class declaration
6751 
6752 #define UCM_TABLE_MAX_ENTRIES 16
6753 void StubGenerator_generate(CodeBuffer* code, bool all) {
6754   if (UnsafeCopyMemory::_table == NULL) {
6755     UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);
6756   }
6757   StubGenerator g(code, all);
6758 }
<a name="14" id="anc14"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="14" type="hidden" />
</body>
</html>
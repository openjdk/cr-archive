<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/loopnode.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="live.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="loopopts.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/loopnode.hpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  59   virtual uint size_of() const { return sizeof(*this); }
  60 protected:
  61   uint _loop_flags;
  62   // Names for flag bitfields
  63   enum { Normal=0, Pre=1, Main=2, Post=3, PreMainPostFlagsMask=3,
  64          MainHasNoPreLoop=4,
  65          HasExactTripCount=8,
  66          InnerLoop=16,
  67          PartialPeelLoop=32,
  68          PartialPeelFailed=64,
  69          HasReductions=128,
  70          WasSlpAnalyzed=256,
  71          PassedSlpAnalysis=512,
  72          DoUnrollOnly=1024,
  73          VectorizedLoop=2048,
  74          HasAtomicPostLoop=4096,
  75          HasRangeChecks=8192,
  76          IsMultiversioned=16384,
  77          StripMined=32768,
  78          SubwordLoop=65536,
<span class="line-modified">  79          ProfileTripFailed=131072};</span>

  80   char _unswitch_count;
  81   enum { _unswitch_max=3 };
  82   char _postloop_flags;
  83   enum { LoopNotRCEChecked = 0, LoopRCEChecked = 1, RCEPostLoop = 2 };
  84 
  85   // Expected trip count from profile data
  86   float _profile_trip_cnt;
  87 
  88 public:
  89   // Names for edge indices
  90   enum { Self=0, EntryControl, LoopBackControl };
  91 
  92   bool is_inner_loop() const { return _loop_flags &amp; InnerLoop; }
  93   void set_inner_loop() { _loop_flags |= InnerLoop; }
  94 
  95   bool range_checks_present() const { return _loop_flags &amp; HasRangeChecks; }
  96   bool is_multiversioned() const { return _loop_flags &amp; IsMultiversioned; }
  97   bool is_vectorized_loop() const { return _loop_flags &amp; VectorizedLoop; }
  98   bool is_partial_peel_loop() const { return _loop_flags &amp; PartialPeelLoop; }
  99   void set_partial_peel_loop() { _loop_flags |= PartialPeelLoop; }
 100   bool partial_peel_has_failed() const { return _loop_flags &amp; PartialPeelFailed; }
 101   bool is_strip_mined() const { return _loop_flags &amp; StripMined; }
 102   bool is_profile_trip_failed() const { return _loop_flags &amp; ProfileTripFailed; }
 103   bool is_subword_loop() const { return _loop_flags &amp; SubwordLoop; }

 104 
 105   void mark_partial_peel_failed() { _loop_flags |= PartialPeelFailed; }
 106   void mark_has_reductions() { _loop_flags |= HasReductions; }
 107   void mark_was_slp() { _loop_flags |= WasSlpAnalyzed; }
 108   void mark_passed_slp() { _loop_flags |= PassedSlpAnalysis; }
 109   void mark_do_unroll_only() { _loop_flags |= DoUnrollOnly; }
 110   void mark_loop_vectorized() { _loop_flags |= VectorizedLoop; }
 111   void mark_has_atomic_post_loop() { _loop_flags |= HasAtomicPostLoop; }
 112   void mark_has_range_checks() { _loop_flags |=  HasRangeChecks; }
 113   void mark_is_multiversioned() { _loop_flags |= IsMultiversioned; }
 114   void mark_strip_mined() { _loop_flags |= StripMined; }
 115   void clear_strip_mined() { _loop_flags &amp;= ~StripMined; }
 116   void mark_profile_trip_failed() { _loop_flags |= ProfileTripFailed; }
 117   void mark_subword_loop() { _loop_flags |= SubwordLoop; }

 118 
 119   int unswitch_max() { return _unswitch_max; }
 120   int unswitch_count() { return _unswitch_count; }
 121 
 122   int has_been_range_checked() const { return _postloop_flags &amp; LoopRCEChecked; }
 123   void set_has_been_range_checked() { _postloop_flags |= LoopRCEChecked; }
 124   int is_rce_post_loop() const { return _postloop_flags &amp; RCEPostLoop; }
 125   void set_is_rce_post_loop() { _postloop_flags |= RCEPostLoop; }
 126 
 127   void set_unswitch_count(int val) {
 128     assert (val &lt;= unswitch_max(), &quot;too many unswitches&quot;);
 129     _unswitch_count = val;
 130   }
 131 
 132   void set_profile_trip_cnt(float ptc) { _profile_trip_cnt = ptc; }
 133   float profile_trip_cnt()             { return _profile_trip_cnt; }
 134 
 135   LoopNode(Node *entry, Node *backedge)
 136     : RegionNode(3), _loop_flags(0), _unswitch_count(0),
 137       _postloop_flags(0), _profile_trip_cnt(COUNT_UNKNOWN)  {
</pre>
<hr />
<pre>
1212                                         Node_List &amp;old_new,
1213                                         int opcode,
1214                                         CloneLoopMode mode);
1215 
1216   // Clone a loop and return the clone head (clone_loop_head).
1217   // Added nodes include int(1), int(0) - disconnected, If, IfTrue, IfFalse,
1218   // This routine was created for usage in CountedLoopReserveKit.
1219   //
1220   //    int(1) -&gt; If -&gt; IfTrue -&gt; original_loop_head
1221   //              |
1222   //              V
1223   //           IfFalse -&gt; clone_loop_head (returned by function pointer)
1224   //
1225   LoopNode* create_reserve_version_of_loop(IdealLoopTree *loop, CountedLoopReserveKit* lk);
1226   // Clone loop with an invariant test (that does not exit) and
1227   // insert a clone of the test that selects which version to
1228   // execute.
1229   void do_unswitching (IdealLoopTree *loop, Node_List &amp;old_new);
1230 
1231   // Find candidate &quot;if&quot; for unswitching
<span class="line-modified">1232   IfNode* find_unswitching_candidate(const IdealLoopTree *loop) const;</span>
1233 
1234   // Range Check Elimination uses this function!
1235   // Constrain the main loop iterations so the affine function:
1236   //    low_limit &lt;= scale_con * I + offset  &lt;  upper_limit
1237   // always holds true.  That is, either increase the number of iterations in
1238   // the pre-loop or the post-loop until the condition holds true in the main
1239   // loop.  Scale_con, offset and limit are all loop invariant.
1240   void add_constraint( int stride_con, int scale_con, Node *offset, Node *low_limit, Node *upper_limit, Node *pre_ctrl, Node **pre_limit, Node **main_limit );
1241   // Helper function for add_constraint().
1242   Node* adjust_limit(int stride_con, Node * scale, Node *offset, Node *rc_limit, Node *loop_limit, Node *pre_ctrl, bool round_up);
1243 
1244   // Partially peel loop up through last_peel node.
1245   bool partial_peel( IdealLoopTree *loop, Node_List &amp;old_new );
1246 
1247   // Create a scheduled list of nodes control dependent on ctrl set.
1248   void scheduled_nodelist( IdealLoopTree *loop, VectorSet&amp; ctrl, Node_List &amp;sched );
1249   // Has a use in the vector set
1250   bool has_use_in_set( Node* n, VectorSet&amp; vset );
1251   // Has use internal to the vector set (ie. not in a phi at the loop head)
1252   bool has_use_internal_to_set( Node* n, VectorSet&amp; vset, IdealLoopTree *loop );
</pre>
<hr />
<pre>
1334   bool match_fill_loop(IdealLoopTree* lpt, Node*&amp; store, Node*&amp; store_value,
1335                        Node*&amp; shift, Node*&amp; offset);
1336 
1337 private:
1338   // Return a type based on condition control flow
1339   const TypeInt* filtered_type( Node *n, Node* n_ctrl);
1340   const TypeInt* filtered_type( Node *n ) { return filtered_type(n, NULL); }
1341  // Helpers for filtered type
1342   const TypeInt* filtered_type_from_dominators( Node* val, Node *val_ctrl);
1343 
1344   // Helper functions
1345   Node *spinup( Node *iff, Node *new_false, Node *new_true, Node *region, Node *phi, small_cache *cache );
1346   Node *find_use_block( Node *use, Node *def, Node *old_false, Node *new_false, Node *old_true, Node *new_true );
1347   void handle_use( Node *use, Node *def, small_cache *cache, Node *region_dom, Node *new_false, Node *new_true, Node *old_false, Node *old_true );
1348   bool split_up( Node *n, Node *blk1, Node *blk2 );
1349   void sink_use( Node *use, Node *post_loop );
1350   Node *place_near_use( Node *useblock ) const;
1351   Node* try_move_store_before_loop(Node* n, Node *n_ctrl);
1352   void try_move_store_after_loop(Node* n);
1353   bool identical_backtoback_ifs(Node *n);

1354   bool can_split_if(Node *n_ctrl);
1355 
1356   // Determine if a method is too big for a/another round of split-if, based on
1357   // a magic (approximate) ratio derived from the equally magic constant 35000,
1358   // previously used for this purpose (but without relating to the node limit).
1359   bool must_throttle_split_if() {
1360     uint threshold = C-&gt;max_node_limit() * 2 / 5;
1361     return C-&gt;live_nodes() &gt; threshold;
1362   }
1363 
1364   // A simplistic node request tracking mechanism, where
1365   //   = UINT_MAX   Request not valid or made final.
1366   //   &lt; UINT_MAX   Nodes currently requested (estimate).
1367   uint _nodes_required;
1368 
1369   enum { REQUIRE_MIN = 70 };
1370 
1371   uint nodes_required() const { return _nodes_required; }
1372 
1373   // Given the _currently_  available number of nodes, check  whether there is
</pre>
</td>
<td>
<hr />
<pre>
  59   virtual uint size_of() const { return sizeof(*this); }
  60 protected:
  61   uint _loop_flags;
  62   // Names for flag bitfields
  63   enum { Normal=0, Pre=1, Main=2, Post=3, PreMainPostFlagsMask=3,
  64          MainHasNoPreLoop=4,
  65          HasExactTripCount=8,
  66          InnerLoop=16,
  67          PartialPeelLoop=32,
  68          PartialPeelFailed=64,
  69          HasReductions=128,
  70          WasSlpAnalyzed=256,
  71          PassedSlpAnalysis=512,
  72          DoUnrollOnly=1024,
  73          VectorizedLoop=2048,
  74          HasAtomicPostLoop=4096,
  75          HasRangeChecks=8192,
  76          IsMultiversioned=16384,
  77          StripMined=32768,
  78          SubwordLoop=65536,
<span class="line-modified">  79          ProfileTripFailed=131072,</span>
<span class="line-added">  80          FlattenedArrays=262144};</span>
  81   char _unswitch_count;
  82   enum { _unswitch_max=3 };
  83   char _postloop_flags;
  84   enum { LoopNotRCEChecked = 0, LoopRCEChecked = 1, RCEPostLoop = 2 };
  85 
  86   // Expected trip count from profile data
  87   float _profile_trip_cnt;
  88 
  89 public:
  90   // Names for edge indices
  91   enum { Self=0, EntryControl, LoopBackControl };
  92 
  93   bool is_inner_loop() const { return _loop_flags &amp; InnerLoop; }
  94   void set_inner_loop() { _loop_flags |= InnerLoop; }
  95 
  96   bool range_checks_present() const { return _loop_flags &amp; HasRangeChecks; }
  97   bool is_multiversioned() const { return _loop_flags &amp; IsMultiversioned; }
  98   bool is_vectorized_loop() const { return _loop_flags &amp; VectorizedLoop; }
  99   bool is_partial_peel_loop() const { return _loop_flags &amp; PartialPeelLoop; }
 100   void set_partial_peel_loop() { _loop_flags |= PartialPeelLoop; }
 101   bool partial_peel_has_failed() const { return _loop_flags &amp; PartialPeelFailed; }
 102   bool is_strip_mined() const { return _loop_flags &amp; StripMined; }
 103   bool is_profile_trip_failed() const { return _loop_flags &amp; ProfileTripFailed; }
 104   bool is_subword_loop() const { return _loop_flags &amp; SubwordLoop; }
<span class="line-added"> 105   bool is_flattened_arrays() const { return _loop_flags &amp; FlattenedArrays; }</span>
 106 
 107   void mark_partial_peel_failed() { _loop_flags |= PartialPeelFailed; }
 108   void mark_has_reductions() { _loop_flags |= HasReductions; }
 109   void mark_was_slp() { _loop_flags |= WasSlpAnalyzed; }
 110   void mark_passed_slp() { _loop_flags |= PassedSlpAnalysis; }
 111   void mark_do_unroll_only() { _loop_flags |= DoUnrollOnly; }
 112   void mark_loop_vectorized() { _loop_flags |= VectorizedLoop; }
 113   void mark_has_atomic_post_loop() { _loop_flags |= HasAtomicPostLoop; }
 114   void mark_has_range_checks() { _loop_flags |=  HasRangeChecks; }
 115   void mark_is_multiversioned() { _loop_flags |= IsMultiversioned; }
 116   void mark_strip_mined() { _loop_flags |= StripMined; }
 117   void clear_strip_mined() { _loop_flags &amp;= ~StripMined; }
 118   void mark_profile_trip_failed() { _loop_flags |= ProfileTripFailed; }
 119   void mark_subword_loop() { _loop_flags |= SubwordLoop; }
<span class="line-added"> 120   void mark_flattened_arrays() { _loop_flags |= FlattenedArrays; }</span>
 121 
 122   int unswitch_max() { return _unswitch_max; }
 123   int unswitch_count() { return _unswitch_count; }
 124 
 125   int has_been_range_checked() const { return _postloop_flags &amp; LoopRCEChecked; }
 126   void set_has_been_range_checked() { _postloop_flags |= LoopRCEChecked; }
 127   int is_rce_post_loop() const { return _postloop_flags &amp; RCEPostLoop; }
 128   void set_is_rce_post_loop() { _postloop_flags |= RCEPostLoop; }
 129 
 130   void set_unswitch_count(int val) {
 131     assert (val &lt;= unswitch_max(), &quot;too many unswitches&quot;);
 132     _unswitch_count = val;
 133   }
 134 
 135   void set_profile_trip_cnt(float ptc) { _profile_trip_cnt = ptc; }
 136   float profile_trip_cnt()             { return _profile_trip_cnt; }
 137 
 138   LoopNode(Node *entry, Node *backedge)
 139     : RegionNode(3), _loop_flags(0), _unswitch_count(0),
 140       _postloop_flags(0), _profile_trip_cnt(COUNT_UNKNOWN)  {
</pre>
<hr />
<pre>
1215                                         Node_List &amp;old_new,
1216                                         int opcode,
1217                                         CloneLoopMode mode);
1218 
1219   // Clone a loop and return the clone head (clone_loop_head).
1220   // Added nodes include int(1), int(0) - disconnected, If, IfTrue, IfFalse,
1221   // This routine was created for usage in CountedLoopReserveKit.
1222   //
1223   //    int(1) -&gt; If -&gt; IfTrue -&gt; original_loop_head
1224   //              |
1225   //              V
1226   //           IfFalse -&gt; clone_loop_head (returned by function pointer)
1227   //
1228   LoopNode* create_reserve_version_of_loop(IdealLoopTree *loop, CountedLoopReserveKit* lk);
1229   // Clone loop with an invariant test (that does not exit) and
1230   // insert a clone of the test that selects which version to
1231   // execute.
1232   void do_unswitching (IdealLoopTree *loop, Node_List &amp;old_new);
1233 
1234   // Find candidate &quot;if&quot; for unswitching
<span class="line-modified">1235   IfNode* find_unswitching_candidate(const IdealLoopTree *loop, Node_List&amp; unswitch_iffs) const;</span>
1236 
1237   // Range Check Elimination uses this function!
1238   // Constrain the main loop iterations so the affine function:
1239   //    low_limit &lt;= scale_con * I + offset  &lt;  upper_limit
1240   // always holds true.  That is, either increase the number of iterations in
1241   // the pre-loop or the post-loop until the condition holds true in the main
1242   // loop.  Scale_con, offset and limit are all loop invariant.
1243   void add_constraint( int stride_con, int scale_con, Node *offset, Node *low_limit, Node *upper_limit, Node *pre_ctrl, Node **pre_limit, Node **main_limit );
1244   // Helper function for add_constraint().
1245   Node* adjust_limit(int stride_con, Node * scale, Node *offset, Node *rc_limit, Node *loop_limit, Node *pre_ctrl, bool round_up);
1246 
1247   // Partially peel loop up through last_peel node.
1248   bool partial_peel( IdealLoopTree *loop, Node_List &amp;old_new );
1249 
1250   // Create a scheduled list of nodes control dependent on ctrl set.
1251   void scheduled_nodelist( IdealLoopTree *loop, VectorSet&amp; ctrl, Node_List &amp;sched );
1252   // Has a use in the vector set
1253   bool has_use_in_set( Node* n, VectorSet&amp; vset );
1254   // Has use internal to the vector set (ie. not in a phi at the loop head)
1255   bool has_use_internal_to_set( Node* n, VectorSet&amp; vset, IdealLoopTree *loop );
</pre>
<hr />
<pre>
1337   bool match_fill_loop(IdealLoopTree* lpt, Node*&amp; store, Node*&amp; store_value,
1338                        Node*&amp; shift, Node*&amp; offset);
1339 
1340 private:
1341   // Return a type based on condition control flow
1342   const TypeInt* filtered_type( Node *n, Node* n_ctrl);
1343   const TypeInt* filtered_type( Node *n ) { return filtered_type(n, NULL); }
1344  // Helpers for filtered type
1345   const TypeInt* filtered_type_from_dominators( Node* val, Node *val_ctrl);
1346 
1347   // Helper functions
1348   Node *spinup( Node *iff, Node *new_false, Node *new_true, Node *region, Node *phi, small_cache *cache );
1349   Node *find_use_block( Node *use, Node *def, Node *old_false, Node *new_false, Node *old_true, Node *new_true );
1350   void handle_use( Node *use, Node *def, small_cache *cache, Node *region_dom, Node *new_false, Node *new_true, Node *old_false, Node *old_true );
1351   bool split_up( Node *n, Node *blk1, Node *blk2 );
1352   void sink_use( Node *use, Node *post_loop );
1353   Node *place_near_use( Node *useblock ) const;
1354   Node* try_move_store_before_loop(Node* n, Node *n_ctrl);
1355   void try_move_store_after_loop(Node* n);
1356   bool identical_backtoback_ifs(Node *n);
<span class="line-added">1357   bool flatten_array_element_type_check(Node *n);</span>
1358   bool can_split_if(Node *n_ctrl);
1359 
1360   // Determine if a method is too big for a/another round of split-if, based on
1361   // a magic (approximate) ratio derived from the equally magic constant 35000,
1362   // previously used for this purpose (but without relating to the node limit).
1363   bool must_throttle_split_if() {
1364     uint threshold = C-&gt;max_node_limit() * 2 / 5;
1365     return C-&gt;live_nodes() &gt; threshold;
1366   }
1367 
1368   // A simplistic node request tracking mechanism, where
1369   //   = UINT_MAX   Request not valid or made final.
1370   //   &lt; UINT_MAX   Nodes currently requested (estimate).
1371   uint _nodes_required;
1372 
1373   enum { REQUIRE_MIN = 70 };
1374 
1375   uint nodes_required() const { return _nodes_required; }
1376 
1377   // Given the _currently_  available number of nodes, check  whether there is
</pre>
</td>
</tr>
</table>
<center><a href="live.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="loopopts.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
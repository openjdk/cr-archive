<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
    <script type="text/javascript" src="../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>   1 /*
   2  * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #include &quot;asm/macroAssembler.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;ci/ciReplay.hpp&quot;
  29 #include &quot;classfile/systemDictionary.hpp&quot;
  30 #include &quot;code/exceptionHandlerTable.hpp&quot;
  31 #include &quot;code/nmethod.hpp&quot;
  32 #include &quot;compiler/compileBroker.hpp&quot;
  33 #include &quot;compiler/compileLog.hpp&quot;
  34 #include &quot;compiler/disassembler.hpp&quot;
  35 #include &quot;compiler/oopMap.hpp&quot;
  36 #include &quot;gc/shared/barrierSet.hpp&quot;
  37 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;memory/resourceArea.hpp&quot;
  40 #include &quot;opto/addnode.hpp&quot;
  41 #include &quot;opto/block.hpp&quot;
  42 #include &quot;opto/c2compiler.hpp&quot;
  43 #include &quot;opto/callGenerator.hpp&quot;
  44 #include &quot;opto/callnode.hpp&quot;
  45 #include &quot;opto/castnode.hpp&quot;
  46 #include &quot;opto/cfgnode.hpp&quot;
  47 #include &quot;opto/chaitin.hpp&quot;
  48 #include &quot;opto/compile.hpp&quot;
  49 #include &quot;opto/connode.hpp&quot;
  50 #include &quot;opto/convertnode.hpp&quot;
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added">  54 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  55 #include &quot;opto/loopnode.hpp&quot;
  56 #include &quot;opto/machnode.hpp&quot;
  57 #include &quot;opto/macro.hpp&quot;
  58 #include &quot;opto/matcher.hpp&quot;
  59 #include &quot;opto/mathexactnode.hpp&quot;
  60 #include &quot;opto/memnode.hpp&quot;
  61 #include &quot;opto/mulnode.hpp&quot;
  62 #include &quot;opto/narrowptrnode.hpp&quot;
  63 #include &quot;opto/node.hpp&quot;
  64 #include &quot;opto/opcodes.hpp&quot;
  65 #include &quot;opto/output.hpp&quot;
  66 #include &quot;opto/parse.hpp&quot;
  67 #include &quot;opto/phaseX.hpp&quot;
  68 #include &quot;opto/rootnode.hpp&quot;
  69 #include &quot;opto/runtime.hpp&quot;
  70 #include &quot;opto/stringopts.hpp&quot;
  71 #include &quot;opto/type.hpp&quot;
  72 #include &quot;opto/vectornode.hpp&quot;
  73 #include &quot;runtime/arguments.hpp&quot;
  74 #include &quot;runtime/sharedRuntime.hpp&quot;
  75 #include &quot;runtime/signature.hpp&quot;
  76 #include &quot;runtime/stubRoutines.hpp&quot;
  77 #include &quot;runtime/timer.hpp&quot;
  78 #include &quot;utilities/align.hpp&quot;
  79 #include &quot;utilities/copy.hpp&quot;
  80 #include &quot;utilities/macros.hpp&quot;
  81 #include &quot;utilities/resourceHash.hpp&quot;
  82 
  83 
  84 // -------------------- Compile::mach_constant_base_node -----------------------
  85 // Constant table base node singleton.
  86 MachConstantBaseNode* Compile::mach_constant_base_node() {
  87   if (_mach_constant_base_node == NULL) {
  88     _mach_constant_base_node = new MachConstantBaseNode();
  89     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  90   }
  91   return _mach_constant_base_node;
  92 }
  93 
  94 
  95 /// Support for intrinsics.
  96 
  97 // Return the index at which m must be inserted (or already exists).
  98 // The sort order is by the address of the ciMethod, with is_virtual as minor key.
  99 class IntrinsicDescPair {
 100  private:
 101   ciMethod* _m;
 102   bool _is_virtual;
 103  public:
 104   IntrinsicDescPair(ciMethod* m, bool is_virtual) : _m(m), _is_virtual(is_virtual) {}
 105   static int compare(IntrinsicDescPair* const&amp; key, CallGenerator* const&amp; elt) {
 106     ciMethod* m= elt-&gt;method();
 107     ciMethod* key_m = key-&gt;_m;
 108     if (key_m &lt; m)      return -1;
 109     else if (key_m &gt; m) return 1;
 110     else {
 111       bool is_virtual = elt-&gt;is_virtual();
 112       bool key_virtual = key-&gt;_is_virtual;
 113       if (key_virtual &lt; is_virtual)      return -1;
 114       else if (key_virtual &gt; is_virtual) return 1;
 115       else                               return 0;
 116     }
 117   }
 118 };
 119 int Compile::intrinsic_insertion_index(ciMethod* m, bool is_virtual, bool&amp; found) {
 120 #ifdef ASSERT
 121   for (int i = 1; i &lt; _intrinsics-&gt;length(); i++) {
 122     CallGenerator* cg1 = _intrinsics-&gt;at(i-1);
 123     CallGenerator* cg2 = _intrinsics-&gt;at(i);
 124     assert(cg1-&gt;method() != cg2-&gt;method()
 125            ? cg1-&gt;method()     &lt; cg2-&gt;method()
 126            : cg1-&gt;is_virtual() &lt; cg2-&gt;is_virtual(),
 127            &quot;compiler intrinsics list must stay sorted&quot;);
 128   }
 129 #endif
 130   IntrinsicDescPair pair(m, is_virtual);
 131   return _intrinsics-&gt;find_sorted&lt;IntrinsicDescPair*, IntrinsicDescPair::compare&gt;(&amp;pair, found);
 132 }
 133 
 134 void Compile::register_intrinsic(CallGenerator* cg) {
 135   if (_intrinsics == NULL) {
 136     _intrinsics = new (comp_arena())GrowableArray&lt;CallGenerator*&gt;(comp_arena(), 60, 0, NULL);
 137   }
 138   int len = _intrinsics-&gt;length();
 139   bool found = false;
 140   int index = intrinsic_insertion_index(cg-&gt;method(), cg-&gt;is_virtual(), found);
 141   assert(!found, &quot;registering twice&quot;);
 142   _intrinsics-&gt;insert_before(index, cg);
 143   assert(find_intrinsic(cg-&gt;method(), cg-&gt;is_virtual()) == cg, &quot;registration worked&quot;);
 144 }
 145 
 146 CallGenerator* Compile::find_intrinsic(ciMethod* m, bool is_virtual) {
 147   assert(m-&gt;is_loaded(), &quot;don&#39;t try this on unloaded methods&quot;);
 148   if (_intrinsics != NULL) {
 149     bool found = false;
 150     int index = intrinsic_insertion_index(m, is_virtual, found);
 151      if (found) {
 152       return _intrinsics-&gt;at(index);
 153     }
 154   }
 155   // Lazily create intrinsics for intrinsic IDs well-known in the runtime.
 156   if (m-&gt;intrinsic_id() != vmIntrinsics::_none &amp;&amp;
 157       m-&gt;intrinsic_id() &lt;= vmIntrinsics::LAST_COMPILER_INLINE) {
 158     CallGenerator* cg = make_vm_intrinsic(m, is_virtual);
 159     if (cg != NULL) {
 160       // Save it for next time:
 161       register_intrinsic(cg);
 162       return cg;
 163     } else {
 164       gather_intrinsic_statistics(m-&gt;intrinsic_id(), is_virtual, _intrinsic_disabled);
 165     }
 166   }
 167   return NULL;
 168 }
 169 
 170 // Compile:: register_library_intrinsics and make_vm_intrinsic are defined
 171 // in library_call.cpp.
 172 
 173 
 174 #ifndef PRODUCT
 175 // statistics gathering...
 176 
 177 juint  Compile::_intrinsic_hist_count[vmIntrinsics::ID_LIMIT] = {0};
 178 jubyte Compile::_intrinsic_hist_flags[vmIntrinsics::ID_LIMIT] = {0};
 179 
 180 bool Compile::gather_intrinsic_statistics(vmIntrinsics::ID id, bool is_virtual, int flags) {
 181   assert(id &gt; vmIntrinsics::_none &amp;&amp; id &lt; vmIntrinsics::ID_LIMIT, &quot;oob&quot;);
 182   int oflags = _intrinsic_hist_flags[id];
 183   assert(flags != 0, &quot;what happened?&quot;);
 184   if (is_virtual) {
 185     flags |= _intrinsic_virtual;
 186   }
 187   bool changed = (flags != oflags);
 188   if ((flags &amp; _intrinsic_worked) != 0) {
 189     juint count = (_intrinsic_hist_count[id] += 1);
 190     if (count == 1) {
 191       changed = true;           // first time
 192     }
 193     // increment the overall count also:
 194     _intrinsic_hist_count[vmIntrinsics::_none] += 1;
 195   }
 196   if (changed) {
 197     if (((oflags ^ flags) &amp; _intrinsic_virtual) != 0) {
 198       // Something changed about the intrinsic&#39;s virtuality.
 199       if ((flags &amp; _intrinsic_virtual) != 0) {
 200         // This is the first use of this intrinsic as a virtual call.
 201         if (oflags != 0) {
 202           // We already saw it as a non-virtual, so note both cases.
 203           flags |= _intrinsic_both;
 204         }
 205       } else if ((oflags &amp; _intrinsic_both) == 0) {
 206         // This is the first use of this intrinsic as a non-virtual
 207         flags |= _intrinsic_both;
 208       }
 209     }
 210     _intrinsic_hist_flags[id] = (jubyte) (oflags | flags);
 211   }
 212   // update the overall flags also:
 213   _intrinsic_hist_flags[vmIntrinsics::_none] |= (jubyte) flags;
 214   return changed;
 215 }
 216 
 217 static char* format_flags(int flags, char* buf) {
 218   buf[0] = 0;
 219   if ((flags &amp; Compile::_intrinsic_worked) != 0)    strcat(buf, &quot;,worked&quot;);
 220   if ((flags &amp; Compile::_intrinsic_failed) != 0)    strcat(buf, &quot;,failed&quot;);
 221   if ((flags &amp; Compile::_intrinsic_disabled) != 0)  strcat(buf, &quot;,disabled&quot;);
 222   if ((flags &amp; Compile::_intrinsic_virtual) != 0)   strcat(buf, &quot;,virtual&quot;);
 223   if ((flags &amp; Compile::_intrinsic_both) != 0)      strcat(buf, &quot;,nonvirtual&quot;);
 224   if (buf[0] == 0)  strcat(buf, &quot;,&quot;);
 225   assert(buf[0] == &#39;,&#39;, &quot;must be&quot;);
 226   return &amp;buf[1];
 227 }
 228 
 229 void Compile::print_intrinsic_statistics() {
 230   char flagsbuf[100];
 231   ttyLocker ttyl;
 232   if (xtty != NULL)  xtty-&gt;head(&quot;statistics type=&#39;intrinsic&#39;&quot;);
 233   tty-&gt;print_cr(&quot;Compiler intrinsic usage:&quot;);
 234   juint total = _intrinsic_hist_count[vmIntrinsics::_none];
 235   if (total == 0)  total = 1;  // avoid div0 in case of no successes
 236   #define PRINT_STAT_LINE(name, c, f) \
 237     tty-&gt;print_cr(&quot;  %4d (%4.1f%%) %s (%s)&quot;, (int)(c), ((c) * 100.0) / total, name, f);
 238   for (int index = 1 + (int)vmIntrinsics::_none; index &lt; (int)vmIntrinsics::ID_LIMIT; index++) {
 239     vmIntrinsics::ID id = (vmIntrinsics::ID) index;
 240     int   flags = _intrinsic_hist_flags[id];
 241     juint count = _intrinsic_hist_count[id];
 242     if ((flags | count) != 0) {
 243       PRINT_STAT_LINE(vmIntrinsics::name_at(id), count, format_flags(flags, flagsbuf));
 244     }
 245   }
 246   PRINT_STAT_LINE(&quot;total&quot;, total, format_flags(_intrinsic_hist_flags[vmIntrinsics::_none], flagsbuf));
 247   if (xtty != NULL)  xtty-&gt;tail(&quot;statistics&quot;);
 248 }
 249 
 250 void Compile::print_statistics() {
 251   { ttyLocker ttyl;
 252     if (xtty != NULL)  xtty-&gt;head(&quot;statistics type=&#39;opto&#39;&quot;);
 253     Parse::print_statistics();
 254     PhaseCCP::print_statistics();
 255     PhaseRegAlloc::print_statistics();
 256     PhaseOutput::print_statistics();
 257     PhasePeephole::print_statistics();
 258     PhaseIdealLoop::print_statistics();
 259     if (xtty != NULL)  xtty-&gt;tail(&quot;statistics&quot;);
 260   }
 261   if (_intrinsic_hist_flags[vmIntrinsics::_none] != 0) {
 262     // put this under its own &lt;statistics&gt; element.
 263     print_intrinsic_statistics();
 264   }
 265 }
 266 #endif //PRODUCT
 267 
 268 void Compile::gvn_replace_by(Node* n, Node* nn) {
 269   for (DUIterator_Last imin, i = n-&gt;last_outs(imin); i &gt;= imin; ) {
 270     Node* use = n-&gt;last_out(i);
 271     bool is_in_table = initial_gvn()-&gt;hash_delete(use);
 272     uint uses_found = 0;
 273     for (uint j = 0; j &lt; use-&gt;len(); j++) {
 274       if (use-&gt;in(j) == n) {
 275         if (j &lt; use-&gt;req())
 276           use-&gt;set_req(j, nn);
 277         else
 278           use-&gt;set_prec(j, nn);
 279         uses_found++;
 280       }
 281     }
 282     if (is_in_table) {
 283       // reinsert into table
 284       initial_gvn()-&gt;hash_find_insert(use);
 285     }
 286     record_for_igvn(use);
 287     i -= uses_found;    // we deleted 1 or more copies of this edge
 288   }
 289 }
 290 
 291 
 292 static inline bool not_a_node(const Node* n) {
 293   if (n == NULL)                   return true;
 294   if (((intptr_t)n &amp; 1) != 0)      return true;  // uninitialized, etc.
 295   if (*(address*)n == badAddress)  return true;  // kill by Node::destruct
 296   return false;
 297 }
 298 
 299 // Identify all nodes that are reachable from below, useful.
 300 // Use breadth-first pass that records state in a Unique_Node_List,
 301 // recursive traversal is slower.
 302 void Compile::identify_useful_nodes(Unique_Node_List &amp;useful) {
 303   int estimated_worklist_size = live_nodes();
 304   useful.map( estimated_worklist_size, NULL );  // preallocate space
 305 
 306   // Initialize worklist
 307   if (root() != NULL)     { useful.push(root()); }
 308   // If &#39;top&#39; is cached, declare it useful to preserve cached node
 309   if( cached_top_node() ) { useful.push(cached_top_node()); }
 310 
 311   // Push all useful nodes onto the list, breadthfirst
 312   for( uint next = 0; next &lt; useful.size(); ++next ) {
 313     assert( next &lt; unique(), &quot;Unique useful nodes &lt; total nodes&quot;);
 314     Node *n  = useful.at(next);
 315     uint max = n-&gt;len();
 316     for( uint i = 0; i &lt; max; ++i ) {
 317       Node *m = n-&gt;in(i);
 318       if (not_a_node(m))  continue;
 319       useful.push(m);
 320     }
 321   }
 322 }
 323 
 324 // Update dead_node_list with any missing dead nodes using useful
 325 // list. Consider all non-useful nodes to be useless i.e., dead nodes.
 326 void Compile::update_dead_node_list(Unique_Node_List &amp;useful) {
 327   uint max_idx = unique();
 328   VectorSet&amp; useful_node_set = useful.member_set();
 329 
 330   for (uint node_idx = 0; node_idx &lt; max_idx; node_idx++) {
 331     // If node with index node_idx is not in useful set,
 332     // mark it as dead in dead node list.
 333     if (!useful_node_set.test(node_idx)) {
 334       record_dead_node(node_idx);
 335     }
 336   }
 337 }
 338 
 339 void Compile::remove_useless_late_inlines(GrowableArray&lt;CallGenerator*&gt;* inlines, Unique_Node_List &amp;useful) {
 340   int shift = 0;
 341   for (int i = 0; i &lt; inlines-&gt;length(); i++) {
 342     CallGenerator* cg = inlines-&gt;at(i);
 343     CallNode* call = cg-&gt;call_node();
 344     if (shift &gt; 0) {
 345       inlines-&gt;at_put(i-shift, cg);
 346     }
 347     if (!useful.member(call)) {
 348       shift++;
 349     }
 350   }
 351   inlines-&gt;trunc_to(inlines-&gt;length()-shift);
 352 }
 353 
 354 // Disconnect all useless nodes by disconnecting those at the boundary.
 355 void Compile::remove_useless_nodes(Unique_Node_List &amp;useful) {
 356   uint next = 0;
 357   while (next &lt; useful.size()) {
 358     Node *n = useful.at(next++);
 359     if (n-&gt;is_SafePoint()) {
 360       // We&#39;re done with a parsing phase. Replaced nodes are not valid
 361       // beyond that point.
 362       n-&gt;as_SafePoint()-&gt;delete_replaced_nodes();
 363     }
 364     // Use raw traversal of out edges since this code removes out edges
 365     int max = n-&gt;outcnt();
 366     for (int j = 0; j &lt; max; ++j) {
 367       Node* child = n-&gt;raw_out(j);
 368       if (! useful.member(child)) {
 369         assert(!child-&gt;is_top() || child != top(),
 370                &quot;If top is cached in Compile object it is in useful list&quot;);
 371         // Only need to remove this out-edge to the useless node
 372         n-&gt;raw_del_out(j);
 373         --j;
 374         --max;
 375       }
 376     }
 377     if (n-&gt;outcnt() == 1 &amp;&amp; n-&gt;has_special_unique_user()) {
 378       record_for_igvn(n-&gt;unique_out());
 379     }
 380   }
 381   // Remove useless macro and predicate opaq nodes
 382   for (int i = C-&gt;macro_count()-1; i &gt;= 0; i--) {
 383     Node* n = C-&gt;macro_node(i);
 384     if (!useful.member(n)) {
 385       remove_macro_node(n);
 386     }
 387   }
 388   // Remove useless CastII nodes with range check dependency
 389   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 390     Node* cast = range_check_cast_node(i);
 391     if (!useful.member(cast)) {
 392       remove_range_check_cast(cast);
 393     }
 394   }
 395   // Remove useless expensive nodes
 396   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 397     Node* n = C-&gt;expensive_node(i);
 398     if (!useful.member(n)) {
 399       remove_expensive_node(n);
 400     }
 401   }
 402   // Remove useless Opaque4 nodes
 403   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 404     Node* opaq = opaque4_node(i);
 405     if (!useful.member(opaq)) {
 406       remove_opaque4_node(opaq);
 407     }
 408   }
<a name="2" id="anc2"></a><span class="line-added"> 409   // Remove useless inline type nodes</span>
<span class="line-added"> 410   for (int i = _inline_type_nodes-&gt;length() - 1; i &gt;= 0; i--) {</span>
<span class="line-added"> 411     Node* vt = _inline_type_nodes-&gt;at(i);</span>
<span class="line-added"> 412     if (!useful.member(vt)) {</span>
<span class="line-added"> 413       _inline_type_nodes-&gt;remove(vt);</span>
<span class="line-added"> 414     }</span>
<span class="line-added"> 415   }</span>
 416   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 417   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 418   // clean up the late inline lists
 419   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 420   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 421   remove_useless_late_inlines(&amp;_late_inlines, useful);
 422   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 423 }
 424 
 425 // ============================================================================
 426 //------------------------------CompileWrapper---------------------------------
 427 class CompileWrapper : public StackObj {
 428   Compile *const _compile;
 429  public:
 430   CompileWrapper(Compile* compile);
 431 
 432   ~CompileWrapper();
 433 };
 434 
 435 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
 436   // the Compile* pointer is stored in the current ciEnv:
 437   ciEnv* env = compile-&gt;env();
 438   assert(env == ciEnv::current(), &quot;must already be a ciEnv active&quot;);
 439   assert(env-&gt;compiler_data() == NULL, &quot;compile already active?&quot;);
 440   env-&gt;set_compiler_data(compile);
 441   assert(compile == Compile::current(), &quot;sanity&quot;);
 442 
 443   compile-&gt;set_type_dict(NULL);
 444   compile-&gt;set_clone_map(new Dict(cmpkey, hashkey, _compile-&gt;comp_arena()));
 445   compile-&gt;clone_map().set_clone_idx(0);
 446   compile-&gt;set_type_last_size(0);
 447   compile-&gt;set_last_tf(NULL, NULL);
 448   compile-&gt;set_indexSet_arena(NULL);
 449   compile-&gt;set_indexSet_free_block_list(NULL);
 450   compile-&gt;init_type_arena();
 451   Type::Initialize(compile);
 452   _compile-&gt;begin_method();
 453   _compile-&gt;clone_map().set_debug(_compile-&gt;has_method() &amp;&amp; _compile-&gt;directive()-&gt;CloneMapDebugOption);
 454 }
 455 CompileWrapper::~CompileWrapper() {
 456   _compile-&gt;end_method();
 457   _compile-&gt;env()-&gt;set_compiler_data(NULL);
 458 }
 459 
 460 
 461 //----------------------------print_compile_messages---------------------------
 462 void Compile::print_compile_messages() {
 463 #ifndef PRODUCT
 464   // Check if recompiling
 465   if (_subsume_loads == false &amp;&amp; PrintOpto) {
 466     // Recompiling without allowing machine instructions to subsume loads
 467     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 468     tty-&gt;print_cr(&quot;** Bailout: Recompile without subsuming loads          **&quot;);
 469     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 470   }
 471   if (_do_escape_analysis != DoEscapeAnalysis &amp;&amp; PrintOpto) {
 472     // Recompiling without escape analysis
 473     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 474     tty-&gt;print_cr(&quot;** Bailout: Recompile without escape analysis          **&quot;);
 475     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 476   }
 477   if (_eliminate_boxing != EliminateAutoBox &amp;&amp; PrintOpto) {
 478     // Recompiling without boxing elimination
 479     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 480     tty-&gt;print_cr(&quot;** Bailout: Recompile without boxing elimination       **&quot;);
 481     tty-&gt;print_cr(&quot;*********************************************************&quot;);
 482   }
 483   if (C-&gt;directive()-&gt;BreakAtCompileOption) {
 484     // Open the debugger when compiling this method.
 485     tty-&gt;print(&quot;### Breaking when compiling: &quot;);
 486     method()-&gt;print_short_name();
 487     tty-&gt;cr();
 488     BREAKPOINT;
 489   }
 490 
 491   if( PrintOpto ) {
 492     if (is_osr_compilation()) {
 493       tty-&gt;print(&quot;[OSR]%3d&quot;, _compile_id);
 494     } else {
 495       tty-&gt;print(&quot;%3d&quot;, _compile_id);
 496     }
 497   }
 498 #endif
 499 }
 500 
 501 // ============================================================================
 502 //------------------------------Compile standard-------------------------------
 503 debug_only( int Compile::_debug_idx = 100000; )
 504 
 505 // Compile a method.  entry_bci is -1 for normal compilations and indicates
 506 // the continuation bci for on stack replacement.
 507 
 508 
 509 Compile::Compile( ciEnv* ci_env, ciMethod* target, int osr_bci,
 510                   bool subsume_loads, bool do_escape_analysis, bool eliminate_boxing, DirectiveSet* directive)
 511                 : Phase(Compiler),
 512                   _compile_id(ci_env-&gt;compile_id()),
 513                   _save_argument_registers(false),
 514                   _subsume_loads(subsume_loads),
 515                   _do_escape_analysis(do_escape_analysis),
 516                   _eliminate_boxing(eliminate_boxing),
 517                   _method(target),
 518                   _entry_bci(osr_bci),
 519                   _stub_function(NULL),
 520                   _stub_name(NULL),
 521                   _stub_entry_point(NULL),
 522                   _max_node_limit(MaxNodeLimit),
 523                   _inlining_progress(false),
 524                   _inlining_incrementally(false),
 525                   _do_cleanup(false),
 526                   _has_reserved_stack_access(target-&gt;has_reserved_stack_access()),
 527 #ifndef PRODUCT
 528                   _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 529                   _print_ideal(directive-&gt;PrintIdealOption),
 530 #endif
 531                   _has_method_handle_invokes(false),
 532                   _clinit_barrier_on_entry(false),
 533                   _comp_arena(mtCompiler),
 534                   _barrier_set_state(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;create_barrier_state(comp_arena())),
 535                   _env(ci_env),
 536                   _directive(directive),
 537                   _log(ci_env-&gt;log()),
 538                   _failure_reason(NULL),
 539                   _congraph(NULL),
 540                   NOT_PRODUCT(_printer(NULL) COMMA)
 541                   _dead_node_list(comp_arena()),
 542                   _dead_node_count(0),
 543                   _node_arena(mtCompiler),
 544                   _old_arena(mtCompiler),
 545                   _mach_constant_base_node(NULL),
 546                   _Compile_types(mtCompiler),
 547                   _initial_gvn(NULL),
 548                   _for_igvn(NULL),
 549                   _warm_calls(NULL),
 550                   _late_inlines(comp_arena(), 2, 0, NULL),
 551                   _string_late_inlines(comp_arena(), 2, 0, NULL),
 552                   _boxing_late_inlines(comp_arena(), 2, 0, NULL),
 553                   _late_inlines_pos(0),
 554                   _number_of_mh_late_inlines(0),
 555                   _print_inlining_stream(NULL),
 556                   _print_inlining_list(NULL),
 557                   _print_inlining_idx(0),
 558                   _print_inlining_output(NULL),
 559                   _replay_inline_data(NULL),
 560                   _java_calls(0),
 561                   _inner_loops(0),
 562                   _interpreter_frame_size(0)
 563 #ifndef PRODUCT
 564                   , _in_dump_cnt(0)
 565 #endif
 566 {
 567   C = this;
 568   CompileWrapper cw(this);
 569 
 570   if (CITimeVerbose) {
 571     tty-&gt;print(&quot; &quot;);
 572     target-&gt;holder()-&gt;name()-&gt;print();
 573     tty-&gt;print(&quot;.&quot;);
 574     target-&gt;print_short_name();
 575     tty-&gt;print(&quot;  &quot;);
 576   }
 577   TraceTime t1(&quot;Total compilation time&quot;, &amp;_t_totalCompilation, CITime, CITimeVerbose);
 578   TraceTime t2(NULL, &amp;_t_methodCompilation, CITime, false);
 579 
 580 #if defined(SUPPORT_ASSEMBLY) || defined(SUPPORT_ABSTRACT_ASSEMBLY)
 581   bool print_opto_assembly = directive-&gt;PrintOptoAssemblyOption;
 582   // We can always print a disassembly, either abstract (hex dump) or
 583   // with the help of a suitable hsdis library. Thus, we should not
 584   // couple print_assembly and print_opto_assembly controls.
 585   // But: always print opto and regular assembly on compile command &#39;print&#39;.
 586   bool print_assembly = directive-&gt;PrintAssemblyOption;
 587   set_print_assembly(print_opto_assembly || print_assembly);
 588 #else
 589   set_print_assembly(false); // must initialize.
 590 #endif
 591 
 592 #ifndef PRODUCT
 593   set_parsed_irreducible_loop(false);
 594 
 595   if (directive-&gt;ReplayInlineOption) {
 596     _replay_inline_data = ciReplay::load_inline_data(method(), entry_bci(), ci_env-&gt;comp_level());
 597   }
 598 #endif
 599   set_print_inlining(directive-&gt;PrintInliningOption || PrintOptoInlining);
 600   set_print_intrinsics(directive-&gt;PrintIntrinsicsOption);
 601   set_has_irreducible_loop(true); // conservative until build_loop_tree() reset it
 602 
 603   if (ProfileTraps RTM_OPT_ONLY( || UseRTMLocking )) {
 604     // Make sure the method being compiled gets its own MDO,
 605     // so we can at least track the decompile_count().
 606     // Need MDO to record RTM code generation state.
 607     method()-&gt;ensure_method_data();
 608   }
 609 
 610   Init(::AliasLevel);
 611 
 612 
 613   print_compile_messages();
 614 
 615   _ilt = InlineTree::build_inline_tree_root();
 616 
 617   // Even if NO memory addresses are used, MergeMem nodes must have at least 1 slice
 618   assert(num_alias_types() &gt;= AliasIdxRaw, &quot;&quot;);
 619 
 620 #define MINIMUM_NODE_HASH  1023
 621   // Node list that Iterative GVN will start with
 622   Unique_Node_List for_igvn(comp_arena());
 623   set_for_igvn(&amp;for_igvn);
 624 
 625   // GVN that will be run immediately on new nodes
 626   uint estimated_size = method()-&gt;code_size()*4+64;
 627   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 628   PhaseGVN gvn(node_arena(), estimated_size);
 629   set_initial_gvn(&amp;gvn);
 630 
 631   print_inlining_init();
 632   { // Scope for timing the parser
 633     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 634 
 635     // Put top into the hash table ASAP.
 636     initial_gvn()-&gt;transform_no_reclaim(top());
 637 
 638     // Set up tf(), start(), and find a CallGenerator.
 639     CallGenerator* cg = NULL;
 640     if (is_osr_compilation()) {
<a name="3" id="anc3"></a><span class="line-modified"> 641       init_tf(TypeFunc::make(method(), /* is_osr_compilation = */ true));</span>
<span class="line-modified"> 642       StartNode* s = new StartOSRNode(root(), tf()-&gt;domain_sig());</span>


 643       initial_gvn()-&gt;set_type_bottom(s);
 644       init_start(s);
 645       cg = CallGenerator::for_osr(method(), entry_bci());
 646     } else {
 647       // Normal case.
 648       init_tf(TypeFunc::make(method()));
<a name="4" id="anc4"></a><span class="line-modified"> 649       StartNode* s = new StartNode(root(), tf()-&gt;domain_cc());</span>
 650       initial_gvn()-&gt;set_type_bottom(s);
 651       init_start(s);
 652       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 653         // With java.lang.ref.reference.get() we must go through the
 654         // intrinsic - even when get() is the root
 655         // method of the compile - so that, if necessary, the value in
 656         // the referent field of the reference object gets recorded by
 657         // the pre-barrier code.
 658         cg = find_intrinsic(method(), false);
 659       }
 660       if (cg == NULL) {
 661         float past_uses = method()-&gt;interpreter_invocation_count();
 662         float expected_uses = past_uses;
 663         cg = CallGenerator::for_inline(method(), expected_uses);
 664       }
 665     }
 666     if (failing())  return;
 667     if (cg == NULL) {
 668       record_method_not_compilable(&quot;cannot parse method&quot;);
 669       return;
 670     }
 671     JVMState* jvms = build_start_state(start(), tf());
 672     if ((jvms = cg-&gt;generate(jvms)) == NULL) {
 673       if (!failure_reason_is(C2Compiler::retry_class_loading_during_parsing())) {
 674         record_method_not_compilable(&quot;method parse failed&quot;);
 675       }
 676       return;
 677     }
 678     GraphKit kit(jvms);
 679 
 680     if (!kit.stopped()) {
 681       // Accept return values, and transfer control we know not where.
 682       // This is done by a special, unique ReturnNode bound to root.
 683       return_values(kit.jvms());
 684     }
 685 
 686     if (kit.has_exceptions()) {
 687       // Any exceptions that escape from this call must be rethrown
 688       // to whatever caller is dynamically above us on the stack.
 689       // This is done by a special, unique RethrowNode bound to root.
 690       rethrow_exceptions(kit.transfer_exceptions_into_jvms());
 691     }
 692 
 693     assert(IncrementalInline || (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines()), &quot;incremental inlining is off&quot;);
 694 
 695     if (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines() &amp;&amp; !failing() &amp;&amp; has_stringbuilder()) {
 696       inline_string_calls(true);
 697     }
 698 
 699     if (failing())  return;
 700 
 701     print_method(PHASE_BEFORE_REMOVEUSELESS, 3);
 702 
 703     // Remove clutter produced by parsing.
 704     if (!failing()) {
 705       ResourceMark rm;
 706       PhaseRemoveUseless pru(initial_gvn(), &amp;for_igvn);
 707     }
 708   }
 709 
 710   // Note:  Large methods are capped off in do_one_bytecode().
 711   if (failing())  return;
 712 
 713   // After parsing, node notes are no longer automagic.
 714   // They must be propagated by register_new_node_with_optimizer(),
 715   // clone(), or the like.
 716   set_default_node_notes(NULL);
 717 
 718   for (;;) {
 719     int successes = Inline_Warm();
 720     if (failing())  return;
 721     if (successes == 0)  break;
 722   }
 723 
 724   // Drain the list.
 725   Finish_Warm();
 726 #ifndef PRODUCT
 727   if (should_print(1)) {
 728     _printer-&gt;print_inlining();
 729   }
 730 #endif
 731 
 732   if (failing())  return;
 733   NOT_PRODUCT( verify_graph_edges(); )
 734 
 735   // Now optimize
 736   Optimize();
 737   if (failing())  return;
 738   NOT_PRODUCT( verify_graph_edges(); )
 739 
 740 #ifndef PRODUCT
 741   if (print_ideal()) {
 742     ttyLocker ttyl;  // keep the following output all in one block
 743     // This output goes directly to the tty, not the compiler log.
 744     // To enable tools to match it up with the compilation activity,
 745     // be sure to tag this tty output with the compile ID.
 746     if (xtty != NULL) {
 747       xtty-&gt;head(&quot;ideal compile_id=&#39;%d&#39;%s&quot;, compile_id(),
 748                  is_osr_compilation()    ? &quot; compile_kind=&#39;osr&#39;&quot; :
 749                  &quot;&quot;);
 750     }
 751     root()-&gt;dump(9999);
 752     if (xtty != NULL) {
 753       xtty-&gt;tail(&quot;ideal&quot;);
 754     }
 755   }
 756 #endif
 757 
 758 #ifdef ASSERT
 759   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 760   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 761 #endif
 762 
 763   // Dump compilation data to replay it.
 764   if (directive-&gt;DumpReplayOption) {
 765     env()-&gt;dump_replay_data(_compile_id);
 766   }
 767   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 768     env()-&gt;dump_inline_data(_compile_id);
 769   }
 770 
 771   // Now that we know the size of all the monitors we can add a fixed slot
 772   // for the original deopt pc.
 773   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
<a name="5" id="anc5"></a><span class="line-added"> 774   if (needs_stack_repair()) {</span>
<span class="line-added"> 775     // One extra slot for the special stack increment value</span>
<span class="line-added"> 776     next_slot += 2;</span>
<span class="line-added"> 777   }</span>
 778   set_fixed_slots(next_slot);
 779 
 780   // Compute when to use implicit null checks. Used by matching trap based
 781   // nodes and NullCheck optimization.
 782   set_allowed_deopt_reasons();
 783 
 784   // Now generate code
 785   Code_Gen();
 786 }
 787 
 788 //------------------------------Compile----------------------------------------
 789 // Compile a runtime stub
 790 Compile::Compile( ciEnv* ci_env,
 791                   TypeFunc_generator generator,
 792                   address stub_function,
 793                   const char *stub_name,
 794                   int is_fancy_jump,
 795                   bool pass_tls,
 796                   bool save_arg_registers,
 797                   bool return_pc,
 798                   DirectiveSet* directive)
 799   : Phase(Compiler),
 800     _compile_id(0),
 801     _save_argument_registers(save_arg_registers),
 802     _subsume_loads(true),
 803     _do_escape_analysis(false),
 804     _eliminate_boxing(false),
 805     _method(NULL),
 806     _entry_bci(InvocationEntryBci),
 807     _stub_function(stub_function),
 808     _stub_name(stub_name),
 809     _stub_entry_point(NULL),
 810     _max_node_limit(MaxNodeLimit),
 811     _inlining_progress(false),
 812     _inlining_incrementally(false),
 813     _has_reserved_stack_access(false),
 814 #ifndef PRODUCT
 815     _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 816     _print_ideal(directive-&gt;PrintIdealOption),
 817 #endif
 818     _has_method_handle_invokes(false),
 819     _clinit_barrier_on_entry(false),
 820     _comp_arena(mtCompiler),
 821     _barrier_set_state(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;create_barrier_state(comp_arena())),
 822     _env(ci_env),
 823     _directive(directive),
 824     _log(ci_env-&gt;log()),
 825     _failure_reason(NULL),
 826     _congraph(NULL),
 827     NOT_PRODUCT(_printer(NULL) COMMA)
 828     _dead_node_list(comp_arena()),
 829     _dead_node_count(0),
 830     _node_arena(mtCompiler),
 831     _old_arena(mtCompiler),
 832     _mach_constant_base_node(NULL),
 833     _Compile_types(mtCompiler),
 834     _initial_gvn(NULL),
 835     _for_igvn(NULL),
 836     _warm_calls(NULL),
 837     _number_of_mh_late_inlines(0),
 838     _print_inlining_stream(NULL),
 839     _print_inlining_list(NULL),
 840     _print_inlining_idx(0),
 841     _print_inlining_output(NULL),
 842     _replay_inline_data(NULL),
 843     _java_calls(0),
 844     _inner_loops(0),
 845     _interpreter_frame_size(0),
 846 #ifndef PRODUCT
 847     _in_dump_cnt(0),
 848 #endif
 849     _allowed_reasons(0) {
 850   C = this;
 851 
 852   TraceTime t1(NULL, &amp;_t_totalCompilation, CITime, false);
 853   TraceTime t2(NULL, &amp;_t_stubCompilation, CITime, false);
 854 
 855 #ifndef PRODUCT
 856   set_print_assembly(PrintFrameConverterAssembly);
 857   set_parsed_irreducible_loop(false);
 858 #else
 859   set_print_assembly(false); // Must initialize.
 860 #endif
 861   set_has_irreducible_loop(false); // no loops
 862 
 863   CompileWrapper cw(this);
 864   Init(/*AliasLevel=*/ 0);
 865   init_tf((*generator)());
 866 
 867   {
 868     // The following is a dummy for the sake of GraphKit::gen_stub
 869     Unique_Node_List for_igvn(comp_arena());
 870     set_for_igvn(&amp;for_igvn);  // not used, but some GraphKit guys push on this
 871     PhaseGVN gvn(Thread::current()-&gt;resource_area(),255);
 872     set_initial_gvn(&amp;gvn);    // not significant, but GraphKit guys use it pervasively
 873     gvn.transform_no_reclaim(top());
 874 
 875     GraphKit kit;
 876     kit.gen_stub(stub_function, stub_name, is_fancy_jump, pass_tls, return_pc);
 877   }
 878 
 879   NOT_PRODUCT( verify_graph_edges(); )
 880 
 881   Code_Gen();
 882 }
 883 
 884 //------------------------------Init-------------------------------------------
 885 // Prepare for a single compilation
 886 void Compile::Init(int aliaslevel) {
 887   _unique  = 0;
 888   _regalloc = NULL;
 889 
 890   _tf      = NULL;  // filled in later
 891   _top     = NULL;  // cached later
 892   _matcher = NULL;  // filled in later
 893   _cfg     = NULL;  // filled in later
 894 
 895   IA32_ONLY( set_24_bit_selection_and_mode(true, false); )
 896 
 897   _node_note_array = NULL;
 898   _default_node_notes = NULL;
 899   DEBUG_ONLY( _modified_nodes = NULL; ) // Used in Optimize()
 900 
 901   _immutable_memory = NULL; // filled in at first inquiry
 902 
 903   // Globally visible Nodes
 904   // First set TOP to NULL to give safe behavior during creation of RootNode
 905   set_cached_top_node(NULL);
 906   set_root(new RootNode());
 907   // Now that you have a Root to point to, create the real TOP
 908   set_cached_top_node( new ConNode(Type::TOP) );
 909   set_recent_alloc(NULL, NULL);
 910 
 911   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 912   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 913   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 914   env()-&gt;set_dependencies(new Dependencies(env()));
 915 
 916   _fixed_slots = 0;
 917   set_has_split_ifs(false);
 918   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 919   set_has_stringbuilder(false);
 920   set_has_boxed_value(false);
 921   _trap_can_recompile = false;  // no traps emitted yet
 922   _major_progress = true; // start out assuming good things will happen
 923   set_has_unsafe_access(false);
 924   set_max_vector_size(0);
 925   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 926   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 927   set_decompile_count(0);
 928 
 929   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 930   _loop_opts_cnt = LoopOptsCount;
<a name="6" id="anc6"></a><span class="line-added"> 931   _has_flattened_accesses = false;</span>
<span class="line-added"> 932   _flattened_accesses_share_alias = true;</span>
<span class="line-added"> 933 </span>
 934   set_do_inlining(Inline);
 935   set_max_inline_size(MaxInlineSize);
 936   set_freq_inline_size(FreqInlineSize);
 937   set_do_scheduling(OptoScheduling);
 938   set_do_count_invocations(false);
 939   set_do_method_data_update(false);
 940 
 941   set_do_vector_loop(false);
 942 
 943   if (AllowVectorizeOnDemand) {
 944     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 945       set_do_vector_loop(true);
 946       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 947     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 948                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 949       set_do_vector_loop(true);
 950     }
 951   }
 952   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 953   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 954 
 955   set_age_code(has_method() &amp;&amp; method()-&gt;profile_aging());
 956   set_rtm_state(NoRTM); // No RTM lock eliding by default
 957   _max_node_limit = _directive-&gt;MaxNodeLimitOption;
 958 
 959 #if INCLUDE_RTM_OPT
 960   if (UseRTMLocking &amp;&amp; has_method() &amp;&amp; (method()-&gt;method_data_or_null() != NULL)) {
 961     int rtm_state = method()-&gt;method_data()-&gt;rtm_state();
 962     if (method_has_option(&quot;NoRTMLockEliding&quot;) || ((rtm_state &amp; NoRTM) != 0)) {
 963       // Don&#39;t generate RTM lock eliding code.
 964       set_rtm_state(NoRTM);
 965     } else if (method_has_option(&quot;UseRTMLockEliding&quot;) || ((rtm_state &amp; UseRTM) != 0) || !UseRTMDeopt) {
 966       // Generate RTM lock eliding code without abort ratio calculation code.
 967       set_rtm_state(UseRTM);
 968     } else if (UseRTMDeopt) {
 969       // Generate RTM lock eliding code and include abort ratio calculation
 970       // code if UseRTMDeopt is on.
 971       set_rtm_state(ProfileRTM);
 972     }
 973   }
 974 #endif
 975   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; has_method() &amp;&amp; !is_osr_compilation() &amp;&amp; method()-&gt;needs_clinit_barrier()) {
 976     set_clinit_barrier_on_entry(true);
 977   }
 978   if (debug_info()-&gt;recording_non_safepoints()) {
 979     set_node_note_array(new(comp_arena()) GrowableArray&lt;Node_Notes*&gt;
 980                         (comp_arena(), 8, 0, NULL));
 981     set_default_node_notes(Node_Notes::make(this));
 982   }
 983 
 984   // // -- Initialize types before each compile --
 985   // // Update cached type information
 986   // if( _method &amp;&amp; _method-&gt;constants() )
 987   //   Type::update_loaded_types(_method, _method-&gt;constants());
 988 
 989   // Init alias_type map.
 990   if (!_do_escape_analysis &amp;&amp; aliaslevel == 3)
 991     aliaslevel = 2;  // No unique types without escape analysis
 992   _AliasLevel = aliaslevel;
 993   const int grow_ats = 16;
 994   _max_alias_types = grow_ats;
 995   _alias_types   = NEW_ARENA_ARRAY(comp_arena(), AliasType*, grow_ats);
 996   AliasType* ats = NEW_ARENA_ARRAY(comp_arena(), AliasType,  grow_ats);
 997   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 998   {
 999     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1000   }
1001   // Initialize the first few types.
1002   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1003   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1004   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1005   _num_alias_types = AliasIdxRaw+1;
1006   // Zero out the alias type cache.
1007   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1008   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1009   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1010 
1011   _intrinsics = NULL;
1012   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1013   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1014   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1015   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1016   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
<a name="7" id="anc7"></a><span class="line-added">1017   _inline_type_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);</span>
1018   register_library_intrinsics();
1019 #ifdef ASSERT
1020   _type_verify_symmetry = true;
1021   _phase_optimize_finished = false;
1022 #endif
1023 }
1024 
1025 //---------------------------init_start----------------------------------------
1026 // Install the StartNode on this compile object.
1027 void Compile::init_start(StartNode* s) {
1028   if (failing())
1029     return; // already failing
1030   assert(s == start(), &quot;&quot;);
1031 }
1032 
1033 /**
1034  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1035  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1036  * the ideal graph.
1037  */
1038 StartNode* Compile::start() const {
1039   assert (!failing(), &quot;Must not have pending failure. Reason is: %s&quot;, failure_reason());
1040   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1041     Node* start = root()-&gt;fast_out(i);
1042     if (start-&gt;is_Start()) {
1043       return start-&gt;as_Start();
1044     }
1045   }
1046   fatal(&quot;Did not find Start node!&quot;);
1047   return NULL;
1048 }
1049 
1050 //-------------------------------immutable_memory-------------------------------------
1051 // Access immutable memory
1052 Node* Compile::immutable_memory() {
1053   if (_immutable_memory != NULL) {
1054     return _immutable_memory;
1055   }
1056   StartNode* s = start();
1057   for (DUIterator_Fast imax, i = s-&gt;fast_outs(imax); true; i++) {
1058     Node *p = s-&gt;fast_out(i);
1059     if (p != s &amp;&amp; p-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
1060       _immutable_memory = p;
1061       return _immutable_memory;
1062     }
1063   }
1064   ShouldNotReachHere();
1065   return NULL;
1066 }
1067 
1068 //----------------------set_cached_top_node------------------------------------
1069 // Install the cached top node, and make sure Node::is_top works correctly.
1070 void Compile::set_cached_top_node(Node* tn) {
1071   if (tn != NULL)  verify_top(tn);
1072   Node* old_top = _top;
1073   _top = tn;
1074   // Calling Node::setup_is_top allows the nodes the chance to adjust
1075   // their _out arrays.
1076   if (_top != NULL)     _top-&gt;setup_is_top();
1077   if (old_top != NULL)  old_top-&gt;setup_is_top();
1078   assert(_top == NULL || top()-&gt;is_top(), &quot;&quot;);
1079 }
1080 
1081 #ifdef ASSERT
1082 uint Compile::count_live_nodes_by_graph_walk() {
1083   Unique_Node_List useful(comp_arena());
1084   // Get useful node list by walking the graph.
1085   identify_useful_nodes(useful);
1086   return useful.size();
1087 }
1088 
1089 void Compile::print_missing_nodes() {
1090 
1091   // Return if CompileLog is NULL and PrintIdealNodeCount is false.
1092   if ((_log == NULL) &amp;&amp; (! PrintIdealNodeCount)) {
1093     return;
1094   }
1095 
1096   // This is an expensive function. It is executed only when the user
1097   // specifies VerifyIdealNodeCount option or otherwise knows the
1098   // additional work that needs to be done to identify reachable nodes
1099   // by walking the flow graph and find the missing ones using
1100   // _dead_node_list.
1101 
1102   Unique_Node_List useful(comp_arena());
1103   // Get useful node list by walking the graph.
1104   identify_useful_nodes(useful);
1105 
1106   uint l_nodes = C-&gt;live_nodes();
1107   uint l_nodes_by_walk = useful.size();
1108 
1109   if (l_nodes != l_nodes_by_walk) {
1110     if (_log != NULL) {
1111       _log-&gt;begin_head(&quot;mismatched_nodes count=&#39;%d&#39;&quot;, abs((int) (l_nodes - l_nodes_by_walk)));
1112       _log-&gt;stamp();
1113       _log-&gt;end_head();
1114     }
1115     VectorSet&amp; useful_member_set = useful.member_set();
1116     int last_idx = l_nodes_by_walk;
1117     for (int i = 0; i &lt; last_idx; i++) {
1118       if (useful_member_set.test(i)) {
1119         if (_dead_node_list.test(i)) {
1120           if (_log != NULL) {
1121             _log-&gt;elem(&quot;mismatched_node_info node_idx=&#39;%d&#39; type=&#39;both live and dead&#39;&quot;, i);
1122           }
1123           if (PrintIdealNodeCount) {
1124             // Print the log message to tty
1125               tty-&gt;print_cr(&quot;mismatched_node idx=&#39;%d&#39; both live and dead&#39;&quot;, i);
1126               useful.at(i)-&gt;dump();
1127           }
1128         }
1129       }
1130       else if (! _dead_node_list.test(i)) {
1131         if (_log != NULL) {
1132           _log-&gt;elem(&quot;mismatched_node_info node_idx=&#39;%d&#39; type=&#39;neither live nor dead&#39;&quot;, i);
1133         }
1134         if (PrintIdealNodeCount) {
1135           // Print the log message to tty
1136           tty-&gt;print_cr(&quot;mismatched_node idx=&#39;%d&#39; type=&#39;neither live nor dead&#39;&quot;, i);
1137         }
1138       }
1139     }
1140     if (_log != NULL) {
1141       _log-&gt;tail(&quot;mismatched_nodes&quot;);
1142     }
1143   }
1144 }
1145 void Compile::record_modified_node(Node* n) {
1146   if (_modified_nodes != NULL &amp;&amp; !_inlining_incrementally &amp;&amp;
1147       n-&gt;outcnt() != 0 &amp;&amp; !n-&gt;is_Con()) {
1148     _modified_nodes-&gt;push(n);
1149   }
1150 }
1151 
1152 void Compile::remove_modified_node(Node* n) {
1153   if (_modified_nodes != NULL) {
1154     _modified_nodes-&gt;remove(n);
1155   }
1156 }
1157 #endif
1158 
1159 #ifndef PRODUCT
1160 void Compile::verify_top(Node* tn) const {
1161   if (tn != NULL) {
1162     assert(tn-&gt;is_Con(), &quot;top node must be a constant&quot;);
1163     assert(((ConNode*)tn)-&gt;type() == Type::TOP, &quot;top node must have correct type&quot;);
1164     assert(tn-&gt;in(0) != NULL, &quot;must have live top node&quot;);
1165   }
1166 }
1167 #endif
1168 
1169 
1170 ///-------------------Managing Per-Node Debug &amp; Profile Info-------------------
1171 
1172 void Compile::grow_node_notes(GrowableArray&lt;Node_Notes*&gt;* arr, int grow_by) {
1173   guarantee(arr != NULL, &quot;&quot;);
1174   int num_blocks = arr-&gt;length();
1175   if (grow_by &lt; num_blocks)  grow_by = num_blocks;
1176   int num_notes = grow_by * _node_notes_block_size;
1177   Node_Notes* notes = NEW_ARENA_ARRAY(node_arena(), Node_Notes, num_notes);
1178   Copy::zero_to_bytes(notes, num_notes * sizeof(Node_Notes));
1179   while (num_notes &gt; 0) {
1180     arr-&gt;append(notes);
1181     notes     += _node_notes_block_size;
1182     num_notes -= _node_notes_block_size;
1183   }
1184   assert(num_notes == 0, &quot;exact multiple, please&quot;);
1185 }
1186 
1187 bool Compile::copy_node_notes_to(Node* dest, Node* source) {
1188   if (source == NULL || dest == NULL)  return false;
1189 
1190   if (dest-&gt;is_Con())
1191     return false;               // Do not push debug info onto constants.
1192 
1193 #ifdef ASSERT
1194   // Leave a bread crumb trail pointing to the original node:
1195   if (dest != NULL &amp;&amp; dest != source &amp;&amp; dest-&gt;debug_orig() == NULL) {
1196     dest-&gt;set_debug_orig(source);
1197   }
1198 #endif
1199 
1200   if (node_note_array() == NULL)
1201     return false;               // Not collecting any notes now.
1202 
1203   // This is a copy onto a pre-existing node, which may already have notes.
1204   // If both nodes have notes, do not overwrite any pre-existing notes.
1205   Node_Notes* source_notes = node_notes_at(source-&gt;_idx);
1206   if (source_notes == NULL || source_notes-&gt;is_clear())  return false;
1207   Node_Notes* dest_notes   = node_notes_at(dest-&gt;_idx);
1208   if (dest_notes == NULL || dest_notes-&gt;is_clear()) {
1209     return set_node_notes_at(dest-&gt;_idx, source_notes);
1210   }
1211 
1212   Node_Notes merged_notes = (*source_notes);
1213   // The order of operations here ensures that dest notes will win...
1214   merged_notes.update_from(dest_notes);
1215   return set_node_notes_at(dest-&gt;_idx, &amp;merged_notes);
1216 }
1217 
1218 
1219 //--------------------------allow_range_check_smearing-------------------------
1220 // Gating condition for coalescing similar range checks.
1221 // Sometimes we try &#39;speculatively&#39; replacing a series of a range checks by a
1222 // single covering check that is at least as strong as any of them.
1223 // If the optimization succeeds, the simplified (strengthened) range check
1224 // will always succeed.  If it fails, we will deopt, and then give up
1225 // on the optimization.
1226 bool Compile::allow_range_check_smearing() const {
1227   // If this method has already thrown a range-check,
1228   // assume it was because we already tried range smearing
1229   // and it failed.
1230   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1231   return !already_trapped;
1232 }
1233 
1234 
1235 //------------------------------flatten_alias_type-----------------------------
1236 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1237   int offset = tj-&gt;offset();
1238   TypePtr::PTR ptr = tj-&gt;ptr();
1239 
1240   // Known instance (scalarizable allocation) alias only with itself.
1241   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1242                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1243 
1244   // Process weird unsafe references.
1245   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<a name="8" id="anc8"></a><span class="line-modified">1246     bool default_value_load = EnableValhalla &amp;&amp; tj-&gt;is_instptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass();</span>
<span class="line-added">1247     assert(InlineUnsafeOps || default_value_load, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>
1248     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1249     tj = TypeOopPtr::BOTTOM;
1250     ptr = tj-&gt;ptr();
1251     offset = tj-&gt;offset();
1252   }
1253 
1254   // Array pointers need some flattening
1255   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1256   if (ta &amp;&amp; ta-&gt;is_stable()) {
1257     // Erase stability property for alias analysis.
1258     tj = ta = ta-&gt;cast_to_stable(false);
1259   }
<a name="9" id="anc9"></a><span class="line-added">1260   if (ta &amp;&amp; ta-&gt;is_not_flat()) {</span>
<span class="line-added">1261     // Erase not flat property for alias analysis.</span>
<span class="line-added">1262     tj = ta = ta-&gt;cast_to_not_flat(false);</span>
<span class="line-added">1263   }</span>
<span class="line-added">1264   if (ta &amp;&amp; ta-&gt;is_not_null_free()) {</span>
<span class="line-added">1265     // Erase not null free property for alias analysis.</span>
<span class="line-added">1266     tj = ta = ta-&gt;cast_to_not_null_free(false);</span>
<span class="line-added">1267   }</span>
<span class="line-added">1268 </span>
1269   if( ta &amp;&amp; is_known_inst ) {
1270     if ( offset != Type::OffsetBot &amp;&amp;
1271          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1272       offset = Type::OffsetBot; // Flatten constant access into array body only
<a name="10" id="anc10"></a><span class="line-modified">1273       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, Type::Offset(offset), ta-&gt;field_offset(), ta-&gt;instance_id());</span>
1274     }
1275   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1276     // For arrays indexed by constant indices, we flatten the alias
1277     // space to include all of the array body.  Only the header, klass
1278     // and array length can be accessed un-aliased.
<a name="11" id="anc11"></a><span class="line-added">1279     // For flattened inline type array, each field has its own slice so</span>
<span class="line-added">1280     // we must include the field offset.</span>
1281     if( offset != Type::OffsetBot ) {
1282       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1283         offset = Type::OffsetBot;   // Flatten constant access into array body
<a name="12" id="anc12"></a><span class="line-modified">1284         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1285       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1286         // range is OK as-is.
1287         tj = ta = TypeAryPtr::RANGE;
1288       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1289         tj = TypeInstPtr::KLASS; // all klass loads look alike
1290         ta = TypeAryPtr::RANGE; // generic ignored junk
1291         ptr = TypePtr::BotPTR;
1292       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1293         tj = TypeInstPtr::MARK;
1294         ta = TypeAryPtr::RANGE; // generic ignored junk
1295         ptr = TypePtr::BotPTR;
1296       } else {                  // Random constant offset into array body
1297         offset = Type::OffsetBot;   // Flatten constant access into array body
<a name="13" id="anc13"></a><span class="line-modified">1298         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1299       }
1300     }
1301     // Arrays of fixed size alias with arrays of unknown size.
1302     if (ta-&gt;size() != TypeInt::POS) {
1303       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<a name="14" id="anc14"></a><span class="line-modified">1304       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1305     }
1306     // Arrays of known objects become arrays of unknown objects.
1307     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1308       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<a name="15" id="anc15"></a><span class="line-modified">1309       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1310     }
1311     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1312       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<a name="16" id="anc16"></a><span class="line-modified">1313       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
<span class="line-added">1314     }</span>
<span class="line-added">1315     // Initially all flattened array accesses share a single slice</span>
<span class="line-added">1316     if (ta-&gt;elem()-&gt;isa_inlinetype() &amp;&amp; ta-&gt;elem() != TypeInlineType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
<span class="line-added">1317       const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta-&gt;size());</span>
<span class="line-added">1318       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));</span>
1319     }
1320     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1321     // cannot be distinguished by bytecode alone.
1322     if (ta-&gt;elem() == TypeInt::BOOL) {
1323       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1324       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<a name="17" id="anc17"></a><span class="line-modified">1325       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1326     }
1327     // During the 2nd round of IterGVN, NotNull castings are removed.
1328     // Make sure the Bottom and NotNull variants alias the same.
1329     // Also, make sure exact and non-exact variants alias the same.
1330     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<a name="18" id="anc18"></a><span class="line-modified">1331       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1332     }
1333   }
1334 
1335   // Oop pointers need some flattening
1336   const TypeInstPtr *to = tj-&gt;isa_instptr();
1337   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1338     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1339     if( ptr == TypePtr::Constant ) {
1340       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1341           offset &lt; k-&gt;size_helper() * wordSize) {
1342         // No constant oop pointers (such as Strings); they alias with
1343         // unknown strings.
1344         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<a name="19" id="anc19"></a><span class="line-modified">1345         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1346       }
1347     } else if( is_known_inst ) {
1348       tj = to; // Keep NotNull and klass_is_exact for instance type
1349     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1350       // During the 2nd round of IterGVN, NotNull castings are removed.
1351       // Make sure the Bottom and NotNull variants alias the same.
1352       // Also, make sure exact and non-exact variants alias the same.
<a name="20" id="anc20"></a><span class="line-modified">1353       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1354     }
1355     if (to-&gt;speculative() != NULL) {
<a name="21" id="anc21"></a><span class="line-modified">1356       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),Type::Offset(to-&gt;offset()), to-&gt;klass()-&gt;flatten_array(), to-&gt;instance_id());</span>
1357     }
1358     // Canonicalize the holder of this field
1359     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1360       // First handle header references such as a LoadKlassNode, even if the
1361       // object&#39;s klass is unloaded at compile time (4965979).
1362       if (!is_known_inst) { // Do it only for non-instance types
<a name="22" id="anc22"></a><span class="line-modified">1363         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, Type::Offset(offset), false);</span>
1364       }
1365     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1366       // Static fields are in the space above the normal instance
1367       // fields in the java.lang.Class instance.
1368       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1369         to = NULL;
1370         tj = TypeOopPtr::BOTTOM;
1371         offset = tj-&gt;offset();
1372       }
1373     } else {
1374       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1375       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1376         if( is_known_inst ) {
<a name="23" id="anc23"></a><span class="line-modified">1377           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array(), to-&gt;instance_id());</span>
1378         } else {
<a name="24" id="anc24"></a><span class="line-modified">1379           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array());</span>
1380         }
1381       }
1382     }
1383   }
1384 
1385   // Klass pointers to object array klasses need some flattening
1386   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1387   if( tk ) {
1388     // If we are referencing a field within a Klass, we need
1389     // to assume the worst case of an Object.  Both exact and
1390     // inexact types must flatten to the same alias class so
1391     // use NotNull as the PTR.
1392     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1393 
1394       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1395                                    TypeKlassPtr::OBJECT-&gt;klass(),
<a name="25" id="anc25"></a><span class="line-modified">1396                                    Type::Offset(offset),</span>
<span class="line-added">1397                                    false);</span>
1398     }
1399 
1400     ciKlass* klass = tk-&gt;klass();
<a name="26" id="anc26"></a><span class="line-modified">1401     if (klass != NULL &amp;&amp; klass-&gt;is_obj_array_klass()) {</span>
1402       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1403       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1404         k = TypeInstPtr::BOTTOM-&gt;klass();
<a name="27" id="anc27"></a><span class="line-modified">1405       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset), false);</span>
1406     }
1407 
1408     // Check for precise loads from the primary supertype array and force them
1409     // to the supertype cache alias index.  Check for generic array loads from
1410     // the primary supertype array and also force them to the supertype cache
1411     // alias index.  Since the same load can reach both, we need to merge
1412     // these 2 disparate memories into the same alias class.  Since the
1413     // primary supertype array is read-only, there&#39;s no chance of confusion
1414     // where we bypass an array load and an array store.
1415     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1416     if (offset == Type::OffsetBot ||
1417         (offset &gt;= primary_supers_offset &amp;&amp;
1418          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1419         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1420       offset = in_bytes(Klass::secondary_super_cache_offset());
<a name="28" id="anc28"></a><span class="line-modified">1421       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk-&gt;klass(), Type::Offset(offset), tk-&gt;flat_array());</span>
1422     }
1423   }
1424 
1425   // Flatten all Raw pointers together.
1426   if (tj-&gt;base() == Type::RawPtr)
1427     tj = TypeRawPtr::BOTTOM;
1428 
1429   if (tj-&gt;base() == Type::AnyPtr)
1430     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1431 
1432   // Flatten all to bottom for now
1433   switch( _AliasLevel ) {
1434   case 0:
1435     tj = TypePtr::BOTTOM;
1436     break;
1437   case 1:                       // Flatten to: oop, static, field or array
1438     switch (tj-&gt;base()) {
1439     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1440     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1441     case Type::AryPtr:   // do not distinguish arrays at all
1442     case Type::InstPtr:  tj = TypeInstPtr::BOTTOM;  break;
1443     case Type::KlassPtr: tj = TypeKlassPtr::OBJECT; break;
1444     case Type::AnyPtr:   tj = TypePtr::BOTTOM;      break;  // caller checks it
1445     default: ShouldNotReachHere();
1446     }
1447     break;
1448   case 2:                       // No collapsing at level 2; keep all splits
1449   case 3:                       // No collapsing at level 3; keep all splits
1450     break;
1451   default:
1452     Unimplemented();
1453   }
1454 
1455   offset = tj-&gt;offset();
1456   assert( offset != Type::OffsetTop, &quot;Offset has fallen from constant&quot; );
1457 
1458   assert( (offset != Type::OffsetBot &amp;&amp; tj-&gt;base() != Type::AryPtr) ||
1459           (offset == Type::OffsetBot &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1460           (offset == Type::OffsetBot &amp;&amp; tj == TypeOopPtr::BOTTOM) ||
1461           (offset == Type::OffsetBot &amp;&amp; tj == TypePtr::BOTTOM) ||
1462           (offset == oopDesc::mark_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1463           (offset == oopDesc::klass_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1464           (offset == arrayOopDesc::length_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr),
1465           &quot;For oops, klasses, raw offset must be constant; for arrays the offset is never known&quot; );
1466   assert( tj-&gt;ptr() != TypePtr::TopPTR &amp;&amp;
1467           tj-&gt;ptr() != TypePtr::AnyNull &amp;&amp;
1468           tj-&gt;ptr() != TypePtr::Null, &quot;No imprecise addresses&quot; );
1469 //    assert( tj-&gt;ptr() != TypePtr::Constant ||
1470 //            tj-&gt;base() == Type::RawPtr ||
1471 //            tj-&gt;base() == Type::KlassPtr, &quot;No constant oop addresses&quot; );
1472 
1473   return tj;
1474 }
1475 
1476 void Compile::AliasType::Init(int i, const TypePtr* at) {
1477   assert(AliasIdxTop &lt;= i &amp;&amp; i &lt; Compile::current()-&gt;_max_alias_types, &quot;Invalid alias index&quot;);
1478   _index = i;
1479   _adr_type = at;
1480   _field = NULL;
1481   _element = NULL;
1482   _is_rewritable = true; // default
1483   const TypeOopPtr *atoop = (at != NULL) ? at-&gt;isa_oopptr() : NULL;
1484   if (atoop != NULL &amp;&amp; atoop-&gt;is_known_instance()) {
1485     const TypeOopPtr *gt = atoop-&gt;cast_to_instance_id(TypeOopPtr::InstanceBot);
1486     _general_index = Compile::current()-&gt;get_alias_index(gt);
1487   } else {
1488     _general_index = 0;
1489   }
1490 }
1491 
1492 BasicType Compile::AliasType::basic_type() const {
1493   if (element() != NULL) {
1494     const Type* element = adr_type()-&gt;is_aryptr()-&gt;elem();
1495     return element-&gt;isa_narrowoop() ? T_OBJECT : element-&gt;array_element_basic_type();
1496   } if (field() != NULL) {
1497     return field()-&gt;layout_type();
1498   } else {
1499     return T_ILLEGAL; // unknown
1500   }
1501 }
1502 
1503 //---------------------------------print_on------------------------------------
1504 #ifndef PRODUCT
1505 void Compile::AliasType::print_on(outputStream* st) {
1506   if (index() &lt; 10)
1507         st-&gt;print(&quot;@ &lt;%d&gt; &quot;, index());
1508   else  st-&gt;print(&quot;@ &lt;%d&gt;&quot;,  index());
1509   st-&gt;print(is_rewritable() ? &quot;   &quot; : &quot; RO&quot;);
1510   int offset = adr_type()-&gt;offset();
1511   if (offset == Type::OffsetBot)
1512         st-&gt;print(&quot; +any&quot;);
1513   else  st-&gt;print(&quot; +%-3d&quot;, offset);
1514   st-&gt;print(&quot; in &quot;);
1515   adr_type()-&gt;dump_on(st);
1516   const TypeOopPtr* tjp = adr_type()-&gt;isa_oopptr();
1517   if (field() != NULL &amp;&amp; tjp) {
1518     if (tjp-&gt;klass()  != field()-&gt;holder() ||
1519         tjp-&gt;offset() != field()-&gt;offset_in_bytes()) {
1520       st-&gt;print(&quot; != &quot;);
1521       field()-&gt;print();
1522       st-&gt;print(&quot; ***&quot;);
1523     }
1524   }
1525 }
1526 
1527 void print_alias_types() {
1528   Compile* C = Compile::current();
1529   tty-&gt;print_cr(&quot;--- Alias types, AliasIdxBot .. %d&quot;, C-&gt;num_alias_types()-1);
1530   for (int idx = Compile::AliasIdxBot; idx &lt; C-&gt;num_alias_types(); idx++) {
1531     C-&gt;alias_type(idx)-&gt;print_on(tty);
1532     tty-&gt;cr();
1533   }
1534 }
1535 #endif
1536 
1537 
1538 //----------------------------probe_alias_cache--------------------------------
1539 Compile::AliasCacheEntry* Compile::probe_alias_cache(const TypePtr* adr_type) {
1540   intptr_t key = (intptr_t) adr_type;
1541   key ^= key &gt;&gt; logAliasCacheSize;
1542   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1543 }
1544 
1545 
1546 //-----------------------------grow_alias_types--------------------------------
1547 void Compile::grow_alias_types() {
1548   const int old_ats  = _max_alias_types; // how many before?
1549   const int new_ats  = old_ats;          // how many more?
1550   const int grow_ats = old_ats+new_ats;  // how many now?
1551   _max_alias_types = grow_ats;
1552   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1553   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1554   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1555   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1556 }
1557 
1558 
1559 //--------------------------------find_alias_type------------------------------
<a name="29" id="anc29"></a><span class="line-modified">1560 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {</span>
1561   if (_AliasLevel == 0)
1562     return alias_type(AliasIdxBot);
1563 
<a name="30" id="anc30"></a><span class="line-modified">1564   AliasCacheEntry* ace = NULL;</span>
<span class="line-modified">1565   if (!uncached) {</span>
<span class="line-modified">1566     ace = probe_alias_cache(adr_type);</span>
<span class="line-added">1567     if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-added">1568       return alias_type(ace-&gt;_index);</span>
<span class="line-added">1569     }</span>
1570   }
1571 
1572   // Handle special cases.
1573   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1574   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1575 
1576   // Do it the slow way.
1577   const TypePtr* flat = flatten_alias_type(adr_type);
1578 
1579 #ifdef ASSERT
1580   {
1581     ResourceMark rm;
1582     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1583            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1584     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1585            Type::str(adr_type));
1586     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1587       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1588       // Scalarizable allocations have exact klass always.
1589       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
1590       const TypePtr* xoop = foop-&gt;cast_to_exactness(exact)-&gt;is_ptr();
1591       assert(foop == flatten_alias_type(xoop), &quot;exactness must not affect alias type: foop = %s; xoop = %s&quot;,
1592              Type::str(foop), Type::str(xoop));
1593     }
1594   }
1595 #endif
1596 
1597   int idx = AliasIdxTop;
1598   for (int i = 0; i &lt; num_alias_types(); i++) {
1599     if (alias_type(i)-&gt;adr_type() == flat) {
1600       idx = i;
1601       break;
1602     }
1603   }
1604 
1605   if (idx == AliasIdxTop) {
1606     if (no_create)  return NULL;
1607     // Grow the array if necessary.
1608     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1609     // Add a new alias type.
1610     idx = _num_alias_types++;
1611     _alias_types[idx]-&gt;Init(idx, flat);
1612     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1613     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1614     if (flat-&gt;isa_instptr()) {
1615       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1616           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1617         alias_type(idx)-&gt;set_rewritable(false);
1618     }
<a name="31" id="anc31"></a><span class="line-added">1619     ciField* field = NULL;</span>
1620     if (flat-&gt;isa_aryptr()) {
1621 #ifdef ASSERT
1622       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1623       // (T_BYTE has the weakest alignment and size restrictions...)
1624       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1625 #endif
<a name="32" id="anc32"></a><span class="line-added">1626       const Type* elemtype = flat-&gt;is_aryptr()-&gt;elem();</span>
1627       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<a name="33" id="anc33"></a><span class="line-modified">1628         alias_type(idx)-&gt;set_element(elemtype);</span>
<span class="line-added">1629       }</span>
<span class="line-added">1630       int field_offset = flat-&gt;is_aryptr()-&gt;field_offset().get();</span>
<span class="line-added">1631       if (elemtype-&gt;isa_inlinetype() &amp;&amp;</span>
<span class="line-added">1632           elemtype-&gt;inline_klass() != NULL &amp;&amp;</span>
<span class="line-added">1633           field_offset != Type::OffsetBot) {</span>
<span class="line-added">1634         ciInlineKlass* vk = elemtype-&gt;inline_klass();</span>
<span class="line-added">1635         field_offset += vk-&gt;first_field_offset();</span>
<span class="line-added">1636         field = vk-&gt;get_field_by_offset(field_offset, false);</span>
1637       }
1638     }
1639     if (flat-&gt;isa_klassptr()) {
1640       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1641         alias_type(idx)-&gt;set_rewritable(false);
1642       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1643         alias_type(idx)-&gt;set_rewritable(false);
1644       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1645         alias_type(idx)-&gt;set_rewritable(false);
1646       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1647         alias_type(idx)-&gt;set_rewritable(false);
<a name="34" id="anc34"></a><span class="line-added">1648       if (flat-&gt;offset() == in_bytes(Klass::layout_helper_offset()))</span>
<span class="line-added">1649         alias_type(idx)-&gt;set_rewritable(false);</span>
1650       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1651         alias_type(idx)-&gt;set_rewritable(false);
1652     }
1653     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1654     // but the base pointer type is not distinctive enough to identify
1655     // references into JavaThread.)
1656 
1657     // Check for final fields.
1658     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1659     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
<a name="35" id="anc35"></a>
1660       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1661           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1662           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1663         // static field
1664         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1665         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
<a name="36" id="anc36"></a><span class="line-added">1666       } else if (tinst-&gt;klass()-&gt;is_inlinetype()) {</span>
<span class="line-added">1667         // Inline type field</span>
<span class="line-added">1668         ciInlineKlass* vk = tinst-&gt;inline_klass();</span>
<span class="line-added">1669         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);</span>
1670       } else {
<a name="37" id="anc37"></a><span class="line-modified">1671         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1672         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1673       }
<a name="38" id="anc38"></a><span class="line-modified">1674     }</span>
<span class="line-modified">1675     assert(field == NULL ||</span>
<span class="line-modified">1676            original_field == NULL ||</span>
<span class="line-modified">1677            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1678             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1679             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1680     // Set field() and is_rewritable() attributes.</span>
<span class="line-added">1681     if (field != NULL) {</span>
<span class="line-added">1682       alias_type(idx)-&gt;set_field(field);</span>
<span class="line-added">1683       if (flat-&gt;isa_aryptr()) {</span>
<span class="line-added">1684         // Fields of flat arrays are rewritable although they are declared final</span>
<span class="line-added">1685         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype(), &quot;must be a flat array&quot;);</span>
<span class="line-added">1686         alias_type(idx)-&gt;set_rewritable(true);</span>
<span class="line-added">1687       }</span>
1688     }
1689   }
1690 
1691   // Fill the cache for next time.
<a name="39" id="anc39"></a><span class="line-modified">1692   if (!uncached) {</span>
<span class="line-modified">1693     ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1694     ace-&gt;_index    = idx;</span>
<span class="line-added">1695     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>
1696 
<a name="40" id="anc40"></a><span class="line-modified">1697     // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1698     AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1699     if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1700       face-&gt;_adr_type = flat;</span>
<span class="line-modified">1701       face-&gt;_index    = idx;</span>
<span class="line-modified">1702       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>
<span class="line-added">1703     }</span>
1704   }
1705 
1706   return alias_type(idx);
1707 }
1708 
1709 
1710 Compile::AliasType* Compile::alias_type(ciField* field) {
1711   const TypeOopPtr* t;
1712   if (field-&gt;is_static())
1713     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1714   else
1715     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1716   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1717   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1718   return atp;
1719 }
1720 
1721 
1722 //------------------------------have_alias_type--------------------------------
1723 bool Compile::have_alias_type(const TypePtr* adr_type) {
1724   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1725   if (ace-&gt;_adr_type == adr_type) {
1726     return true;
1727   }
1728 
1729   // Handle special cases.
1730   if (adr_type == NULL)             return true;
1731   if (adr_type == TypePtr::BOTTOM)  return true;
1732 
1733   return find_alias_type(adr_type, true, NULL) != NULL;
1734 }
1735 
1736 //-----------------------------must_alias--------------------------------------
1737 // True if all values of the given address type are in the given alias category.
1738 bool Compile::must_alias(const TypePtr* adr_type, int alias_idx) {
1739   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1740   if (adr_type == NULL)                 return true;  // NULL serves as TypePtr::TOP
1741   if (alias_idx == AliasIdxTop)         return false; // the empty category
1742   if (adr_type-&gt;base() == Type::AnyPtr) return false; // TypePtr::BOTTOM or its twins
1743 
1744   // the only remaining possible overlap is identity
1745   int adr_idx = get_alias_index(adr_type);
1746   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, &quot;&quot;);
1747   assert(adr_idx == alias_idx ||
1748          (alias_type(alias_idx)-&gt;adr_type() != TypeOopPtr::BOTTOM
1749           &amp;&amp; adr_type                       != TypeOopPtr::BOTTOM),
1750          &quot;should not be testing for overlap with an unsafe pointer&quot;);
1751   return adr_idx == alias_idx;
1752 }
1753 
1754 //------------------------------can_alias--------------------------------------
1755 // True if any values of the given address type are in the given alias category.
1756 bool Compile::can_alias(const TypePtr* adr_type, int alias_idx) {
1757   if (alias_idx == AliasIdxTop)         return false; // the empty category
1758   if (adr_type == NULL)                 return false; // NULL serves as TypePtr::TOP
1759   // Known instance doesn&#39;t alias with bottom memory
1760   if (alias_idx == AliasIdxBot)         return !adr_type-&gt;is_known_instance();                   // the universal category
1761   if (adr_type-&gt;base() == Type::AnyPtr) return !C-&gt;get_adr_type(alias_idx)-&gt;is_known_instance(); // TypePtr::BOTTOM or its twins
1762 
1763   // the only remaining possible overlap is identity
1764   int adr_idx = get_alias_index(adr_type);
1765   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, &quot;&quot;);
1766   return adr_idx == alias_idx;
1767 }
1768 
1769 
1770 
1771 //---------------------------pop_warm_call-------------------------------------
1772 WarmCallInfo* Compile::pop_warm_call() {
1773   WarmCallInfo* wci = _warm_calls;
1774   if (wci != NULL)  _warm_calls = wci-&gt;remove_from(wci);
1775   return wci;
1776 }
1777 
1778 //----------------------------Inline_Warm--------------------------------------
1779 int Compile::Inline_Warm() {
1780   // If there is room, try to inline some more warm call sites.
1781   // %%% Do a graph index compaction pass when we think we&#39;re out of space?
1782   if (!InlineWarmCalls)  return 0;
1783 
1784   int calls_made_hot = 0;
1785   int room_to_grow   = NodeCountInliningCutoff - unique();
1786   int amount_to_grow = MIN2(room_to_grow, (int)NodeCountInliningStep);
1787   int amount_grown   = 0;
1788   WarmCallInfo* call;
1789   while (amount_to_grow &gt; 0 &amp;&amp; (call = pop_warm_call()) != NULL) {
1790     int est_size = (int)call-&gt;size();
1791     if (est_size &gt; (room_to_grow - amount_grown)) {
1792       // This one won&#39;t fit anyway.  Get rid of it.
1793       call-&gt;make_cold();
1794       continue;
1795     }
1796     call-&gt;make_hot();
1797     calls_made_hot++;
1798     amount_grown   += est_size;
1799     amount_to_grow -= est_size;
1800   }
1801 
1802   if (calls_made_hot &gt; 0)  set_major_progress();
1803   return calls_made_hot;
1804 }
1805 
1806 
1807 //----------------------------Finish_Warm--------------------------------------
1808 void Compile::Finish_Warm() {
1809   if (!InlineWarmCalls)  return;
1810   if (failing())  return;
1811   if (warm_calls() == NULL)  return;
1812 
1813   // Clean up loose ends, if we are out of space for inlining.
1814   WarmCallInfo* call;
1815   while ((call = pop_warm_call()) != NULL) {
1816     call-&gt;make_cold();
1817   }
1818 }
1819 
1820 //---------------------cleanup_loop_predicates-----------------------
1821 // Remove the opaque nodes that protect the predicates so that all unused
1822 // checks and uncommon_traps will be eliminated from the ideal graph
1823 void Compile::cleanup_loop_predicates(PhaseIterGVN &amp;igvn) {
1824   if (predicate_count()==0) return;
1825   for (int i = predicate_count(); i &gt; 0; i--) {
1826     Node * n = predicate_opaque1_node(i-1);
1827     assert(n-&gt;Opcode() == Op_Opaque1, &quot;must be&quot;);
1828     igvn.replace_node(n, n-&gt;in(1));
1829   }
1830   assert(predicate_count()==0, &quot;should be clean!&quot;);
1831 }
1832 
1833 void Compile::add_range_check_cast(Node* n) {
1834   assert(n-&gt;isa_CastII()-&gt;has_range_check(), &quot;CastII should have range check dependency&quot;);
1835   assert(!_range_check_casts-&gt;contains(n), &quot;duplicate entry in range check casts&quot;);
1836   _range_check_casts-&gt;append(n);
1837 }
1838 
1839 // Remove all range check dependent CastIINodes.
1840 void Compile::remove_range_check_casts(PhaseIterGVN &amp;igvn) {
1841   for (int i = range_check_cast_count(); i &gt; 0; i--) {
1842     Node* cast = range_check_cast_node(i-1);
1843     assert(cast-&gt;isa_CastII()-&gt;has_range_check(), &quot;CastII should have range check dependency&quot;);
1844     igvn.replace_node(cast, cast-&gt;in(1));
1845   }
1846   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1847 }
1848 
1849 void Compile::add_opaque4_node(Node* n) {
1850   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1851   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1852   _opaque4_nodes-&gt;append(n);
1853 }
1854 
1855 // Remove all Opaque4 nodes.
1856 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1857   for (int i = opaque4_count(); i &gt; 0; i--) {
1858     Node* opaq = opaque4_node(i-1);
1859     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1860     igvn.replace_node(opaq, opaq-&gt;in(2));
1861   }
1862   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1863 }
1864 
<a name="41" id="anc41"></a><span class="line-added">1865 void Compile::add_inline_type(Node* n) {</span>
<span class="line-added">1866   assert(n-&gt;is_InlineTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1867   if (_inline_type_nodes != NULL) {</span>
<span class="line-added">1868     _inline_type_nodes-&gt;push(n);</span>
<span class="line-added">1869   }</span>
<span class="line-added">1870 }</span>
<span class="line-added">1871 </span>
<span class="line-added">1872 void Compile::remove_inline_type(Node* n) {</span>
<span class="line-added">1873   assert(n-&gt;is_InlineTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1874   if (_inline_type_nodes != NULL &amp;&amp; _inline_type_nodes-&gt;contains(n)) {</span>
<span class="line-added">1875     _inline_type_nodes-&gt;remove(n);</span>
<span class="line-added">1876   }</span>
<span class="line-added">1877 }</span>
<span class="line-added">1878 </span>
<span class="line-added">1879 // Does the return value keep otherwise useless inline type allocations alive?</span>
<span class="line-added">1880 static bool return_val_keeps_allocations_alive(Node* ret_val) {</span>
<span class="line-added">1881   ResourceMark rm;</span>
<span class="line-added">1882   Unique_Node_List wq;</span>
<span class="line-added">1883   wq.push(ret_val);</span>
<span class="line-added">1884   bool some_allocations = false;</span>
<span class="line-added">1885   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1886     Node* n = wq.at(i);</span>
<span class="line-added">1887     assert(!n-&gt;is_InlineType(), &quot;chain of inline type nodes&quot;);</span>
<span class="line-added">1888     if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">1889       // Some other use for the allocation</span>
<span class="line-added">1890       return false;</span>
<span class="line-added">1891     } else if (n-&gt;is_InlineTypePtr()) {</span>
<span class="line-added">1892       wq.push(n-&gt;in(1));</span>
<span class="line-added">1893     } else if (n-&gt;is_Phi()) {</span>
<span class="line-added">1894       for (uint j = 1; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1895         wq.push(n-&gt;in(j));</span>
<span class="line-added">1896       }</span>
<span class="line-added">1897     } else if (n-&gt;is_CheckCastPP() &amp;&amp;</span>
<span class="line-added">1898                n-&gt;in(1)-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1899                n-&gt;in(1)-&gt;in(0)-&gt;is_Allocate()) {</span>
<span class="line-added">1900       some_allocations = true;</span>
<span class="line-added">1901     }</span>
<span class="line-added">1902   }</span>
<span class="line-added">1903   return some_allocations;</span>
<span class="line-added">1904 }</span>
<span class="line-added">1905 </span>
<span class="line-added">1906 void Compile::process_inline_types(PhaseIterGVN &amp;igvn, bool post_ea) {</span>
<span class="line-added">1907   // Make inline types scalar in safepoints</span>
<span class="line-added">1908   for (int i = _inline_type_nodes-&gt;length()-1; i &gt;= 0; i--) {</span>
<span class="line-added">1909     InlineTypeBaseNode* vt = _inline_type_nodes-&gt;at(i)-&gt;as_InlineTypeBase();</span>
<span class="line-added">1910     vt-&gt;make_scalar_in_safepoints(&amp;igvn);</span>
<span class="line-added">1911   }</span>
<span class="line-added">1912   // Remove InlineTypePtr nodes only after EA to give scalar replacement a chance</span>
<span class="line-added">1913   // to remove buffer allocations. InlineType nodes are kept until loop opts and</span>
<span class="line-added">1914   // removed via InlineTypeNode::remove_redundant_allocations.</span>
<span class="line-added">1915   if (post_ea) {</span>
<span class="line-added">1916     while (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-added">1917       InlineTypeBaseNode* vt = _inline_type_nodes-&gt;pop()-&gt;as_InlineTypeBase();</span>
<span class="line-added">1918       if (vt-&gt;is_InlineTypePtr()) {</span>
<span class="line-added">1919         igvn.replace_node(vt, vt-&gt;get_oop());</span>
<span class="line-added">1920       }</span>
<span class="line-added">1921     }</span>
<span class="line-added">1922   }</span>
<span class="line-added">1923   // Make sure that the return value does not keep an unused allocation alive</span>
<span class="line-added">1924   if (tf()-&gt;returns_inline_type_as_fields()) {</span>
<span class="line-added">1925     Node* ret = NULL;</span>
<span class="line-added">1926     for (uint i = 1; i &lt; root()-&gt;req(); i++){</span>
<span class="line-added">1927       Node* in = root()-&gt;in(i);</span>
<span class="line-added">1928       if (in-&gt;Opcode() == Op_Return) {</span>
<span class="line-added">1929         assert(ret == NULL, &quot;only one return&quot;);</span>
<span class="line-added">1930         ret = in;</span>
<span class="line-added">1931       }</span>
<span class="line-added">1932     }</span>
<span class="line-added">1933     if (ret != NULL) {</span>
<span class="line-added">1934       Node* ret_val = ret-&gt;in(TypeFunc::Parms);</span>
<span class="line-added">1935       if (igvn.type(ret_val)-&gt;isa_oopptr() &amp;&amp;</span>
<span class="line-added">1936           return_val_keeps_allocations_alive(ret_val)) {</span>
<span class="line-added">1937         igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)-&gt;inline_klass(), igvn));</span>
<span class="line-added">1938         assert(ret_val-&gt;outcnt() == 0, &quot;should be dead now&quot;);</span>
<span class="line-added">1939         igvn.remove_dead_node(ret_val);</span>
<span class="line-added">1940       }</span>
<span class="line-added">1941     }</span>
<span class="line-added">1942   }</span>
<span class="line-added">1943   igvn.optimize();</span>
<span class="line-added">1944 }</span>
<span class="line-added">1945 </span>
<span class="line-added">1946 void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN&amp; igvn) {</span>
<span class="line-added">1947   if (!_has_flattened_accesses) {</span>
<span class="line-added">1948     return;</span>
<span class="line-added">1949   }</span>
<span class="line-added">1950   // Initially, all flattened array accesses share the same slice to</span>
<span class="line-added">1951   // keep dependencies with Object[] array accesses (that could be</span>
<span class="line-added">1952   // to a flattened array) correct. We&#39;re done with parsing so we</span>
<span class="line-added">1953   // now know all flattened array accesses in this compile</span>
<span class="line-added">1954   // unit. Let&#39;s move flattened array accesses to their own slice,</span>
<span class="line-added">1955   // one per element field. This should help memory access</span>
<span class="line-added">1956   // optimizations.</span>
<span class="line-added">1957   ResourceMark rm;</span>
<span class="line-added">1958   Unique_Node_List wq;</span>
<span class="line-added">1959   wq.push(root());</span>
<span class="line-added">1960 </span>
<span class="line-added">1961   Node_List mergememnodes;</span>
<span class="line-added">1962   Node_List memnodes;</span>
<span class="line-added">1963 </span>
<span class="line-added">1964   // Alias index currently shared by all flattened memory accesses</span>
<span class="line-added">1965   int index = get_alias_index(TypeAryPtr::INLINES);</span>
<span class="line-added">1966 </span>
<span class="line-added">1967   // Find MergeMem nodes and flattened array accesses</span>
<span class="line-added">1968   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1969     Node* n = wq.at(i);</span>
<span class="line-added">1970     if (n-&gt;is_Mem()) {</span>
<span class="line-added">1971       const TypePtr* adr_type = NULL;</span>
<span class="line-added">1972       if (n-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">1973         adr_type = get_adr_type(get_alias_index(n-&gt;in(MemNode::OopStore)-&gt;adr_type()));</span>
<span class="line-added">1974       } else {</span>
<span class="line-added">1975         adr_type = get_adr_type(get_alias_index(n-&gt;adr_type()));</span>
<span class="line-added">1976       }</span>
<span class="line-added">1977       if (adr_type == TypeAryPtr::INLINES) {</span>
<span class="line-added">1978         memnodes.push(n);</span>
<span class="line-added">1979       }</span>
<span class="line-added">1980     } else if (n-&gt;is_MergeMem()) {</span>
<span class="line-added">1981       MergeMemNode* mm = n-&gt;as_MergeMem();</span>
<span class="line-added">1982       if (mm-&gt;memory_at(index) != mm-&gt;base_memory()) {</span>
<span class="line-added">1983         mergememnodes.push(n);</span>
<span class="line-added">1984       }</span>
<span class="line-added">1985     }</span>
<span class="line-added">1986     for (uint j = 0; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1987       Node* m = n-&gt;in(j);</span>
<span class="line-added">1988       if (m != NULL) {</span>
<span class="line-added">1989         wq.push(m);</span>
<span class="line-added">1990       }</span>
<span class="line-added">1991     }</span>
<span class="line-added">1992   }</span>
<span class="line-added">1993 </span>
<span class="line-added">1994   if (memnodes.size() &gt; 0) {</span>
<span class="line-added">1995     _flattened_accesses_share_alias = false;</span>
<span class="line-added">1996 </span>
<span class="line-added">1997     // We are going to change the slice for the flattened array</span>
<span class="line-added">1998     // accesses so we need to clear the cache entries that refer to</span>
<span class="line-added">1999     // them.</span>
<span class="line-added">2000     for (uint i = 0; i &lt; AliasCacheSize; i++) {</span>
<span class="line-added">2001       AliasCacheEntry* ace = &amp;_alias_cache[i];</span>
<span class="line-added">2002       if (ace-&gt;_adr_type != NULL &amp;&amp;</span>
<span class="line-added">2003           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;</span>
<span class="line-added">2004           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2005         ace-&gt;_adr_type = NULL;</span>
<span class="line-added">2006         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop</span>
<span class="line-added">2007       }</span>
<span class="line-added">2008     }</span>
<span class="line-added">2009 </span>
<span class="line-added">2010     // Find what aliases we are going to add</span>
<span class="line-added">2011     int start_alias = num_alias_types()-1;</span>
<span class="line-added">2012     int stop_alias = 0;</span>
<span class="line-added">2013 </span>
<span class="line-added">2014     for (uint i = 0; i &lt; memnodes.size(); i++) {</span>
<span class="line-added">2015       Node* m = memnodes.at(i);</span>
<span class="line-added">2016       const TypePtr* adr_type = NULL;</span>
<span class="line-added">2017       if (m-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">2018         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();</span>
<span class="line-added">2019         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),</span>
<span class="line-added">2020                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),</span>
<span class="line-added">2021                                       get_alias_index(adr_type));</span>
<span class="line-added">2022         igvn.register_new_node_with_optimizer(clone);</span>
<span class="line-added">2023         igvn.replace_node(m, clone);</span>
<span class="line-added">2024       } else {</span>
<span class="line-added">2025         adr_type = m-&gt;adr_type();</span>
<span class="line-added">2026 #ifdef ASSERT</span>
<span class="line-added">2027         m-&gt;as_Mem()-&gt;set_adr_type(adr_type);</span>
<span class="line-added">2028 #endif</span>
<span class="line-added">2029       }</span>
<span class="line-added">2030       int idx = get_alias_index(adr_type);</span>
<span class="line-added">2031       start_alias = MIN2(start_alias, idx);</span>
<span class="line-added">2032       stop_alias = MAX2(stop_alias, idx);</span>
<span class="line-added">2033     }</span>
<span class="line-added">2034 </span>
<span class="line-added">2035     assert(stop_alias &gt;= start_alias, &quot;should have expanded aliases&quot;);</span>
<span class="line-added">2036 </span>
<span class="line-added">2037     Node_Stack stack(0);</span>
<span class="line-added">2038 #ifdef ASSERT</span>
<span class="line-added">2039     VectorSet seen(Thread::current()-&gt;resource_area());</span>
<span class="line-added">2040 #endif</span>
<span class="line-added">2041     // Now let&#39;s fix the memory graph so each flattened array access</span>
<span class="line-added">2042     // is moved to the right slice. Start from the MergeMem nodes.</span>
<span class="line-added">2043     uint last = unique();</span>
<span class="line-added">2044     for (uint i = 0; i &lt; mergememnodes.size(); i++) {</span>
<span class="line-added">2045       MergeMemNode* current = mergememnodes.at(i)-&gt;as_MergeMem();</span>
<span class="line-added">2046       Node* n = current-&gt;memory_at(index);</span>
<span class="line-added">2047       MergeMemNode* mm = NULL;</span>
<span class="line-added">2048       do {</span>
<span class="line-added">2049         // Follow memory edges through memory accesses, phis and</span>
<span class="line-added">2050         // narrow membars and push nodes on the stack. Once we hit</span>
<span class="line-added">2051         // bottom memory, we pop element off the stack one at a</span>
<span class="line-added">2052         // time, in reverse order, and move them to the right slice</span>
<span class="line-added">2053         // by changing their memory edges.</span>
<span class="line-added">2054         if ((n-&gt;is_Phi() &amp;&amp; n-&gt;adr_type() != TypePtr::BOTTOM) || n-&gt;is_Mem() || n-&gt;adr_type() == TypeAryPtr::INLINES) {</span>
<span class="line-added">2055           assert(!seen.test_set(n-&gt;_idx), &quot;&quot;);</span>
<span class="line-added">2056           // Uses (a load for instance) will need to be moved to the</span>
<span class="line-added">2057           // right slice as well and will get a new memory state</span>
<span class="line-added">2058           // that we don&#39;t know yet. The use could also be the</span>
<span class="line-added">2059           // backedge of a loop. We put a place holder node between</span>
<span class="line-added">2060           // the memory node and its uses. We replace that place</span>
<span class="line-added">2061           // holder with the correct memory state once we know it,</span>
<span class="line-added">2062           // i.e. when nodes are popped off the stack. Using the</span>
<span class="line-added">2063           // place holder make the logic work in the presence of</span>
<span class="line-added">2064           // loops.</span>
<span class="line-added">2065           if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">2066             Node* place_holder = NULL;</span>
<span class="line-added">2067             assert(!n-&gt;has_out_with(Op_Node), &quot;&quot;);</span>
<span class="line-added">2068             for (DUIterator k = n-&gt;outs(); n-&gt;has_out(k); k++) {</span>
<span class="line-added">2069               Node* u = n-&gt;out(k);</span>
<span class="line-added">2070               if (u != current &amp;&amp; u-&gt;_idx &lt; last) {</span>
<span class="line-added">2071                 bool success = false;</span>
<span class="line-added">2072                 for (uint l = 0; l &lt; u-&gt;req(); l++) {</span>
<span class="line-added">2073                   if (!stack.is_empty() &amp;&amp; u == stack.node() &amp;&amp; l == stack.index()) {</span>
<span class="line-added">2074                     continue;</span>
<span class="line-added">2075                   }</span>
<span class="line-added">2076                   Node* in = u-&gt;in(l);</span>
<span class="line-added">2077                   if (in == n) {</span>
<span class="line-added">2078                     if (place_holder == NULL) {</span>
<span class="line-added">2079                       place_holder = new Node(1);</span>
<span class="line-added">2080                       place_holder-&gt;init_req(0, n);</span>
<span class="line-added">2081                     }</span>
<span class="line-added">2082                     igvn.replace_input_of(u, l, place_holder);</span>
<span class="line-added">2083                     success = true;</span>
<span class="line-added">2084                   }</span>
<span class="line-added">2085                 }</span>
<span class="line-added">2086                 if (success) {</span>
<span class="line-added">2087                   --k;</span>
<span class="line-added">2088                 }</span>
<span class="line-added">2089               }</span>
<span class="line-added">2090             }</span>
<span class="line-added">2091           }</span>
<span class="line-added">2092           if (n-&gt;is_Phi()) {</span>
<span class="line-added">2093             stack.push(n, 1);</span>
<span class="line-added">2094             n = n-&gt;in(1);</span>
<span class="line-added">2095           } else if (n-&gt;is_Mem()) {</span>
<span class="line-added">2096             stack.push(n, n-&gt;req());</span>
<span class="line-added">2097             n = n-&gt;in(MemNode::Memory);</span>
<span class="line-added">2098           } else {</span>
<span class="line-added">2099             assert(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;Opcode() == Op_MemBarCPUOrder, &quot;&quot;);</span>
<span class="line-added">2100             stack.push(n, n-&gt;req());</span>
<span class="line-added">2101             n = n-&gt;in(0)-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">2102           }</span>
<span class="line-added">2103         } else {</span>
<span class="line-added">2104           assert(n-&gt;adr_type() == TypePtr::BOTTOM || (n-&gt;Opcode() == Op_Node &amp;&amp; n-&gt;_idx &gt;= last) || (n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;is_Initialize()), &quot;&quot;);</span>
<span class="line-added">2105           // Build a new MergeMem node to carry the new memory state</span>
<span class="line-added">2106           // as we build it. IGVN should fold extraneous MergeMem</span>
<span class="line-added">2107           // nodes.</span>
<span class="line-added">2108           mm = MergeMemNode::make(n);</span>
<span class="line-added">2109           igvn.register_new_node_with_optimizer(mm);</span>
<span class="line-added">2110           while (stack.size() &gt; 0) {</span>
<span class="line-added">2111             Node* m = stack.node();</span>
<span class="line-added">2112             uint idx = stack.index();</span>
<span class="line-added">2113             if (m-&gt;is_Mem()) {</span>
<span class="line-added">2114               // Move memory node to its new slice</span>
<span class="line-added">2115               const TypePtr* adr_type = m-&gt;adr_type();</span>
<span class="line-added">2116               int alias = get_alias_index(adr_type);</span>
<span class="line-added">2117               Node* prev = mm-&gt;memory_at(alias);</span>
<span class="line-added">2118               igvn.replace_input_of(m, MemNode::Memory, prev);</span>
<span class="line-added">2119               mm-&gt;set_memory_at(alias, m);</span>
<span class="line-added">2120             } else if (m-&gt;is_Phi()) {</span>
<span class="line-added">2121               // We need as many new phis as there are new aliases</span>
<span class="line-added">2122               igvn.replace_input_of(m, idx, mm);</span>
<span class="line-added">2123               if (idx == m-&gt;req()-1) {</span>
<span class="line-added">2124                 Node* r = m-&gt;in(0);</span>
<span class="line-added">2125                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2126                   const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2127                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2128                     continue;</span>
<span class="line-added">2129                   }</span>
<span class="line-added">2130                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));</span>
<span class="line-added">2131                   igvn.register_new_node_with_optimizer(phi);</span>
<span class="line-added">2132                   for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2133                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));</span>
<span class="line-added">2134                   }</span>
<span class="line-added">2135                   mm-&gt;set_memory_at(j, phi);</span>
<span class="line-added">2136                 }</span>
<span class="line-added">2137                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);</span>
<span class="line-added">2138                 igvn.register_new_node_with_optimizer(base_phi);</span>
<span class="line-added">2139                 for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2140                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());</span>
<span class="line-added">2141                 }</span>
<span class="line-added">2142                 mm-&gt;set_base_memory(base_phi);</span>
<span class="line-added">2143               }</span>
<span class="line-added">2144             } else {</span>
<span class="line-added">2145               // This is a MemBarCPUOrder node from</span>
<span class="line-added">2146               // Parse::array_load()/Parse::array_store(), in the</span>
<span class="line-added">2147               // branch that handles flattened arrays hidden under</span>
<span class="line-added">2148               // an Object[] array. We also need one new membar per</span>
<span class="line-added">2149               // new alias to keep the unknown access that the</span>
<span class="line-added">2150               // membars protect properly ordered with accesses to</span>
<span class="line-added">2151               // known flattened array.</span>
<span class="line-added">2152               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);</span>
<span class="line-added">2153               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);</span>
<span class="line-added">2154               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());</span>
<span class="line-added">2155               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2156                 const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2157                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2158                   continue;</span>
<span class="line-added">2159                 }</span>
<span class="line-added">2160                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);</span>
<span class="line-added">2161                 igvn.register_new_node_with_optimizer(mb);</span>
<span class="line-added">2162                 Node* mem = mm-&gt;memory_at(j);</span>
<span class="line-added">2163                 mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">2164                 mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-added">2165                 ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">2166                 igvn.register_new_node_with_optimizer(ctrl);</span>
<span class="line-added">2167                 mem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">2168                 igvn.register_new_node_with_optimizer(mem);</span>
<span class="line-added">2169                 mm-&gt;set_memory_at(j, mem);</span>
<span class="line-added">2170               }</span>
<span class="line-added">2171               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);</span>
<span class="line-added">2172             }</span>
<span class="line-added">2173             if (idx &lt; m-&gt;req()-1) {</span>
<span class="line-added">2174               idx += 1;</span>
<span class="line-added">2175               stack.set_index(idx);</span>
<span class="line-added">2176               n = m-&gt;in(idx);</span>
<span class="line-added">2177               break;</span>
<span class="line-added">2178             }</span>
<span class="line-added">2179             // Take care of place holder nodes</span>
<span class="line-added">2180             if (m-&gt;has_out_with(Op_Node)) {</span>
<span class="line-added">2181               Node* place_holder = m-&gt;find_out_with(Op_Node);</span>
<span class="line-added">2182               if (place_holder != NULL) {</span>
<span class="line-added">2183                 Node* mm_clone = mm-&gt;clone();</span>
<span class="line-added">2184                 igvn.register_new_node_with_optimizer(mm_clone);</span>
<span class="line-added">2185                 Node* hook = new Node(1);</span>
<span class="line-added">2186                 hook-&gt;init_req(0, mm);</span>
<span class="line-added">2187                 igvn.replace_node(place_holder, mm_clone);</span>
<span class="line-added">2188                 hook-&gt;destruct();</span>
<span class="line-added">2189               }</span>
<span class="line-added">2190               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);</span>
<span class="line-added">2191             }</span>
<span class="line-added">2192             stack.pop();</span>
<span class="line-added">2193           }</span>
<span class="line-added">2194         }</span>
<span class="line-added">2195       } while(stack.size() &gt; 0);</span>
<span class="line-added">2196       // Fix the memory state at the MergeMem we started from</span>
<span class="line-added">2197       igvn.rehash_node_delayed(current);</span>
<span class="line-added">2198       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2199         const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2200         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2201           continue;</span>
<span class="line-added">2202         }</span>
<span class="line-added">2203         current-&gt;set_memory_at(j, mm);</span>
<span class="line-added">2204       }</span>
<span class="line-added">2205       current-&gt;set_memory_at(index, current-&gt;base_memory());</span>
<span class="line-added">2206     }</span>
<span class="line-added">2207     igvn.optimize();</span>
<span class="line-added">2208   }</span>
<span class="line-added">2209   print_method(PHASE_SPLIT_INLINES_ARRAY, 2);</span>
<span class="line-added">2210 }</span>
<span class="line-added">2211 </span>
<span class="line-added">2212 </span>
2213 // StringOpts and late inlining of string methods
2214 void Compile::inline_string_calls(bool parse_time) {
2215   {
2216     // remove useless nodes to make the usage analysis simpler
2217     ResourceMark rm;
2218     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2219   }
2220 
2221   {
2222     ResourceMark rm;
2223     print_method(PHASE_BEFORE_STRINGOPTS, 3);
2224     PhaseStringOpts pso(initial_gvn(), for_igvn());
2225     print_method(PHASE_AFTER_STRINGOPTS, 3);
2226   }
2227 
2228   // now inline anything that we skipped the first time around
2229   if (!parse_time) {
2230     _late_inlines_pos = _late_inlines.length();
2231   }
2232 
2233   while (_string_late_inlines.length() &gt; 0) {
2234     CallGenerator* cg = _string_late_inlines.pop();
2235     cg-&gt;do_late_inline();
2236     if (failing())  return;
2237   }
2238   _string_late_inlines.trunc_to(0);
2239 }
2240 
2241 // Late inlining of boxing methods
2242 void Compile::inline_boxing_calls(PhaseIterGVN&amp; igvn) {
2243   if (_boxing_late_inlines.length() &gt; 0) {
2244     assert(has_boxed_value(), &quot;inconsistent&quot;);
2245 
2246     PhaseGVN* gvn = initial_gvn();
2247     set_inlining_incrementally(true);
2248 
2249     assert( igvn._worklist.size() == 0, &quot;should be done with igvn&quot; );
2250     for_igvn()-&gt;clear();
2251     gvn-&gt;replace_with(&amp;igvn);
2252 
2253     _late_inlines_pos = _late_inlines.length();
2254 
2255     while (_boxing_late_inlines.length() &gt; 0) {
2256       CallGenerator* cg = _boxing_late_inlines.pop();
2257       cg-&gt;do_late_inline();
2258       if (failing())  return;
2259     }
2260     _boxing_late_inlines.trunc_to(0);
2261 
2262     inline_incrementally_cleanup(igvn);
2263 
2264     set_inlining_incrementally(false);
2265   }
2266 }
2267 
2268 bool Compile::inline_incrementally_one() {
2269   assert(IncrementalInline, &quot;incremental inlining should be on&quot;);
2270 
2271   TracePhase tp(&quot;incrementalInline_inline&quot;, &amp;timers[_t_incrInline_inline]);
2272   set_inlining_progress(false);
2273   set_do_cleanup(false);
2274   int i = 0;
2275   for (; i &lt;_late_inlines.length() &amp;&amp; !inlining_progress(); i++) {
2276     CallGenerator* cg = _late_inlines.at(i);
2277     _late_inlines_pos = i+1;
2278     cg-&gt;do_late_inline();
2279     if (failing())  return false;
2280   }
2281   int j = 0;
2282   for (; i &lt; _late_inlines.length(); i++, j++) {
2283     _late_inlines.at_put(j, _late_inlines.at(i));
2284   }
2285   _late_inlines.trunc_to(j);
2286   assert(inlining_progress() || _late_inlines.length() == 0, &quot;&quot;);
2287 
2288   bool needs_cleanup = do_cleanup() || over_inlining_cutoff();
2289 
2290   set_inlining_progress(false);
2291   set_do_cleanup(false);
2292   return (_late_inlines.length() &gt; 0) &amp;&amp; !needs_cleanup;
2293 }
2294 
2295 void Compile::inline_incrementally_cleanup(PhaseIterGVN&amp; igvn) {
2296   {
2297     TracePhase tp(&quot;incrementalInline_pru&quot;, &amp;timers[_t_incrInline_pru]);
2298     ResourceMark rm;
2299     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2300   }
2301   {
2302     TracePhase tp(&quot;incrementalInline_igvn&quot;, &amp;timers[_t_incrInline_igvn]);
2303     igvn = PhaseIterGVN(initial_gvn());
2304     igvn.optimize();
2305   }
2306 }
2307 
2308 // Perform incremental inlining until bound on number of live nodes is reached
2309 void Compile::inline_incrementally(PhaseIterGVN&amp; igvn) {
2310   TracePhase tp(&quot;incrementalInline&quot;, &amp;timers[_t_incrInline]);
2311 
2312   set_inlining_incrementally(true);
2313   uint low_live_nodes = 0;
2314 
2315   while (_late_inlines.length() &gt; 0) {
2316     if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2317       if (low_live_nodes &lt; (uint)LiveNodeCountInliningCutoff * 8 / 10) {
2318         TracePhase tp(&quot;incrementalInline_ideal&quot;, &amp;timers[_t_incrInline_ideal]);
2319         // PhaseIdealLoop is expensive so we only try it once we are
2320         // out of live nodes and we only try it again if the previous
2321         // helped got the number of nodes down significantly
2322         PhaseIdealLoop::optimize(igvn, LoopOptsNone);
2323         if (failing())  return;
2324         low_live_nodes = live_nodes();
2325         _major_progress = true;
2326       }
2327 
2328       if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2329         break; // finish
2330       }
2331     }
2332 
2333     for_igvn()-&gt;clear();
2334     initial_gvn()-&gt;replace_with(&amp;igvn);
2335 
2336     while (inline_incrementally_one()) {
2337       assert(!failing(), &quot;inconsistent&quot;);
2338     }
2339 
2340     if (failing())  return;
2341 
2342     inline_incrementally_cleanup(igvn);
2343 
2344     if (failing())  return;
2345   }
2346   assert( igvn._worklist.size() == 0, &quot;should be done with igvn&quot; );
2347 
2348   if (_string_late_inlines.length() &gt; 0) {
2349     assert(has_stringbuilder(), &quot;inconsistent&quot;);
2350     for_igvn()-&gt;clear();
2351     initial_gvn()-&gt;replace_with(&amp;igvn);
2352 
2353     inline_string_calls(false);
2354 
2355     if (failing())  return;
2356 
2357     inline_incrementally_cleanup(igvn);
2358   }
2359 
2360   set_inlining_incrementally(false);
2361 }
2362 
2363 
2364 bool Compile::optimize_loops(PhaseIterGVN&amp; igvn, LoopOptsMode mode) {
2365   if(_loop_opts_cnt &gt; 0) {
2366     debug_only( int cnt = 0; );
2367     while(major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2368       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2369       assert( cnt++ &lt; 40, &quot;infinite cycle in loop optimization&quot; );
2370       PhaseIdealLoop::optimize(igvn, mode);
2371       _loop_opts_cnt--;
2372       if (failing())  return false;
2373       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP_ITERATIONS, 2);
2374     }
2375   }
2376   return true;
2377 }
2378 
2379 // Remove edges from &quot;root&quot; to each SafePoint at a backward branch.
2380 // They were inserted during parsing (see add_safepoint()) to make
2381 // infinite loops without calls or exceptions visible to root, i.e.,
2382 // useful.
2383 void Compile::remove_root_to_sfpts_edges(PhaseIterGVN&amp; igvn) {
2384   Node *r = root();
2385   if (r != NULL) {
2386     for (uint i = r-&gt;req(); i &lt; r-&gt;len(); ++i) {
2387       Node *n = r-&gt;in(i);
2388       if (n != NULL &amp;&amp; n-&gt;is_SafePoint()) {
2389         r-&gt;rm_prec(i);
2390         if (n-&gt;outcnt() == 0) {
2391           igvn.remove_dead_node(n);
2392         }
2393         --i;
2394       }
2395     }
2396     // Parsing may have added top inputs to the root node (Path
2397     // leading to the Halt node proven dead). Make sure we get a
2398     // chance to clean them up.
2399     igvn._worklist.push(r);
2400     igvn.optimize();
2401   }
2402 }
2403 
2404 //------------------------------Optimize---------------------------------------
2405 // Given a graph, optimize it.
2406 void Compile::Optimize() {
2407   TracePhase tp(&quot;optimizer&quot;, &amp;timers[_t_optimizer]);
2408 
2409 #ifndef PRODUCT
2410   if (_directive-&gt;BreakAtCompileOption) {
2411     BREAKPOINT;
2412   }
2413 
2414 #endif
2415 
2416   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
2417 #ifdef ASSERT
2418   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeOptimize);
2419 #endif
2420 
2421   ResourceMark rm;
2422 
2423   print_inlining_reinit();
2424 
2425   NOT_PRODUCT( verify_graph_edges(); )
2426 
2427   print_method(PHASE_AFTER_PARSING);
2428 
2429  {
2430   // Iterative Global Value Numbering, including ideal transforms
2431   // Initialize IterGVN with types and values from parse-time GVN
2432   PhaseIterGVN igvn(initial_gvn());
2433 #ifdef ASSERT
2434   _modified_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
2435 #endif
2436   {
2437     TracePhase tp(&quot;iterGVN&quot;, &amp;timers[_t_iterGVN]);
2438     igvn.optimize();
2439   }
2440 
2441   if (failing())  return;
2442 
2443   print_method(PHASE_ITER_GVN1, 2);
2444 
2445   inline_incrementally(igvn);
2446 
2447   print_method(PHASE_INCREMENTAL_INLINE, 2);
2448 
2449   if (failing())  return;
2450 
2451   if (eliminate_boxing()) {
2452     // Inline valueOf() methods now.
2453     inline_boxing_calls(igvn);
2454 
2455     if (AlwaysIncrementalInline) {
2456       inline_incrementally(igvn);
2457     }
2458 
2459     print_method(PHASE_INCREMENTAL_BOXING_INLINE, 2);
2460 
2461     if (failing())  return;
2462   }
2463 
2464   // Now that all inlining is over, cut edge from root to loop
2465   // safepoints
2466   remove_root_to_sfpts_edges(igvn);
2467 
2468   // Remove the speculative part of types and clean up the graph from
2469   // the extra CastPP nodes whose only purpose is to carry them. Do
2470   // that early so that optimizations are not disrupted by the extra
2471   // CastPP nodes.
2472   remove_speculative_types(igvn);
2473 
2474   // No more new expensive nodes will be added to the list from here
2475   // so keep only the actual candidates for optimizations.
2476   cleanup_expensive_nodes(igvn);
2477 
2478   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2479     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2480     initial_gvn()-&gt;replace_with(&amp;igvn);
2481     for_igvn()-&gt;clear();
2482     Unique_Node_List new_worklist(C-&gt;comp_arena());
2483     {
2484       ResourceMark rm;
2485       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2486     }
2487     set_for_igvn(&amp;new_worklist);
2488     igvn = PhaseIterGVN(initial_gvn());
2489     igvn.optimize();
2490   }
2491 
<a name="42" id="anc42"></a><span class="line-added">2492   if (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-added">2493     // Do this once all inlining is over to avoid getting inconsistent debug info</span>
<span class="line-added">2494     process_inline_types(igvn);</span>
<span class="line-added">2495   }</span>
<span class="line-added">2496 </span>
<span class="line-added">2497   adjust_flattened_array_access_aliases(igvn);</span>
<span class="line-added">2498 </span>
2499   // Perform escape analysis
2500   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2501     if (has_loops()) {
2502       // Cleanup graph (remove dead nodes).
2503       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2504       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2505       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2506       if (failing())  return;
2507     }
2508     ConnectionGraph::do_analysis(this, &amp;igvn);
2509 
2510     if (failing())  return;
2511 
2512     // Optimize out fields loads from scalar replaceable allocations.
2513     igvn.optimize();
2514     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2515 
2516     if (failing())  return;
2517 
2518     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2519       TracePhase tp(&quot;macroEliminate&quot;, &amp;timers[_t_macroEliminate]);
2520       PhaseMacroExpand mexp(igvn);
2521       mexp.eliminate_macro_nodes();
2522       igvn.set_delay_transform(false);
2523 
2524       igvn.optimize();
2525       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2526 
2527       if (failing())  return;
2528     }
2529   }
2530 
<a name="43" id="anc43"></a><span class="line-added">2531   if (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-added">2532     // Process inline types again now that EA might have simplified the graph</span>
<span class="line-added">2533     process_inline_types(igvn, /* post_ea= */ true);</span>
<span class="line-added">2534   }</span>
<span class="line-added">2535 </span>
2536   // Loop transforms on the ideal graph.  Range Check Elimination,
2537   // peeling, unrolling, etc.
2538 
2539   // Set loop opts counter
2540   if((_loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2541     {
2542       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2543       PhaseIdealLoop::optimize(igvn, LoopOptsDefault);
2544       _loop_opts_cnt--;
2545       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2546       if (failing())  return;
2547     }
2548     // Loop opts pass if partial peeling occurred in previous pass
2549     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2550       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2551       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2552       _loop_opts_cnt--;
2553       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2554       if (failing())  return;
2555     }
2556     // Loop opts pass for loop-unrolling before CCP
2557     if(major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2558       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2559       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2560       _loop_opts_cnt--;
2561       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP3, 2);
2562     }
2563     if (!failing()) {
2564       // Verify that last round of loop opts produced a valid graph
2565       TracePhase tp(&quot;idealLoopVerify&quot;, &amp;timers[_t_idealLoopVerify]);
2566       PhaseIdealLoop::verify(igvn);
2567     }
2568   }
2569   if (failing())  return;
2570 
2571   // Conditional Constant Propagation;
2572   PhaseCCP ccp( &amp;igvn );
2573   assert( true, &quot;Break here to ccp.dump_nodes_and_types(_root,999,1)&quot;);
2574   {
2575     TracePhase tp(&quot;ccp&quot;, &amp;timers[_t_ccp]);
2576     ccp.do_transform();
2577   }
2578   print_method(PHASE_CPP1, 2);
2579 
2580   assert( true, &quot;Break here to ccp.dump_old2new_map()&quot;);
2581 
2582   // Iterative Global Value Numbering, including ideal transforms
2583   {
2584     TracePhase tp(&quot;iterGVN2&quot;, &amp;timers[_t_iterGVN2]);
2585     igvn = ccp;
2586     igvn.optimize();
2587   }
2588   print_method(PHASE_ITER_GVN2, 2);
2589 
2590   if (failing())  return;
2591 
2592   // Loop transforms on the ideal graph.  Range Check Elimination,
2593   // peeling, unrolling, etc.
2594   if (!optimize_loops(igvn, LoopOptsDefault)) {
2595     return;
2596   }
2597 
2598   if (failing())  return;
2599 
2600   // Ensure that major progress is now clear
2601   C-&gt;clear_major_progress();
2602 
2603   {
2604     // Verify that all previous optimizations produced a valid graph
2605     // at least to this point, even if no loop optimizations were done.
2606     TracePhase tp(&quot;idealLoopVerify&quot;, &amp;timers[_t_idealLoopVerify]);
2607     PhaseIdealLoop::verify(igvn);
2608   }
2609 
2610   if (range_check_cast_count() &gt; 0) {
2611     // No more loop optimizations. Remove all range check dependent CastIINodes.
2612     C-&gt;remove_range_check_casts(igvn);
2613     igvn.optimize();
2614   }
2615 
2616 #ifdef ASSERT
2617   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeMacroExpand);
2618 #endif
2619 
2620   {
2621     TracePhase tp(&quot;macroExpand&quot;, &amp;timers[_t_macroExpand]);
2622     PhaseMacroExpand  mex(igvn);
2623     if (mex.expand_macro_nodes()) {
2624       assert(failing(), &quot;must bail out w/ explicit message&quot;);
2625       return;
2626     }
2627     print_method(PHASE_MACRO_EXPANSION, 2);
2628   }
2629 
2630   {
2631     TracePhase tp(&quot;barrierExpand&quot;, &amp;timers[_t_barrierExpand]);
2632     if (bs-&gt;expand_barriers(this, igvn)) {
2633       assert(failing(), &quot;must bail out w/ explicit message&quot;);
2634       return;
2635     }
2636     print_method(PHASE_BARRIER_EXPANSION, 2);
2637   }
2638 
2639   if (opaque4_count() &gt; 0) {
2640     C-&gt;remove_opaque4_nodes(igvn);
2641     igvn.optimize();
2642   }
2643 
2644   if (C-&gt;max_vector_size() &gt; 0) {
2645     C-&gt;optimize_logic_cones(igvn);
2646     igvn.optimize();
2647   }
2648 
2649   DEBUG_ONLY( _modified_nodes = NULL; )
2650  } // (End scope of igvn; run destructor if necessary for asserts.)
2651 
2652  process_print_inlining();
2653  // A method with only infinite loops has no edges entering loops from root
2654  {
2655    TracePhase tp(&quot;graphReshape&quot;, &amp;timers[_t_graphReshaping]);
2656    if (final_graph_reshaping()) {
2657      assert(failing(), &quot;must bail out w/ explicit message&quot;);
2658      return;
2659    }
2660  }
2661 
2662  print_method(PHASE_OPTIMIZE_FINISHED, 2);
2663  DEBUG_ONLY(set_phase_optimize_finished();)
2664 }
2665 
2666 //---------------------------- Bitwise operation packing optimization ---------------------------
2667 
2668 static bool is_vector_unary_bitwise_op(Node* n) {
2669   return n-&gt;Opcode() == Op_XorV &amp;&amp;
2670          VectorNode::is_vector_bitwise_not_pattern(n);
2671 }
2672 
2673 static bool is_vector_binary_bitwise_op(Node* n) {
2674   switch (n-&gt;Opcode()) {
2675     case Op_AndV:
2676     case Op_OrV:
2677       return true;
2678 
2679     case Op_XorV:
2680       return !is_vector_unary_bitwise_op(n);
2681 
2682     default:
2683       return false;
2684   }
2685 }
2686 
2687 static bool is_vector_ternary_bitwise_op(Node* n) {
2688   return n-&gt;Opcode() == Op_MacroLogicV;
2689 }
2690 
2691 static bool is_vector_bitwise_op(Node* n) {
2692   return is_vector_unary_bitwise_op(n)  ||
2693          is_vector_binary_bitwise_op(n) ||
2694          is_vector_ternary_bitwise_op(n);
2695 }
2696 
2697 static bool is_vector_bitwise_cone_root(Node* n) {
2698   if (!is_vector_bitwise_op(n)) {
2699     return false;
2700   }
2701   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
2702     if (is_vector_bitwise_op(n-&gt;fast_out(i))) {
2703       return false;
2704     }
2705   }
2706   return true;
2707 }
2708 
2709 static uint collect_unique_inputs(Node* n, Unique_Node_List&amp; partition, Unique_Node_List&amp; inputs) {
2710   uint cnt = 0;
2711   if (is_vector_bitwise_op(n)) {
2712     if (VectorNode::is_vector_bitwise_not_pattern(n)) {
2713       for (uint i = 1; i &lt; n-&gt;req(); i++) {
2714         Node* in = n-&gt;in(i);
2715         bool skip = VectorNode::is_all_ones_vector(in);
2716         if (!skip &amp;&amp; !inputs.member(in)) {
2717           inputs.push(in);
2718           cnt++;
2719         }
2720       }
2721       assert(cnt &lt;= 1, &quot;not unary&quot;);
2722     } else {
2723       uint last_req = n-&gt;req();
2724       if (is_vector_ternary_bitwise_op(n)) {
2725         last_req = n-&gt;req() - 1; // skip last input
2726       }
2727       for (uint i = 1; i &lt; last_req; i++) {
2728         Node* def = n-&gt;in(i);
2729         if (!inputs.member(def)) {
2730           inputs.push(def);
2731           cnt++;
2732         }
2733       }
2734     }
2735     partition.push(n);
2736   } else { // not a bitwise operations
2737     if (!inputs.member(n)) {
2738       inputs.push(n);
2739       cnt++;
2740     }
2741   }
2742   return cnt;
2743 }
2744 
2745 void Compile::collect_logic_cone_roots(Unique_Node_List&amp; list) {
2746   Unique_Node_List useful_nodes;
2747   C-&gt;identify_useful_nodes(useful_nodes);
2748 
2749   for (uint i = 0; i &lt; useful_nodes.size(); i++) {
2750     Node* n = useful_nodes.at(i);
2751     if (is_vector_bitwise_cone_root(n)) {
2752       list.push(n);
2753     }
2754   }
2755 }
2756 
2757 Node* Compile::xform_to_MacroLogicV(PhaseIterGVN&amp; igvn,
2758                                     const TypeVect* vt,
2759                                     Unique_Node_List&amp; partition,
2760                                     Unique_Node_List&amp; inputs) {
2761   assert(partition.size() == 2 || partition.size() == 3, &quot;not supported&quot;);
2762   assert(inputs.size()    == 2 || inputs.size()    == 3, &quot;not supported&quot;);
2763   assert(Matcher::match_rule_supported_vector(Op_MacroLogicV, vt-&gt;length(), vt-&gt;element_basic_type()), &quot;not supported&quot;);
2764 
2765   Node* in1 = inputs.at(0);
2766   Node* in2 = inputs.at(1);
2767   Node* in3 = (inputs.size() == 3 ? inputs.at(2) : in2);
2768 
2769   uint func = compute_truth_table(partition, inputs);
2770   return igvn.transform(MacroLogicVNode::make(igvn, in3, in2, in1, func, vt));
2771 }
2772 
2773 static uint extract_bit(uint func, uint pos) {
2774   return (func &amp; (1 &lt;&lt; pos)) &gt;&gt; pos;
2775 }
2776 
2777 //
2778 //  A macro logic node represents a truth table. It has 4 inputs,
2779 //  First three inputs corresponds to 3 columns of a truth table
2780 //  and fourth input captures the logic function.
2781 //
2782 //  eg.  fn = (in1 AND in2) OR in3;
2783 //
2784 //      MacroNode(in1,in2,in3,fn)
2785 //
2786 //  -----------------
2787 //  in1 in2 in3  fn
2788 //  -----------------
2789 //  0    0   0    0
2790 //  0    0   1    1
2791 //  0    1   0    0
2792 //  0    1   1    1
2793 //  1    0   0    0
2794 //  1    0   1    1
2795 //  1    1   0    1
2796 //  1    1   1    1
2797 //
2798 
2799 uint Compile::eval_macro_logic_op(uint func, uint in1 , uint in2, uint in3) {
2800   int res = 0;
2801   for (int i = 0; i &lt; 8; i++) {
2802     int bit1 = extract_bit(in1, i);
2803     int bit2 = extract_bit(in2, i);
2804     int bit3 = extract_bit(in3, i);
2805 
2806     int func_bit_pos = (bit1 &lt;&lt; 2 | bit2 &lt;&lt; 1 | bit3);
2807     int func_bit = extract_bit(func, func_bit_pos);
2808 
2809     res |= func_bit &lt;&lt; i;
2810   }
2811   return res;
2812 }
2813 
2814 static uint eval_operand(Node* n, ResourceHashtable&lt;Node*,uint&gt;&amp; eval_map) {
2815   assert(n != NULL, &quot;&quot;);
2816   assert(eval_map.contains(n), &quot;absent&quot;);
2817   return *(eval_map.get(n));
2818 }
2819 
2820 static void eval_operands(Node* n,
2821                           uint&amp; func1, uint&amp; func2, uint&amp; func3,
2822                           ResourceHashtable&lt;Node*,uint&gt;&amp; eval_map) {
2823   assert(is_vector_bitwise_op(n), &quot;&quot;);
2824   func1 = eval_operand(n-&gt;in(1), eval_map);
2825 
2826   if (is_vector_binary_bitwise_op(n)) {
2827     func2 = eval_operand(n-&gt;in(2), eval_map);
2828   } else if (is_vector_ternary_bitwise_op(n)) {
2829     func2 = eval_operand(n-&gt;in(2), eval_map);
2830     func3 = eval_operand(n-&gt;in(3), eval_map);
2831   } else {
2832     assert(is_vector_unary_bitwise_op(n), &quot;not unary&quot;);
2833   }
2834 }
2835 
2836 uint Compile::compute_truth_table(Unique_Node_List&amp; partition, Unique_Node_List&amp; inputs) {
2837   assert(inputs.size() &lt;= 3, &quot;sanity&quot;);
2838   ResourceMark rm;
2839   uint res = 0;
2840   ResourceHashtable&lt;Node*,uint&gt; eval_map;
2841 
2842   // Populate precomputed functions for inputs.
2843   // Each input corresponds to one column of 3 input truth-table.
2844   uint input_funcs[] = { 0xAA,   // (_, _, a) -&gt; a
2845                          0xCC,   // (_, b, _) -&gt; b
2846                          0xF0 }; // (c, _, _) -&gt; c
2847   for (uint i = 0; i &lt; inputs.size(); i++) {
2848     eval_map.put(inputs.at(i), input_funcs[i]);
2849   }
2850 
2851   for (uint i = 0; i &lt; partition.size(); i++) {
2852     Node* n = partition.at(i);
2853 
2854     uint func1 = 0, func2 = 0, func3 = 0;
2855     eval_operands(n, func1, func2, func3, eval_map);
2856 
2857     switch (n-&gt;Opcode()) {
2858       case Op_OrV:
2859         assert(func3 == 0, &quot;not binary&quot;);
2860         res = func1 | func2;
2861         break;
2862       case Op_AndV:
2863         assert(func3 == 0, &quot;not binary&quot;);
2864         res = func1 &amp; func2;
2865         break;
2866       case Op_XorV:
2867         if (VectorNode::is_vector_bitwise_not_pattern(n)) {
2868           assert(func2 == 0 &amp;&amp; func3 == 0, &quot;not unary&quot;);
2869           res = (~func1) &amp; 0xFF;
2870         } else {
2871           assert(func3 == 0, &quot;not binary&quot;);
2872           res = func1 ^ func2;
2873         }
2874         break;
2875       case Op_MacroLogicV:
2876         // Ordering of inputs may change during evaluation of sub-tree
2877         // containing MacroLogic node as a child node, thus a re-evaluation
2878         // makes sure that function is evaluated in context of current
2879         // inputs.
2880         res = eval_macro_logic_op(n-&gt;in(4)-&gt;get_int(), func1, func2, func3);
2881         break;
2882 
2883       default: assert(false, &quot;not supported: %s&quot;, n-&gt;Name());
2884     }
2885     assert(res &lt;= 0xFF, &quot;invalid&quot;);
2886     eval_map.put(n, res);
2887   }
2888   return res;
2889 }
2890 
2891 bool Compile::compute_logic_cone(Node* n, Unique_Node_List&amp; partition, Unique_Node_List&amp; inputs) {
2892   assert(partition.size() == 0, &quot;not empty&quot;);
2893   assert(inputs.size() == 0, &quot;not empty&quot;);
2894   if (is_vector_ternary_bitwise_op(n)) {
2895     return false;
2896   }
2897 
2898   bool is_unary_op = is_vector_unary_bitwise_op(n);
2899   if (is_unary_op) {
2900     assert(collect_unique_inputs(n, partition, inputs) == 1, &quot;not unary&quot;);
2901     return false; // too few inputs
2902   }
2903 
2904   assert(is_vector_binary_bitwise_op(n), &quot;not binary&quot;);
2905   Node* in1 = n-&gt;in(1);
2906   Node* in2 = n-&gt;in(2);
2907 
2908   int in1_unique_inputs_cnt = collect_unique_inputs(in1, partition, inputs);
2909   int in2_unique_inputs_cnt = collect_unique_inputs(in2, partition, inputs);
2910   partition.push(n);
2911 
2912   // Too many inputs?
2913   if (inputs.size() &gt; 3) {
2914     partition.clear();
2915     inputs.clear();
2916     { // Recompute in2 inputs
2917       Unique_Node_List not_used;
2918       in2_unique_inputs_cnt = collect_unique_inputs(in2, not_used, not_used);
2919     }
2920     // Pick the node with minimum number of inputs.
2921     if (in1_unique_inputs_cnt &gt;= 3 &amp;&amp; in2_unique_inputs_cnt &gt;= 3) {
2922       return false; // still too many inputs
2923     }
2924     // Recompute partition &amp; inputs.
2925     Node* child       = (in1_unique_inputs_cnt &lt; in2_unique_inputs_cnt ? in1 : in2);
2926     collect_unique_inputs(child, partition, inputs);
2927 
2928     Node* other_input = (in1_unique_inputs_cnt &lt; in2_unique_inputs_cnt ? in2 : in1);
2929     inputs.push(other_input);
2930 
2931     partition.push(n);
2932   }
2933 
2934   return (partition.size() == 2 || partition.size() == 3) &amp;&amp;
2935          (inputs.size()    == 2 || inputs.size()    == 3);
2936 }
2937 
2938 
2939 void Compile::process_logic_cone_root(PhaseIterGVN &amp;igvn, Node *n, VectorSet &amp;visited) {
2940   assert(is_vector_bitwise_op(n), &quot;not a root&quot;);
2941 
2942   visited.set(n-&gt;_idx);
2943 
2944   // 1) Do a DFS walk over the logic cone.
2945   for (uint i = 1; i &lt; n-&gt;req(); i++) {
2946     Node* in = n-&gt;in(i);
2947     if (!visited.test(in-&gt;_idx) &amp;&amp; is_vector_bitwise_op(in)) {
2948       process_logic_cone_root(igvn, in, visited);
2949     }
2950   }
2951 
2952   // 2) Bottom up traversal: Merge node[s] with
2953   // the parent to form macro logic node.
2954   Unique_Node_List partition;
2955   Unique_Node_List inputs;
2956   if (compute_logic_cone(n, partition, inputs)) {
2957     const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
2958     Node* macro_logic = xform_to_MacroLogicV(igvn, vt, partition, inputs);
2959     igvn.replace_node(n, macro_logic);
2960   }
2961 }
2962 
2963 void Compile::optimize_logic_cones(PhaseIterGVN &amp;igvn) {
2964   ResourceMark rm;
2965   if (Matcher::match_rule_supported(Op_MacroLogicV)) {
2966     Unique_Node_List list;
2967     collect_logic_cone_roots(list);
2968 
2969     while (list.size() &gt; 0) {
2970       Node* n = list.pop();
2971       const TypeVect* vt = n-&gt;bottom_type()-&gt;is_vect();
2972       bool supported = Matcher::match_rule_supported_vector(Op_MacroLogicV, vt-&gt;length(), vt-&gt;element_basic_type());
2973       if (supported) {
2974         VectorSet visited(comp_arena());
2975         process_logic_cone_root(igvn, n, visited);
2976       }
2977     }
2978   }
2979 }
2980 
2981 //------------------------------Code_Gen---------------------------------------
2982 // Given a graph, generate code for it
2983 void Compile::Code_Gen() {
2984   if (failing()) {
2985     return;
2986   }
2987 
2988   // Perform instruction selection.  You might think we could reclaim Matcher
2989   // memory PDQ, but actually the Matcher is used in generating spill code.
2990   // Internals of the Matcher (including some VectorSets) must remain live
2991   // for awhile - thus I cannot reclaim Matcher memory lest a VectorSet usage
2992   // set a bit in reclaimed memory.
2993 
2994   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2995   // nodes.  Mapping is only valid at the root of each matched subtree.
2996   NOT_PRODUCT( verify_graph_edges(); )
2997 
2998   Matcher matcher;
2999   _matcher = &amp;matcher;
3000   {
3001     TracePhase tp(&quot;matcher&quot;, &amp;timers[_t_matcher]);
3002     matcher.match();
3003     if (failing()) {
3004       return;
3005     }
3006   }
3007 
3008   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
3009   // nodes.  Mapping is only valid at the root of each matched subtree.
3010   NOT_PRODUCT( verify_graph_edges(); )
3011 
3012   // If you have too many nodes, or if matching has failed, bail out
3013   check_node_count(0, &quot;out of nodes matching instructions&quot;);
3014   if (failing()) {
3015     return;
3016   }
3017 
3018   print_method(PHASE_MATCHING, 2);
3019 
3020   // Build a proper-looking CFG
3021   PhaseCFG cfg(node_arena(), root(), matcher);
3022   _cfg = &amp;cfg;
3023   {
3024     TracePhase tp(&quot;scheduler&quot;, &amp;timers[_t_scheduler]);
3025     bool success = cfg.do_global_code_motion();
3026     if (!success) {
3027       return;
3028     }
3029 
3030     print_method(PHASE_GLOBAL_CODE_MOTION, 2);
3031     NOT_PRODUCT( verify_graph_edges(); )
3032     debug_only( cfg.verify(); )
3033   }
3034 
3035   PhaseChaitin regalloc(unique(), cfg, matcher, false);
3036   _regalloc = &amp;regalloc;
3037   {
3038     TracePhase tp(&quot;regalloc&quot;, &amp;timers[_t_registerAllocation]);
3039     // Perform register allocation.  After Chaitin, use-def chains are
3040     // no longer accurate (at spill code) and so must be ignored.
3041     // Node-&gt;LRG-&gt;reg mappings are still accurate.
3042     _regalloc-&gt;Register_Allocate();
3043 
3044     // Bail out if the allocator builds too many nodes
3045     if (failing()) {
3046       return;
3047     }
3048   }
3049 
3050   // Prior to register allocation we kept empty basic blocks in case the
3051   // the allocator needed a place to spill.  After register allocation we
3052   // are not adding any new instructions.  If any basic block is empty, we
3053   // can now safely remove it.
3054   {
3055     TracePhase tp(&quot;blockOrdering&quot;, &amp;timers[_t_blockOrdering]);
3056     cfg.remove_empty_blocks();
3057     if (do_freq_based_layout()) {
3058       PhaseBlockLayout layout(cfg);
3059     } else {
3060       cfg.set_loop_alignment();
3061     }
3062     cfg.fixup_flow();
3063   }
3064 
3065   // Apply peephole optimizations
3066   if( OptoPeephole ) {
3067     TracePhase tp(&quot;peephole&quot;, &amp;timers[_t_peephole]);
3068     PhasePeephole peep( _regalloc, cfg);
3069     peep.do_transform();
3070   }
3071 
3072   // Do late expand if CPU requires this.
3073   if (Matcher::require_postalloc_expand) {
3074     TracePhase tp(&quot;postalloc_expand&quot;, &amp;timers[_t_postalloc_expand]);
3075     cfg.postalloc_expand(_regalloc);
3076   }
3077 
3078   // Convert Nodes to instruction bits in a buffer
3079   {
3080     TracePhase tp(&quot;output&quot;, &amp;timers[_t_output]);
3081     PhaseOutput output;
3082     output.Output();
3083     if (failing())  return;
3084     output.install();
3085   }
3086 
3087   print_method(PHASE_FINAL_CODE);
3088 
3089   // He&#39;s dead, Jim.
3090   _cfg     = (PhaseCFG*)((intptr_t)0xdeadbeef);
3091   _regalloc = (PhaseChaitin*)((intptr_t)0xdeadbeef);
3092 }
3093 
3094 //------------------------------Final_Reshape_Counts---------------------------
3095 // This class defines counters to help identify when a method
3096 // may/must be executed using hardware with only 24-bit precision.
3097 struct Final_Reshape_Counts : public StackObj {
3098   int  _call_count;             // count non-inlined &#39;common&#39; calls
3099   int  _float_count;            // count float ops requiring 24-bit precision
3100   int  _double_count;           // count double ops requiring more precision
3101   int  _java_call_count;        // count non-inlined &#39;java&#39; calls
3102   int  _inner_loop_count;       // count loops which need alignment
3103   VectorSet _visited;           // Visitation flags
3104   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
3105 
3106   Final_Reshape_Counts() :
3107     _call_count(0), _float_count(0), _double_count(0),
3108     _java_call_count(0), _inner_loop_count(0) { }
3109 
3110   void inc_call_count  () { _call_count  ++; }
3111   void inc_float_count () { _float_count ++; }
3112   void inc_double_count() { _double_count++; }
3113   void inc_java_call_count() { _java_call_count++; }
3114   void inc_inner_loop_count() { _inner_loop_count++; }
3115 
3116   int  get_call_count  () const { return _call_count  ; }
3117   int  get_float_count () const { return _float_count ; }
3118   int  get_double_count() const { return _double_count; }
3119   int  get_java_call_count() const { return _java_call_count; }
3120   int  get_inner_loop_count() const { return _inner_loop_count; }
3121 };
3122 
3123 #ifdef ASSERT
3124 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
3125   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
3126   // Make sure the offset goes inside the instance layout.
3127   return k-&gt;contains_field_offset(tp-&gt;offset());
3128   // Note that OffsetBot and OffsetTop are very negative.
3129 }
3130 #endif
3131 
3132 // Eliminate trivially redundant StoreCMs and accumulate their
3133 // precedence edges.
3134 void Compile::eliminate_redundant_card_marks(Node* n) {
3135   assert(n-&gt;Opcode() == Op_StoreCM, &quot;expected StoreCM&quot;);
3136   if (n-&gt;in(MemNode::Address)-&gt;outcnt() &gt; 1) {
3137     // There are multiple users of the same address so it might be
3138     // possible to eliminate some of the StoreCMs
3139     Node* mem = n-&gt;in(MemNode::Memory);
3140     Node* adr = n-&gt;in(MemNode::Address);
3141     Node* val = n-&gt;in(MemNode::ValueIn);
3142     Node* prev = n;
3143     bool done = false;
3144     // Walk the chain of StoreCMs eliminating ones that match.  As
3145     // long as it&#39;s a chain of single users then the optimization is
3146     // safe.  Eliminating partially redundant StoreCMs would require
3147     // cloning copies down the other paths.
3148     while (mem-&gt;Opcode() == Op_StoreCM &amp;&amp; mem-&gt;outcnt() == 1 &amp;&amp; !done) {
3149       if (adr == mem-&gt;in(MemNode::Address) &amp;&amp;
3150           val == mem-&gt;in(MemNode::ValueIn)) {
3151         // redundant StoreCM
3152         if (mem-&gt;req() &gt; MemNode::OopStore) {
3153           // Hasn&#39;t been processed by this code yet.
3154           n-&gt;add_prec(mem-&gt;in(MemNode::OopStore));
3155         } else {
3156           // Already converted to precedence edge
3157           for (uint i = mem-&gt;req(); i &lt; mem-&gt;len(); i++) {
3158             // Accumulate any precedence edges
3159             if (mem-&gt;in(i) != NULL) {
3160               n-&gt;add_prec(mem-&gt;in(i));
3161             }
3162           }
3163           // Everything above this point has been processed.
3164           done = true;
3165         }
3166         // Eliminate the previous StoreCM
3167         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
3168         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
3169         mem-&gt;disconnect_inputs(NULL, this);
3170       } else {
3171         prev = mem;
3172       }
3173       mem = prev-&gt;in(MemNode::Memory);
3174     }
3175   }
3176 }
3177 
<a name="44" id="anc44"></a><span class="line-added">3178 </span>
3179 //------------------------------final_graph_reshaping_impl----------------------
3180 // Implement items 1-5 from final_graph_reshaping below.
3181 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
3182 
3183   if ( n-&gt;outcnt() == 0 ) return; // dead node
3184   uint nop = n-&gt;Opcode();
3185 
3186   // Check for 2-input instruction with &quot;last use&quot; on right input.
3187   // Swap to left input.  Implements item (2).
3188   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
3189       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
3190       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
3191       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
3192       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
3193     // Check for commutative opcode
3194     switch( nop ) {
3195     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
3196     case Op_MaxI:  case Op_MinI:
3197     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
3198     case Op_AndL:  case Op_XorL:  case Op_OrL:
3199     case Op_AndI:  case Op_XorI:  case Op_OrI: {
3200       // Move &quot;last use&quot; input to left by swapping inputs
3201       n-&gt;swap_edges(1, 2);
3202       break;
3203     }
3204     default:
3205       break;
3206     }
3207   }
3208 
3209 #ifdef ASSERT
3210   if( n-&gt;is_Mem() ) {
3211     int alias_idx = get_alias_index(n-&gt;as_Mem()-&gt;adr_type());
3212     assert( n-&gt;in(0) != NULL || alias_idx != Compile::AliasIdxRaw ||
3213             // oop will be recorded in oop map if load crosses safepoint
3214             n-&gt;is_Load() &amp;&amp; (n-&gt;as_Load()-&gt;bottom_type()-&gt;isa_oopptr() ||
3215                              LoadNode::is_immutable_value(n-&gt;in(MemNode::Address))),
3216             &quot;raw memory operations should have control edge&quot;);
3217   }
3218   if (n-&gt;is_MemBar()) {
3219     MemBarNode* mb = n-&gt;as_MemBar();
3220     if (mb-&gt;trailing_store() || mb-&gt;trailing_load_store()) {
3221       assert(mb-&gt;leading_membar()-&gt;trailing_membar() == mb, &quot;bad membar pair&quot;);
3222       Node* mem = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;step_over_gc_barrier(mb-&gt;in(MemBarNode::Precedent));
3223       assert((mb-&gt;trailing_store() &amp;&amp; mem-&gt;is_Store() &amp;&amp; mem-&gt;as_Store()-&gt;is_release()) ||
3224              (mb-&gt;trailing_load_store() &amp;&amp; mem-&gt;is_LoadStore()), &quot;missing mem op&quot;);
3225     } else if (mb-&gt;leading()) {
3226       assert(mb-&gt;trailing_membar()-&gt;leading_membar() == mb, &quot;bad membar pair&quot;);
3227     }
3228   }
3229 #endif
3230   // Count FPU ops and common calls, implements item (3)
3231   bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;final_graph_reshaping(this, n, nop);
3232   if (!gc_handled) {
3233     final_graph_reshaping_main_switch(n, frc, nop);
3234   }
3235 
3236   // Collect CFG split points
3237   if (n-&gt;is_MultiBranch() &amp;&amp; !n-&gt;is_RangeCheck()) {
3238     frc._tests.push(n);
3239   }
3240 }
3241 
3242 void Compile::final_graph_reshaping_main_switch(Node* n, Final_Reshape_Counts&amp; frc, uint nop) {
3243   switch( nop ) {
3244   // Count all float operations that may use FPU
3245   case Op_AddF:
3246   case Op_SubF:
3247   case Op_MulF:
3248   case Op_DivF:
3249   case Op_NegF:
3250   case Op_ModF:
3251   case Op_ConvI2F:
3252   case Op_ConF:
3253   case Op_CmpF:
3254   case Op_CmpF3:
3255   // case Op_ConvL2F: // longs are split into 32-bit halves
3256     frc.inc_float_count();
3257     break;
3258 
3259   case Op_ConvF2D:
3260   case Op_ConvD2F:
3261     frc.inc_float_count();
3262     frc.inc_double_count();
3263     break;
3264 
3265   // Count all double operations that may use FPU
3266   case Op_AddD:
3267   case Op_SubD:
3268   case Op_MulD:
3269   case Op_DivD:
3270   case Op_NegD:
3271   case Op_ModD:
3272   case Op_ConvI2D:
3273   case Op_ConvD2I:
3274   // case Op_ConvL2D: // handled by leaf call
3275   // case Op_ConvD2L: // handled by leaf call
3276   case Op_ConD:
3277   case Op_CmpD:
3278   case Op_CmpD3:
3279     frc.inc_double_count();
3280     break;
3281   case Op_Opaque1:              // Remove Opaque Nodes before matching
3282   case Op_Opaque2:              // Remove Opaque Nodes before matching
3283   case Op_Opaque3:
3284     n-&gt;subsume_by(n-&gt;in(1), this);
3285     break;
3286   case Op_CallStaticJava:
3287   case Op_CallJava:
3288   case Op_CallDynamicJava:
3289     frc.inc_java_call_count(); // Count java call site;
3290   case Op_CallRuntime:
3291   case Op_CallLeaf:
3292   case Op_CallLeafNoFP: {
3293     assert (n-&gt;is_Call(), &quot;&quot;);
3294     CallNode *call = n-&gt;as_Call();
3295     // Count call sites where the FP mode bit would have to be flipped.
3296     // Do not count uncommon runtime calls:
3297     // uncommon_trap, _complete_monitor_locking, _complete_monitor_unlocking,
3298     // _new_Java, _new_typeArray, _new_objArray, _rethrow_Java, ...
3299     if (!call-&gt;is_CallStaticJava() || !call-&gt;as_CallStaticJava()-&gt;_name) {
3300       frc.inc_call_count();   // Count the call site
3301     } else {                  // See if uncommon argument is shared
3302       Node *n = call-&gt;in(TypeFunc::Parms);
3303       int nop = n-&gt;Opcode();
3304       // Clone shared simple arguments to uncommon calls, item (1).
3305       if (n-&gt;outcnt() &gt; 1 &amp;&amp;
3306           !n-&gt;is_Proj() &amp;&amp;
3307           nop != Op_CreateEx &amp;&amp;
3308           nop != Op_CheckCastPP &amp;&amp;
3309           nop != Op_DecodeN &amp;&amp;
3310           nop != Op_DecodeNKlass &amp;&amp;
3311           !n-&gt;is_Mem() &amp;&amp;
3312           !n-&gt;is_Phi()) {
3313         Node *x = n-&gt;clone();
3314         call-&gt;set_req(TypeFunc::Parms, x);
3315       }
3316     }
3317     break;
3318   }
3319 
3320   case Op_StoreD:
3321   case Op_LoadD:
3322   case Op_LoadD_unaligned:
3323     frc.inc_double_count();
3324     goto handle_mem;
3325   case Op_StoreF:
3326   case Op_LoadF:
3327     frc.inc_float_count();
3328     goto handle_mem;
3329 
3330   case Op_StoreCM:
3331     {
3332       // Convert OopStore dependence into precedence edge
3333       Node* prec = n-&gt;in(MemNode::OopStore);
3334       n-&gt;del_req(MemNode::OopStore);
3335       n-&gt;add_prec(prec);
3336       eliminate_redundant_card_marks(n);
3337     }
3338 
3339     // fall through
3340 
3341   case Op_StoreB:
3342   case Op_StoreC:
3343   case Op_StorePConditional:
3344   case Op_StoreI:
3345   case Op_StoreL:
3346   case Op_StoreIConditional:
3347   case Op_StoreLConditional:
3348   case Op_CompareAndSwapB:
3349   case Op_CompareAndSwapS:
3350   case Op_CompareAndSwapI:
3351   case Op_CompareAndSwapL:
3352   case Op_CompareAndSwapP:
3353   case Op_CompareAndSwapN:
3354   case Op_WeakCompareAndSwapB:
3355   case Op_WeakCompareAndSwapS:
3356   case Op_WeakCompareAndSwapI:
3357   case Op_WeakCompareAndSwapL:
3358   case Op_WeakCompareAndSwapP:
3359   case Op_WeakCompareAndSwapN:
3360   case Op_CompareAndExchangeB:
3361   case Op_CompareAndExchangeS:
3362   case Op_CompareAndExchangeI:
3363   case Op_CompareAndExchangeL:
3364   case Op_CompareAndExchangeP:
3365   case Op_CompareAndExchangeN:
3366   case Op_GetAndAddS:
3367   case Op_GetAndAddB:
3368   case Op_GetAndAddI:
3369   case Op_GetAndAddL:
3370   case Op_GetAndSetS:
3371   case Op_GetAndSetB:
3372   case Op_GetAndSetI:
3373   case Op_GetAndSetL:
3374   case Op_GetAndSetP:
3375   case Op_GetAndSetN:
3376   case Op_StoreP:
3377   case Op_StoreN:
3378   case Op_StoreNKlass:
3379   case Op_LoadB:
3380   case Op_LoadUB:
3381   case Op_LoadUS:
3382   case Op_LoadI:
3383   case Op_LoadKlass:
3384   case Op_LoadNKlass:
3385   case Op_LoadL:
3386   case Op_LoadL_unaligned:
3387   case Op_LoadPLocked:
3388   case Op_LoadP:
3389   case Op_LoadN:
3390   case Op_LoadRange:
3391   case Op_LoadS: {
3392   handle_mem:
3393 #ifdef ASSERT
3394     if( VerifyOptoOopOffsets ) {
3395       MemNode* mem  = n-&gt;as_Mem();
3396       // Check to see if address types have grounded out somehow.
3397       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
3398       assert( !tp || oop_offset_is_sane(tp), &quot;&quot; );
3399     }
3400 #endif
3401     break;
3402   }
3403 
3404   case Op_AddP: {               // Assert sane base pointers
3405     Node *addp = n-&gt;in(AddPNode::Address);
3406     assert( !addp-&gt;is_AddP() ||
3407             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
3408             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
3409             &quot;Base pointers must match (addp %u)&quot;, addp-&gt;_idx );
3410 #ifdef _LP64
3411     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
3412         addp-&gt;Opcode() == Op_ConP &amp;&amp;
3413         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
3414         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
3415       // If the transformation of ConP to ConN+DecodeN is beneficial depends
3416       // on the platform and on the compressed oops mode.
3417       // Use addressing with narrow klass to load with offset on x86.
3418       // Some platforms can use the constant pool to load ConP.
3419       // Do this transformation here since IGVN will convert ConN back to ConP.
3420       const Type* t = addp-&gt;bottom_type();
3421       bool is_oop   = t-&gt;isa_oopptr() != NULL;
3422       bool is_klass = t-&gt;isa_klassptr() != NULL;
3423 
3424       if ((is_oop   &amp;&amp; Matcher::const_oop_prefer_decode()  ) ||
3425           (is_klass &amp;&amp; Matcher::const_klass_prefer_decode())) {
3426         Node* nn = NULL;
3427 
3428         int op = is_oop ? Op_ConN : Op_ConNKlass;
3429 
3430         // Look for existing ConN node of the same exact type.
3431         Node* r  = root();
3432         uint cnt = r-&gt;outcnt();
3433         for (uint i = 0; i &lt; cnt; i++) {
3434           Node* m = r-&gt;raw_out(i);
3435           if (m!= NULL &amp;&amp; m-&gt;Opcode() == op &amp;&amp;
3436               m-&gt;bottom_type()-&gt;make_ptr() == t) {
3437             nn = m;
3438             break;
3439           }
3440         }
3441         if (nn != NULL) {
3442           // Decode a narrow oop to match address
3443           // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
3444           if (is_oop) {
3445             nn = new DecodeNNode(nn, t);
3446           } else {
3447             nn = new DecodeNKlassNode(nn, t);
3448           }
3449           // Check for succeeding AddP which uses the same Base.
3450           // Otherwise we will run into the assertion above when visiting that guy.
3451           for (uint i = 0; i &lt; n-&gt;outcnt(); ++i) {
3452             Node *out_i = n-&gt;raw_out(i);
3453             if (out_i &amp;&amp; out_i-&gt;is_AddP() &amp;&amp; out_i-&gt;in(AddPNode::Base) == addp) {
3454               out_i-&gt;set_req(AddPNode::Base, nn);
3455 #ifdef ASSERT
3456               for (uint j = 0; j &lt; out_i-&gt;outcnt(); ++j) {
3457                 Node *out_j = out_i-&gt;raw_out(j);
3458                 assert(out_j == NULL || !out_j-&gt;is_AddP() || out_j-&gt;in(AddPNode::Base) != addp,
3459                        &quot;more than 2 AddP nodes in a chain (out_j %u)&quot;, out_j-&gt;_idx);
3460               }
3461 #endif
3462             }
3463           }
3464           n-&gt;set_req(AddPNode::Base, nn);
3465           n-&gt;set_req(AddPNode::Address, nn);
3466           if (addp-&gt;outcnt() == 0) {
3467             addp-&gt;disconnect_inputs(NULL, this);
3468           }
3469         }
3470       }
3471     }
3472 #endif
3473     // platform dependent reshaping of the address expression
3474     reshape_address(n-&gt;as_AddP());
3475     break;
3476   }
3477 
3478   case Op_CastPP: {
3479     // Remove CastPP nodes to gain more freedom during scheduling but
3480     // keep the dependency they encode as control or precedence edges
3481     // (if control is set already) on memory operations. Some CastPP
3482     // nodes don&#39;t have a control (don&#39;t carry a dependency): skip
3483     // those.
3484     if (n-&gt;in(0) != NULL) {
3485       ResourceMark rm;
3486       Unique_Node_List wq;
3487       wq.push(n);
3488       for (uint next = 0; next &lt; wq.size(); ++next) {
3489         Node *m = wq.at(next);
3490         for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax; i++) {
3491           Node* use = m-&gt;fast_out(i);
3492           if (use-&gt;is_Mem() || use-&gt;is_EncodeNarrowPtr()) {
3493             use-&gt;ensure_control_or_add_prec(n-&gt;in(0));
3494           } else {
3495             switch(use-&gt;Opcode()) {
3496             case Op_AddP:
3497             case Op_DecodeN:
3498             case Op_DecodeNKlass:
3499             case Op_CheckCastPP:
3500             case Op_CastPP:
3501               wq.push(use);
3502               break;
3503             }
3504           }
3505         }
3506       }
3507     }
3508     const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);
3509     if (is_LP64 &amp;&amp; n-&gt;in(1)-&gt;is_DecodeN() &amp;&amp; Matcher::gen_narrow_oop_implicit_null_checks()) {
3510       Node* in1 = n-&gt;in(1);
3511       const Type* t = n-&gt;bottom_type();
3512       Node* new_in1 = in1-&gt;clone();
3513       new_in1-&gt;as_DecodeN()-&gt;set_type(t);
3514 
3515       if (!Matcher::narrow_oop_use_complex_address()) {
3516         //
3517         // x86, ARM and friends can handle 2 adds in addressing mode
3518         // and Matcher can fold a DecodeN node into address by using
3519         // a narrow oop directly and do implicit NULL check in address:
3520         //
3521         // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
3522         // NullCheck narrow_oop_reg
3523         //
3524         // On other platforms (Sparc) we have to keep new DecodeN node and
3525         // use it to do implicit NULL check in address:
3526         //
3527         // decode_not_null narrow_oop_reg, base_reg
3528         // [base_reg + offset]
3529         // NullCheck base_reg
3530         //
3531         // Pin the new DecodeN node to non-null path on these platform (Sparc)
3532         // to keep the information to which NULL check the new DecodeN node
3533         // corresponds to use it as value in implicit_null_check().
3534         //
3535         new_in1-&gt;set_req(0, n-&gt;in(0));
3536       }
3537 
3538       n-&gt;subsume_by(new_in1, this);
3539       if (in1-&gt;outcnt() == 0) {
3540         in1-&gt;disconnect_inputs(NULL, this);
3541       }
3542     } else {
3543       n-&gt;subsume_by(n-&gt;in(1), this);
3544       if (n-&gt;outcnt() == 0) {
3545         n-&gt;disconnect_inputs(NULL, this);
3546       }
3547     }
3548     break;
3549   }
3550 #ifdef _LP64
3551   case Op_CmpP:
3552     // Do this transformation here to preserve CmpPNode::sub() and
3553     // other TypePtr related Ideal optimizations (for example, ptr nullness).
3554     if (n-&gt;in(1)-&gt;is_DecodeNarrowPtr() || n-&gt;in(2)-&gt;is_DecodeNarrowPtr()) {
3555       Node* in1 = n-&gt;in(1);
3556       Node* in2 = n-&gt;in(2);
3557       if (!in1-&gt;is_DecodeNarrowPtr()) {
3558         in2 = in1;
3559         in1 = n-&gt;in(2);
3560       }
3561       assert(in1-&gt;is_DecodeNarrowPtr(), &quot;sanity&quot;);
3562 
3563       Node* new_in2 = NULL;
3564       if (in2-&gt;is_DecodeNarrowPtr()) {
3565         assert(in2-&gt;Opcode() == in1-&gt;Opcode(), &quot;must be same node type&quot;);
3566         new_in2 = in2-&gt;in(1);
3567       } else if (in2-&gt;Opcode() == Op_ConP) {
3568         const Type* t = in2-&gt;bottom_type();
3569         if (t == TypePtr::NULL_PTR) {
3570           assert(in1-&gt;is_DecodeN(), &quot;compare klass to null?&quot;);
3571           // Don&#39;t convert CmpP null check into CmpN if compressed
3572           // oops implicit null check is not generated.
3573           // This will allow to generate normal oop implicit null check.
3574           if (Matcher::gen_narrow_oop_implicit_null_checks())
3575             new_in2 = ConNode::make(TypeNarrowOop::NULL_PTR);
3576           //
3577           // This transformation together with CastPP transformation above
3578           // will generated code for implicit NULL checks for compressed oops.
3579           //
3580           // The original code after Optimize()
3581           //
3582           //    LoadN memory, narrow_oop_reg
3583           //    decode narrow_oop_reg, base_reg
3584           //    CmpP base_reg, NULL
3585           //    CastPP base_reg // NotNull
3586           //    Load [base_reg + offset], val_reg
3587           //
3588           // after these transformations will be
3589           //
3590           //    LoadN memory, narrow_oop_reg
3591           //    CmpN narrow_oop_reg, NULL
3592           //    decode_not_null narrow_oop_reg, base_reg
3593           //    Load [base_reg + offset], val_reg
3594           //
3595           // and the uncommon path (== NULL) will use narrow_oop_reg directly
3596           // since narrow oops can be used in debug info now (see the code in
3597           // final_graph_reshaping_walk()).
3598           //
3599           // At the end the code will be matched to
3600           // on x86:
3601           //
3602           //    Load_narrow_oop memory, narrow_oop_reg
3603           //    Load [R12 + narrow_oop_reg&lt;&lt;3 + offset], val_reg
3604           //    NullCheck narrow_oop_reg
3605           //
3606           // and on sparc:
3607           //
3608           //    Load_narrow_oop memory, narrow_oop_reg
3609           //    decode_not_null narrow_oop_reg, base_reg
3610           //    Load [base_reg + offset], val_reg
3611           //    NullCheck base_reg
3612           //
3613         } else if (t-&gt;isa_oopptr()) {
3614           new_in2 = ConNode::make(t-&gt;make_narrowoop());
3615         } else if (t-&gt;isa_klassptr()) {
3616           new_in2 = ConNode::make(t-&gt;make_narrowklass());
3617         }
3618       }
3619       if (new_in2 != NULL) {
3620         Node* cmpN = new CmpNNode(in1-&gt;in(1), new_in2);
3621         n-&gt;subsume_by(cmpN, this);
3622         if (in1-&gt;outcnt() == 0) {
3623           in1-&gt;disconnect_inputs(NULL, this);
3624         }
3625         if (in2-&gt;outcnt() == 0) {
3626           in2-&gt;disconnect_inputs(NULL, this);
3627         }
3628       }
3629     }
3630     break;
3631 
3632   case Op_DecodeN:
3633   case Op_DecodeNKlass:
3634     assert(!n-&gt;in(1)-&gt;is_EncodeNarrowPtr(), &quot;should be optimized out&quot;);
3635     // DecodeN could be pinned when it can&#39;t be fold into
3636     // an address expression, see the code for Op_CastPP above.
3637     assert(n-&gt;in(0) == NULL || (UseCompressedOops &amp;&amp; !Matcher::narrow_oop_use_complex_address()), &quot;no control&quot;);
3638     break;
3639 
3640   case Op_EncodeP:
3641   case Op_EncodePKlass: {
3642     Node* in1 = n-&gt;in(1);
3643     if (in1-&gt;is_DecodeNarrowPtr()) {
3644       n-&gt;subsume_by(in1-&gt;in(1), this);
3645     } else if (in1-&gt;Opcode() == Op_ConP) {
3646       const Type* t = in1-&gt;bottom_type();
3647       if (t == TypePtr::NULL_PTR) {
3648         assert(t-&gt;isa_oopptr(), &quot;null klass?&quot;);
3649         n-&gt;subsume_by(ConNode::make(TypeNarrowOop::NULL_PTR), this);
3650       } else if (t-&gt;isa_oopptr()) {
3651         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowoop()), this);
3652       } else if (t-&gt;isa_klassptr()) {
3653         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowklass()), this);
3654       }
3655     }
3656     if (in1-&gt;outcnt() == 0) {
3657       in1-&gt;disconnect_inputs(NULL, this);
3658     }
3659     break;
3660   }
3661 
3662   case Op_Proj: {
3663     if (OptimizeStringConcat) {
3664       ProjNode* p = n-&gt;as_Proj();
3665       if (p-&gt;_is_io_use) {
3666         // Separate projections were used for the exception path which
3667         // are normally removed by a late inline.  If it wasn&#39;t inlined
3668         // then they will hang around and should just be replaced with
3669         // the original one.
3670         Node* proj = NULL;
3671         // Replace with just one
3672         for (SimpleDUIterator i(p-&gt;in(0)); i.has_next(); i.next()) {
3673           Node *use = i.get();
3674           if (use-&gt;is_Proj() &amp;&amp; p != use &amp;&amp; use-&gt;as_Proj()-&gt;_con == p-&gt;_con) {
3675             proj = use;
3676             break;
3677           }
3678         }
3679         assert(proj != NULL || p-&gt;_con == TypeFunc::I_O, &quot;io may be dropped at an infinite loop&quot;);
3680         if (proj != NULL) {
3681           p-&gt;subsume_by(proj, this);
3682         }
3683       }
3684     }
3685     break;
3686   }
3687 
3688   case Op_Phi:
3689     if (n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowoop() || n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowklass()) {
3690       // The EncodeP optimization may create Phi with the same edges
3691       // for all paths. It is not handled well by Register Allocator.
3692       Node* unique_in = n-&gt;in(1);
3693       assert(unique_in != NULL, &quot;&quot;);
3694       uint cnt = n-&gt;req();
3695       for (uint i = 2; i &lt; cnt; i++) {
3696         Node* m = n-&gt;in(i);
3697         assert(m != NULL, &quot;&quot;);
3698         if (unique_in != m)
3699           unique_in = NULL;
3700       }
3701       if (unique_in != NULL) {
3702         n-&gt;subsume_by(unique_in, this);
3703       }
3704     }
3705     break;
3706 
3707 #endif
3708 
3709 #ifdef ASSERT
3710   case Op_CastII:
3711     // Verify that all range check dependent CastII nodes were removed.
3712     if (n-&gt;isa_CastII()-&gt;has_range_check()) {
3713       n-&gt;dump(3);
3714       assert(false, &quot;Range check dependent CastII node was not removed&quot;);
3715     }
3716     break;
3717 #endif
3718 
3719   case Op_ModI:
3720     if (UseDivMod) {
3721       // Check if a%b and a/b both exist
3722       Node* d = n-&gt;find_similar(Op_DivI);
3723       if (d) {
3724         // Replace them with a fused divmod if supported
3725         if (Matcher::has_match_rule(Op_DivModI)) {
3726           DivModINode* divmod = DivModINode::make(n);
3727           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3728           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3729         } else {
3730           // replace a%b with a-((a/b)*b)
3731           Node* mult = new MulINode(d, d-&gt;in(2));
3732           Node* sub  = new SubINode(d-&gt;in(1), mult);
3733           n-&gt;subsume_by(sub, this);
3734         }
3735       }
3736     }
3737     break;
3738 
3739   case Op_ModL:
3740     if (UseDivMod) {
3741       // Check if a%b and a/b both exist
3742       Node* d = n-&gt;find_similar(Op_DivL);
3743       if (d) {
3744         // Replace them with a fused divmod if supported
3745         if (Matcher::has_match_rule(Op_DivModL)) {
3746           DivModLNode* divmod = DivModLNode::make(n);
3747           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3748           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3749         } else {
3750           // replace a%b with a-((a/b)*b)
3751           Node* mult = new MulLNode(d, d-&gt;in(2));
3752           Node* sub  = new SubLNode(d-&gt;in(1), mult);
3753           n-&gt;subsume_by(sub, this);
3754         }
3755       }
3756     }
3757     break;
3758 
3759   case Op_LoadVector:
3760   case Op_StoreVector:
3761     break;
3762 
3763   case Op_AddReductionVI:
3764   case Op_AddReductionVL:
3765   case Op_AddReductionVF:
3766   case Op_AddReductionVD:
3767   case Op_MulReductionVI:
3768   case Op_MulReductionVL:
3769   case Op_MulReductionVF:
3770   case Op_MulReductionVD:
3771   case Op_MinReductionV:
3772   case Op_MaxReductionV:
3773   case Op_AndReductionV:
3774   case Op_OrReductionV:
3775   case Op_XorReductionV:
3776     break;
3777 
3778   case Op_PackB:
3779   case Op_PackS:
3780   case Op_PackI:
3781   case Op_PackF:
3782   case Op_PackL:
3783   case Op_PackD:
3784     if (n-&gt;req()-1 &gt; 2) {
3785       // Replace many operand PackNodes with a binary tree for matching
3786       PackNode* p = (PackNode*) n;
3787       Node* btp = p-&gt;binary_tree_pack(1, n-&gt;req());
3788       n-&gt;subsume_by(btp, this);
3789     }
3790     break;
3791   case Op_Loop:
3792   case Op_CountedLoop:
3793   case Op_OuterStripMinedLoop:
3794     if (n-&gt;as_Loop()-&gt;is_inner_loop()) {
3795       frc.inc_inner_loop_count();
3796     }
3797     n-&gt;as_Loop()-&gt;verify_strip_mined(0);
3798     break;
3799   case Op_LShiftI:
3800   case Op_RShiftI:
3801   case Op_URShiftI:
3802   case Op_LShiftL:
3803   case Op_RShiftL:
3804   case Op_URShiftL:
3805     if (Matcher::need_masked_shift_count) {
3806       // The cpu&#39;s shift instructions don&#39;t restrict the count to the
3807       // lower 5/6 bits. We need to do the masking ourselves.
3808       Node* in2 = n-&gt;in(2);
3809       juint mask = (n-&gt;bottom_type() == TypeInt::INT) ? (BitsPerInt - 1) : (BitsPerLong - 1);
3810       const TypeInt* t = in2-&gt;find_int_type();
3811       if (t != NULL &amp;&amp; t-&gt;is_con()) {
3812         juint shift = t-&gt;get_con();
3813         if (shift &gt; mask) { // Unsigned cmp
3814           n-&gt;set_req(2, ConNode::make(TypeInt::make(shift &amp; mask)));
3815         }
3816       } else {
3817         if (t == NULL || t-&gt;_lo &lt; 0 || t-&gt;_hi &gt; (int)mask) {
3818           Node* shift = new AndINode(in2, ConNode::make(TypeInt::make(mask)));
3819           n-&gt;set_req(2, shift);
3820         }
3821       }
3822       if (in2-&gt;outcnt() == 0) { // Remove dead node
3823         in2-&gt;disconnect_inputs(NULL, this);
3824       }
3825     }
3826     break;
3827   case Op_MemBarStoreStore:
3828   case Op_MemBarRelease:
3829     // Break the link with AllocateNode: it is no longer useful and
3830     // confuses register allocation.
3831     if (n-&gt;req() &gt; MemBarNode::Precedent) {
3832       n-&gt;set_req(MemBarNode::Precedent, top());
3833     }
3834     break;
3835   case Op_MemBarAcquire: {
3836     if (n-&gt;as_MemBar()-&gt;trailing_load() &amp;&amp; n-&gt;req() &gt; MemBarNode::Precedent) {
3837       // At parse time, the trailing MemBarAcquire for a volatile load
3838       // is created with an edge to the load. After optimizations,
3839       // that input may be a chain of Phis. If those phis have no
3840       // other use, then the MemBarAcquire keeps them alive and
3841       // register allocation can be confused.
3842       ResourceMark rm;
3843       Unique_Node_List wq;
3844       wq.push(n-&gt;in(MemBarNode::Precedent));
3845       n-&gt;set_req(MemBarNode::Precedent, top());
3846       while (wq.size() &gt; 0) {
3847         Node* m = wq.pop();
3848         if (m-&gt;outcnt() == 0) {
3849           for (uint j = 0; j &lt; m-&gt;req(); j++) {
3850             Node* in = m-&gt;in(j);
3851             if (in != NULL) {
3852               wq.push(in);
3853             }
3854           }
3855           m-&gt;disconnect_inputs(NULL, this);
3856         }
3857       }
3858     }
3859     break;
3860   }
3861   case Op_RangeCheck: {
3862     RangeCheckNode* rc = n-&gt;as_RangeCheck();
3863     Node* iff = new IfNode(rc-&gt;in(0), rc-&gt;in(1), rc-&gt;_prob, rc-&gt;_fcnt);
3864     n-&gt;subsume_by(iff, this);
3865     frc._tests.push(iff);
3866     break;
3867   }
3868   case Op_ConvI2L: {
3869     if (!Matcher::convi2l_type_required) {
3870       // Code generation on some platforms doesn&#39;t need accurate
3871       // ConvI2L types. Widening the type can help remove redundant
3872       // address computations.
3873       n-&gt;as_Type()-&gt;set_type(TypeLong::INT);
3874       ResourceMark rm;
3875       Unique_Node_List wq;
3876       wq.push(n);
3877       for (uint next = 0; next &lt; wq.size(); next++) {
3878         Node *m = wq.at(next);
3879 
3880         for(;;) {
3881           // Loop over all nodes with identical inputs edges as m
3882           Node* k = m-&gt;find_similar(m-&gt;Opcode());
3883           if (k == NULL) {
3884             break;
3885           }
3886           // Push their uses so we get a chance to remove node made
3887           // redundant
3888           for (DUIterator_Fast imax, i = k-&gt;fast_outs(imax); i &lt; imax; i++) {
3889             Node* u = k-&gt;fast_out(i);
3890             if (u-&gt;Opcode() == Op_LShiftL ||
3891                 u-&gt;Opcode() == Op_AddL ||
3892                 u-&gt;Opcode() == Op_SubL ||
3893                 u-&gt;Opcode() == Op_AddP) {
3894               wq.push(u);
3895             }
3896           }
3897           // Replace all nodes with identical edges as m with m
3898           k-&gt;subsume_by(m, this);
3899         }
3900       }
3901     }
3902     break;
3903   }
3904   case Op_CmpUL: {
3905     if (!Matcher::has_match_rule(Op_CmpUL)) {
3906       // No support for unsigned long comparisons
3907       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3908       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3909       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3910       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3911       Node* andl = new AndLNode(orl, remove_sign_mask);
3912       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3913       n-&gt;subsume_by(cmp, this);
3914     }
3915     break;
3916   }
<a name="45" id="anc45"></a><span class="line-added">3917 #ifdef ASSERT</span>
<span class="line-added">3918   case Op_InlineTypePtr:</span>
<span class="line-added">3919   case Op_InlineType: {</span>
<span class="line-added">3920     n-&gt;dump(-1);</span>
<span class="line-added">3921     assert(false, &quot;inline type node was not removed&quot;);</span>
<span class="line-added">3922     break;</span>
<span class="line-added">3923   }</span>
<span class="line-added">3924 #endif</span>
3925   default:
3926     assert(!n-&gt;is_Call(), &quot;&quot;);
3927     assert(!n-&gt;is_Mem(), &quot;&quot;);
3928     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3929     break;
3930   }
3931 }
3932 
3933 //------------------------------final_graph_reshaping_walk---------------------
3934 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3935 // requires that the walk visits a node&#39;s inputs before visiting the node.
3936 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3937   Unique_Node_List sfpt;
3938 
3939   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3940   uint cnt = root-&gt;req();
3941   Node *n = root;
3942   uint  i = 0;
3943   while (true) {
3944     if (i &lt; cnt) {
3945       // Place all non-visited non-null inputs onto stack
3946       Node* m = n-&gt;in(i);
3947       ++i;
3948       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3949         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3950           // compute worst case interpreter size in case of a deoptimization
3951           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3952 
3953           sfpt.push(m);
3954         }
3955         cnt = m-&gt;req();
3956         nstack.push(n, i); // put on stack parent and next input&#39;s index
3957         n = m;
3958         i = 0;
3959       }
3960     } else {
3961       // Now do post-visit work
3962       final_graph_reshaping_impl( n, frc );
3963       if (nstack.is_empty())
3964         break;             // finished
3965       n = nstack.node();   // Get node from stack
3966       cnt = n-&gt;req();
3967       i = nstack.index();
3968       nstack.pop();        // Shift to the next node on stack
3969     }
3970   }
3971 
3972   // Skip next transformation if compressed oops are not used.
3973   if ((UseCompressedOops &amp;&amp; !Matcher::gen_narrow_oop_implicit_null_checks()) ||
3974       (!UseCompressedOops &amp;&amp; !UseCompressedClassPointers))
3975     return;
3976 
3977   // Go over safepoints nodes to skip DecodeN/DecodeNKlass nodes for debug edges.
3978   // It could be done for an uncommon traps or any safepoints/calls
3979   // if the DecodeN/DecodeNKlass node is referenced only in a debug info.
3980   while (sfpt.size() &gt; 0) {
3981     n = sfpt.pop();
3982     JVMState *jvms = n-&gt;as_SafePoint()-&gt;jvms();
3983     assert(jvms != NULL, &quot;sanity&quot;);
3984     int start = jvms-&gt;debug_start();
3985     int end   = n-&gt;req();
3986     bool is_uncommon = (n-&gt;is_CallStaticJava() &amp;&amp;
3987                         n-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() != 0);
3988     for (int j = start; j &lt; end; j++) {
3989       Node* in = n-&gt;in(j);
3990       if (in-&gt;is_DecodeNarrowPtr()) {
3991         bool safe_to_skip = true;
3992         if (!is_uncommon ) {
3993           // Is it safe to skip?
3994           for (uint i = 0; i &lt; in-&gt;outcnt(); i++) {
3995             Node* u = in-&gt;raw_out(i);
3996             if (!u-&gt;is_SafePoint() ||
3997                 (u-&gt;is_Call() &amp;&amp; u-&gt;as_Call()-&gt;has_non_debug_use(n))) {
3998               safe_to_skip = false;
3999             }
4000           }
4001         }
4002         if (safe_to_skip) {
4003           n-&gt;set_req(j, in-&gt;in(1));
4004         }
4005         if (in-&gt;outcnt() == 0) {
4006           in-&gt;disconnect_inputs(NULL, this);
4007         }
4008       }
4009     }
4010   }
4011 }
4012 
4013 //------------------------------final_graph_reshaping--------------------------
4014 // Final Graph Reshaping.
4015 //
4016 // (1) Clone simple inputs to uncommon calls, so they can be scheduled late
4017 //     and not commoned up and forced early.  Must come after regular
4018 //     optimizations to avoid GVN undoing the cloning.  Clone constant
4019 //     inputs to Loop Phis; these will be split by the allocator anyways.
4020 //     Remove Opaque nodes.
4021 // (2) Move last-uses by commutative operations to the left input to encourage
4022 //     Intel update-in-place two-address operations and better register usage
4023 //     on RISCs.  Must come after regular optimizations to avoid GVN Ideal
4024 //     calls canonicalizing them back.
4025 // (3) Count the number of double-precision FP ops, single-precision FP ops
4026 //     and call sites.  On Intel, we can get correct rounding either by
4027 //     forcing singles to memory (requires extra stores and loads after each
4028 //     FP bytecode) or we can set a rounding mode bit (requires setting and
4029 //     clearing the mode bit around call sites).  The mode bit is only used
4030 //     if the relative frequency of single FP ops to calls is low enough.
4031 //     This is a key transform for SPEC mpeg_audio.
4032 // (4) Detect infinite loops; blobs of code reachable from above but not
4033 //     below.  Several of the Code_Gen algorithms fail on such code shapes,
4034 //     so we simply bail out.  Happens a lot in ZKM.jar, but also happens
4035 //     from time to time in other codes (such as -Xcomp finalizer loops, etc).
4036 //     Detection is by looking for IfNodes where only 1 projection is
4037 //     reachable from below or CatchNodes missing some targets.
4038 // (5) Assert for insane oop offsets in debug mode.
4039 
4040 bool Compile::final_graph_reshaping() {
4041   // an infinite loop may have been eliminated by the optimizer,
4042   // in which case the graph will be empty.
4043   if (root()-&gt;req() == 1) {
4044     record_method_not_compilable(&quot;trivial infinite loop&quot;);
4045     return true;
4046   }
4047 
4048   // Expensive nodes have their control input set to prevent the GVN
4049   // from freely commoning them. There&#39;s no GVN beyond this point so
4050   // no need to keep the control input. We want the expensive nodes to
4051   // be freely moved to the least frequent code path by gcm.
4052   assert(OptimizeExpensiveOps || expensive_count() == 0, &quot;optimization off but list non empty?&quot;);
4053   for (int i = 0; i &lt; expensive_count(); i++) {
4054     _expensive_nodes-&gt;at(i)-&gt;set_req(0, NULL);
4055   }
4056 
4057   Final_Reshape_Counts frc;
4058 
4059   // Visit everybody reachable!
4060   // Allocate stack of size C-&gt;live_nodes()/2 to avoid frequent realloc
4061   Node_Stack nstack(live_nodes() &gt;&gt; 1);
4062   final_graph_reshaping_walk(nstack, root(), frc);
4063 
4064   // Check for unreachable (from below) code (i.e., infinite loops).
4065   for( uint i = 0; i &lt; frc._tests.size(); i++ ) {
4066     MultiBranchNode *n = frc._tests[i]-&gt;as_MultiBranch();
4067     // Get number of CFG targets.
4068     // Note that PCTables include exception targets after calls.
4069     uint required_outcnt = n-&gt;required_outcnt();
4070     if (n-&gt;outcnt() != required_outcnt) {
4071       // Check for a few special cases.  Rethrow Nodes never take the
4072       // &#39;fall-thru&#39; path, so expected kids is 1 less.
4073       if (n-&gt;is_PCTable() &amp;&amp; n-&gt;in(0) &amp;&amp; n-&gt;in(0)-&gt;in(0)) {
4074         if (n-&gt;in(0)-&gt;in(0)-&gt;is_Call()) {
4075           CallNode *call = n-&gt;in(0)-&gt;in(0)-&gt;as_Call();
4076           if (call-&gt;entry_point() == OptoRuntime::rethrow_stub()) {
4077             required_outcnt--;      // Rethrow always has 1 less kid
4078           } else if (call-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
4079                      call-&gt;is_CallDynamicJava()) {
4080             // Check for null receiver. In such case, the optimizer has
4081             // detected that the virtual call will always result in a null
4082             // pointer exception. The fall-through projection of this CatchNode
4083             // will not be populated.
4084             Node *arg0 = call-&gt;in(TypeFunc::Parms);
4085             if (arg0-&gt;is_Type() &amp;&amp;
4086                 arg0-&gt;as_Type()-&gt;type()-&gt;higher_equal(TypePtr::NULL_PTR)) {
4087               required_outcnt--;
4088             }
4089           } else if (call-&gt;entry_point() == OptoRuntime::new_array_Java() &amp;&amp;
4090                      call-&gt;req() &gt; TypeFunc::Parms+1 &amp;&amp;
4091                      call-&gt;is_CallStaticJava()) {
4092             // Check for negative array length. In such case, the optimizer has
4093             // detected that the allocation attempt will always result in an
4094             // exception. There is no fall-through projection of this CatchNode .
4095             Node *arg1 = call-&gt;in(TypeFunc::Parms+1);
4096             if (arg1-&gt;is_Type() &amp;&amp;
4097                 arg1-&gt;as_Type()-&gt;type()-&gt;join(TypeInt::POS)-&gt;empty()) {
4098               required_outcnt--;
4099             }
4100           }
4101         }
4102       }
4103       // Recheck with a better notion of &#39;required_outcnt&#39;
4104       if (n-&gt;outcnt() != required_outcnt) {
4105         record_method_not_compilable(&quot;malformed control flow&quot;);
4106         return true;            // Not all targets reachable!
4107       }
4108     }
4109     // Check that I actually visited all kids.  Unreached kids
4110     // must be infinite loops.
4111     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++)
4112       if (!frc._visited.test(n-&gt;fast_out(j)-&gt;_idx)) {
4113         record_method_not_compilable(&quot;infinite loop&quot;);
4114         return true;            // Found unvisited kid; must be unreach
4115       }
4116 
4117     // Here so verification code in final_graph_reshaping_walk()
4118     // always see an OuterStripMinedLoopEnd
4119     if (n-&gt;is_OuterStripMinedLoopEnd()) {
4120       IfNode* init_iff = n-&gt;as_If();
4121       Node* iff = new IfNode(init_iff-&gt;in(0), init_iff-&gt;in(1), init_iff-&gt;_prob, init_iff-&gt;_fcnt);
4122       n-&gt;subsume_by(iff, this);
4123     }
4124   }
4125 
4126 #ifdef IA32
4127   // If original bytecodes contained a mixture of floats and doubles
4128   // check if the optimizer has made it homogenous, item (3).
4129   if (UseSSE == 0 &amp;&amp;
4130       frc.get_float_count() &gt; 32 &amp;&amp;
4131       frc.get_double_count() == 0 &amp;&amp;
4132       (10 * frc.get_call_count() &lt; frc.get_float_count()) ) {
4133     set_24_bit_selection_and_mode(false, true);
4134   }
4135 #endif // IA32
4136 
4137   set_java_calls(frc.get_java_call_count());
4138   set_inner_loops(frc.get_inner_loop_count());
4139 
4140   // No infinite loops, no reason to bail out.
4141   return false;
4142 }
4143 
4144 //-----------------------------too_many_traps----------------------------------
4145 // Report if there are too many traps at the current method and bci.
4146 // Return true if there was a trap, and/or PerMethodTrapLimit is exceeded.
4147 bool Compile::too_many_traps(ciMethod* method,
4148                              int bci,
4149                              Deoptimization::DeoptReason reason) {
4150   ciMethodData* md = method-&gt;method_data();
4151   if (md-&gt;is_empty()) {
4152     // Assume the trap has not occurred, or that it occurred only
4153     // because of a transient condition during start-up in the interpreter.
4154     return false;
4155   }
4156   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
4157   if (md-&gt;has_trap_at(bci, m, reason) != 0) {
4158     // Assume PerBytecodeTrapLimit==0, for a more conservative heuristic.
4159     // Also, if there are multiple reasons, or if there is no per-BCI record,
4160     // assume the worst.
4161     if (log())
4162       log()-&gt;elem(&quot;observe trap=&#39;%s&#39; count=&#39;%d&#39;&quot;,
4163                   Deoptimization::trap_reason_name(reason),
4164                   md-&gt;trap_count(reason));
4165     return true;
4166   } else {
4167     // Ignore method/bci and see if there have been too many globally.
4168     return too_many_traps(reason, md);
4169   }
4170 }
4171 
4172 // Less-accurate variant which does not require a method and bci.
4173 bool Compile::too_many_traps(Deoptimization::DeoptReason reason,
4174                              ciMethodData* logmd) {
4175   if (trap_count(reason) &gt;= Deoptimization::per_method_trap_limit(reason)) {
4176     // Too many traps globally.
4177     // Note that we use cumulative trap_count, not just md-&gt;trap_count.
4178     if (log()) {
4179       int mcount = (logmd == NULL)? -1: (int)logmd-&gt;trap_count(reason);
4180       log()-&gt;elem(&quot;observe trap=&#39;%s&#39; count=&#39;0&#39; mcount=&#39;%d&#39; ccount=&#39;%d&#39;&quot;,
4181                   Deoptimization::trap_reason_name(reason),
4182                   mcount, trap_count(reason));
4183     }
4184     return true;
4185   } else {
4186     // The coast is clear.
4187     return false;
4188   }
4189 }
4190 
4191 //--------------------------too_many_recompiles--------------------------------
4192 // Report if there are too many recompiles at the current method and bci.
4193 // Consults PerBytecodeRecompilationCutoff and PerMethodRecompilationCutoff.
4194 // Is not eager to return true, since this will cause the compiler to use
4195 // Action_none for a trap point, to avoid too many recompilations.
4196 bool Compile::too_many_recompiles(ciMethod* method,
4197                                   int bci,
4198                                   Deoptimization::DeoptReason reason) {
4199   ciMethodData* md = method-&gt;method_data();
4200   if (md-&gt;is_empty()) {
4201     // Assume the trap has not occurred, or that it occurred only
4202     // because of a transient condition during start-up in the interpreter.
4203     return false;
4204   }
4205   // Pick a cutoff point well within PerBytecodeRecompilationCutoff.
4206   uint bc_cutoff = (uint) PerBytecodeRecompilationCutoff / 8;
4207   uint m_cutoff  = (uint) PerMethodRecompilationCutoff / 2 + 1;  // not zero
4208   Deoptimization::DeoptReason per_bc_reason
4209     = Deoptimization::reason_recorded_per_bytecode_if_any(reason);
4210   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
4211   if ((per_bc_reason == Deoptimization::Reason_none
4212        || md-&gt;has_trap_at(bci, m, reason) != 0)
4213       // The trap frequency measure we care about is the recompile count:
4214       &amp;&amp; md-&gt;trap_recompiled_at(bci, m)
4215       &amp;&amp; md-&gt;overflow_recompile_count() &gt;= bc_cutoff) {
4216     // Do not emit a trap here if it has already caused recompilations.
4217     // Also, if there are multiple reasons, or if there is no per-BCI record,
4218     // assume the worst.
4219     if (log())
4220       log()-&gt;elem(&quot;observe trap=&#39;%s recompiled&#39; count=&#39;%d&#39; recompiles2=&#39;%d&#39;&quot;,
4221                   Deoptimization::trap_reason_name(reason),
4222                   md-&gt;trap_count(reason),
4223                   md-&gt;overflow_recompile_count());
4224     return true;
4225   } else if (trap_count(reason) != 0
4226              &amp;&amp; decompile_count() &gt;= m_cutoff) {
4227     // Too many recompiles globally, and we have seen this sort of trap.
4228     // Use cumulative decompile_count, not just md-&gt;decompile_count.
4229     if (log())
4230       log()-&gt;elem(&quot;observe trap=&#39;%s&#39; count=&#39;%d&#39; mcount=&#39;%d&#39; decompiles=&#39;%d&#39; mdecompiles=&#39;%d&#39;&quot;,
4231                   Deoptimization::trap_reason_name(reason),
4232                   md-&gt;trap_count(reason), trap_count(reason),
4233                   md-&gt;decompile_count(), decompile_count());
4234     return true;
4235   } else {
4236     // The coast is clear.
4237     return false;
4238   }
4239 }
4240 
4241 // Compute when not to trap. Used by matching trap based nodes and
4242 // NullCheck optimization.
4243 void Compile::set_allowed_deopt_reasons() {
4244   _allowed_reasons = 0;
4245   if (is_method_compilation()) {
4246     for (int rs = (int)Deoptimization::Reason_none+1; rs &lt; Compile::trapHistLength; rs++) {
4247       assert(rs &lt; BitsPerInt, &quot;recode bit map&quot;);
4248       if (!too_many_traps((Deoptimization::DeoptReason) rs)) {
4249         _allowed_reasons |= nth_bit(rs);
4250       }
4251     }
4252   }
4253 }
4254 
4255 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
4256   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
4257 }
4258 
4259 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
4260   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
4261 }
4262 
4263 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
4264   if (holder-&gt;is_initialized()) {
4265     return false;
4266   }
4267   if (holder-&gt;is_being_initialized()) {
4268     if (accessing_method-&gt;holder() == holder) {
4269       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
4270       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
4271       // barrier on the holder klass passed.
<a name="46" id="anc46"></a><span class="line-modified">4272       if (accessing_method-&gt;is_class_initializer() ||</span>
<span class="line-modified">4273           accessing_method-&gt;is_object_constructor() ||</span>
4274           accessing_method-&gt;is_static()) {
4275         return false;
4276       }
4277     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
4278       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
4279       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
4280       // child class can become fully initialized while its parent class is still being initialized.
<a name="47" id="anc47"></a><span class="line-modified">4281       if (accessing_method-&gt;is_class_initializer()) {</span>
4282         return false;
4283       }
4284     }
4285     ciMethod* root = method(); // the root method of compilation
4286     if (root != accessing_method) {
4287       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
4288     }
4289   }
4290   return true;
4291 }
4292 
4293 #ifndef PRODUCT
4294 //------------------------------verify_graph_edges---------------------------
4295 // Walk the Graph and verify that there is a one-to-one correspondence
4296 // between Use-Def edges and Def-Use edges in the graph.
4297 void Compile::verify_graph_edges(bool no_dead_code) {
4298   if (VerifyGraphEdges) {
4299     Unique_Node_List visited;
4300     // Call recursive graph walk to check edges
4301     _root-&gt;verify_edges(visited);
4302     if (no_dead_code) {
4303       // Now make sure that no visited node is used by an unvisited node.
4304       bool dead_nodes = false;
4305       Unique_Node_List checked;
4306       while (visited.size() &gt; 0) {
4307         Node* n = visited.pop();
4308         checked.push(n);
4309         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
4310           Node* use = n-&gt;raw_out(i);
4311           if (checked.member(use))  continue;  // already checked
4312           if (visited.member(use))  continue;  // already in the graph
4313           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
4314           // At this point, we have found a dead node which is DU-reachable.
4315           if (!dead_nodes) {
4316             tty-&gt;print_cr(&quot;*** Dead nodes reachable via DU edges:&quot;);
4317             dead_nodes = true;
4318           }
4319           use-&gt;dump(2);
4320           tty-&gt;print_cr(&quot;---&quot;);
4321           checked.push(use);  // No repeats; pretend it is now checked.
4322         }
4323       }
4324       assert(!dead_nodes, &quot;using nodes must be reachable from root&quot;);
4325     }
4326   }
4327 }
4328 #endif
4329 
4330 // The Compile object keeps track of failure reasons separately from the ciEnv.
4331 // This is required because there is not quite a 1-1 relation between the
4332 // ciEnv and its compilation task and the Compile object.  Note that one
4333 // ciEnv might use two Compile objects, if C2Compiler::compile_method decides
4334 // to backtrack and retry without subsuming loads.  Other than this backtracking
4335 // behavior, the Compile&#39;s failure reason is quietly copied up to the ciEnv
4336 // by the logic in C2Compiler.
4337 void Compile::record_failure(const char* reason) {
4338   if (log() != NULL) {
4339     log()-&gt;elem(&quot;failure reason=&#39;%s&#39; phase=&#39;compile&#39;&quot;, reason);
4340   }
4341   if (_failure_reason == NULL) {
4342     // Record the first failure reason.
4343     _failure_reason = reason;
4344   }
4345 
4346   if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
4347     C-&gt;print_method(PHASE_FAILURE);
4348   }
4349   _root = NULL;  // flush the graph, too
4350 }
4351 
4352 Compile::TracePhase::TracePhase(const char* name, elapsedTimer* accumulator)
4353   : TraceTime(name, accumulator, CITime, CITimeVerbose),
4354     _phase_name(name), _dolog(CITimeVerbose)
4355 {
4356   if (_dolog) {
4357     C = Compile::current();
4358     _log = C-&gt;log();
4359   } else {
4360     C = NULL;
4361     _log = NULL;
4362   }
4363   if (_log != NULL) {
4364     _log-&gt;begin_head(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
4365     _log-&gt;stamp();
4366     _log-&gt;end_head();
4367   }
4368 }
4369 
4370 Compile::TracePhase::~TracePhase() {
4371 
4372   C = Compile::current();
4373   if (_dolog) {
4374     _log = C-&gt;log();
4375   } else {
4376     _log = NULL;
4377   }
4378 
4379 #ifdef ASSERT
4380   if (PrintIdealNodeCount) {
4381     tty-&gt;print_cr(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39; live_graph_walk=&#39;%d&#39;&quot;,
4382                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
4383   }
4384 
4385   if (VerifyIdealNodeCount) {
4386     Compile::current()-&gt;print_missing_nodes();
4387   }
4388 #endif
4389 
4390   if (_log != NULL) {
4391     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
4392   }
4393 }
4394 
4395 //----------------------------static_subtype_check-----------------------------
4396 // Shortcut important common cases when superklass is exact:
4397 // (0) superklass is java.lang.Object (can occur in reflective code)
4398 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
4399 // (2) subklass does not overlap with superklass =&gt; always fail
4400 // (3) superklass has NO subtypes and we can check with a simple compare.
4401 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<a name="48" id="anc48"></a><span class="line-modified">4402   if (StressReflectiveCode || superk == NULL || subk == NULL) {</span>
4403     return SSC_full_test;       // Let caller generate the general case.
4404   }
4405 
4406   if (superk == env()-&gt;Object_klass()) {
4407     return SSC_always_true;     // (0) this test cannot fail
4408   }
4409 
4410   ciType* superelem = superk;
<a name="49" id="anc49"></a><span class="line-modified">4411   if (superelem-&gt;is_array_klass()) {</span>
<span class="line-added">4412     ciArrayKlass* ak = superelem-&gt;as_array_klass();</span>
4413     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
<a name="50" id="anc50"></a><span class="line-added">4414   }</span>
4415 
4416   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
4417     if (subk-&gt;is_subtype_of(superk)) {
4418       return SSC_always_true;   // (1) false path dead; no dynamic test needed
4419     }
4420     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
4421         !superk-&gt;is_subtype_of(subk)) {
4422       return SSC_always_false;
4423     }
4424   }
4425 
4426   // If casting to an instance klass, it must have no subtypes
4427   if (superk-&gt;is_interface()) {
4428     // Cannot trust interfaces yet.
4429     // %%% S.B. superk-&gt;nof_implementors() == 1
4430   } else if (superelem-&gt;is_instance_klass()) {
4431     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4432     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4433       if (!ik-&gt;is_final()) {
4434         // Add a dependency if there is a chance of a later subclass.
4435         dependencies()-&gt;assert_leaf_type(ik);
4436       }
4437       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
4438     }
4439   } else {
4440     // A primitive array type has no subtypes.
4441     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
4442   }
4443 
4444   return SSC_full_test;
4445 }
4446 
4447 Node* Compile::conv_I2X_index(PhaseGVN* phase, Node* idx, const TypeInt* sizetype, Node* ctrl) {
4448 #ifdef _LP64
4449   // The scaled index operand to AddP must be a clean 64-bit value.
4450   // Java allows a 32-bit int to be incremented to a negative
4451   // value, which appears in a 64-bit register as a large
4452   // positive number.  Using that large positive number as an
4453   // operand in pointer arithmetic has bad consequences.
4454   // On the other hand, 32-bit overflow is rare, and the possibility
4455   // can often be excluded, if we annotate the ConvI2L node with
4456   // a type assertion that its value is known to be a small positive
4457   // number.  (The prior range check has ensured this.)
4458   // This assertion is used by ConvI2LNode::Ideal.
4459   int index_max = max_jint - 1;  // array size is max_jint, index is one less
4460   if (sizetype != NULL) index_max = sizetype-&gt;_hi - 1;
4461   const TypeInt* iidxtype = TypeInt::make(0, index_max, Type::WidenMax);
4462   idx = constrained_convI2L(phase, idx, iidxtype, ctrl);
4463 #endif
4464   return idx;
4465 }
4466 
4467 // Convert integer value to a narrowed long type dependent on ctrl (for example, a range check)
4468 Node* Compile::constrained_convI2L(PhaseGVN* phase, Node* value, const TypeInt* itype, Node* ctrl) {
4469   if (ctrl != NULL) {
4470     // Express control dependency by a CastII node with a narrow type.
4471     value = new CastIINode(value, itype, false, true /* range check dependency */);
4472     // Make the CastII node dependent on the control input to prevent the narrowed ConvI2L
4473     // node from floating above the range check during loop optimizations. Otherwise, the
4474     // ConvI2L node may be eliminated independently of the range check, causing the data path
4475     // to become TOP while the control path is still there (although it&#39;s unreachable).
4476     value-&gt;set_req(0, ctrl);
4477     // Save CastII node to remove it after loop optimizations.
4478     phase-&gt;C-&gt;add_range_check_cast(value);
4479     value = phase-&gt;transform(value);
4480   }
4481   const TypeLong* ltype = TypeLong::make(itype-&gt;_lo, itype-&gt;_hi, itype-&gt;_widen);
4482   return phase-&gt;transform(new ConvI2LNode(value, ltype));
4483 }
4484 
4485 void Compile::print_inlining_stream_free() {
4486   if (_print_inlining_stream != NULL) {
4487     _print_inlining_stream-&gt;~stringStream();
4488     _print_inlining_stream = NULL;
4489   }
4490 }
4491 
4492 // The message about the current inlining is accumulated in
4493 // _print_inlining_stream and transfered into the _print_inlining_list
4494 // once we know whether inlining succeeds or not. For regular
4495 // inlining, messages are appended to the buffer pointed by
4496 // _print_inlining_idx in the _print_inlining_list. For late inlining,
4497 // a new buffer is added after _print_inlining_idx in the list. This
4498 // way we can update the inlining message for late inlining call site
4499 // when the inlining is attempted again.
4500 void Compile::print_inlining_init() {
4501   if (print_inlining() || print_intrinsics()) {
4502     // print_inlining_init is actually called several times.
4503     print_inlining_stream_free();
4504     _print_inlining_stream = new stringStream();
4505     // Watch out: The memory initialized by the constructor call PrintInliningBuffer()
4506     // will be copied into the only initial element. The default destructor of
4507     // PrintInliningBuffer will be called when leaving the scope here. If it
4508     // would destuct the  enclosed stringStream _print_inlining_list[0]-&gt;_ss
4509     // would be destructed, too!
4510     _print_inlining_list = new (comp_arena())GrowableArray&lt;PrintInliningBuffer&gt;(comp_arena(), 1, 1, PrintInliningBuffer());
4511   }
4512 }
4513 
4514 void Compile::print_inlining_reinit() {
4515   if (print_inlining() || print_intrinsics()) {
4516     print_inlining_stream_free();
4517     // Re allocate buffer when we change ResourceMark
4518     _print_inlining_stream = new stringStream();
4519   }
4520 }
4521 
4522 void Compile::print_inlining_reset() {
4523   _print_inlining_stream-&gt;reset();
4524 }
4525 
4526 void Compile::print_inlining_commit() {
4527   assert(print_inlining() || print_intrinsics(), &quot;PrintInlining off?&quot;);
4528   // Transfer the message from _print_inlining_stream to the current
4529   // _print_inlining_list buffer and clear _print_inlining_stream.
4530   _print_inlining_list-&gt;at(_print_inlining_idx).ss()-&gt;write(_print_inlining_stream-&gt;base(), _print_inlining_stream-&gt;size());
4531   print_inlining_reset();
4532 }
4533 
4534 void Compile::print_inlining_push() {
4535   // Add new buffer to the _print_inlining_list at current position
4536   _print_inlining_idx++;
4537   _print_inlining_list-&gt;insert_before(_print_inlining_idx, PrintInliningBuffer());
4538 }
4539 
4540 Compile::PrintInliningBuffer&amp; Compile::print_inlining_current() {
4541   return _print_inlining_list-&gt;at(_print_inlining_idx);
4542 }
4543 
4544 void Compile::print_inlining_update(CallGenerator* cg) {
4545   if (print_inlining() || print_intrinsics()) {
4546     if (!cg-&gt;is_late_inline()) {
4547       if (print_inlining_current().cg() != NULL) {
4548         print_inlining_push();
4549       }
4550       print_inlining_commit();
4551     } else {
4552       if (print_inlining_current().cg() != cg &amp;&amp;
4553           (print_inlining_current().cg() != NULL ||
4554            print_inlining_current().ss()-&gt;size() != 0)) {
4555         print_inlining_push();
4556       }
4557       print_inlining_commit();
4558       print_inlining_current().set_cg(cg);
4559     }
4560   }
4561 }
4562 
4563 void Compile::print_inlining_move_to(CallGenerator* cg) {
4564   // We resume inlining at a late inlining call site. Locate the
4565   // corresponding inlining buffer so that we can update it.
4566   if (print_inlining()) {
4567     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4568       if (_print_inlining_list-&gt;adr_at(i)-&gt;cg() == cg) {
4569         _print_inlining_idx = i;
4570         return;
4571       }
4572     }
4573     ShouldNotReachHere();
4574   }
4575 }
4576 
4577 void Compile::print_inlining_update_delayed(CallGenerator* cg) {
4578   if (print_inlining()) {
4579     assert(_print_inlining_stream-&gt;size() &gt; 0, &quot;missing inlining msg&quot;);
4580     assert(print_inlining_current().cg() == cg, &quot;wrong entry&quot;);
4581     // replace message with new message
4582     _print_inlining_list-&gt;at_put(_print_inlining_idx, PrintInliningBuffer());
4583     print_inlining_commit();
4584     print_inlining_current().set_cg(cg);
4585   }
4586 }
4587 
4588 void Compile::print_inlining_assert_ready() {
4589   assert(!_print_inlining || _print_inlining_stream-&gt;size() == 0, &quot;loosing data&quot;);
4590 }
4591 
4592 void Compile::process_print_inlining() {
4593   bool do_print_inlining = print_inlining() || print_intrinsics();
4594   if (do_print_inlining || log() != NULL) {
4595     // Print inlining message for candidates that we couldn&#39;t inline
4596     // for lack of space
4597     for (int i = 0; i &lt; _late_inlines.length(); i++) {
4598       CallGenerator* cg = _late_inlines.at(i);
4599       if (!cg-&gt;is_mh_late_inline()) {
4600         const char* msg = &quot;live nodes &gt; LiveNodeCountInliningCutoff&quot;;
4601         if (do_print_inlining) {
4602           cg-&gt;print_inlining_late(msg);
4603         }
4604         log_late_inline_failure(cg, msg);
4605       }
4606     }
4607   }
4608   if (do_print_inlining) {
4609     ResourceMark rm;
4610     stringStream ss;
4611     assert(_print_inlining_list != NULL, &quot;process_print_inlining should be called only once.&quot;);
4612     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4613       ss.print(&quot;%s&quot;, _print_inlining_list-&gt;adr_at(i)-&gt;ss()-&gt;as_string());
4614       _print_inlining_list-&gt;at(i).freeStream();
4615     }
4616     // Reset _print_inlining_list, it only contains destructed objects.
4617     // It is on the arena, so it will be freed when the arena is reset.
4618     _print_inlining_list = NULL;
4619     // _print_inlining_stream won&#39;t be used anymore, either.
4620     print_inlining_stream_free();
4621     size_t end = ss.size();
4622     _print_inlining_output = NEW_ARENA_ARRAY(comp_arena(), char, end+1);
4623     strncpy(_print_inlining_output, ss.base(), end+1);
4624     _print_inlining_output[end] = 0;
4625   }
4626 }
4627 
4628 void Compile::dump_print_inlining() {
4629   if (_print_inlining_output != NULL) {
4630     tty-&gt;print_raw(_print_inlining_output);
4631   }
4632 }
4633 
4634 void Compile::log_late_inline(CallGenerator* cg) {
4635   if (log() != NULL) {
4636     log()-&gt;head(&quot;late_inline method=&#39;%d&#39;  inline_id=&#39;&quot; JLONG_FORMAT &quot;&#39;&quot;, log()-&gt;identify(cg-&gt;method()),
4637                 cg-&gt;unique_id());
4638     JVMState* p = cg-&gt;call_node()-&gt;jvms();
4639     while (p != NULL) {
4640       log()-&gt;elem(&quot;jvms bci=&#39;%d&#39; method=&#39;%d&#39;&quot;, p-&gt;bci(), log()-&gt;identify(p-&gt;method()));
4641       p = p-&gt;caller();
4642     }
4643     log()-&gt;tail(&quot;late_inline&quot;);
4644   }
4645 }
4646 
4647 void Compile::log_late_inline_failure(CallGenerator* cg, const char* msg) {
4648   log_late_inline(cg);
4649   if (log() != NULL) {
4650     log()-&gt;inline_fail(msg);
4651   }
4652 }
4653 
4654 void Compile::log_inline_id(CallGenerator* cg) {
4655   if (log() != NULL) {
4656     // The LogCompilation tool needs a unique way to identify late
4657     // inline call sites. This id must be unique for this call site in
4658     // this compilation. Try to have it unique across compilations as
4659     // well because it can be convenient when grepping through the log
4660     // file.
4661     // Distinguish OSR compilations from others in case CICountOSR is
4662     // on.
4663     jlong id = ((jlong)unique()) + (((jlong)compile_id()) &lt;&lt; 33) + (CICountOSR &amp;&amp; is_osr_compilation() ? ((jlong)1) &lt;&lt; 32 : 0);
4664     cg-&gt;set_unique_id(id);
4665     log()-&gt;elem(&quot;inline_id id=&#39;&quot; JLONG_FORMAT &quot;&#39;&quot;, id);
4666   }
4667 }
4668 
4669 void Compile::log_inline_failure(const char* msg) {
4670   if (C-&gt;log() != NULL) {
4671     C-&gt;log()-&gt;inline_fail(msg);
4672   }
4673 }
4674 
4675 
4676 // Dump inlining replay data to the stream.
4677 // Don&#39;t change thread state and acquire any locks.
4678 void Compile::dump_inline_data(outputStream* out) {
4679   InlineTree* inl_tree = ilt();
4680   if (inl_tree != NULL) {
4681     out-&gt;print(&quot; inline %d&quot;, inl_tree-&gt;count());
4682     inl_tree-&gt;dump_replay_data(out);
4683   }
4684 }
4685 
4686 int Compile::cmp_expensive_nodes(Node* n1, Node* n2) {
4687   if (n1-&gt;Opcode() &lt; n2-&gt;Opcode())      return -1;
4688   else if (n1-&gt;Opcode() &gt; n2-&gt;Opcode()) return 1;
4689 
4690   assert(n1-&gt;req() == n2-&gt;req(), &quot;can&#39;t compare %s nodes: n1-&gt;req() = %d, n2-&gt;req() = %d&quot;, NodeClassNames[n1-&gt;Opcode()], n1-&gt;req(), n2-&gt;req());
4691   for (uint i = 1; i &lt; n1-&gt;req(); i++) {
4692     if (n1-&gt;in(i) &lt; n2-&gt;in(i))      return -1;
4693     else if (n1-&gt;in(i) &gt; n2-&gt;in(i)) return 1;
4694   }
4695 
4696   return 0;
4697 }
4698 
4699 int Compile::cmp_expensive_nodes(Node** n1p, Node** n2p) {
4700   Node* n1 = *n1p;
4701   Node* n2 = *n2p;
4702 
4703   return cmp_expensive_nodes(n1, n2);
4704 }
4705 
4706 void Compile::sort_expensive_nodes() {
4707   if (!expensive_nodes_sorted()) {
4708     _expensive_nodes-&gt;sort(cmp_expensive_nodes);
4709   }
4710 }
4711 
4712 bool Compile::expensive_nodes_sorted() const {
4713   for (int i = 1; i &lt; _expensive_nodes-&gt;length(); i++) {
4714     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i-1)) &lt; 0) {
4715       return false;
4716     }
4717   }
4718   return true;
4719 }
4720 
4721 bool Compile::should_optimize_expensive_nodes(PhaseIterGVN &amp;igvn) {
4722   if (_expensive_nodes-&gt;length() == 0) {
4723     return false;
4724   }
4725 
4726   assert(OptimizeExpensiveOps, &quot;optimization off?&quot;);
4727 
4728   // Take this opportunity to remove dead nodes from the list
4729   int j = 0;
4730   for (int i = 0; i &lt; _expensive_nodes-&gt;length(); i++) {
4731     Node* n = _expensive_nodes-&gt;at(i);
4732     if (!n-&gt;is_unreachable(igvn)) {
4733       assert(n-&gt;is_expensive(), &quot;should be expensive&quot;);
4734       _expensive_nodes-&gt;at_put(j, n);
4735       j++;
4736     }
4737   }
4738   _expensive_nodes-&gt;trunc_to(j);
4739 
4740   // Then sort the list so that similar nodes are next to each other
4741   // and check for at least two nodes of identical kind with same data
4742   // inputs.
4743   sort_expensive_nodes();
4744 
4745   for (int i = 0; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4746     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i+1)) == 0) {
4747       return true;
4748     }
4749   }
4750 
4751   return false;
4752 }
4753 
4754 void Compile::cleanup_expensive_nodes(PhaseIterGVN &amp;igvn) {
4755   if (_expensive_nodes-&gt;length() == 0) {
4756     return;
4757   }
4758 
4759   assert(OptimizeExpensiveOps, &quot;optimization off?&quot;);
4760 
4761   // Sort to bring similar nodes next to each other and clear the
4762   // control input of nodes for which there&#39;s only a single copy.
4763   sort_expensive_nodes();
4764 
4765   int j = 0;
4766   int identical = 0;
4767   int i = 0;
4768   bool modified = false;
4769   for (; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4770     assert(j &lt;= i, &quot;can&#39;t write beyond current index&quot;);
4771     if (_expensive_nodes-&gt;at(i)-&gt;Opcode() == _expensive_nodes-&gt;at(i+1)-&gt;Opcode()) {
4772       identical++;
4773       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4774       continue;
4775     }
4776     if (identical &gt; 0) {
4777       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4778       identical = 0;
4779     } else {
4780       Node* n = _expensive_nodes-&gt;at(i);
4781       igvn.replace_input_of(n, 0, NULL);
4782       igvn.hash_insert(n);
4783       modified = true;
4784     }
4785   }
4786   if (identical &gt; 0) {
4787     _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4788   } else if (_expensive_nodes-&gt;length() &gt;= 1) {
4789     Node* n = _expensive_nodes-&gt;at(i);
4790     igvn.replace_input_of(n, 0, NULL);
4791     igvn.hash_insert(n);
4792     modified = true;
4793   }
4794   _expensive_nodes-&gt;trunc_to(j);
4795   if (modified) {
4796     igvn.optimize();
4797   }
4798 }
4799 
4800 void Compile::add_expensive_node(Node * n) {
4801   assert(!_expensive_nodes-&gt;contains(n), &quot;duplicate entry in expensive list&quot;);
4802   assert(n-&gt;is_expensive(), &quot;expensive nodes with non-null control here only&quot;);
4803   assert(!n-&gt;is_CFG() &amp;&amp; !n-&gt;is_Mem(), &quot;no cfg or memory nodes here&quot;);
4804   if (OptimizeExpensiveOps) {
4805     _expensive_nodes-&gt;append(n);
4806   } else {
4807     // Clear control input and let IGVN optimize expensive nodes if
4808     // OptimizeExpensiveOps is off.
4809     n-&gt;set_req(0, NULL);
4810   }
4811 }
4812 
4813 /**
4814  * Remove the speculative part of types and clean up the graph
4815  */
4816 void Compile::remove_speculative_types(PhaseIterGVN &amp;igvn) {
4817   if (UseTypeSpeculation) {
4818     Unique_Node_List worklist;
4819     worklist.push(root());
4820     int modified = 0;
4821     // Go over all type nodes that carry a speculative type, drop the
4822     // speculative part of the type and enqueue the node for an igvn
4823     // which may optimize it out.
4824     for (uint next = 0; next &lt; worklist.size(); ++next) {
4825       Node *n  = worklist.at(next);
4826       if (n-&gt;is_Type()) {
4827         TypeNode* tn = n-&gt;as_Type();
4828         const Type* t = tn-&gt;type();
4829         const Type* t_no_spec = t-&gt;remove_speculative();
4830         if (t_no_spec != t) {
4831           bool in_hash = igvn.hash_delete(n);
4832           assert(in_hash, &quot;node should be in igvn hash table&quot;);
4833           tn-&gt;set_type(t_no_spec);
4834           igvn.hash_insert(n);
4835           igvn._worklist.push(n); // give it a chance to go away
4836           modified++;
4837         }
4838       }
4839       uint max = n-&gt;len();
4840       for( uint i = 0; i &lt; max; ++i ) {
4841         Node *m = n-&gt;in(i);
4842         if (not_a_node(m))  continue;
4843         worklist.push(m);
4844       }
4845     }
4846     // Drop the speculative part of all types in the igvn&#39;s type table
4847     igvn.remove_speculative_types();
4848     if (modified &gt; 0) {
4849       igvn.optimize();
4850     }
4851 #ifdef ASSERT
4852     // Verify that after the IGVN is over no speculative type has resurfaced
4853     worklist.clear();
4854     worklist.push(root());
4855     for (uint next = 0; next &lt; worklist.size(); ++next) {
4856       Node *n  = worklist.at(next);
4857       const Type* t = igvn.type_or_null(n);
4858       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4859       if (n-&gt;is_Type()) {
4860         t = n-&gt;as_Type()-&gt;type();
4861         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4862       }
4863       uint max = n-&gt;len();
4864       for( uint i = 0; i &lt; max; ++i ) {
4865         Node *m = n-&gt;in(i);
4866         if (not_a_node(m))  continue;
4867         worklist.push(m);
4868       }
4869     }
4870     igvn.check_no_speculative_types();
4871 #endif
4872   }
4873 }
4874 
<a name="51" id="anc51"></a><span class="line-added">4875 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {</span>
<span class="line-added">4876   const TypeInstPtr* ta = phase-&gt;type(a)-&gt;isa_instptr();</span>
<span class="line-added">4877   const TypeInstPtr* tb = phase-&gt;type(b)-&gt;isa_instptr();</span>
<span class="line-added">4878   if (!EnableValhalla || ta == NULL || tb == NULL ||</span>
<span class="line-added">4879       ta-&gt;is_zero_type() || tb-&gt;is_zero_type() ||</span>
<span class="line-added">4880       !ta-&gt;can_be_inline_type() || !tb-&gt;can_be_inline_type()) {</span>
<span class="line-added">4881     // Use old acmp if one operand is null or not an inline type</span>
<span class="line-added">4882     return new CmpPNode(a, b);</span>
<span class="line-added">4883   } else if (ta-&gt;is_inlinetypeptr() || tb-&gt;is_inlinetypeptr()) {</span>
<span class="line-added">4884     // We know that one operand is an inline type. Therefore,</span>
<span class="line-added">4885     // new acmp will only return true if both operands are NULL.</span>
<span class="line-added">4886     // Check if both operands are null by or&#39;ing the oops.</span>
<span class="line-added">4887     a = phase-&gt;transform(new CastP2XNode(NULL, a));</span>
<span class="line-added">4888     b = phase-&gt;transform(new CastP2XNode(NULL, b));</span>
<span class="line-added">4889     a = phase-&gt;transform(new OrXNode(a, b));</span>
<span class="line-added">4890     return new CmpXNode(a, phase-&gt;MakeConX(0));</span>
<span class="line-added">4891   }</span>
<span class="line-added">4892   // Use new acmp</span>
<span class="line-added">4893   return NULL;</span>
<span class="line-added">4894 }</span>
<span class="line-added">4895 </span>
4896 // Auxiliary method to support randomized stressing/fuzzing.
4897 //
4898 // This method can be called the arbitrary number of times, with current count
4899 // as the argument. The logic allows selecting a single candidate from the
4900 // running list of candidates as follows:
4901 //    int count = 0;
4902 //    Cand* selected = null;
4903 //    while(cand = cand-&gt;next()) {
4904 //      if (randomized_select(++count)) {
4905 //        selected = cand;
4906 //      }
4907 //    }
4908 //
4909 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4910 // This is useful when we don&#39;t have the complete list of candidates to choose
4911 // from uniformly. In this case, we need to adjust the randomicity of the
4912 // selection, or else we will end up biasing the selection towards the latter
4913 // candidates.
4914 //
4915 // Quick back-envelope calculation shows that for the list of n candidates
4916 // the equal probability for the candidate to persist as &quot;best&quot; can be
4917 // achieved by replacing it with &quot;next&quot; k-th candidate with the probability
4918 // of 1/k. It can be easily shown that by the end of the run, the
4919 // probability for any candidate is converged to 1/n, thus giving the
4920 // uniform distribution among all the candidates.
4921 //
4922 // We don&#39;t care about the domain size as long as (RANDOMIZED_DOMAIN / count) is large.
4923 #define RANDOMIZED_DOMAIN_POW 29
4924 #define RANDOMIZED_DOMAIN (1 &lt;&lt; RANDOMIZED_DOMAIN_POW)
4925 #define RANDOMIZED_DOMAIN_MASK ((1 &lt;&lt; (RANDOMIZED_DOMAIN_POW + 1)) - 1)
4926 bool Compile::randomized_select(int count) {
4927   assert(count &gt; 0, &quot;only positive&quot;);
4928   return (os::random() &amp; RANDOMIZED_DOMAIN_MASK) &lt; (RANDOMIZED_DOMAIN / count);
4929 }
4930 
4931 CloneMap&amp;     Compile::clone_map()                 { return _clone_map; }
4932 void          Compile::set_clone_map(Dict* d)      { _clone_map._dict = d; }
4933 
4934 void NodeCloneInfo::dump() const {
4935   tty-&gt;print(&quot; {%d:%d} &quot;, idx(), gen());
4936 }
4937 
4938 void CloneMap::clone(Node* old, Node* nnn, int gen) {
4939   uint64_t val = value(old-&gt;_idx);
4940   NodeCloneInfo cio(val);
4941   assert(val != 0, &quot;old node should be in the map&quot;);
4942   NodeCloneInfo cin(cio.idx(), gen + cio.gen());
4943   insert(nnn-&gt;_idx, cin.get());
4944 #ifndef PRODUCT
4945   if (is_debug()) {
4946     tty-&gt;print_cr(&quot;CloneMap::clone inserted node %d info {%d:%d} into CloneMap&quot;, nnn-&gt;_idx, cin.idx(), cin.gen());
4947   }
4948 #endif
4949 }
4950 
4951 void CloneMap::verify_insert_and_clone(Node* old, Node* nnn, int gen) {
4952   NodeCloneInfo cio(value(old-&gt;_idx));
4953   if (cio.get() == 0) {
4954     cio.set(old-&gt;_idx, 0);
4955     insert(old-&gt;_idx, cio.get());
4956 #ifndef PRODUCT
4957     if (is_debug()) {
4958       tty-&gt;print_cr(&quot;CloneMap::verify_insert_and_clone inserted node %d info {%d:%d} into CloneMap&quot;, old-&gt;_idx, cio.idx(), cio.gen());
4959     }
4960 #endif
4961   }
4962   clone(old, nnn, gen);
4963 }
4964 
4965 int CloneMap::max_gen() const {
4966   int g = 0;
4967   DictI di(_dict);
4968   for(; di.test(); ++di) {
4969     int t = gen(di._key);
4970     if (g &lt; t) {
4971       g = t;
4972 #ifndef PRODUCT
4973       if (is_debug()) {
4974         tty-&gt;print_cr(&quot;CloneMap::max_gen() update max=%d from %d&quot;, g, _2_node_idx_t(di._key));
4975       }
4976 #endif
4977     }
4978   }
4979   return g;
4980 }
4981 
4982 void CloneMap::dump(node_idx_t key) const {
4983   uint64_t val = value(key);
4984   if (val != 0) {
4985     NodeCloneInfo ni(val);
4986     ni.dump();
4987   }
4988 }
4989 
4990 // Move Allocate nodes to the start of the list
4991 void Compile::sort_macro_nodes() {
4992   int count = macro_count();
4993   int allocates = 0;
4994   for (int i = 0; i &lt; count; i++) {
4995     Node* n = macro_node(i);
4996     if (n-&gt;is_Allocate()) {
4997       if (i != allocates) {
4998         Node* tmp = macro_node(allocates);
4999         _macro_nodes-&gt;at_put(allocates, n);
5000         _macro_nodes-&gt;at_put(i, tmp);
5001       }
5002       allocates++;
5003     }
5004   }
5005 }
5006 
5007 void Compile::print_method(CompilerPhaseType cpt, int level, int idx) {
5008   EventCompilerPhase event;
5009   if (event.should_commit()) {
5010     CompilerEvent::PhaseEvent::post(event, C-&gt;_latest_stage_start_counter, cpt, C-&gt;_compile_id, level);
5011   }
5012 
5013 #ifndef PRODUCT
5014   if (should_print(level)) {
5015     char output[1024];
5016     if (idx != 0) {
5017       jio_snprintf(output, sizeof(output), &quot;%s:%d&quot;, CompilerPhaseTypeHelper::to_string(cpt), idx);
5018     } else {
5019       jio_snprintf(output, sizeof(output), &quot;%s&quot;, CompilerPhaseTypeHelper::to_string(cpt));
5020     }
5021     _printer-&gt;print_method(output, level);
5022   }
5023 #endif
5024   C-&gt;_latest_stage_start_counter.stamp();
5025 }
5026 
5027 void Compile::end_method(int level) {
5028   EventCompilerPhase event;
5029   if (event.should_commit()) {
5030     CompilerEvent::PhaseEvent::post(event, C-&gt;_latest_stage_start_counter, PHASE_END, C-&gt;_compile_id, level);
5031   }
5032 
5033 #ifndef PRODUCT
5034   if (_method != NULL &amp;&amp; should_print(level)) {
5035     _printer-&gt;end_method();
5036   }
5037 #endif
5038 }
5039 
5040 
5041 #ifndef PRODUCT
5042 IdealGraphPrinter* Compile::_debug_file_printer = NULL;
5043 IdealGraphPrinter* Compile::_debug_network_printer = NULL;
5044 
5045 // Called from debugger. Prints method to the default file with the default phase name.
5046 // This works regardless of any Ideal Graph Visualizer flags set or not.
5047 void igv_print() {
5048   Compile::current()-&gt;igv_print_method_to_file();
5049 }
5050 
5051 // Same as igv_print() above but with a specified phase name.
5052 void igv_print(const char* phase_name) {
5053   Compile::current()-&gt;igv_print_method_to_file(phase_name);
5054 }
5055 
5056 // Called from debugger. Prints method with the default phase name to the default network or the one specified with
5057 // the network flags for the Ideal Graph Visualizer, or to the default file depending on the &#39;network&#39; argument.
5058 // This works regardless of any Ideal Graph Visualizer flags set or not.
5059 void igv_print(bool network) {
5060   if (network) {
5061     Compile::current()-&gt;igv_print_method_to_network();
5062   } else {
5063     Compile::current()-&gt;igv_print_method_to_file();
5064   }
5065 }
5066 
5067 // Same as igv_print(bool network) above but with a specified phase name.
5068 void igv_print(bool network, const char* phase_name) {
5069   if (network) {
5070     Compile::current()-&gt;igv_print_method_to_network(phase_name);
5071   } else {
5072     Compile::current()-&gt;igv_print_method_to_file(phase_name);
5073   }
5074 }
5075 
5076 // Called from debugger. Normal write to the default _printer. Only works if Ideal Graph Visualizer printing flags are set.
5077 void igv_print_default() {
5078   Compile::current()-&gt;print_method(PHASE_DEBUG, 0, 0);
5079 }
5080 
5081 // Called from debugger, especially when replaying a trace in which the program state cannot be altered like with rr replay.
5082 // A method is appended to an existing default file with the default phase name. This means that igv_append() must follow
5083 // an earlier igv_print(*) call which sets up the file. This works regardless of any Ideal Graph Visualizer flags set or not.
5084 void igv_append() {
5085   Compile::current()-&gt;igv_print_method_to_file(&quot;Debug&quot;, true);
5086 }
5087 
5088 // Same as igv_append() above but with a specified phase name.
5089 void igv_append(const char* phase_name) {
5090   Compile::current()-&gt;igv_print_method_to_file(phase_name, true);
5091 }
5092 
5093 void Compile::igv_print_method_to_file(const char* phase_name, bool append) {
5094   const char* file_name = &quot;custom_debug.xml&quot;;
5095   if (_debug_file_printer == NULL) {
5096     _debug_file_printer = new IdealGraphPrinter(C, file_name, append);
5097   } else {
5098     _debug_file_printer-&gt;update_compiled_method(C-&gt;method());
5099   }
5100   tty-&gt;print_cr(&quot;Method %s to %s&quot;, append ? &quot;appended&quot; : &quot;printed&quot;, file_name);
5101   _debug_file_printer-&gt;print_method(phase_name, 0);
5102 }
5103 
5104 void Compile::igv_print_method_to_network(const char* phase_name) {
5105   if (_debug_network_printer == NULL) {
5106     _debug_network_printer = new IdealGraphPrinter(C);
5107   } else {
5108     _debug_network_printer-&gt;update_compiled_method(C-&gt;method());
5109   }
5110   tty-&gt;print_cr(&quot;Method printed over network stream to IGV&quot;);
5111   _debug_network_printer-&gt;print_method(phase_name, 0);
5112 }
5113 #endif
5114 
<a name="52" id="anc52"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="52" type="hidden" />
</body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/memnode.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="matcher.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="node.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/memnode.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;

  26 #include &quot;classfile/systemDictionary.hpp&quot;
  27 #include &quot;compiler/compileLog.hpp&quot;
  28 #include &quot;gc/shared/barrierSet.hpp&quot;
  29 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  30 #include &quot;memory/allocation.inline.hpp&quot;
  31 #include &quot;memory/resourceArea.hpp&quot;
  32 #include &quot;oops/objArrayKlass.hpp&quot;
  33 #include &quot;opto/addnode.hpp&quot;
  34 #include &quot;opto/arraycopynode.hpp&quot;
  35 #include &quot;opto/cfgnode.hpp&quot;
  36 #include &quot;opto/compile.hpp&quot;
  37 #include &quot;opto/connode.hpp&quot;
  38 #include &quot;opto/convertnode.hpp&quot;

  39 #include &quot;opto/loopnode.hpp&quot;
  40 #include &quot;opto/machnode.hpp&quot;
  41 #include &quot;opto/matcher.hpp&quot;
  42 #include &quot;opto/memnode.hpp&quot;
  43 #include &quot;opto/mulnode.hpp&quot;
  44 #include &quot;opto/narrowptrnode.hpp&quot;
  45 #include &quot;opto/phaseX.hpp&quot;
  46 #include &quot;opto/regmask.hpp&quot;
  47 #include &quot;opto/rootnode.hpp&quot;
  48 #include &quot;utilities/align.hpp&quot;
  49 #include &quot;utilities/copy.hpp&quot;
  50 #include &quot;utilities/macros.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 #include &quot;utilities/vmError.hpp&quot;
  53 
  54 // Portions of code courtesy of Clifford Click
  55 
  56 // Optimization - Graph Style
  57 
  58 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
</pre>
<hr />
<pre>
 203 
 204 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 205   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 206   if (t_oop == NULL)
 207     return mchain;  // don&#39;t try to optimize non-oop types
 208   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 209   bool is_instance = t_oop-&gt;is_known_instance_field();
 210   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 211   if (is_instance &amp;&amp; igvn != NULL &amp;&amp; result-&gt;is_Phi()) {
 212     PhiNode *mphi = result-&gt;as_Phi();
 213     assert(mphi-&gt;bottom_type() == Type::MEMORY, &quot;memory phi required&quot;);
 214     const TypePtr *t = mphi-&gt;adr_type();
 215     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 216         (t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 217          t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 218            -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 219             -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop)) {
 220       // clone the Phi with our address type
 221       result = mphi-&gt;split_out_instance(t_adr, igvn);
 222     } else {





 223       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), &quot;correct memory chain&quot;);
 224     }
 225   }
 226   return result;
 227 }
 228 
 229 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 230   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 231   Node *mem = mmem;
 232 #ifdef ASSERT
 233   {
 234     // Check that current type is consistent with the alias index used during graph construction
 235     assert(alias_idx &gt;= Compile::AliasIdxRaw, &quot;must not be a bad alias_idx&quot;);
 236     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 237                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 238     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 239     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
<span class="line-modified"> 240                tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;</span>
 241         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 242         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 243           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 244           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 245       // don&#39;t assert if it is dead code.
 246       consistent = true;
 247     }
 248     if( !consistent ) {
 249       st-&gt;print(&quot;alias_idx==%d, adr_check==&quot;, alias_idx);
 250       if( adr_check == NULL ) {
 251         st-&gt;print(&quot;NULL&quot;);
 252       } else {
 253         adr_check-&gt;dump();
 254       }
 255       st-&gt;cr();
 256       print_alias_types();
 257       assert(consistent, &quot;adr_check must match alias idx&quot;);
 258     }
 259   }
 260 #endif
</pre>
<hr />
<pre>
 814          &quot;use LoadKlassNode instead&quot;);
 815   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 816            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 817          &quot;use LoadRangeNode instead&quot;);
 818   // Check control edge of raw loads
 819   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 820           // oop will be recorded in oop map if load crosses safepoint
 821           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 822           &quot;raw memory operations should have control edge&quot;);
 823   LoadNode* load = NULL;
 824   switch (bt) {
 825   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 826   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 827   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 828   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 829   case T_SHORT:   load = new LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 830   case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency); break;
 831   case T_FLOAT:   load = new LoadFNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 832   case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 833   case T_ADDRESS: load = new LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo, control_dependency); break;

 834   case T_OBJECT:
 835 #ifdef _LP64
 836     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 837       load = new LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo, control_dependency);
 838     } else
 839 #endif
 840     {
 841       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), &quot;should have got back a narrow oop&quot;);
 842       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_ptr(), mo, control_dependency);
 843     }
 844     break;
 845   default:
 846     ShouldNotReachHere();
 847     break;
 848   }
 849   assert(load != NULL, &quot;LoadNode should have been created&quot;);
 850   if (unaligned) {
 851     load-&gt;set_unaligned_access();
 852   }
 853   if (mismatched) {
</pre>
<hr />
<pre>
 941 
 942     LoadNode* ld = clone()-&gt;as_Load();
 943     Node* addp = in(MemNode::Address)-&gt;clone();
 944     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 945       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 946       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
 947       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 948       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Base)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 949       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Address)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 950       addp-&gt;set_req(AddPNode::Base, src);
 951       addp-&gt;set_req(AddPNode::Address, src);
 952     } else {
 953       assert(ac-&gt;as_ArrayCopy()-&gt;is_arraycopy_validated() ||
 954              ac-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 955              ac-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated(), &quot;only supported cases&quot;);
 956       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), &quot;should be&quot;);
 957       addp-&gt;set_req(AddPNode::Base, src);
 958       addp-&gt;set_req(AddPNode::Address, src);
 959 
 960       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
<span class="line-modified"> 961       BasicType ary_elem  = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();</span>
 962       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 963       uint shift  = exact_log2(type2aelembytes(ary_elem));




 964 
 965       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 966 #ifdef _LP64
 967       diff = phase-&gt;transform(new ConvI2LNode(diff));
 968 #endif
 969       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 970 
 971       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 972       addp-&gt;set_req(AddPNode::Offset, offset);
 973     }
 974     addp = phase-&gt;transform(addp);
 975 #ifdef ASSERT
 976     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 977     ld-&gt;_adr_type = adr_type;
 978 #endif
 979     ld-&gt;set_req(MemNode::Address, addp);
 980     ld-&gt;set_req(0, ctl);
 981     ld-&gt;set_req(MemNode::Memory, mem);
 982     // load depends on the tests that validate the arraycopy
 983     ld-&gt;_control_dependency = UnknownControl;
</pre>
<hr />
<pre>
1071         // the same pointer-and-offset that we stored to.
1072         // Casted version may carry a dependency and it is respected.
1073         // Thus, we are able to replace L by V.
1074       }
1075       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1076       if (store_Opcode() != st-&gt;Opcode())
1077         return NULL;
1078       return st-&gt;in(MemNode::ValueIn);
1079     }
1080 
1081     // A load from a freshly-created object always returns zero.
1082     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1083     // to find_captured_store, which returned InitializeNode::zero_memory.)
1084     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1085         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1086         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1087       // return a zero value for the load&#39;s basic type
1088       // (This is one of the few places where a generic PhaseTransform
1089       // can create new nodes.  Think of it as lazily manifesting
1090       // virtually pre-existing constants.)






1091       return phase-&gt;zerocon(memory_type());
1092     }
1093 
1094     // A load from an initialization barrier can match a captured store.
1095     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1096       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1097       AllocateNode* alloc = init-&gt;allocation();
1098       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1099         // examine a captured store value
1100         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1101         if (st != NULL) {
1102           continue;             // take one more trip around
1103         }
1104       }
1105     }
1106 
1107     // Load boxed value from result of valueOf() call is input parameter.
1108     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1109         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1110       intptr_t ignore = 0;
</pre>
<hr />
<pre>
1128 //----------------------is_instance_field_load_with_local_phi------------------
1129 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1130   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1131       in(Address)-&gt;is_AddP() ) {
1132     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1133     // Only instances and boxed values.
1134     if( t_oop != NULL &amp;&amp;
1135         (t_oop-&gt;is_ptr_to_boxed_value() ||
1136          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1137         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1138         t_oop-&gt;offset() != Type::OffsetTop) {
1139       return true;
1140     }
1141   }
1142   return false;
1143 }
1144 
1145 //------------------------------Identity---------------------------------------
1146 // Loads are identity if previous store is to same address
1147 Node* LoadNode::Identity(PhaseGVN* phase) {



























1148   // If the previous store-maker is the right kind of Store, and the store is
1149   // to the same address, then we are equal to the value stored.
1150   Node* mem = in(Memory);
1151   Node* value = can_see_stored_value(mem, phase);
1152   if( value ) {
1153     // byte, short &amp; char stores truncate naturally.
1154     // A load has to load the truncated value which requires
1155     // some sort of masking operation and that requires an
1156     // Ideal call instead of an Identity call.
1157     if (memory_size() &lt; BytesPerInt) {
1158       // If the input to the store does not fit with the load&#39;s result type,
1159       // it must be truncated via an Ideal call.
1160       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1161         return this;
1162     }
1163     // (This works even when value is a Con, but LoadNode::Value
1164     // usually runs first, producing the singleton type of the Con.)
1165     return value;
1166   }
1167 
</pre>
<hr />
<pre>
1685   // fold up, do so.
1686   Node* prev_mem = find_previous_store(phase);
1687   if (prev_mem != NULL) {
1688     Node* value = can_see_arraycopy_value(prev_mem, phase);
1689     if (value != NULL) {
1690       return value;
1691     }
1692   }
1693   // Steps (a), (b):  Walk past independent stores to find an exact match.
1694   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1695     // (c) See if we can fold up on the spot, but don&#39;t fold up here.
1696     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1697     // just return a prior value, which is done by Identity calls.
1698     if (can_see_stored_value(prev_mem, phase)) {
1699       // Make ready for step (d):
1700       set_req(MemNode::Memory, prev_mem);
1701       return this;
1702     }
1703   }
1704 
<span class="line-modified">1705   AllocateNode* alloc = is_new_object_mark_load(phase);</span>
<span class="line-modified">1706   if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate &amp;&amp; UseBiasedLocking) {</span>




1707     InitializeNode* init = alloc-&gt;initialization();
1708     Node* control = init-&gt;proj_out(0);
<span class="line-modified">1709     return alloc-&gt;make_ideal_mark(phase, address, control, mem);</span>
1710   }
1711 
1712   return progress ? this : NULL;
1713 }
1714 
1715 // Helper to recognize certain Klass fields which are invariant across
1716 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1717 const Type*
1718 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1719                                  ciKlass* klass) const {
1720   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1721     // The field is Klass::_modifier_flags.  Return its (constant) value.
1722     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1723     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _modifier_flags&quot;);
1724     return TypeInt::make(klass-&gt;modifier_flags());
1725   }
1726   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1727     // The field is Klass::_access_flags.  Return its (constant) value.
1728     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1729     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _access_flags&quot;);
</pre>
<hr />
<pre>
1781       }
1782     }
1783 
1784     // Don&#39;t do this for integer types. There is only potential profit if
1785     // the element type t is lower than _type; that is, for int types, if _type is
1786     // more restrictive than t.  This only happens here if one is short and the other
1787     // char (both 16 bits), and in those cases we&#39;ve made an intentional decision
1788     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1789     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1790     //
1791     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1792     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1793     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1794     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1795     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1796     // In fact, that could have been the original type of p1, and p1 could have
1797     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1798     // expression (LShiftL quux 3) independently optimized to the constant 8.
1799     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1800         &amp;&amp; (_type-&gt;isa_vect() == NULL)

1801         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1802       // t might actually be lower than _type, if _type is a unique
1803       // concrete subclass of abstract class t.
1804       if (off_beyond_header || off == Type::OffsetBot) {  // is the offset beyond the header?
1805         const Type* jt = t-&gt;join_speculative(_type);
1806         // In any case, do not allow the join, per se, to empty out the type.
1807         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1808           // This can happen if a interface-typed array narrows to a class type.
1809           jt = _type;
1810         }
1811 #ifdef ASSERT
1812         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1813           // The pointers in the autobox arrays are always non-null
1814           Node* base = adr-&gt;in(AddPNode::Base);
1815           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1816             // Get LoadN node which loads IntegerCache.cache field
1817             base = base-&gt;in(1);
1818           }
1819           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1820             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1821             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1822               // It could be narrow oop
1823               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,&quot;sanity&quot;);
1824             }
1825           }
1826         }
1827 #endif
1828         return jt;
1829       }
1830     }
1831   } else if (tp-&gt;base() == Type::InstPtr) {
1832     assert( off != Type::OffsetBot ||
1833             // arrays can be cast to Objects
1834             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||

1835             // unsafe field access may not have a constant offset
1836             C-&gt;has_unsafe_access(),
1837             &quot;Field accesses must be precise&quot; );
1838     // For oop loads, we expect the _type to be precise.
1839 
<span class="line-modified">1840     // Optimize loads from constant fields.</span>


1841     const TypeInstPtr* tinst = tp-&gt;is_instptr();
1842     ciObject* const_oop = tinst-&gt;const_oop();
1843     if (!is_mismatched_access() &amp;&amp; off != Type::OffsetBot &amp;&amp; const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
<span class="line-modified">1844       const Type* con_type = Type::make_constant_from_field(const_oop-&gt;as_instance(), off, is_unsigned(), memory_type());</span>









1845       if (con_type != NULL) {
1846         return con_type;
1847       }
1848     }
1849   } else if (tp-&gt;base() == Type::KlassPtr) {
1850     assert( off != Type::OffsetBot ||
1851             // arrays can be cast to Objects

1852             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1853             // also allow array-loading from the primary supertype
1854             // array during subtype checks
1855             Opcode() == Op_LoadKlass,
1856             &quot;Field accesses must be precise&quot; );
1857     // For klass/static loads, we expect the _type to be precise
<span class="line-modified">1858   } else if (tp-&gt;base() == Type::RawPtr &amp;&amp; adr-&gt;is_Load() &amp;&amp; off == 0) {</span>
<span class="line-modified">1859     /* With mirrors being an indirect in the Klass*</span>
<span class="line-modified">1860      * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))</span>
<span class="line-modified">1861      * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).</span>
<span class="line-modified">1862      *</span>
<span class="line-modified">1863      * So check the type and klass of the node before the LoadP.</span>
<span class="line-modified">1864      */</span>
<span class="line-modified">1865     Node* adr2 = adr-&gt;in(MemNode::Address);</span>
<span class="line-modified">1866     const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();</span>
<span class="line-modified">1867     if (tkls != NULL &amp;&amp; !StressReflectiveCode) {</span>
<span class="line-modified">1868       ciKlass* klass = tkls-&gt;klass();</span>
<span class="line-modified">1869       if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {</span>
<span class="line-modified">1870         assert(adr-&gt;Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-modified">1871         assert(Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-modified">1872         return TypeInstPtr::make(klass-&gt;java_mirror());</span>
















1873       }
1874     }
1875   }
1876 
1877   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1878   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1879     ciKlass* klass = tkls-&gt;klass();
<span class="line-modified">1880     if (klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {</span>
1881       // We are loading a field from a Klass metaobject whose identity
1882       // is known at compile time (the type is &quot;exact&quot; or &quot;precise&quot;).
1883       // Check for fields we know are maintained as constants by the VM.
1884       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1885         // The field is Klass::_super_check_offset.  Return its (constant) value.
1886         // (Folds up type checking code.)
1887         assert(Opcode() == Op_LoadI, &quot;must load an int from _super_check_offset&quot;);
1888         return TypeInt::make(klass-&gt;super_check_offset());
1889       }
1890       // Compute index into primary_supers array
1891       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1892       // Check for overflowing; use unsigned compare to handle the negative case.
1893       if( depth &lt; ciKlass::primary_super_limit() ) {
1894         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1895         // (Folds up type checking code.)
1896         assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1897         ciKlass *ss = klass-&gt;super_of_depth(depth);
1898         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1899       }
1900       const Type* aift = load_array_final_field(tkls, klass);
1901       if (aift != NULL)  return aift;
1902     }
1903 
1904     // We can still check if we are loading from the primary_supers array at a
1905     // shallow enough depth.  Even though the klass is not exact, entries less
1906     // than or equal to its super depth are correct.
<span class="line-modified">1907     if (klass-&gt;is_loaded() ) {</span>
1908       ciType *inner = klass;
1909       while( inner-&gt;is_obj_array_klass() )
1910         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1911       if( inner-&gt;is_instance_klass() &amp;&amp;
1912           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1913         // Compute index into primary_supers array
1914         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1915         // Check for overflowing; use unsigned compare to handle the negative case.
1916         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1917             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1918           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1919           // (Folds up type checking code.)
1920           assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1921           ciKlass *ss = klass-&gt;super_of_depth(depth);
1922           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1923         }
1924       }
1925     }
1926 
1927     // If the type is enough to determine that the thing is not an array,
</pre>
<hr />
<pre>
2092   return LoadNode::Ideal(phase, can_reshape);
2093 }
2094 
2095 const Type* LoadSNode::Value(PhaseGVN* phase) const {
2096   Node* mem = in(MemNode::Memory);
2097   Node* value = can_see_stored_value(mem,phase);
2098   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2099       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2100     // If the input to the store does not fit with the load&#39;s result type,
2101     // it must be truncated. We can&#39;t delay until Ideal call since
2102     // a singleton Value is needed for split_thru_phi optimization.
2103     int con = value-&gt;get_int();
2104     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2105   }
2106   return LoadNode::Value(phase);
2107 }
2108 
2109 //=============================================================================
2110 //----------------------------LoadKlassNode::make------------------------------
2111 // Polymorphic factory method:
<span class="line-modified">2112 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at, const TypeKlassPtr* tk) {</span>

2113   // sanity check the alias category against the created node type
2114   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2115   assert(adr_type != NULL, &quot;expecting TypeKlassPtr&quot;);
2116 #ifdef _LP64
2117   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2118     assert(UseCompressedClassPointers, &quot;no compressed klasses&quot;);
2119     Node* load_klass = gvn.transform(new LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2120     return new DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2121   }
2122 #endif
2123   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), &quot;should have got back a narrow oop&quot;);
2124   return new LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2125 }
2126 
2127 //------------------------------Value------------------------------------------
2128 const Type* LoadKlassNode::Value(PhaseGVN* phase) const {
2129   return klass_value_common(phase);
2130 }
2131 
2132 // In most cases, LoadKlassNode does not have the control input set. If the control
</pre>
<hr />
<pre>
2179     }
2180     if( !ik-&gt;is_loaded() )
2181       return _type;             // Bail out if not loaded
2182     if (offset == oopDesc::klass_offset_in_bytes()) {
2183       if (tinst-&gt;klass_is_exact()) {
2184         return TypeKlassPtr::make(ik);
2185       }
2186       // See if we can become precise: no subklasses and no interface
2187       // (Note:  We need to support verified interfaces.)
2188       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2189         // Add a dependence; if any subclass added we need to recompile
2190         if (!ik-&gt;is_final()) {
2191           // %%% should use stronger assert_unique_concrete_subtype instead
2192           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2193         }
2194         // Return precise klass
2195         return TypeKlassPtr::make(ik);
2196       }
2197 
2198       // Return root of possible klass
<span class="line-modified">2199       return TypeKlassPtr::make(TypePtr::NotNull, ik, 0/*offset*/);</span>
2200     }
2201   }
2202 
2203   // Check for loading klass from an array
2204   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
<span class="line-modified">2205   if( tary != NULL ) {</span>
2206     ciKlass *tary_klass = tary-&gt;klass();
2207     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2208         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2209       if (tary-&gt;klass_is_exact()) {
2210         return TypeKlassPtr::make(tary_klass);
2211       }
<span class="line-modified">2212       ciArrayKlass *ak = tary-&gt;klass()-&gt;as_array_klass();</span>
2213       // If the klass is an object array, we defer the question to the
2214       // array component klass.
<span class="line-modified">2215       if( ak-&gt;is_obj_array_klass() ) {</span>
<span class="line-modified">2216         assert( ak-&gt;is_loaded(), &quot;&quot; );</span>
2217         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
<span class="line-modified">2218         if( base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass() ) {</span>
<span class="line-modified">2219           ciInstanceKlass* ik = base_k-&gt;as_instance_klass();</span>
2220           // See if we can become precise: no subklasses and no interface
2221           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2222             // Add a dependence; if any subclass added we need to recompile
2223             if (!ik-&gt;is_final()) {
2224               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2225             }
2226             // Return precise array klass
2227             return TypeKlassPtr::make(ak);
2228           }
2229         }
<span class="line-modified">2230         return TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);</span>
<span class="line-modified">2231       } else {                  // Found a type-array?</span>
<span class="line-modified">2232         assert( ak-&gt;is_type_array_klass(), &quot;&quot; );</span>
2233         return TypeKlassPtr::make(ak); // These are always precise
2234       }
2235     }
2236   }
2237 
2238   // Check for loading klass from an array klass
2239   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2240   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
<span class="line-modified">2241     ciKlass* klass = tkls-&gt;klass();</span>
<span class="line-removed">2242     if( !klass-&gt;is_loaded() )</span>
2243       return _type;             // Bail out if not loaded


2244     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2245         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2246       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2247       // // Always returning precise element type is incorrect,
2248       // // e.g., element type could be object and array may contain strings
2249       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2250 
2251       // The array&#39;s TypeKlassPtr was declared &#39;precise&#39; or &#39;not precise&#39;
2252       // according to the element type&#39;s subclassing.
<span class="line-modified">2253       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, 0/*offset*/);</span>




2254     }
2255     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2256         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2257       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2258       // The field is Klass::_super.  Return its (constant) value.
2259       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2260       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2261     }
2262   }
2263 
2264   // Bailout case
2265   return LoadNode::Value(phase);
2266 }
2267 
2268 //------------------------------Identity---------------------------------------
2269 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2270 // Also feed through the klass in Allocate(...klass...)._klass.
2271 Node* LoadKlassNode::Identity(PhaseGVN* phase) {
2272   return klass_identity_common(phase);
2273 }
</pre>
<hr />
<pre>
2441 //=============================================================================
2442 //---------------------------StoreNode::make-----------------------------------
2443 // Polymorphic factory method:
2444 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2445   assert((mo == unordered || mo == release), &quot;unexpected&quot;);
2446   Compile* C = gvn.C;
2447   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2448          ctl != NULL, &quot;raw memory operations should have control edge&quot;);
2449 
2450   switch (bt) {
2451   case T_BOOLEAN: val = gvn.transform(new AndINode(val, gvn.intcon(0x1))); // Fall through to T_BYTE case
2452   case T_BYTE:    return new StoreBNode(ctl, mem, adr, adr_type, val, mo);
2453   case T_INT:     return new StoreINode(ctl, mem, adr, adr_type, val, mo);
2454   case T_CHAR:
2455   case T_SHORT:   return new StoreCNode(ctl, mem, adr, adr_type, val, mo);
2456   case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);
2457   case T_FLOAT:   return new StoreFNode(ctl, mem, adr, adr_type, val, mo);
2458   case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);
2459   case T_METADATA:
2460   case T_ADDRESS:

2461   case T_OBJECT:
2462 #ifdef _LP64
2463     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2464       val = gvn.transform(new EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2465       return new StoreNNode(ctl, mem, adr, adr_type, val, mo);
2466     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2467                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2468                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2469       val = gvn.transform(new EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2470       return new StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2471     }
2472 #endif
2473     {
2474       return new StorePNode(ctl, mem, adr, adr_type, val, mo);
2475     }
2476   default:
2477     ShouldNotReachHere();
2478     return (StoreNode*)NULL;
2479   }
2480 }
</pre>
<hr />
<pre>
2501   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2502 
2503   // Since they are not commoned, do not hash them:
2504   return NO_HASH;
2505 }
2506 
2507 //------------------------------Ideal------------------------------------------
2508 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2509 // When a store immediately follows a relevant allocation/initialization,
2510 // try to capture it into the initialization, or hoist it above.
2511 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2512   Node* p = MemNode::Ideal_common(phase, can_reshape);
2513   if (p)  return (p == NodeSentinel) ? NULL : p;
2514 
2515   Node* mem     = in(MemNode::Memory);
2516   Node* address = in(MemNode::Address);
2517   // Back-to-back stores to same address?  Fold em up.  Generally
2518   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2519   // since they must follow each StoreP operation.  Redundant StoreCMs
2520   // are eliminated just before matching in final_graph_reshape.
<span class="line-modified">2521   {</span>
2522     Node* st = mem;
2523     // If Store &#39;st&#39; has more than one use, we cannot fold &#39;st&#39; away.
2524     // For example, &#39;st&#39; might be the final state at a conditional
2525     // return.  Or, &#39;st&#39; might be used by some node which is live at
2526     // the same time &#39;st&#39; is live, which might be unschedulable.  So,
2527     // require exactly ONE user until such time as we clone &#39;mem&#39; for
2528     // each of &#39;mem&#39;s uses (thus making the exactly-1-user-rule hold
2529     // true).
2530     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2531       // Looking at a dead closed cycle of memory?
2532       assert(st != st-&gt;in(MemNode::Memory), &quot;dead loop in StoreNode::Ideal&quot;);
2533       assert(Opcode() == st-&gt;Opcode() ||
2534              st-&gt;Opcode() == Op_StoreVector ||
2535              Opcode() == Op_StoreVector ||
2536              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2537              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2538              (Opcode() == Op_StoreI &amp;&amp; st-&gt;Opcode() == Op_StoreL) || // initialization by arraycopy

2539              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2540              &quot;no mismatched stores, except on raw memory: %s %s&quot;, NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
2541 
2542       if (st-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2543           st-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2544         Node* use = st-&gt;raw_out(0);
2545         phase-&gt;igvn_rehash_node_delayed(use);
2546         if (can_reshape) {
2547           use-&gt;set_req_X(MemNode::Memory, st-&gt;in(MemNode::Memory), phase-&gt;is_IterGVN());
2548         } else {
2549           // It&#39;s OK to do this in the parser, since DU info is always accurate,
2550           // and the parser always refers to nodes via SafePointNode maps.
2551           use-&gt;set_req(MemNode::Memory, st-&gt;in(MemNode::Memory));
2552         }
2553         return this;
2554       }
2555       st = st-&gt;in(MemNode::Memory);
2556     }
2557   }
2558 
</pre>
<hr />
<pre>
2604   // Load then Store?  Then the Store is useless
2605   if (val-&gt;is_Load() &amp;&amp;
2606       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2607       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2608       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2609     result = mem;
2610   }
2611 
2612   // Two stores in a row of the same value?
2613   if (result == this &amp;&amp;
2614       mem-&gt;is_Store() &amp;&amp;
2615       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2616       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2617       mem-&gt;Opcode() == Opcode()) {
2618     result = mem;
2619   }
2620 
2621   // Store of zero anywhere into a freshly-allocated object?
2622   // Then the store is useless.
2623   // (It must already have been captured by the InitializeNode.)
<span class="line-modified">2624   if (result == this &amp;&amp;</span>
<span class="line-removed">2625       ReduceFieldZeroing &amp;&amp; phase-&gt;type(val)-&gt;is_zero_type()) {</span>
2626     // a newly allocated object is already all-zeroes everywhere
<span class="line-modified">2627     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate()) {</span>


2628       result = mem;
2629     }
2630 
2631     if (result == this) {
2632       // the store may also apply to zero-bits in an earlier object
2633       Node* prev_mem = find_previous_store(phase);
2634       // Steps (a), (b):  Walk past independent stores to find an exact match.
2635       if (prev_mem != NULL) {
2636         Node* prev_val = can_see_stored_value(prev_mem, phase);
2637         if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2638           // prev_val and val might differ by a cast; it would be good
2639           // to keep the more informative of the two.
<span class="line-modified">2640           result = mem;</span>








2641         }
2642       }
2643     }
2644   }
2645 
2646   if (result != this &amp;&amp; phase-&gt;is_IterGVN() != NULL) {
2647     MemBarNode* trailing = trailing_membar();
2648     if (trailing != NULL) {
2649 #ifdef ASSERT
2650       const TypeOopPtr* t_oop = phase-&gt;type(in(Address))-&gt;isa_oopptr();
2651       assert(t_oop == NULL || t_oop-&gt;is_known_instance_field(), &quot;only for non escaping objects&quot;);
2652 #endif
2653       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2654       trailing-&gt;remove(igvn);
2655     }
2656   }
2657 
2658   return result;
2659 }
2660 
</pre>
<hr />
<pre>
2929 // Clearing a short array is faster with stores
2930 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2931   // Already know this is a large node, do not try to ideal it
2932   if (!IdealizeClearArrayNode || _is_large) return NULL;
2933 
2934   const int unit = BytesPerLong;
2935   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
2936   if (!t)  return NULL;
2937   if (!t-&gt;is_con())  return NULL;
2938   intptr_t raw_count = t-&gt;get_con();
2939   intptr_t size = raw_count;
2940   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
2941   // Clearing nothing uses the Identity call.
2942   // Negative clears are possible on dead ClearArrays
2943   // (see jck test stmt114.stmt11402.val).
2944   if (size &lt;= 0 || size % unit != 0)  return NULL;
2945   intptr_t count = size / unit;
2946   // Length too long; communicate this to matchers and assemblers.
2947   // Assemblers are responsible to produce fast hardware clears for it.
2948   if (size &gt; InitArrayShortSize) {
<span class="line-modified">2949     return new ClearArrayNode(in(0), in(1), in(2), in(3), true);</span>
2950   }
2951   Node *mem = in(1);
2952   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
2953   Node *adr = in(3);
2954   const Type* at = phase-&gt;type(adr);
2955   if( at==Type::TOP ) return NULL;
2956   const TypePtr* atp = at-&gt;isa_ptr();
2957   // adjust atp to be the correct array element address type
2958   if (atp == NULL)  atp = TypePtr::BOTTOM;
2959   else              atp = atp-&gt;add_offset(Type::OffsetBot);
2960   // Get base for derived pointer purposes
2961   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
2962   Node *base = adr-&gt;in(1);
2963 
<span class="line-modified">2964   Node *zero = phase-&gt;makecon(TypeLong::ZERO);</span>
2965   Node *off  = phase-&gt;MakeConX(BytesPerLong);
<span class="line-modified">2966   mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);</span>
2967   count--;
2968   while( count-- ) {
2969     mem = phase-&gt;transform(mem);
2970     adr = phase-&gt;transform(new AddPNode(base,adr,off));
<span class="line-modified">2971     mem = new StoreLNode(in(0),mem,adr,atp,zero,MemNode::unordered,false);</span>
2972   }
2973   return mem;
2974 }
2975 
2976 //----------------------------step_through----------------------------------
2977 // Return allocation input memory edge if it is different instance
2978 // or itself if it is the one we are looking for.
2979 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
2980   Node* n = *np;
2981   assert(n-&gt;is_ClearArray(), &quot;sanity&quot;);
2982   intptr_t offset;
2983   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
2984   // This method is called only before Allocate nodes are expanded
2985   // during macro nodes expansion. Before that ClearArray nodes are
2986   // only generated in PhaseMacroExpand::generate_arraycopy() (before
2987   // Allocate nodes are expanded) which follows allocations.
2988   assert(alloc != NULL, &quot;should have allocation&quot;);
2989   if (alloc-&gt;_idx == instance_id) {
2990     // Can not bypass initialization of the instance we are looking for.
2991     return false;
2992   }
2993   // Otherwise skip it.
2994   InitializeNode* init = alloc-&gt;initialization();
2995   if (init != NULL)
2996     *np = init-&gt;in(TypeFunc::Memory);
2997   else
2998     *np = alloc-&gt;in(TypeFunc::Memory);
2999   return true;
3000 }
3001 
3002 //----------------------------clear_memory-------------------------------------
3003 // Generate code to initialize object storage to zero.
3004 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,


3005                                    intptr_t start_offset,
3006                                    Node* end_offset,
3007                                    PhaseGVN* phase) {
3008   intptr_t offset = start_offset;
3009 
3010   int unit = BytesPerLong;
3011   if ((offset % unit) != 0) {
3012     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(offset));
3013     adr = phase-&gt;transform(adr);
3014     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3015     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>






3016     mem = phase-&gt;transform(mem);
3017     offset += BytesPerInt;
3018   }
3019   assert((offset % unit) == 0, &quot;&quot;);
3020 
3021   // Initialize the remaining stuff, if any, with a ClearArray.
<span class="line-modified">3022   return clear_memory(ctl, mem, dest, phase-&gt;MakeConX(offset), end_offset, phase);</span>
3023 }
3024 
3025 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,

3026                                    Node* start_offset,
3027                                    Node* end_offset,
3028                                    PhaseGVN* phase) {
3029   if (start_offset == end_offset) {
3030     // nothing to do
3031     return mem;
3032   }
3033 
3034   int unit = BytesPerLong;
3035   Node* zbase = start_offset;
3036   Node* zend  = end_offset;
3037 
3038   // Scale to the unit required by the CPU:
3039   if (!Matcher::init_array_count_is_in_bytes) {
3040     Node* shift = phase-&gt;intcon(exact_log2(unit));
3041     zbase = phase-&gt;transform(new URShiftXNode(zbase, shift) );
3042     zend  = phase-&gt;transform(new URShiftXNode(zend,  shift) );
3043   }
3044 
3045   // Bulk clear double-words
3046   Node* zsize = phase-&gt;transform(new SubXNode(zend, zbase) );
3047   Node* adr = phase-&gt;transform(new AddPNode(dest, dest, start_offset) );
<span class="line-modified">3048   mem = new ClearArrayNode(ctl, mem, zsize, adr, false);</span>



3049   return phase-&gt;transform(mem);
3050 }
3051 
3052 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,


3053                                    intptr_t start_offset,
3054                                    intptr_t end_offset,
3055                                    PhaseGVN* phase) {
3056   if (start_offset == end_offset) {
3057     // nothing to do
3058     return mem;
3059   }
3060 
3061   assert((end_offset % BytesPerInt) == 0, &quot;odd end offset&quot;);
3062   intptr_t done_offset = end_offset;
3063   if ((done_offset % BytesPerLong) != 0) {
3064     done_offset -= BytesPerInt;
3065   }
3066   if (done_offset &gt; start_offset) {
<span class="line-modified">3067     mem = clear_memory(ctl, mem, dest,</span>
3068                        start_offset, phase-&gt;MakeConX(done_offset), phase);
3069   }
3070   if (done_offset &lt; end_offset) { // emit the final 32-bit store
3071     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
3072     adr = phase-&gt;transform(adr);
3073     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3074     mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>






3075     mem = phase-&gt;transform(mem);
3076     done_offset += BytesPerInt;
3077   }
3078   assert(done_offset == end_offset, &quot;&quot;);
3079   return mem;
3080 }
3081 
3082 //=============================================================================
3083 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
3084   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
3085     _adr_type(C-&gt;get_adr_type(alias_idx)), _kind(Standalone)
3086 #ifdef ASSERT
3087   , _pair_idx(0)
3088 #endif
3089 {
3090   init_class_id(Class_MemBar);
3091   Node* top = C-&gt;top();
3092   init_req(TypeFunc::I_O,top);
3093   init_req(TypeFunc::FramePtr,top);
3094   init_req(TypeFunc::ReturnAdr,top);
</pre>
<hr />
<pre>
3193       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3194       remove(igvn);
3195       // Must return either the original node (now dead) or a new node
3196       // (Do not return a top here, since that would break the uniqueness of top.)
3197       return new ConINode(TypeInt::ZERO);
3198     }
3199   }
3200   return progress ? this : NULL;
3201 }
3202 
3203 //------------------------------Value------------------------------------------
3204 const Type* MemBarNode::Value(PhaseGVN* phase) const {
3205   if( !in(0) ) return Type::TOP;
3206   if( phase-&gt;type(in(0)) == Type::TOP )
3207     return Type::TOP;
3208   return TypeTuple::MEMBAR;
3209 }
3210 
3211 //------------------------------match------------------------------------------
3212 // Construct projections for memory.
<span class="line-modified">3213 Node *MemBarNode::match( const ProjNode *proj, const Matcher *m ) {</span>
3214   switch (proj-&gt;_con) {
3215   case TypeFunc::Control:
3216   case TypeFunc::Memory:
3217     return new MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3218   }
3219   ShouldNotReachHere();
3220   return NULL;
3221 }
3222 
3223 void MemBarNode::set_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3224   trailing-&gt;_kind = TrailingStore;
3225   leading-&gt;_kind = LeadingStore;
3226 #ifdef ASSERT
3227   trailing-&gt;_pair_idx = leading-&gt;_idx;
3228   leading-&gt;_pair_idx = leading-&gt;_idx;
3229 #endif
3230 }
3231 
3232 void MemBarNode::set_load_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3233   trailing-&gt;_kind = TrailingLoadStore;
</pre>
<hr />
<pre>
3479   return (req() &gt; RawStores);
3480 }
3481 
3482 void InitializeNode::set_complete(PhaseGVN* phase) {
3483   assert(!is_complete(), &quot;caller responsibility&quot;);
3484   _is_complete = Complete;
3485 
3486   // After this node is complete, it contains a bunch of
3487   // raw-memory initializations.  There is no need for
3488   // it to have anything to do with non-raw memory effects.
3489   // Therefore, tell all non-raw users to re-optimize themselves,
3490   // after skipping the memory effects of this initialization.
3491   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3492   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3493 }
3494 
3495 // convenience function
3496 // return false if the init contains any stores already
3497 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3498   InitializeNode* init = initialization();
<span class="line-modified">3499   if (init == NULL || init-&gt;is_complete())  return false;</span>


3500   init-&gt;remove_extra_zeroes();
3501   // for now, if this allocation has already collected any inits, bail:
3502   if (init-&gt;is_non_zero())  return false;
3503   init-&gt;set_complete(phase);
3504   return true;
3505 }
3506 
3507 void InitializeNode::remove_extra_zeroes() {
3508   if (req() == RawStores)  return;
3509   Node* zmem = zero_memory();
3510   uint fill = RawStores;
3511   for (uint i = fill; i &lt; req(); i++) {
3512     Node* n = in(i);
3513     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3514     if (fill &lt; i)  set_req(fill, n);          // compact
3515     ++fill;
3516   }
3517   // delete any empty spaces created:
3518   while (fill &lt; req()) {
3519     del_req(fill);
</pre>
<hr />
<pre>
4237         //   z&#39;s_done      12  16  16  16    12  16    12
4238         //   z&#39;s_needed    12  16  16  16    16  16    16
4239         //   zsize          0   0   0   0     4   0     4
4240         if (next_full_store &lt; 0) {
4241           // Conservative tack:  Zero to end of current word.
4242           zeroes_needed = align_up(zeroes_needed, BytesPerInt);
4243         } else {
4244           // Zero to beginning of next fully initialized word.
4245           // Or, don&#39;t zero at all, if we are already in that word.
4246           assert(next_full_store &gt;= zeroes_needed, &quot;must go forward&quot;);
4247           assert((next_full_store &amp; (BytesPerInt-1)) == 0, &quot;even boundary&quot;);
4248           zeroes_needed = next_full_store;
4249         }
4250       }
4251 
4252       if (zeroes_needed &gt; zeroes_done) {
4253         intptr_t zsize = zeroes_needed - zeroes_done;
4254         // Do some incremental zeroing on rawmem, in parallel with inits.
4255         zeroes_done = align_down(zeroes_done, BytesPerInt);
4256         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,


4257                                               zeroes_done, zeroes_needed,
4258                                               phase);
4259         zeroes_done = zeroes_needed;
4260         if (zsize &gt; InitArrayShortSize &amp;&amp; ++big_init_gaps &gt; 2)
4261           do_zeroing = false;   // leave the hole, next time
4262       }
4263     }
4264 
4265     // Collect the store and move on:
4266     phase-&gt;replace_input_of(st, MemNode::Memory, inits);
4267     inits = st;                 // put it on the linearized chain
4268     set_req(i, zmem);           // unhook from previous position
4269 
4270     if (zeroes_done == st_off)
4271       zeroes_done = next_init_off;
4272 
4273     assert(!do_zeroing || zeroes_done &gt;= next_init_off, &quot;don&#39;t miss any&quot;);
4274 
4275     #ifdef ASSERT
4276     // Various order invariants.  Weaker than stores_are_sane because
</pre>
<hr />
<pre>
4296   remove_extra_zeroes();        // clear out all the zmems left over
4297   add_req(inits);
4298 
4299   if (!(UseTLAB &amp;&amp; ZeroTLAB)) {
4300     // If anything remains to be zeroed, zero it all now.
4301     zeroes_done = align_down(zeroes_done, BytesPerInt);
4302     // if it is the last unused 4 bytes of an instance, forget about it
4303     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4304     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4305       AllocateNode* alloc = allocation();
4306       assert(alloc != NULL, &quot;must be present&quot;);
4307       if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate) {
4308         Node* klass_node = alloc-&gt;in(AllocateNode::KlassNode);
4309         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4310         if (zeroes_done == k-&gt;layout_helper())
4311           zeroes_done = size_limit;
4312       }
4313     }
4314     if (zeroes_done &lt; size_limit) {
4315       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,


4316                                             zeroes_done, size_in_bytes, phase);
4317     }
4318   }
4319 
4320   set_complete(phase);
4321   return rawmem;
4322 }
4323 
4324 
4325 #ifdef ASSERT
4326 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4327   if (is_complete())
4328     return true;                // stores could be anything at this point
4329   assert(allocation() != NULL, &quot;must be present&quot;);
4330   intptr_t last_off = allocation()-&gt;minimum_header_size();
4331   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4332     Node* st = in(i);
4333     intptr_t st_off = get_store_offset(st, phase);
4334     if (st_off &lt; 0)  continue;  // ignore dead garbage
4335     if (last_off &gt; st_off) {
</pre>
</td>
<td>
<hr />
<pre>
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
<span class="line-added">  26 #include &quot;ci/ciFlatArrayKlass.hpp&quot;</span>
  27 #include &quot;classfile/systemDictionary.hpp&quot;
  28 #include &quot;compiler/compileLog.hpp&quot;
  29 #include &quot;gc/shared/barrierSet.hpp&quot;
  30 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  31 #include &quot;memory/allocation.inline.hpp&quot;
  32 #include &quot;memory/resourceArea.hpp&quot;
  33 #include &quot;oops/objArrayKlass.hpp&quot;
  34 #include &quot;opto/addnode.hpp&quot;
  35 #include &quot;opto/arraycopynode.hpp&quot;
  36 #include &quot;opto/cfgnode.hpp&quot;
  37 #include &quot;opto/compile.hpp&quot;
  38 #include &quot;opto/connode.hpp&quot;
  39 #include &quot;opto/convertnode.hpp&quot;
<span class="line-added">  40 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  41 #include &quot;opto/loopnode.hpp&quot;
  42 #include &quot;opto/machnode.hpp&quot;
  43 #include &quot;opto/matcher.hpp&quot;
  44 #include &quot;opto/memnode.hpp&quot;
  45 #include &quot;opto/mulnode.hpp&quot;
  46 #include &quot;opto/narrowptrnode.hpp&quot;
  47 #include &quot;opto/phaseX.hpp&quot;
  48 #include &quot;opto/regmask.hpp&quot;
  49 #include &quot;opto/rootnode.hpp&quot;
  50 #include &quot;utilities/align.hpp&quot;
  51 #include &quot;utilities/copy.hpp&quot;
  52 #include &quot;utilities/macros.hpp&quot;
  53 #include &quot;utilities/powerOfTwo.hpp&quot;
  54 #include &quot;utilities/vmError.hpp&quot;
  55 
  56 // Portions of code courtesy of Clifford Click
  57 
  58 // Optimization - Graph Style
  59 
  60 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st);
</pre>
<hr />
<pre>
 205 
 206 Node *MemNode::optimize_memory_chain(Node *mchain, const TypePtr *t_adr, Node *load, PhaseGVN *phase) {
 207   const TypeOopPtr* t_oop = t_adr-&gt;isa_oopptr();
 208   if (t_oop == NULL)
 209     return mchain;  // don&#39;t try to optimize non-oop types
 210   Node* result = optimize_simple_memory_chain(mchain, t_oop, load, phase);
 211   bool is_instance = t_oop-&gt;is_known_instance_field();
 212   PhaseIterGVN *igvn = phase-&gt;is_IterGVN();
 213   if (is_instance &amp;&amp; igvn != NULL &amp;&amp; result-&gt;is_Phi()) {
 214     PhiNode *mphi = result-&gt;as_Phi();
 215     assert(mphi-&gt;bottom_type() == Type::MEMORY, &quot;memory phi required&quot;);
 216     const TypePtr *t = mphi-&gt;adr_type();
 217     if (t == TypePtr::BOTTOM || t == TypeRawPtr::BOTTOM ||
 218         (t-&gt;isa_oopptr() &amp;&amp; !t-&gt;is_oopptr()-&gt;is_known_instance() &amp;&amp;
 219          t-&gt;is_oopptr()-&gt;cast_to_exactness(true)
 220            -&gt;is_oopptr()-&gt;cast_to_ptr_type(t_oop-&gt;ptr())
 221             -&gt;is_oopptr()-&gt;cast_to_instance_id(t_oop-&gt;instance_id()) == t_oop)) {
 222       // clone the Phi with our address type
 223       result = mphi-&gt;split_out_instance(t_adr, igvn);
 224     } else {
<span class="line-added"> 225       if (t-&gt;isa_aryptr()) {</span>
<span class="line-added"> 226         // In the case of a flattened inline type array, each field has its own slice.</span>
<span class="line-added"> 227         // TODO This should be re-evaluated with JDK-8251039</span>
<span class="line-added"> 228         t = t-&gt;is_aryptr()-&gt;with_field_offset(t_adr-&gt;is_aryptr()-&gt;field_offset().get());</span>
<span class="line-added"> 229       }</span>
 230       assert(phase-&gt;C-&gt;get_alias_index(t) == phase-&gt;C-&gt;get_alias_index(t_adr), &quot;correct memory chain&quot;);
 231     }
 232   }
 233   return result;
 234 }
 235 
 236 static Node *step_through_mergemem(PhaseGVN *phase, MergeMemNode *mmem,  const TypePtr *tp, const TypePtr *adr_check, outputStream *st) {
 237   uint alias_idx = phase-&gt;C-&gt;get_alias_index(tp);
 238   Node *mem = mmem;
 239 #ifdef ASSERT
 240   {
 241     // Check that current type is consistent with the alias index used during graph construction
 242     assert(alias_idx &gt;= Compile::AliasIdxRaw, &quot;must not be a bad alias_idx&quot;);
 243     bool consistent =  adr_check == NULL || adr_check-&gt;empty() ||
 244                        phase-&gt;C-&gt;must_alias(adr_check, alias_idx );
 245     // Sometimes dead array references collapse to a[-1], a[-2], or a[-3]
 246     if( !consistent &amp;&amp; adr_check != NULL &amp;&amp; !adr_check-&gt;empty() &amp;&amp;
<span class="line-modified"> 247         tp-&gt;isa_aryptr() &amp;&amp;        tp-&gt;offset() == Type::OffsetBot &amp;&amp;</span>
 248         adr_check-&gt;isa_aryptr() &amp;&amp; adr_check-&gt;offset() != Type::OffsetBot &amp;&amp;
 249         ( adr_check-&gt;offset() == arrayOopDesc::length_offset_in_bytes() ||
 250           adr_check-&gt;offset() == oopDesc::klass_offset_in_bytes() ||
 251           adr_check-&gt;offset() == oopDesc::mark_offset_in_bytes() ) ) {
 252       // don&#39;t assert if it is dead code.
 253       consistent = true;
 254     }
 255     if( !consistent ) {
 256       st-&gt;print(&quot;alias_idx==%d, adr_check==&quot;, alias_idx);
 257       if( adr_check == NULL ) {
 258         st-&gt;print(&quot;NULL&quot;);
 259       } else {
 260         adr_check-&gt;dump();
 261       }
 262       st-&gt;cr();
 263       print_alias_types();
 264       assert(consistent, &quot;adr_check must match alias idx&quot;);
 265     }
 266   }
 267 #endif
</pre>
<hr />
<pre>
 821          &quot;use LoadKlassNode instead&quot;);
 822   assert(!(adr_type-&gt;isa_aryptr() &amp;&amp;
 823            adr_type-&gt;offset() == arrayOopDesc::length_offset_in_bytes()),
 824          &quot;use LoadRangeNode instead&quot;);
 825   // Check control edge of raw loads
 826   assert( ctl != NULL || C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
 827           // oop will be recorded in oop map if load crosses safepoint
 828           rt-&gt;isa_oopptr() || is_immutable_value(adr),
 829           &quot;raw memory operations should have control edge&quot;);
 830   LoadNode* load = NULL;
 831   switch (bt) {
 832   case T_BOOLEAN: load = new LoadUBNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 833   case T_BYTE:    load = new LoadBNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 834   case T_INT:     load = new LoadINode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 835   case T_CHAR:    load = new LoadUSNode(ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 836   case T_SHORT:   load = new LoadSNode (ctl, mem, adr, adr_type, rt-&gt;is_int(),  mo, control_dependency); break;
 837   case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt-&gt;is_long(), mo, control_dependency); break;
 838   case T_FLOAT:   load = new LoadFNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 839   case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;
 840   case T_ADDRESS: load = new LoadPNode (ctl, mem, adr, adr_type, rt-&gt;is_ptr(),  mo, control_dependency); break;
<span class="line-added"> 841   case T_INLINE_TYPE:</span>
 842   case T_OBJECT:
 843 #ifdef _LP64
 844     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
 845       load = new LoadNNode(ctl, mem, adr, adr_type, rt-&gt;make_narrowoop(), mo, control_dependency);
 846     } else
 847 #endif
 848     {
 849       assert(!adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop() &amp;&amp; !adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass(), &quot;should have got back a narrow oop&quot;);
 850       load = new LoadPNode(ctl, mem, adr, adr_type, rt-&gt;is_ptr(), mo, control_dependency);
 851     }
 852     break;
 853   default:
 854     ShouldNotReachHere();
 855     break;
 856   }
 857   assert(load != NULL, &quot;LoadNode should have been created&quot;);
 858   if (unaligned) {
 859     load-&gt;set_unaligned_access();
 860   }
 861   if (mismatched) {
</pre>
<hr />
<pre>
 949 
 950     LoadNode* ld = clone()-&gt;as_Load();
 951     Node* addp = in(MemNode::Address)-&gt;clone();
 952     if (ac-&gt;as_ArrayCopy()-&gt;is_clonebasic()) {
 953       assert(ld_alloc != NULL, &quot;need an alloc&quot;);
 954       assert(addp-&gt;is_AddP(), &quot;address must be addp&quot;);
 955       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 956       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Base)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 957       assert(bs-&gt;step_over_gc_barrier(addp-&gt;in(AddPNode::Address)) == bs-&gt;step_over_gc_barrier(ac-&gt;in(ArrayCopyNode::Dest)), &quot;strange pattern&quot;);
 958       addp-&gt;set_req(AddPNode::Base, src);
 959       addp-&gt;set_req(AddPNode::Address, src);
 960     } else {
 961       assert(ac-&gt;as_ArrayCopy()-&gt;is_arraycopy_validated() ||
 962              ac-&gt;as_ArrayCopy()-&gt;is_copyof_validated() ||
 963              ac-&gt;as_ArrayCopy()-&gt;is_copyofrange_validated(), &quot;only supported cases&quot;);
 964       assert(addp-&gt;in(AddPNode::Base) == addp-&gt;in(AddPNode::Address), &quot;should be&quot;);
 965       addp-&gt;set_req(AddPNode::Base, src);
 966       addp-&gt;set_req(AddPNode::Address, src);
 967 
 968       const TypeAryPtr* ary_t = phase-&gt;type(in(MemNode::Address))-&gt;isa_aryptr();
<span class="line-modified"> 969       BasicType ary_elem = ary_t-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();</span>
 970       uint header = arrayOopDesc::base_offset_in_bytes(ary_elem);
 971       uint shift  = exact_log2(type2aelembytes(ary_elem));
<span class="line-added"> 972       if (ary_t-&gt;klass()-&gt;is_flat_array_klass()) {</span>
<span class="line-added"> 973         ciFlatArrayKlass* vak = ary_t-&gt;klass()-&gt;as_flat_array_klass();</span>
<span class="line-added"> 974         shift = vak-&gt;log2_element_size();</span>
<span class="line-added"> 975       }</span>
 976 
 977       Node* diff = phase-&gt;transform(new SubINode(ac-&gt;in(ArrayCopyNode::SrcPos), ac-&gt;in(ArrayCopyNode::DestPos)));
 978 #ifdef _LP64
 979       diff = phase-&gt;transform(new ConvI2LNode(diff));
 980 #endif
 981       diff = phase-&gt;transform(new LShiftXNode(diff, phase-&gt;intcon(shift)));
 982 
 983       Node* offset = phase-&gt;transform(new AddXNode(addp-&gt;in(AddPNode::Offset), diff));
 984       addp-&gt;set_req(AddPNode::Offset, offset);
 985     }
 986     addp = phase-&gt;transform(addp);
 987 #ifdef ASSERT
 988     const TypePtr* adr_type = phase-&gt;type(addp)-&gt;is_ptr();
 989     ld-&gt;_adr_type = adr_type;
 990 #endif
 991     ld-&gt;set_req(MemNode::Address, addp);
 992     ld-&gt;set_req(0, ctl);
 993     ld-&gt;set_req(MemNode::Memory, mem);
 994     // load depends on the tests that validate the arraycopy
 995     ld-&gt;_control_dependency = UnknownControl;
</pre>
<hr />
<pre>
1083         // the same pointer-and-offset that we stored to.
1084         // Casted version may carry a dependency and it is respected.
1085         // Thus, we are able to replace L by V.
1086       }
1087       // Now prove that we have a LoadQ matched to a StoreQ, for some Q.
1088       if (store_Opcode() != st-&gt;Opcode())
1089         return NULL;
1090       return st-&gt;in(MemNode::ValueIn);
1091     }
1092 
1093     // A load from a freshly-created object always returns zero.
1094     // (This can happen after LoadNode::Ideal resets the load&#39;s memory input
1095     // to find_captured_store, which returned InitializeNode::zero_memory.)
1096     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
1097         (st-&gt;in(0) == ld_alloc) &amp;&amp;
1098         (ld_off &gt;= st-&gt;in(0)-&gt;as_Allocate()-&gt;minimum_header_size())) {
1099       // return a zero value for the load&#39;s basic type
1100       // (This is one of the few places where a generic PhaseTransform
1101       // can create new nodes.  Think of it as lazily manifesting
1102       // virtually pre-existing constants.)
<span class="line-added">1103       assert(memory_type() != T_INLINE_TYPE, &quot;should not be used for inline types&quot;);</span>
<span class="line-added">1104       Node* default_value = ld_alloc-&gt;in(AllocateNode::DefaultValue);</span>
<span class="line-added">1105       if (default_value != NULL) {</span>
<span class="line-added">1106         return default_value;</span>
<span class="line-added">1107       }</span>
<span class="line-added">1108       assert(ld_alloc-&gt;in(AllocateNode::RawDefaultValue) == NULL, &quot;default value may not be null&quot;);</span>
1109       return phase-&gt;zerocon(memory_type());
1110     }
1111 
1112     // A load from an initialization barrier can match a captured store.
1113     if (st-&gt;is_Proj() &amp;&amp; st-&gt;in(0)-&gt;is_Initialize()) {
1114       InitializeNode* init = st-&gt;in(0)-&gt;as_Initialize();
1115       AllocateNode* alloc = init-&gt;allocation();
1116       if ((alloc != NULL) &amp;&amp; (alloc == ld_alloc)) {
1117         // examine a captured store value
1118         st = init-&gt;find_captured_store(ld_off, memory_size(), phase);
1119         if (st != NULL) {
1120           continue;             // take one more trip around
1121         }
1122       }
1123     }
1124 
1125     // Load boxed value from result of valueOf() call is input parameter.
1126     if (this-&gt;is_Load() &amp;&amp; ld_adr-&gt;is_AddP() &amp;&amp;
1127         (tp != NULL) &amp;&amp; tp-&gt;is_ptr_to_boxed_value()) {
1128       intptr_t ignore = 0;
</pre>
<hr />
<pre>
1146 //----------------------is_instance_field_load_with_local_phi------------------
1147 bool LoadNode::is_instance_field_load_with_local_phi(Node* ctrl) {
1148   if( in(Memory)-&gt;is_Phi() &amp;&amp; in(Memory)-&gt;in(0) == ctrl &amp;&amp;
1149       in(Address)-&gt;is_AddP() ) {
1150     const TypeOopPtr* t_oop = in(Address)-&gt;bottom_type()-&gt;isa_oopptr();
1151     // Only instances and boxed values.
1152     if( t_oop != NULL &amp;&amp;
1153         (t_oop-&gt;is_ptr_to_boxed_value() ||
1154          t_oop-&gt;is_known_instance_field()) &amp;&amp;
1155         t_oop-&gt;offset() != Type::OffsetBot &amp;&amp;
1156         t_oop-&gt;offset() != Type::OffsetTop) {
1157       return true;
1158     }
1159   }
1160   return false;
1161 }
1162 
1163 //------------------------------Identity---------------------------------------
1164 // Loads are identity if previous store is to same address
1165 Node* LoadNode::Identity(PhaseGVN* phase) {
<span class="line-added">1166   // Loading from an InlineTypePtr? The InlineTypePtr has the values of</span>
<span class="line-added">1167   // all fields as input. Look for the field with matching offset.</span>
<span class="line-added">1168   Node* addr = in(Address);</span>
<span class="line-added">1169   intptr_t offset;</span>
<span class="line-added">1170   Node* base = AddPNode::Ideal_base_and_offset(addr, phase, offset);</span>
<span class="line-added">1171   if (base != NULL &amp;&amp; base-&gt;is_InlineTypePtr() &amp;&amp; offset &gt; oopDesc::klass_offset_in_bytes()) {</span>
<span class="line-added">1172     Node* value = base-&gt;as_InlineTypePtr()-&gt;field_value_by_offset((int)offset, true);</span>
<span class="line-added">1173     if (value-&gt;is_InlineType()) {</span>
<span class="line-added">1174       // Non-flattened inline type field</span>
<span class="line-added">1175       InlineTypeNode* vt = value-&gt;as_InlineType();</span>
<span class="line-added">1176       if (vt-&gt;is_allocated(phase)) {</span>
<span class="line-added">1177         value = vt-&gt;get_oop();</span>
<span class="line-added">1178       } else {</span>
<span class="line-added">1179         // Not yet allocated, bail out</span>
<span class="line-added">1180         value = NULL;</span>
<span class="line-added">1181       }</span>
<span class="line-added">1182     }</span>
<span class="line-added">1183     if (value != NULL) {</span>
<span class="line-added">1184       if (Opcode() == Op_LoadN) {</span>
<span class="line-added">1185         // Encode oop value if we are loading a narrow oop</span>
<span class="line-added">1186         assert(!phase-&gt;type(value)-&gt;isa_narrowoop(), &quot;should already be decoded&quot;);</span>
<span class="line-added">1187         value = phase-&gt;transform(new EncodePNode(value, bottom_type()));</span>
<span class="line-added">1188       }</span>
<span class="line-added">1189       return value;</span>
<span class="line-added">1190     }</span>
<span class="line-added">1191   }</span>
<span class="line-added">1192 </span>
1193   // If the previous store-maker is the right kind of Store, and the store is
1194   // to the same address, then we are equal to the value stored.
1195   Node* mem = in(Memory);
1196   Node* value = can_see_stored_value(mem, phase);
1197   if( value ) {
1198     // byte, short &amp; char stores truncate naturally.
1199     // A load has to load the truncated value which requires
1200     // some sort of masking operation and that requires an
1201     // Ideal call instead of an Identity call.
1202     if (memory_size() &lt; BytesPerInt) {
1203       // If the input to the store does not fit with the load&#39;s result type,
1204       // it must be truncated via an Ideal call.
1205       if (!phase-&gt;type(value)-&gt;higher_equal(phase-&gt;type(this)))
1206         return this;
1207     }
1208     // (This works even when value is a Con, but LoadNode::Value
1209     // usually runs first, producing the singleton type of the Con.)
1210     return value;
1211   }
1212 
</pre>
<hr />
<pre>
1730   // fold up, do so.
1731   Node* prev_mem = find_previous_store(phase);
1732   if (prev_mem != NULL) {
1733     Node* value = can_see_arraycopy_value(prev_mem, phase);
1734     if (value != NULL) {
1735       return value;
1736     }
1737   }
1738   // Steps (a), (b):  Walk past independent stores to find an exact match.
1739   if (prev_mem != NULL &amp;&amp; prev_mem != in(MemNode::Memory)) {
1740     // (c) See if we can fold up on the spot, but don&#39;t fold up here.
1741     // Fold-up might require truncation (for LoadB/LoadS/LoadUS) or
1742     // just return a prior value, which is done by Identity calls.
1743     if (can_see_stored_value(prev_mem, phase)) {
1744       // Make ready for step (d):
1745       set_req(MemNode::Memory, prev_mem);
1746       return this;
1747     }
1748   }
1749 
<span class="line-modified">1750   AllocateNode* alloc = AllocateNode::Ideal_allocation(address, phase);</span>
<span class="line-modified">1751   if (alloc != NULL &amp;&amp; mem-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1752       mem-&gt;in(0) != NULL &amp;&amp;</span>
<span class="line-added">1753       mem-&gt;in(0) == alloc-&gt;initialization() &amp;&amp;</span>
<span class="line-added">1754       Opcode() == Op_LoadX &amp;&amp;</span>
<span class="line-added">1755       alloc-&gt;initialization()-&gt;proj_out_or_null(0) != NULL) {</span>
1756     InitializeNode* init = alloc-&gt;initialization();
1757     Node* control = init-&gt;proj_out(0);
<span class="line-modified">1758     return alloc-&gt;make_ideal_mark(phase, control, mem);</span>
1759   }
1760 
1761   return progress ? this : NULL;
1762 }
1763 
1764 // Helper to recognize certain Klass fields which are invariant across
1765 // some group of array types (e.g., int[] or all T[] where T &lt; Object).
1766 const Type*
1767 LoadNode::load_array_final_field(const TypeKlassPtr *tkls,
1768                                  ciKlass* klass) const {
1769   if (tkls-&gt;offset() == in_bytes(Klass::modifier_flags_offset())) {
1770     // The field is Klass::_modifier_flags.  Return its (constant) value.
1771     // (Folds up the 2nd indirection in aClassConstant.getModifiers().)
1772     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _modifier_flags&quot;);
1773     return TypeInt::make(klass-&gt;modifier_flags());
1774   }
1775   if (tkls-&gt;offset() == in_bytes(Klass::access_flags_offset())) {
1776     // The field is Klass::_access_flags.  Return its (constant) value.
1777     // (Folds up the 2nd indirection in Reflection.getClassAccessFlags(aClassConstant).)
1778     assert(this-&gt;Opcode() == Op_LoadI, &quot;must load an int from _access_flags&quot;);
</pre>
<hr />
<pre>
1830       }
1831     }
1832 
1833     // Don&#39;t do this for integer types. There is only potential profit if
1834     // the element type t is lower than _type; that is, for int types, if _type is
1835     // more restrictive than t.  This only happens here if one is short and the other
1836     // char (both 16 bits), and in those cases we&#39;ve made an intentional decision
1837     // to use one kind of load over the other. See AndINode::Ideal and 4965907.
1838     // Also, do not try to narrow the type for a LoadKlass, regardless of offset.
1839     //
1840     // Yes, it is possible to encounter an expression like (LoadKlass p1:(AddP x x 8))
1841     // where the _gvn.type of the AddP is wider than 8.  This occurs when an earlier
1842     // copy p0 of (AddP x x 8) has been proven equal to p1, and the p0 has been
1843     // subsumed by p1.  If p1 is on the worklist but has not yet been re-transformed,
1844     // it is possible that p1 will have a type like Foo*[int+]:NotNull*+any.
1845     // In fact, that could have been the original type of p1, and p1 could have
1846     // had an original form like p1:(AddP x x (LShiftL quux 3)), where the
1847     // expression (LShiftL quux 3) independently optimized to the constant 8.
1848     if ((t-&gt;isa_int() == NULL) &amp;&amp; (t-&gt;isa_long() == NULL)
1849         &amp;&amp; (_type-&gt;isa_vect() == NULL)
<span class="line-added">1850         &amp;&amp; t-&gt;isa_inlinetype() == NULL</span>
1851         &amp;&amp; Opcode() != Op_LoadKlass &amp;&amp; Opcode() != Op_LoadNKlass) {
1852       // t might actually be lower than _type, if _type is a unique
1853       // concrete subclass of abstract class t.
1854       if (off_beyond_header || off == Type::OffsetBot) {  // is the offset beyond the header?
1855         const Type* jt = t-&gt;join_speculative(_type);
1856         // In any case, do not allow the join, per se, to empty out the type.
1857         if (jt-&gt;empty() &amp;&amp; !t-&gt;empty()) {
1858           // This can happen if a interface-typed array narrows to a class type.
1859           jt = _type;
1860         }
1861 #ifdef ASSERT
1862         if (phase-&gt;C-&gt;eliminate_boxing() &amp;&amp; adr-&gt;is_AddP()) {
1863           // The pointers in the autobox arrays are always non-null
1864           Node* base = adr-&gt;in(AddPNode::Base);
1865           if ((base != NULL) &amp;&amp; base-&gt;is_DecodeN()) {
1866             // Get LoadN node which loads IntegerCache.cache field
1867             base = base-&gt;in(1);
1868           }
1869           if ((base != NULL) &amp;&amp; base-&gt;is_Con()) {
1870             const TypeAryPtr* base_type = base-&gt;bottom_type()-&gt;isa_aryptr();
1871             if ((base_type != NULL) &amp;&amp; base_type-&gt;is_autobox_cache()) {
1872               // It could be narrow oop
1873               assert(jt-&gt;make_ptr()-&gt;ptr() == TypePtr::NotNull,&quot;sanity&quot;);
1874             }
1875           }
1876         }
1877 #endif
1878         return jt;
1879       }
1880     }
1881   } else if (tp-&gt;base() == Type::InstPtr) {
1882     assert( off != Type::OffsetBot ||
1883             // arrays can be cast to Objects
1884             tp-&gt;is_oopptr()-&gt;klass()-&gt;is_java_lang_Object() ||
<span class="line-added">1885             tp-&gt;is_oopptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass() ||</span>
1886             // unsafe field access may not have a constant offset
1887             C-&gt;has_unsafe_access(),
1888             &quot;Field accesses must be precise&quot; );
1889     // For oop loads, we expect the _type to be precise.
1890 
<span class="line-modified">1891     const TypeInstPtr* tinst = tp-&gt;is_instptr();</span>
<span class="line-added">1892     BasicType bt = memory_type();</span>
<span class="line-added">1893 </span>
1894     // Optimize loads from constant fields.
1895     ciObject* const_oop = tinst-&gt;const_oop();
1896     if (!is_mismatched_access() &amp;&amp; off != Type::OffsetBot &amp;&amp; const_oop != NULL &amp;&amp; const_oop-&gt;is_instance()) {
<span class="line-modified">1897       ciType* mirror_type = const_oop-&gt;as_instance()-&gt;java_mirror_type();</span>
<span class="line-added">1898       if (mirror_type != NULL &amp;&amp; mirror_type-&gt;is_inlinetype()) {</span>
<span class="line-added">1899         ciInlineKlass* vk = mirror_type-&gt;as_inline_klass();</span>
<span class="line-added">1900         if (off == vk-&gt;default_value_offset()) {</span>
<span class="line-added">1901           // Loading a special hidden field that contains the oop of the default inline type</span>
<span class="line-added">1902           const Type* const_oop = TypeInstPtr::make(vk-&gt;default_instance());</span>
<span class="line-added">1903           return (bt == T_NARROWOOP) ? const_oop-&gt;make_narrowoop() : const_oop;</span>
<span class="line-added">1904         }</span>
<span class="line-added">1905       }</span>
<span class="line-added">1906       const Type* con_type = Type::make_constant_from_field(const_oop-&gt;as_instance(), off, is_unsigned(), bt);</span>
1907       if (con_type != NULL) {
1908         return con_type;
1909       }
1910     }
1911   } else if (tp-&gt;base() == Type::KlassPtr) {
1912     assert( off != Type::OffsetBot ||
1913             // arrays can be cast to Objects
<span class="line-added">1914             tp-&gt;is_klassptr()-&gt;klass() == NULL ||</span>
1915             tp-&gt;is_klassptr()-&gt;klass()-&gt;is_java_lang_Object() ||
1916             // also allow array-loading from the primary supertype
1917             // array during subtype checks
1918             Opcode() == Op_LoadKlass,
1919             &quot;Field accesses must be precise&quot; );
1920     // For klass/static loads, we expect the _type to be precise
<span class="line-modified">1921   } else if (tp-&gt;base() == Type::RawPtr &amp;&amp; !StressReflectiveCode) {</span>
<span class="line-modified">1922     if (adr-&gt;is_Load() &amp;&amp; off == 0) {</span>
<span class="line-modified">1923       /* With mirrors being an indirect in the Klass*</span>
<span class="line-modified">1924        * the VM is now using two loads. LoadKlass(LoadP(LoadP(Klass, mirror_offset), zero_offset))</span>
<span class="line-modified">1925        * The LoadP from the Klass has a RawPtr type (see LibraryCallKit::load_mirror_from_klass).</span>
<span class="line-modified">1926        *</span>
<span class="line-modified">1927        * So check the type and klass of the node before the LoadP.</span>
<span class="line-modified">1928        */</span>
<span class="line-modified">1929       Node* adr2 = adr-&gt;in(MemNode::Address);</span>
<span class="line-modified">1930       const TypeKlassPtr* tkls = phase-&gt;type(adr2)-&gt;isa_klassptr();</span>
<span class="line-modified">1931       if (tkls != NULL) {</span>
<span class="line-modified">1932         ciKlass* klass = tkls-&gt;klass();</span>
<span class="line-modified">1933         if (klass != NULL &amp;&amp; klass-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;offset() == in_bytes(Klass::java_mirror_offset())) {</span>
<span class="line-modified">1934           assert(adr-&gt;Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-modified">1935           assert(Opcode() == Op_LoadP, &quot;must load an oop from _java_mirror&quot;);</span>
<span class="line-added">1936           return TypeInstPtr::make(klass-&gt;java_mirror());</span>
<span class="line-added">1937         }</span>
<span class="line-added">1938       }</span>
<span class="line-added">1939     } else {</span>
<span class="line-added">1940       // Check for a load of the default value offset from the InlineKlassFixedBlock:</span>
<span class="line-added">1941       // LoadI(LoadP(inline_klass, adr_inlineklass_fixed_block_offset), default_value_offset_offset)</span>
<span class="line-added">1942       intptr_t offset = 0;</span>
<span class="line-added">1943       Node* base = AddPNode::Ideal_base_and_offset(adr, phase, offset);</span>
<span class="line-added">1944       if (base != NULL &amp;&amp; base-&gt;is_Load() &amp;&amp; offset == in_bytes(InlineKlass::default_value_offset_offset())) {</span>
<span class="line-added">1945         const TypeKlassPtr* tkls = phase-&gt;type(base-&gt;in(MemNode::Address))-&gt;isa_klassptr();</span>
<span class="line-added">1946         if (tkls != NULL &amp;&amp; tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp; tkls-&gt;isa_inlinetype() &amp;&amp;</span>
<span class="line-added">1947             tkls-&gt;offset() == in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())) {</span>
<span class="line-added">1948           assert(base-&gt;Opcode() == Op_LoadP, &quot;must load an oop from klass&quot;);</span>
<span class="line-added">1949           assert(Opcode() == Op_LoadI, &quot;must load an int from fixed block&quot;);</span>
<span class="line-added">1950           return TypeInt::make(tkls-&gt;klass()-&gt;as_inline_klass()-&gt;default_value_offset());</span>
<span class="line-added">1951         }</span>
1952       }
1953     }
1954   }
1955 
1956   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
1957   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
1958     ciKlass* klass = tkls-&gt;klass();
<span class="line-modified">1959     if (tkls-&gt;is_loaded() &amp;&amp; tkls-&gt;klass_is_exact()) {</span>
1960       // We are loading a field from a Klass metaobject whose identity
1961       // is known at compile time (the type is &quot;exact&quot; or &quot;precise&quot;).
1962       // Check for fields we know are maintained as constants by the VM.
1963       if (tkls-&gt;offset() == in_bytes(Klass::super_check_offset_offset())) {
1964         // The field is Klass::_super_check_offset.  Return its (constant) value.
1965         // (Folds up type checking code.)
1966         assert(Opcode() == Op_LoadI, &quot;must load an int from _super_check_offset&quot;);
1967         return TypeInt::make(klass-&gt;super_check_offset());
1968       }
1969       // Compute index into primary_supers array
1970       juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1971       // Check for overflowing; use unsigned compare to handle the negative case.
1972       if( depth &lt; ciKlass::primary_super_limit() ) {
1973         // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1974         // (Folds up type checking code.)
1975         assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
1976         ciKlass *ss = klass-&gt;super_of_depth(depth);
1977         return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
1978       }
1979       const Type* aift = load_array_final_field(tkls, klass);
1980       if (aift != NULL)  return aift;
1981     }
1982 
1983     // We can still check if we are loading from the primary_supers array at a
1984     // shallow enough depth.  Even though the klass is not exact, entries less
1985     // than or equal to its super depth are correct.
<span class="line-modified">1986     if (tkls-&gt;is_loaded()) {</span>
1987       ciType *inner = klass;
1988       while( inner-&gt;is_obj_array_klass() )
1989         inner = inner-&gt;as_obj_array_klass()-&gt;base_element_type();
1990       if( inner-&gt;is_instance_klass() &amp;&amp;
1991           !inner-&gt;as_instance_klass()-&gt;flags().is_interface() ) {
1992         // Compute index into primary_supers array
1993         juint depth = (tkls-&gt;offset() - in_bytes(Klass::primary_supers_offset())) / sizeof(Klass*);
1994         // Check for overflowing; use unsigned compare to handle the negative case.
1995         if( depth &lt; ciKlass::primary_super_limit() &amp;&amp;
1996             depth &lt;= klass-&gt;super_depth() ) { // allow self-depth checks to handle self-check case
1997           // The field is an element of Klass::_primary_supers.  Return its (constant) value.
1998           // (Folds up type checking code.)
1999           assert(Opcode() == Op_LoadKlass, &quot;must load a klass from _primary_supers&quot;);
2000           ciKlass *ss = klass-&gt;super_of_depth(depth);
2001           return ss ? TypeKlassPtr::make(ss) : TypePtr::NULL_PTR;
2002         }
2003       }
2004     }
2005 
2006     // If the type is enough to determine that the thing is not an array,
</pre>
<hr />
<pre>
2171   return LoadNode::Ideal(phase, can_reshape);
2172 }
2173 
2174 const Type* LoadSNode::Value(PhaseGVN* phase) const {
2175   Node* mem = in(MemNode::Memory);
2176   Node* value = can_see_stored_value(mem,phase);
2177   if (value != NULL &amp;&amp; value-&gt;is_Con() &amp;&amp;
2178       !value-&gt;bottom_type()-&gt;higher_equal(_type)) {
2179     // If the input to the store does not fit with the load&#39;s result type,
2180     // it must be truncated. We can&#39;t delay until Ideal call since
2181     // a singleton Value is needed for split_thru_phi optimization.
2182     int con = value-&gt;get_int();
2183     return TypeInt::make((con &lt;&lt; 16) &gt;&gt; 16);
2184   }
2185   return LoadNode::Value(phase);
2186 }
2187 
2188 //=============================================================================
2189 //----------------------------LoadKlassNode::make------------------------------
2190 // Polymorphic factory method:
<span class="line-modified">2191 Node* LoadKlassNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* at,</span>
<span class="line-added">2192                           const TypeKlassPtr* tk) {</span>
2193   // sanity check the alias category against the created node type
2194   const TypePtr *adr_type = adr-&gt;bottom_type()-&gt;isa_ptr();
2195   assert(adr_type != NULL, &quot;expecting TypeKlassPtr&quot;);
2196 #ifdef _LP64
2197   if (adr_type-&gt;is_ptr_to_narrowklass()) {
2198     assert(UseCompressedClassPointers, &quot;no compressed klasses&quot;);
2199     Node* load_klass = gvn.transform(new LoadNKlassNode(ctl, mem, adr, at, tk-&gt;make_narrowklass(), MemNode::unordered));
2200     return new DecodeNKlassNode(load_klass, load_klass-&gt;bottom_type()-&gt;make_ptr());
2201   }
2202 #endif
2203   assert(!adr_type-&gt;is_ptr_to_narrowklass() &amp;&amp; !adr_type-&gt;is_ptr_to_narrowoop(), &quot;should have got back a narrow oop&quot;);
2204   return new LoadKlassNode(ctl, mem, adr, at, tk, MemNode::unordered);
2205 }
2206 
2207 //------------------------------Value------------------------------------------
2208 const Type* LoadKlassNode::Value(PhaseGVN* phase) const {
2209   return klass_value_common(phase);
2210 }
2211 
2212 // In most cases, LoadKlassNode does not have the control input set. If the control
</pre>
<hr />
<pre>
2259     }
2260     if( !ik-&gt;is_loaded() )
2261       return _type;             // Bail out if not loaded
2262     if (offset == oopDesc::klass_offset_in_bytes()) {
2263       if (tinst-&gt;klass_is_exact()) {
2264         return TypeKlassPtr::make(ik);
2265       }
2266       // See if we can become precise: no subklasses and no interface
2267       // (Note:  We need to support verified interfaces.)
2268       if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2269         // Add a dependence; if any subclass added we need to recompile
2270         if (!ik-&gt;is_final()) {
2271           // %%% should use stronger assert_unique_concrete_subtype instead
2272           phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2273         }
2274         // Return precise klass
2275         return TypeKlassPtr::make(ik);
2276       }
2277 
2278       // Return root of possible klass
<span class="line-modified">2279       return TypeKlassPtr::make(TypePtr::NotNull, ik, Type::Offset(0), tinst-&gt;flat_array());</span>
2280     }
2281   }
2282 
2283   // Check for loading klass from an array
2284   const TypeAryPtr *tary = tp-&gt;isa_aryptr();
<span class="line-modified">2285   if (tary != NULL) {</span>
2286     ciKlass *tary_klass = tary-&gt;klass();
2287     if (tary_klass != NULL   // can be NULL when at BOTTOM or TOP
2288         &amp;&amp; tary-&gt;offset() == oopDesc::klass_offset_in_bytes()) {
2289       if (tary-&gt;klass_is_exact()) {
2290         return TypeKlassPtr::make(tary_klass);
2291       }
<span class="line-modified">2292       ciArrayKlass* ak = tary_klass-&gt;as_array_klass();</span>
2293       // If the klass is an object array, we defer the question to the
2294       // array component klass.
<span class="line-modified">2295       if (ak-&gt;is_obj_array_klass()) {</span>
<span class="line-modified">2296         assert(ak-&gt;is_loaded(), &quot;&quot;);</span>
2297         ciKlass *base_k = ak-&gt;as_obj_array_klass()-&gt;base_element_klass();
<span class="line-modified">2298         if (base_k-&gt;is_loaded() &amp;&amp; base_k-&gt;is_instance_klass()) {</span>
<span class="line-modified">2299           ciInstanceKlass *ik = base_k-&gt;as_instance_klass();</span>
2300           // See if we can become precise: no subklasses and no interface
2301           if (!ik-&gt;is_interface() &amp;&amp; !ik-&gt;has_subklass()) {
2302             // Add a dependence; if any subclass added we need to recompile
2303             if (!ik-&gt;is_final()) {
2304               phase-&gt;C-&gt;dependencies()-&gt;assert_leaf_type(ik);
2305             }
2306             // Return precise array klass
2307             return TypeKlassPtr::make(ak);
2308           }
2309         }
<span class="line-modified">2310         return TypeKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), false);</span>
<span class="line-modified">2311       } else if (ak-&gt;is_type_array_klass()) {</span>
<span class="line-modified">2312         //assert(!UseExactTypes, &quot;this code should be useless with exact types&quot;);</span>
2313         return TypeKlassPtr::make(ak); // These are always precise
2314       }
2315     }
2316   }
2317 
2318   // Check for loading klass from an array klass
2319   const TypeKlassPtr *tkls = tp-&gt;isa_klassptr();
2320   if (tkls != NULL &amp;&amp; !StressReflectiveCode) {
<span class="line-modified">2321     if (!tkls-&gt;is_loaded()) {</span>

2322       return _type;             // Bail out if not loaded
<span class="line-added">2323     }</span>
<span class="line-added">2324     ciKlass* klass = tkls-&gt;klass();</span>
2325     if( klass-&gt;is_obj_array_klass() &amp;&amp;
2326         tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {
2327       ciKlass* elem = klass-&gt;as_obj_array_klass()-&gt;element_klass();
2328       // // Always returning precise element type is incorrect,
2329       // // e.g., element type could be object and array may contain strings
2330       // return TypeKlassPtr::make(TypePtr::Constant, elem, 0);
2331 
2332       // The array&#39;s TypeKlassPtr was declared &#39;precise&#39; or &#39;not precise&#39;
2333       // according to the element type&#39;s subclassing.
<span class="line-modified">2334       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), elem-&gt;flatten_array());</span>
<span class="line-added">2335     } else if (klass-&gt;is_flat_array_klass() &amp;&amp;</span>
<span class="line-added">2336                tkls-&gt;offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {</span>
<span class="line-added">2337       ciKlass* elem = klass-&gt;as_flat_array_klass()-&gt;element_klass();</span>
<span class="line-added">2338       return TypeKlassPtr::make(tkls-&gt;ptr(), elem, Type::Offset(0), /* flat_array= */ true);</span>
2339     }
2340     if( klass-&gt;is_instance_klass() &amp;&amp; tkls-&gt;klass_is_exact() &amp;&amp;
2341         tkls-&gt;offset() == in_bytes(Klass::super_offset())) {
2342       ciKlass* sup = klass-&gt;as_instance_klass()-&gt;super();
2343       // The field is Klass::_super.  Return its (constant) value.
2344       // (Folds up the 2nd indirection in aClassConstant.getSuperClass().)
2345       return sup ? TypeKlassPtr::make(sup) : TypePtr::NULL_PTR;
2346     }
2347   }
2348 
2349   // Bailout case
2350   return LoadNode::Value(phase);
2351 }
2352 
2353 //------------------------------Identity---------------------------------------
2354 // To clean up reflective code, simplify k.java_mirror.as_klass to plain k.
2355 // Also feed through the klass in Allocate(...klass...)._klass.
2356 Node* LoadKlassNode::Identity(PhaseGVN* phase) {
2357   return klass_identity_common(phase);
2358 }
</pre>
<hr />
<pre>
2526 //=============================================================================
2527 //---------------------------StoreNode::make-----------------------------------
2528 // Polymorphic factory method:
2529 StoreNode* StoreNode::make(PhaseGVN&amp; gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {
2530   assert((mo == unordered || mo == release), &quot;unexpected&quot;);
2531   Compile* C = gvn.C;
2532   assert(C-&gt;get_alias_index(adr_type) != Compile::AliasIdxRaw ||
2533          ctl != NULL, &quot;raw memory operations should have control edge&quot;);
2534 
2535   switch (bt) {
2536   case T_BOOLEAN: val = gvn.transform(new AndINode(val, gvn.intcon(0x1))); // Fall through to T_BYTE case
2537   case T_BYTE:    return new StoreBNode(ctl, mem, adr, adr_type, val, mo);
2538   case T_INT:     return new StoreINode(ctl, mem, adr, adr_type, val, mo);
2539   case T_CHAR:
2540   case T_SHORT:   return new StoreCNode(ctl, mem, adr, adr_type, val, mo);
2541   case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);
2542   case T_FLOAT:   return new StoreFNode(ctl, mem, adr, adr_type, val, mo);
2543   case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);
2544   case T_METADATA:
2545   case T_ADDRESS:
<span class="line-added">2546   case T_INLINE_TYPE:</span>
2547   case T_OBJECT:
2548 #ifdef _LP64
2549     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2550       val = gvn.transform(new EncodePNode(val, val-&gt;bottom_type()-&gt;make_narrowoop()));
2551       return new StoreNNode(ctl, mem, adr, adr_type, val, mo);
2552     } else if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowklass() ||
2553                (UseCompressedClassPointers &amp;&amp; val-&gt;bottom_type()-&gt;isa_klassptr() &amp;&amp;
2554                 adr-&gt;bottom_type()-&gt;isa_rawptr())) {
2555       val = gvn.transform(new EncodePKlassNode(val, val-&gt;bottom_type()-&gt;make_narrowklass()));
2556       return new StoreNKlassNode(ctl, mem, adr, adr_type, val, mo);
2557     }
2558 #endif
2559     {
2560       return new StorePNode(ctl, mem, adr, adr_type, val, mo);
2561     }
2562   default:
2563     ShouldNotReachHere();
2564     return (StoreNode*)NULL;
2565   }
2566 }
</pre>
<hr />
<pre>
2587   //return (uintptr_t)in(Control) + (uintptr_t)in(Memory) + (uintptr_t)in(Address) + (uintptr_t)in(ValueIn);
2588 
2589   // Since they are not commoned, do not hash them:
2590   return NO_HASH;
2591 }
2592 
2593 //------------------------------Ideal------------------------------------------
2594 // Change back-to-back Store(, p, x) -&gt; Store(m, p, y) to Store(m, p, x).
2595 // When a store immediately follows a relevant allocation/initialization,
2596 // try to capture it into the initialization, or hoist it above.
2597 Node *StoreNode::Ideal(PhaseGVN *phase, bool can_reshape) {
2598   Node* p = MemNode::Ideal_common(phase, can_reshape);
2599   if (p)  return (p == NodeSentinel) ? NULL : p;
2600 
2601   Node* mem     = in(MemNode::Memory);
2602   Node* address = in(MemNode::Address);
2603   // Back-to-back stores to same address?  Fold em up.  Generally
2604   // unsafe if I have intervening uses...  Also disallowed for StoreCM
2605   // since they must follow each StoreP operation.  Redundant StoreCMs
2606   // are eliminated just before matching in final_graph_reshape.
<span class="line-modified">2607   if (phase-&gt;C-&gt;get_adr_type(phase-&gt;C-&gt;get_alias_index(adr_type())) != TypeAryPtr::INLINES) {</span>
2608     Node* st = mem;
2609     // If Store &#39;st&#39; has more than one use, we cannot fold &#39;st&#39; away.
2610     // For example, &#39;st&#39; might be the final state at a conditional
2611     // return.  Or, &#39;st&#39; might be used by some node which is live at
2612     // the same time &#39;st&#39; is live, which might be unschedulable.  So,
2613     // require exactly ONE user until such time as we clone &#39;mem&#39; for
2614     // each of &#39;mem&#39;s uses (thus making the exactly-1-user-rule hold
2615     // true).
2616     while (st-&gt;is_Store() &amp;&amp; st-&gt;outcnt() == 1 &amp;&amp; st-&gt;Opcode() != Op_StoreCM) {
2617       // Looking at a dead closed cycle of memory?
2618       assert(st != st-&gt;in(MemNode::Memory), &quot;dead loop in StoreNode::Ideal&quot;);
2619       assert(Opcode() == st-&gt;Opcode() ||
2620              st-&gt;Opcode() == Op_StoreVector ||
2621              Opcode() == Op_StoreVector ||
2622              phase-&gt;C-&gt;get_alias_index(adr_type()) == Compile::AliasIdxRaw ||
2623              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreI) || // expanded ClearArrayNode
2624              (Opcode() == Op_StoreI &amp;&amp; st-&gt;Opcode() == Op_StoreL) || // initialization by arraycopy
<span class="line-added">2625              (Opcode() == Op_StoreL &amp;&amp; st-&gt;Opcode() == Op_StoreN) ||</span>
2626              (is_mismatched_access() || st-&gt;as_Store()-&gt;is_mismatched_access()),
2627              &quot;no mismatched stores, except on raw memory: %s %s&quot;, NodeClassNames[Opcode()], NodeClassNames[st-&gt;Opcode()]);
2628 
2629       if (st-&gt;in(MemNode::Address)-&gt;eqv_uncast(address) &amp;&amp;
2630           st-&gt;as_Store()-&gt;memory_size() &lt;= this-&gt;memory_size()) {
2631         Node* use = st-&gt;raw_out(0);
2632         phase-&gt;igvn_rehash_node_delayed(use);
2633         if (can_reshape) {
2634           use-&gt;set_req_X(MemNode::Memory, st-&gt;in(MemNode::Memory), phase-&gt;is_IterGVN());
2635         } else {
2636           // It&#39;s OK to do this in the parser, since DU info is always accurate,
2637           // and the parser always refers to nodes via SafePointNode maps.
2638           use-&gt;set_req(MemNode::Memory, st-&gt;in(MemNode::Memory));
2639         }
2640         return this;
2641       }
2642       st = st-&gt;in(MemNode::Memory);
2643     }
2644   }
2645 
</pre>
<hr />
<pre>
2691   // Load then Store?  Then the Store is useless
2692   if (val-&gt;is_Load() &amp;&amp;
2693       val-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2694       val-&gt;in(MemNode::Memory )-&gt;eqv_uncast(mem) &amp;&amp;
2695       val-&gt;as_Load()-&gt;store_Opcode() == Opcode()) {
2696     result = mem;
2697   }
2698 
2699   // Two stores in a row of the same value?
2700   if (result == this &amp;&amp;
2701       mem-&gt;is_Store() &amp;&amp;
2702       mem-&gt;in(MemNode::Address)-&gt;eqv_uncast(adr) &amp;&amp;
2703       mem-&gt;in(MemNode::ValueIn)-&gt;eqv_uncast(val) &amp;&amp;
2704       mem-&gt;Opcode() == Opcode()) {
2705     result = mem;
2706   }
2707 
2708   // Store of zero anywhere into a freshly-allocated object?
2709   // Then the store is useless.
2710   // (It must already have been captured by the InitializeNode.)
<span class="line-modified">2711   if (result == this &amp;&amp; ReduceFieldZeroing) {</span>

2712     // a newly allocated object is already all-zeroes everywhere
<span class="line-modified">2713     if (mem-&gt;is_Proj() &amp;&amp; mem-&gt;in(0)-&gt;is_Allocate() &amp;&amp;</span>
<span class="line-added">2714         (phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == val)) {</span>
<span class="line-added">2715       assert(!phase-&gt;type(val)-&gt;is_zero_type() || mem-&gt;in(0)-&gt;in(AllocateNode::DefaultValue) == NULL, &quot;storing null to inline type array is forbidden&quot;);</span>
2716       result = mem;
2717     }
2718 
2719     if (result == this) {
2720       // the store may also apply to zero-bits in an earlier object
2721       Node* prev_mem = find_previous_store(phase);
2722       // Steps (a), (b):  Walk past independent stores to find an exact match.
2723       if (prev_mem != NULL) {
2724         Node* prev_val = can_see_stored_value(prev_mem, phase);
2725         if (prev_val != NULL &amp;&amp; phase-&gt;eqv(prev_val, val)) {
2726           // prev_val and val might differ by a cast; it would be good
2727           // to keep the more informative of the two.
<span class="line-modified">2728           if (phase-&gt;type(val)-&gt;is_zero_type()) {</span>
<span class="line-added">2729             result = mem;</span>
<span class="line-added">2730           } else if (prev_mem-&gt;is_Proj() &amp;&amp; prev_mem-&gt;in(0)-&gt;is_Initialize()) {</span>
<span class="line-added">2731             InitializeNode* init = prev_mem-&gt;in(0)-&gt;as_Initialize();</span>
<span class="line-added">2732             AllocateNode* alloc = init-&gt;allocation();</span>
<span class="line-added">2733             if (alloc != NULL &amp;&amp; alloc-&gt;in(AllocateNode::DefaultValue) == val) {</span>
<span class="line-added">2734               result = mem;</span>
<span class="line-added">2735             }</span>
<span class="line-added">2736           }</span>
2737         }
2738       }
2739     }
2740   }
2741 
2742   if (result != this &amp;&amp; phase-&gt;is_IterGVN() != NULL) {
2743     MemBarNode* trailing = trailing_membar();
2744     if (trailing != NULL) {
2745 #ifdef ASSERT
2746       const TypeOopPtr* t_oop = phase-&gt;type(in(Address))-&gt;isa_oopptr();
2747       assert(t_oop == NULL || t_oop-&gt;is_known_instance_field(), &quot;only for non escaping objects&quot;);
2748 #endif
2749       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
2750       trailing-&gt;remove(igvn);
2751     }
2752   }
2753 
2754   return result;
2755 }
2756 
</pre>
<hr />
<pre>
3025 // Clearing a short array is faster with stores
3026 Node *ClearArrayNode::Ideal(PhaseGVN *phase, bool can_reshape) {
3027   // Already know this is a large node, do not try to ideal it
3028   if (!IdealizeClearArrayNode || _is_large) return NULL;
3029 
3030   const int unit = BytesPerLong;
3031   const TypeX* t = phase-&gt;type(in(2))-&gt;isa_intptr_t();
3032   if (!t)  return NULL;
3033   if (!t-&gt;is_con())  return NULL;
3034   intptr_t raw_count = t-&gt;get_con();
3035   intptr_t size = raw_count;
3036   if (!Matcher::init_array_count_is_in_bytes) size *= unit;
3037   // Clearing nothing uses the Identity call.
3038   // Negative clears are possible on dead ClearArrays
3039   // (see jck test stmt114.stmt11402.val).
3040   if (size &lt;= 0 || size % unit != 0)  return NULL;
3041   intptr_t count = size / unit;
3042   // Length too long; communicate this to matchers and assemblers.
3043   // Assemblers are responsible to produce fast hardware clears for it.
3044   if (size &gt; InitArrayShortSize) {
<span class="line-modified">3045     return new ClearArrayNode(in(0), in(1), in(2), in(3), in(4), true);</span>
3046   }
3047   Node *mem = in(1);
3048   if( phase-&gt;type(mem)==Type::TOP ) return NULL;
3049   Node *adr = in(3);
3050   const Type* at = phase-&gt;type(adr);
3051   if( at==Type::TOP ) return NULL;
3052   const TypePtr* atp = at-&gt;isa_ptr();
3053   // adjust atp to be the correct array element address type
3054   if (atp == NULL)  atp = TypePtr::BOTTOM;
3055   else              atp = atp-&gt;add_offset(Type::OffsetBot);
3056   // Get base for derived pointer purposes
3057   if( adr-&gt;Opcode() != Op_AddP ) Unimplemented();
3058   Node *base = adr-&gt;in(1);
3059 
<span class="line-modified">3060   Node *val = in(4);</span>
3061   Node *off  = phase-&gt;MakeConX(BytesPerLong);
<span class="line-modified">3062   mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);</span>
3063   count--;
3064   while( count-- ) {
3065     mem = phase-&gt;transform(mem);
3066     adr = phase-&gt;transform(new AddPNode(base,adr,off));
<span class="line-modified">3067     mem = new StoreLNode(in(0), mem, adr, atp, val, MemNode::unordered, false);</span>
3068   }
3069   return mem;
3070 }
3071 
3072 //----------------------------step_through----------------------------------
3073 // Return allocation input memory edge if it is different instance
3074 // or itself if it is the one we are looking for.
3075 bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {
3076   Node* n = *np;
3077   assert(n-&gt;is_ClearArray(), &quot;sanity&quot;);
3078   intptr_t offset;
3079   AllocateNode* alloc = AllocateNode::Ideal_allocation(n-&gt;in(3), phase, offset);
3080   // This method is called only before Allocate nodes are expanded
3081   // during macro nodes expansion. Before that ClearArray nodes are
3082   // only generated in PhaseMacroExpand::generate_arraycopy() (before
3083   // Allocate nodes are expanded) which follows allocations.
3084   assert(alloc != NULL, &quot;should have allocation&quot;);
3085   if (alloc-&gt;_idx == instance_id) {
3086     // Can not bypass initialization of the instance we are looking for.
3087     return false;
3088   }
3089   // Otherwise skip it.
3090   InitializeNode* init = alloc-&gt;initialization();
3091   if (init != NULL)
3092     *np = init-&gt;in(TypeFunc::Memory);
3093   else
3094     *np = alloc-&gt;in(TypeFunc::Memory);
3095   return true;
3096 }
3097 
3098 //----------------------------clear_memory-------------------------------------
3099 // Generate code to initialize object storage to zero.
3100 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
<span class="line-added">3101                                    Node* val,</span>
<span class="line-added">3102                                    Node* raw_val,</span>
3103                                    intptr_t start_offset,
3104                                    Node* end_offset,
3105                                    PhaseGVN* phase) {
3106   intptr_t offset = start_offset;
3107 
3108   int unit = BytesPerLong;
3109   if ((offset % unit) != 0) {
3110     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(offset));
3111     adr = phase-&gt;transform(adr);
3112     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3113     if (val != NULL) {</span>
<span class="line-added">3114       assert(phase-&gt;type(val)-&gt;isa_narrowoop(), &quot;should be narrow oop&quot;);</span>
<span class="line-added">3115       mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);</span>
<span class="line-added">3116     } else {</span>
<span class="line-added">3117       assert(raw_val == NULL, &quot;val may not be null&quot;);</span>
<span class="line-added">3118       mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>
<span class="line-added">3119     }</span>
3120     mem = phase-&gt;transform(mem);
3121     offset += BytesPerInt;
3122   }
3123   assert((offset % unit) == 0, &quot;&quot;);
3124 
3125   // Initialize the remaining stuff, if any, with a ClearArray.
<span class="line-modified">3126   return clear_memory(ctl, mem, dest, raw_val, phase-&gt;MakeConX(offset), end_offset, phase);</span>
3127 }
3128 
3129 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
<span class="line-added">3130                                    Node* raw_val,</span>
3131                                    Node* start_offset,
3132                                    Node* end_offset,
3133                                    PhaseGVN* phase) {
3134   if (start_offset == end_offset) {
3135     // nothing to do
3136     return mem;
3137   }
3138 
3139   int unit = BytesPerLong;
3140   Node* zbase = start_offset;
3141   Node* zend  = end_offset;
3142 
3143   // Scale to the unit required by the CPU:
3144   if (!Matcher::init_array_count_is_in_bytes) {
3145     Node* shift = phase-&gt;intcon(exact_log2(unit));
3146     zbase = phase-&gt;transform(new URShiftXNode(zbase, shift) );
3147     zend  = phase-&gt;transform(new URShiftXNode(zend,  shift) );
3148   }
3149 
3150   // Bulk clear double-words
3151   Node* zsize = phase-&gt;transform(new SubXNode(zend, zbase) );
3152   Node* adr = phase-&gt;transform(new AddPNode(dest, dest, start_offset) );
<span class="line-modified">3153   if (raw_val == NULL) {</span>
<span class="line-added">3154     raw_val = phase-&gt;MakeConX(0);</span>
<span class="line-added">3155   }</span>
<span class="line-added">3156   mem = new ClearArrayNode(ctl, mem, zsize, adr, raw_val, false);</span>
3157   return phase-&gt;transform(mem);
3158 }
3159 
3160 Node* ClearArrayNode::clear_memory(Node* ctl, Node* mem, Node* dest,
<span class="line-added">3161                                    Node* val,</span>
<span class="line-added">3162                                    Node* raw_val,</span>
3163                                    intptr_t start_offset,
3164                                    intptr_t end_offset,
3165                                    PhaseGVN* phase) {
3166   if (start_offset == end_offset) {
3167     // nothing to do
3168     return mem;
3169   }
3170 
3171   assert((end_offset % BytesPerInt) == 0, &quot;odd end offset&quot;);
3172   intptr_t done_offset = end_offset;
3173   if ((done_offset % BytesPerLong) != 0) {
3174     done_offset -= BytesPerInt;
3175   }
3176   if (done_offset &gt; start_offset) {
<span class="line-modified">3177     mem = clear_memory(ctl, mem, dest, val, raw_val,</span>
3178                        start_offset, phase-&gt;MakeConX(done_offset), phase);
3179   }
3180   if (done_offset &lt; end_offset) { // emit the final 32-bit store
3181     Node* adr = new AddPNode(dest, dest, phase-&gt;MakeConX(done_offset));
3182     adr = phase-&gt;transform(adr);
3183     const TypePtr* atp = TypeRawPtr::BOTTOM;
<span class="line-modified">3184     if (val != NULL) {</span>
<span class="line-added">3185       assert(phase-&gt;type(val)-&gt;isa_narrowoop(), &quot;should be narrow oop&quot;);</span>
<span class="line-added">3186       mem = new StoreNNode(ctl, mem, adr, atp, val, MemNode::unordered);</span>
<span class="line-added">3187     } else {</span>
<span class="line-added">3188       assert(raw_val == NULL, &quot;val may not be null&quot;);</span>
<span class="line-added">3189       mem = StoreNode::make(*phase, ctl, mem, adr, atp, phase-&gt;zerocon(T_INT), T_INT, MemNode::unordered);</span>
<span class="line-added">3190     }</span>
3191     mem = phase-&gt;transform(mem);
3192     done_offset += BytesPerInt;
3193   }
3194   assert(done_offset == end_offset, &quot;&quot;);
3195   return mem;
3196 }
3197 
3198 //=============================================================================
3199 MemBarNode::MemBarNode(Compile* C, int alias_idx, Node* precedent)
3200   : MultiNode(TypeFunc::Parms + (precedent == NULL? 0: 1)),
3201     _adr_type(C-&gt;get_adr_type(alias_idx)), _kind(Standalone)
3202 #ifdef ASSERT
3203   , _pair_idx(0)
3204 #endif
3205 {
3206   init_class_id(Class_MemBar);
3207   Node* top = C-&gt;top();
3208   init_req(TypeFunc::I_O,top);
3209   init_req(TypeFunc::FramePtr,top);
3210   init_req(TypeFunc::ReturnAdr,top);
</pre>
<hr />
<pre>
3309       PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3310       remove(igvn);
3311       // Must return either the original node (now dead) or a new node
3312       // (Do not return a top here, since that would break the uniqueness of top.)
3313       return new ConINode(TypeInt::ZERO);
3314     }
3315   }
3316   return progress ? this : NULL;
3317 }
3318 
3319 //------------------------------Value------------------------------------------
3320 const Type* MemBarNode::Value(PhaseGVN* phase) const {
3321   if( !in(0) ) return Type::TOP;
3322   if( phase-&gt;type(in(0)) == Type::TOP )
3323     return Type::TOP;
3324   return TypeTuple::MEMBAR;
3325 }
3326 
3327 //------------------------------match------------------------------------------
3328 // Construct projections for memory.
<span class="line-modified">3329 Node *MemBarNode::match(const ProjNode *proj, const Matcher *m, const RegMask* mask) {</span>
3330   switch (proj-&gt;_con) {
3331   case TypeFunc::Control:
3332   case TypeFunc::Memory:
3333     return new MachProjNode(this,proj-&gt;_con,RegMask::Empty,MachProjNode::unmatched_proj);
3334   }
3335   ShouldNotReachHere();
3336   return NULL;
3337 }
3338 
3339 void MemBarNode::set_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3340   trailing-&gt;_kind = TrailingStore;
3341   leading-&gt;_kind = LeadingStore;
3342 #ifdef ASSERT
3343   trailing-&gt;_pair_idx = leading-&gt;_idx;
3344   leading-&gt;_pair_idx = leading-&gt;_idx;
3345 #endif
3346 }
3347 
3348 void MemBarNode::set_load_store_pair(MemBarNode* leading, MemBarNode* trailing) {
3349   trailing-&gt;_kind = TrailingLoadStore;
</pre>
<hr />
<pre>
3595   return (req() &gt; RawStores);
3596 }
3597 
3598 void InitializeNode::set_complete(PhaseGVN* phase) {
3599   assert(!is_complete(), &quot;caller responsibility&quot;);
3600   _is_complete = Complete;
3601 
3602   // After this node is complete, it contains a bunch of
3603   // raw-memory initializations.  There is no need for
3604   // it to have anything to do with non-raw memory effects.
3605   // Therefore, tell all non-raw users to re-optimize themselves,
3606   // after skipping the memory effects of this initialization.
3607   PhaseIterGVN* igvn = phase-&gt;is_IterGVN();
3608   if (igvn)  igvn-&gt;add_users_to_worklist(this);
3609 }
3610 
3611 // convenience function
3612 // return false if the init contains any stores already
3613 bool AllocateNode::maybe_set_complete(PhaseGVN* phase) {
3614   InitializeNode* init = initialization();
<span class="line-modified">3615   if (init == NULL || init-&gt;is_complete()) {</span>
<span class="line-added">3616     return false;</span>
<span class="line-added">3617   }</span>
3618   init-&gt;remove_extra_zeroes();
3619   // for now, if this allocation has already collected any inits, bail:
3620   if (init-&gt;is_non_zero())  return false;
3621   init-&gt;set_complete(phase);
3622   return true;
3623 }
3624 
3625 void InitializeNode::remove_extra_zeroes() {
3626   if (req() == RawStores)  return;
3627   Node* zmem = zero_memory();
3628   uint fill = RawStores;
3629   for (uint i = fill; i &lt; req(); i++) {
3630     Node* n = in(i);
3631     if (n-&gt;is_top() || n == zmem)  continue;  // skip
3632     if (fill &lt; i)  set_req(fill, n);          // compact
3633     ++fill;
3634   }
3635   // delete any empty spaces created:
3636   while (fill &lt; req()) {
3637     del_req(fill);
</pre>
<hr />
<pre>
4355         //   z&#39;s_done      12  16  16  16    12  16    12
4356         //   z&#39;s_needed    12  16  16  16    16  16    16
4357         //   zsize          0   0   0   0     4   0     4
4358         if (next_full_store &lt; 0) {
4359           // Conservative tack:  Zero to end of current word.
4360           zeroes_needed = align_up(zeroes_needed, BytesPerInt);
4361         } else {
4362           // Zero to beginning of next fully initialized word.
4363           // Or, don&#39;t zero at all, if we are already in that word.
4364           assert(next_full_store &gt;= zeroes_needed, &quot;must go forward&quot;);
4365           assert((next_full_store &amp; (BytesPerInt-1)) == 0, &quot;even boundary&quot;);
4366           zeroes_needed = next_full_store;
4367         }
4368       }
4369 
4370       if (zeroes_needed &gt; zeroes_done) {
4371         intptr_t zsize = zeroes_needed - zeroes_done;
4372         // Do some incremental zeroing on rawmem, in parallel with inits.
4373         zeroes_done = align_down(zeroes_done, BytesPerInt);
4374         rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
<span class="line-added">4375                                               allocation()-&gt;in(AllocateNode::DefaultValue),</span>
<span class="line-added">4376                                               allocation()-&gt;in(AllocateNode::RawDefaultValue),</span>
4377                                               zeroes_done, zeroes_needed,
4378                                               phase);
4379         zeroes_done = zeroes_needed;
4380         if (zsize &gt; InitArrayShortSize &amp;&amp; ++big_init_gaps &gt; 2)
4381           do_zeroing = false;   // leave the hole, next time
4382       }
4383     }
4384 
4385     // Collect the store and move on:
4386     phase-&gt;replace_input_of(st, MemNode::Memory, inits);
4387     inits = st;                 // put it on the linearized chain
4388     set_req(i, zmem);           // unhook from previous position
4389 
4390     if (zeroes_done == st_off)
4391       zeroes_done = next_init_off;
4392 
4393     assert(!do_zeroing || zeroes_done &gt;= next_init_off, &quot;don&#39;t miss any&quot;);
4394 
4395     #ifdef ASSERT
4396     // Various order invariants.  Weaker than stores_are_sane because
</pre>
<hr />
<pre>
4416   remove_extra_zeroes();        // clear out all the zmems left over
4417   add_req(inits);
4418 
4419   if (!(UseTLAB &amp;&amp; ZeroTLAB)) {
4420     // If anything remains to be zeroed, zero it all now.
4421     zeroes_done = align_down(zeroes_done, BytesPerInt);
4422     // if it is the last unused 4 bytes of an instance, forget about it
4423     intptr_t size_limit = phase-&gt;find_intptr_t_con(size_in_bytes, max_jint);
4424     if (zeroes_done + BytesPerLong &gt;= size_limit) {
4425       AllocateNode* alloc = allocation();
4426       assert(alloc != NULL, &quot;must be present&quot;);
4427       if (alloc != NULL &amp;&amp; alloc-&gt;Opcode() == Op_Allocate) {
4428         Node* klass_node = alloc-&gt;in(AllocateNode::KlassNode);
4429         ciKlass* k = phase-&gt;type(klass_node)-&gt;is_klassptr()-&gt;klass();
4430         if (zeroes_done == k-&gt;layout_helper())
4431           zeroes_done = size_limit;
4432       }
4433     }
4434     if (zeroes_done &lt; size_limit) {
4435       rawmem = ClearArrayNode::clear_memory(rawctl, rawmem, rawptr,
<span class="line-added">4436                                             allocation()-&gt;in(AllocateNode::DefaultValue),</span>
<span class="line-added">4437                                             allocation()-&gt;in(AllocateNode::RawDefaultValue),</span>
4438                                             zeroes_done, size_in_bytes, phase);
4439     }
4440   }
4441 
4442   set_complete(phase);
4443   return rawmem;
4444 }
4445 
4446 
4447 #ifdef ASSERT
4448 bool InitializeNode::stores_are_sane(PhaseTransform* phase) {
4449   if (is_complete())
4450     return true;                // stores could be anything at this point
4451   assert(allocation() != NULL, &quot;must be present&quot;);
4452   intptr_t last_off = allocation()-&gt;minimum_header_size();
4453   for (uint i = InitializeNode::RawStores; i &lt; req(); i++) {
4454     Node* st = in(i);
4455     intptr_t st_off = get_store_offset(st, phase);
4456     if (st_off &lt; 0)  continue;  // ignore dead garbage
4457     if (last_off &gt; st_off) {
</pre>
</td>
</tr>
</table>
<center><a href="matcher.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="node.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/escape.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="compile.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="lcm.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/escape.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 124   GrowableArray&lt;JavaObjectNode*&gt; non_escaped_worklist;
 125   GrowableArray&lt;FieldNode*&gt;      oop_fields_worklist;
 126   DEBUG_ONLY( GrowableArray&lt;Node*&gt; addp_worklist; )
 127 
 128   { Compile::TracePhase tp(&quot;connectionGraph&quot;, &amp;Phase::timers[Phase::_t_connectionGraph]);
 129 
 130   // 1. Populate Connection Graph (CG) with PointsTo nodes.
 131   ideal_nodes.map(C-&gt;live_nodes(), NULL);  // preallocate space
 132   // Initialize worklist
 133   if (C-&gt;root() != NULL) {
 134     ideal_nodes.push(C-&gt;root());
 135   }
 136   // Processed ideal nodes are unique on ideal_nodes list
 137   // but several ideal nodes are mapped to the phantom_obj.
 138   // To avoid duplicated entries on the following worklists
 139   // add the phantom_obj only once to them.
 140   ptnodes_worklist.append(phantom_obj);
 141   java_objects_worklist.append(phantom_obj);
 142   for( uint next = 0; next &lt; ideal_nodes.size(); ++next ) {
 143     Node* n = ideal_nodes.at(next);










 144     // Create PointsTo nodes and add them to Connection Graph. Called
 145     // only once per ideal node since ideal_nodes is Unique_Node list.
 146     add_node_to_connection_graph(n, &amp;delayed_worklist);
 147     PointsToNode* ptn = ptnode_adr(n-&gt;_idx);
 148     if (ptn != NULL &amp;&amp; ptn != phantom_obj) {
 149       ptnodes_worklist.append(ptn);
 150       if (ptn-&gt;is_JavaObject()) {
 151         java_objects_worklist.append(ptn-&gt;as_JavaObject());
 152         if ((n-&gt;is_Allocate() || n-&gt;is_CallStaticJava()) &amp;&amp;
 153             (ptn-&gt;escape_state() &lt; PointsToNode::GlobalEscape)) {
 154           // Only allocations and java static calls results are interesting.
 155           non_escaped_worklist.append(ptn-&gt;as_JavaObject());
 156         }
 157       } else if (ptn-&gt;is_Field() &amp;&amp; ptn-&gt;as_Field()-&gt;is_oop()) {
 158         oop_fields_worklist.append(ptn-&gt;as_Field());
 159       }
 160     }
 161     if (n-&gt;is_MergeMem()) {
 162       // Collect all MergeMem nodes to add memory slices for
 163       // scalar replaceable objects in split_unique_types().
</pre>
<hr />
<pre>
 362       // Put Lock and Unlock nodes on IGVN worklist to process them during
 363       // first IGVN optimization when escape information is still available.
 364       record_for_optimizer(n);
 365     } else if (n-&gt;is_Allocate()) {
 366       add_call_node(n-&gt;as_Call());
 367       record_for_optimizer(n);
 368     } else {
 369       if (n-&gt;is_CallStaticJava()) {
 370         const char* name = n-&gt;as_CallStaticJava()-&gt;_name;
 371         if (name != NULL &amp;&amp; strcmp(name, &quot;uncommon_trap&quot;) == 0)
 372           return; // Skip uncommon traps
 373       }
 374       // Don&#39;t mark as processed since call&#39;s arguments have to be processed.
 375       delayed_worklist-&gt;push(n);
 376       // Check if a call returns an object.
 377       if ((n-&gt;as_Call()-&gt;returns_pointer() &amp;&amp;
 378            n-&gt;as_Call()-&gt;proj_out_or_null(TypeFunc::Parms) != NULL) ||
 379           (n-&gt;is_CallStaticJava() &amp;&amp;
 380            n-&gt;as_CallStaticJava()-&gt;is_boxing_method())) {
 381         add_call_node(n-&gt;as_Call());











 382       }
 383     }
 384     return;
 385   }
 386   // Put this check here to process call arguments since some call nodes
 387   // point to phantom_obj.
 388   if (n_ptn == phantom_obj || n_ptn == null_obj)
 389     return; // Skip predefined nodes.
 390 
 391   switch (opcode) {
 392     case Op_AddP: {
 393       Node* base = get_addp_base(n);
 394       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 395       // Field nodes are created for all field types. They are used in
 396       // adjust_scalar_replaceable_state() and split_unique_types().
 397       // Note, non-oop fields will have only base edges in Connection
 398       // Graph because such fields are not used for oop loads and stores.
 399       int offset = address_offset(n, igvn);
 400       add_field(n, PointsToNode::NoEscape, offset);
 401       if (ptn_base == NULL) {
 402         delayed_worklist-&gt;push(n); // Process it later.
 403       } else {
 404         n_ptn = ptnode_adr(n_idx);
 405         add_base(n_ptn-&gt;as_Field(), ptn_base);
 406       }
 407       break;
 408     }
 409     case Op_CastX2P: {
 410       map_ideal_node(n, phantom_obj);
 411       break;
 412     }

 413     case Op_CastPP:
 414     case Op_CheckCastPP:
 415     case Op_EncodeP:
 416     case Op_DecodeN:
 417     case Op_EncodePKlass:
 418     case Op_DecodeNKlass: {
 419       add_local_var_and_edge(n, PointsToNode::NoEscape,
 420                              n-&gt;in(1), delayed_worklist);
 421       break;
 422     }
 423     case Op_CMoveP: {
 424       add_local_var(n, PointsToNode::NoEscape);
 425       // Do not add edges during first iteration because some could be
 426       // not defined yet.
 427       delayed_worklist-&gt;push(n);
 428       break;
 429     }
 430     case Op_ConP:
 431     case Op_ConN:
 432     case Op_ConNKlass: {
</pre>
<hr />
<pre>
 465     case Op_PartialSubtypeCheck: {
 466       // Produces Null or notNull and is used in only in CmpP so
 467       // phantom_obj could be used.
 468       map_ideal_node(n, phantom_obj); // Result is unknown
 469       break;
 470     }
 471     case Op_Phi: {
 472       // Using isa_ptr() instead of isa_oopptr() for LoadP and Phi because
 473       // ThreadLocal has RawPtr type.
 474       const Type* t = n-&gt;as_Phi()-&gt;type();
 475       if (t-&gt;make_ptr() != NULL) {
 476         add_local_var(n, PointsToNode::NoEscape);
 477         // Do not add edges during first iteration because some could be
 478         // not defined yet.
 479         delayed_worklist-&gt;push(n);
 480       }
 481       break;
 482     }
 483     case Op_Proj: {
 484       // we are only interested in the oop result projection from a call
<span class="line-modified"> 485       if (n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;</span>
<span class="line-modified"> 486           n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) {</span>


 487         add_local_var_and_edge(n, PointsToNode::NoEscape,
 488                                n-&gt;in(0), delayed_worklist);
 489       }
 490       break;
 491     }
 492     case Op_Rethrow: // Exception object escapes
 493     case Op_Return: {
 494       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 495           igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 496         // Treat Return value as LocalVar with GlobalEscape escape state.
 497         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 498                                n-&gt;in(TypeFunc::Parms), delayed_worklist);
 499       }
 500       break;
 501     }
 502     case Op_CompareAndExchangeP:
 503     case Op_CompareAndExchangeN:
 504     case Op_GetAndSetP:
 505     case Op_GetAndSetN: {
 506       add_objload_to_connection_graph(n, delayed_worklist);
</pre>
<hr />
<pre>
 562   if (n-&gt;is_Call()) {
 563     process_call_arguments(n-&gt;as_Call());
 564     return;
 565   }
 566   assert(n-&gt;is_Store() || n-&gt;is_LoadStore() ||
 567          (n_ptn != NULL) &amp;&amp; (n_ptn-&gt;ideal_node() != NULL),
 568          &quot;node should be registered already&quot;);
 569   int opcode = n-&gt;Opcode();
 570   bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_add_final_edges(this, _igvn, n, opcode);
 571   if (gc_handled) {
 572     return; // Ignore node if already handled by GC.
 573   }
 574   switch (opcode) {
 575     case Op_AddP: {
 576       Node* base = get_addp_base(n);
 577       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 578       assert(ptn_base != NULL, &quot;field&#39;s base should be registered&quot;);
 579       add_base(n_ptn-&gt;as_Field(), ptn_base);
 580       break;
 581     }

 582     case Op_CastPP:
 583     case Op_CheckCastPP:
 584     case Op_EncodeP:
 585     case Op_DecodeN:
 586     case Op_EncodePKlass:
 587     case Op_DecodeNKlass: {
 588       add_local_var_and_edge(n, PointsToNode::NoEscape,
 589                              n-&gt;in(1), NULL);
 590       break;
 591     }
 592     case Op_CMoveP: {
 593       for (uint i = CMoveNode::IfFalse; i &lt; n-&gt;req(); i++) {
 594         Node* in = n-&gt;in(i);
 595         if (in == NULL)
 596           continue;  // ignore NULL
 597         Node* uncast_in = in-&gt;uncast();
 598         if (uncast_in-&gt;is_top() || uncast_in == n)
 599           continue;  // ignore top or inputs which go back this node
 600         PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 601         assert(ptn != NULL, &quot;node should be registered&quot;);
</pre>
<hr />
<pre>
 621       // ThreadLocal has RawPtr type.
 622       const Type* t = n-&gt;as_Phi()-&gt;type();
 623       if (t-&gt;make_ptr() != NULL) {
 624         for (uint i = 1; i &lt; n-&gt;req(); i++) {
 625           Node* in = n-&gt;in(i);
 626           if (in == NULL)
 627             continue;  // ignore NULL
 628           Node* uncast_in = in-&gt;uncast();
 629           if (uncast_in-&gt;is_top() || uncast_in == n)
 630             continue;  // ignore top or inputs which go back this node
 631           PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 632           assert(ptn != NULL, &quot;node should be registered&quot;);
 633           add_edge(n_ptn, ptn);
 634         }
 635         break;
 636       }
 637       ELSE_FAIL(&quot;Op_Phi&quot;);
 638     }
 639     case Op_Proj: {
 640       // we are only interested in the oop result projection from a call
<span class="line-modified"> 641       if (n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;</span>
<span class="line-modified"> 642           n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) {</span>


 643         add_local_var_and_edge(n, PointsToNode::NoEscape, n-&gt;in(0), NULL);
 644         break;
 645       }
 646       ELSE_FAIL(&quot;Op_Proj&quot;);
 647     }
 648     case Op_Rethrow: // Exception object escapes
 649     case Op_Return: {
 650       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 651           _igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 652         // Treat Return value as LocalVar with GlobalEscape escape state.
 653         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 654                                n-&gt;in(TypeFunc::Parms), NULL);
 655         break;
 656       }
 657       ELSE_FAIL(&quot;Op_Return&quot;);
 658     }
 659     case Op_StoreP:
 660     case Op_StoreN:
 661     case Op_StoreNKlass:
 662     case Op_StorePConditional:
</pre>
<hr />
<pre>
 784     return true;
 785   } else if ((opcode == Op_StoreP) &amp;&amp; adr_type-&gt;isa_rawptr()) {
 786     // Stored value escapes in unsafe access.
 787     Node* val = n-&gt;in(MemNode::ValueIn);
 788     PointsToNode* ptn = ptnode_adr(val-&gt;_idx);
 789     assert(ptn != NULL, &quot;node should be registered&quot;);
 790     set_escape_state(ptn, PointsToNode::GlobalEscape);
 791     // Add edge to object for unsafe access with offset.
 792     PointsToNode* adr_ptn = ptnode_adr(adr-&gt;_idx);
 793     assert(adr_ptn != NULL, &quot;node should be registered&quot;);
 794     if (adr_ptn-&gt;is_Field()) {
 795       assert(adr_ptn-&gt;as_Field()-&gt;is_oop(), &quot;should be oop field&quot;);
 796       add_edge(adr_ptn, ptn);
 797     }
 798     return true;
 799   }
 800   return false;
 801 }
 802 
 803 void ConnectionGraph::add_call_node(CallNode* call) {
<span class="line-modified"> 804   assert(call-&gt;returns_pointer(), &quot;only for call which returns pointer&quot;);</span>
 805   uint call_idx = call-&gt;_idx;
 806   if (call-&gt;is_Allocate()) {
 807     Node* k = call-&gt;in(AllocateNode::KlassNode);
 808     const TypeKlassPtr* kt = k-&gt;bottom_type()-&gt;isa_klassptr();
 809     assert(kt != NULL, &quot;TypeKlassPtr  required.&quot;);
 810     ciKlass* cik = kt-&gt;klass();
 811     PointsToNode::EscapeState es = PointsToNode::NoEscape;
 812     bool scalar_replaceable = true;
 813     if (call-&gt;is_AllocateArray()) {
 814       if (!cik-&gt;is_array_klass()) { // StressReflectiveCode
 815         es = PointsToNode::GlobalEscape;
 816       } else {
 817         int length = call-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);
 818         if (length &lt; 0 || length &gt; EliminateAllocationArraySizeLimit) {
 819           // Not scalar replaceable if the length is not constant or too big.
 820           scalar_replaceable = false;
 821         }
 822       }
 823     } else {  // Allocate instance
 824       if (cik-&gt;is_subclass_of(_compile-&gt;env()-&gt;Thread_klass()) ||
</pre>
<hr />
<pre>
 871       if (intr == vmIntrinsics::_floatValue || intr == vmIntrinsics::_doubleValue) {
 872         // It does not escape if object is always allocated.
 873         es = PointsToNode::NoEscape;
 874       } else {
 875         // It escapes globally if object could be loaded from cache.
 876         es = PointsToNode::GlobalEscape;
 877       }
 878       add_java_object(call, es);
 879     } else {
 880       BCEscapeAnalyzer* call_analyzer = meth-&gt;get_bcea();
 881       call_analyzer-&gt;copy_dependencies(_compile-&gt;dependencies());
 882       if (call_analyzer-&gt;is_return_allocated()) {
 883         // Returns a newly allocated unescaped object, simply
 884         // update dependency information.
 885         // Mark it as NoEscape so that objects referenced by
 886         // it&#39;s fields will be marked as NoEscape at least.
 887         add_java_object(call, PointsToNode::NoEscape);
 888         ptnode_adr(call_idx)-&gt;set_scalar_replaceable(false);
 889       } else {
 890         // Determine whether any arguments are returned.
<span class="line-modified"> 891         const TypeTuple* d = call-&gt;tf()-&gt;domain();</span>
 892         bool ret_arg = false;
 893         for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
 894           if (d-&gt;field_at(i)-&gt;isa_ptr() != NULL &amp;&amp;
 895               call_analyzer-&gt;is_arg_returned(i - TypeFunc::Parms)) {
 896             ret_arg = true;
 897             break;
 898           }
 899         }
 900         if (ret_arg) {
 901           add_local_var(call, PointsToNode::ArgEscape);
 902         } else {
 903           // Returns unknown object.
 904           map_ideal_node(call, phantom_obj);
 905         }
 906       }
 907     }
 908   } else {
 909     // An other type of call, assume the worst case:
 910     // returned value is unknown and globally escapes.
 911     assert(call-&gt;Opcode() == Op_CallDynamicJava, &quot;add failed case check&quot;);
</pre>
<hr />
<pre>
 918     switch (call-&gt;Opcode()) {
 919 #ifdef ASSERT
 920     case Op_Allocate:
 921     case Op_AllocateArray:
 922     case Op_Lock:
 923     case Op_Unlock:
 924       assert(false, &quot;should be done already&quot;);
 925       break;
 926 #endif
 927     case Op_ArrayCopy:
 928     case Op_CallLeafNoFP:
 929       // Most array copies are ArrayCopy nodes at this point but there
 930       // are still a few direct calls to the copy subroutines (See
 931       // PhaseStringOpts::copy_string())
 932       is_arraycopy = (call-&gt;Opcode() == Op_ArrayCopy) ||
 933         call-&gt;as_CallLeaf()-&gt;is_call_to_arraycopystub();
 934       // fall through
 935     case Op_CallLeaf: {
 936       // Stub calls, objects do not escape but they are not scale replaceable.
 937       // Adjust escape state for outgoing arguments.
<span class="line-modified"> 938       const TypeTuple * d = call-&gt;tf()-&gt;domain();</span>
 939       bool src_has_oops = false;
 940       for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
 941         const Type* at = d-&gt;field_at(i);
 942         Node *arg = call-&gt;in(i);
 943         if (arg == NULL) {
 944           continue;
 945         }
 946         const Type *aat = _igvn-&gt;type(arg);
 947         if (arg-&gt;is_top() || !at-&gt;isa_ptr() || !aat-&gt;isa_ptr())
 948           continue;
 949         if (arg-&gt;is_AddP()) {
 950           //
 951           // The inline_native_clone() case when the arraycopy stub is called
 952           // after the allocation before Initialize and CheckCastPP nodes.
 953           // Or normal arraycopy for object arrays case.
 954           //
 955           // Set AddP&#39;s base (Allocate) as not scalar replaceable since
 956           // pointer to the base (with offset) is passed as argument.
 957           //
 958           arg = get_addp_base(arg);
 959         }
 960         PointsToNode* arg_ptn = ptnode_adr(arg-&gt;_idx);
 961         assert(arg_ptn != NULL, &quot;should be registered&quot;);
 962         PointsToNode::EscapeState arg_esc = arg_ptn-&gt;escape_state();
 963         if (is_arraycopy || arg_esc &lt; PointsToNode::ArgEscape) {
 964           assert(aat == Type::TOP || aat == TypePtr::NULL_PTR ||
 965                  aat-&gt;isa_ptr() != NULL, &quot;expecting an Ptr&quot;);
 966           bool arg_has_oops = aat-&gt;isa_oopptr() &amp;&amp;
 967                               (aat-&gt;isa_oopptr()-&gt;klass() == NULL || aat-&gt;isa_instptr() ||
<span class="line-modified"> 968                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;klass()-&gt;is_obj_array_klass()));</span>



 969           if (i == TypeFunc::Parms) {
 970             src_has_oops = arg_has_oops;
 971           }
 972           //
 973           // src or dst could be j.l.Object when other is basic type array:
 974           //
 975           //   arraycopy(char[],0,Object*,0,size);
 976           //   arraycopy(Object*,0,char[],0,size);
 977           //
 978           // Don&#39;t add edges in such cases.
 979           //
 980           bool arg_is_arraycopy_dest = src_has_oops &amp;&amp; is_arraycopy &amp;&amp;
 981                                        arg_has_oops &amp;&amp; (i &gt; TypeFunc::Parms);
 982 #ifdef ASSERT
 983           if (!(is_arraycopy ||
 984                 BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(call) ||
 985                 (call-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
 986                  (strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32&quot;) == 0 ||
 987                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32C&quot;) == 0 ||
 988                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesAdler32&quot;) == 0 ||
 989                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;aescrypt_encryptBlock&quot;) == 0 ||
 990                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;aescrypt_decryptBlock&quot;) == 0 ||
 991                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_encryptAESCrypt&quot;) == 0 ||
 992                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_decryptAESCrypt&quot;) == 0 ||
 993                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_encryptAESCrypt&quot;) == 0 ||
 994                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_decryptAESCrypt&quot;) == 0 ||
 995                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;counterMode_AESCrypt&quot;) == 0 ||
 996                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;ghash_processBlocks&quot;) == 0 ||
 997                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;encodeBlock&quot;) == 0 ||
 998                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompress&quot;) == 0 ||
 999                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompressMB&quot;) == 0 ||
1000                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompress&quot;) == 0 ||
1001                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompressMB&quot;) == 0 ||
1002                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompress&quot;) == 0 ||
1003                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompressMB&quot;) == 0 ||
1004                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;multiplyToLen&quot;) == 0 ||
1005                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;squareToLen&quot;) == 0 ||
1006                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;mulAdd&quot;) == 0 ||
1007                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_multiply&quot;) == 0 ||
1008                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_square&quot;) == 0 ||



1009                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerRightShiftWorker&quot;) == 0 ||
1010                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerLeftShiftWorker&quot;) == 0 ||
1011                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0)
1012                  ))) {
1013             call-&gt;dump();
1014             fatal(&quot;EA unexpected CallLeaf %s&quot;, call-&gt;as_CallLeaf()-&gt;_name);
1015           }
1016 #endif
1017           // Always process arraycopy&#39;s destination object since
1018           // we need to add all possible edges to references in
1019           // source object.
1020           if (arg_esc &gt;= PointsToNode::ArgEscape &amp;&amp;
1021               !arg_is_arraycopy_dest) {
1022             continue;
1023           }
1024           PointsToNode::EscapeState es = PointsToNode::ArgEscape;
1025           if (call-&gt;is_ArrayCopy()) {
1026             ArrayCopyNode* ac = call-&gt;as_ArrayCopy();
1027             if (ac-&gt;is_clonebasic() ||
1028                 ac-&gt;is_arraycopy_validated() ||
</pre>
<hr />
<pre>
1050           }
1051         }
1052       }
1053       break;
1054     }
1055     case Op_CallStaticJava: {
1056       // For a static call, we know exactly what method is being called.
1057       // Use bytecode estimator to record the call&#39;s escape affects
1058 #ifdef ASSERT
1059       const char* name = call-&gt;as_CallStaticJava()-&gt;_name;
1060       assert((name == NULL || strcmp(name, &quot;uncommon_trap&quot;) != 0), &quot;normal calls only&quot;);
1061 #endif
1062       ciMethod* meth = call-&gt;as_CallJava()-&gt;method();
1063       if ((meth != NULL) &amp;&amp; meth-&gt;is_boxing_method()) {
1064         break; // Boxing methods do not modify any oops.
1065       }
1066       BCEscapeAnalyzer* call_analyzer = (meth !=NULL) ? meth-&gt;get_bcea() : NULL;
1067       // fall-through if not a Java method or no analyzer information
1068       if (call_analyzer != NULL) {
1069         PointsToNode* call_ptn = ptnode_adr(call-&gt;_idx);
<span class="line-modified">1070         const TypeTuple* d = call-&gt;tf()-&gt;domain();</span>
1071         for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
1072           const Type* at = d-&gt;field_at(i);
1073           int k = i - TypeFunc::Parms;
1074           Node* arg = call-&gt;in(i);
1075           PointsToNode* arg_ptn = ptnode_adr(arg-&gt;_idx);
1076           if (at-&gt;isa_ptr() != NULL &amp;&amp;
1077               call_analyzer-&gt;is_arg_returned(k)) {
1078             // The call returns arguments.
1079             if (call_ptn != NULL) { // Is call&#39;s result used?
1080               assert(call_ptn-&gt;is_LocalVar(), &quot;node should be registered&quot;);
1081               assert(arg_ptn != NULL, &quot;node should be registered&quot;);
1082               add_edge(call_ptn, arg_ptn);
1083             }
1084           }
1085           if (at-&gt;isa_oopptr() != NULL &amp;&amp;
1086               arg_ptn-&gt;escape_state() &lt; PointsToNode::GlobalEscape) {
1087             if (!call_analyzer-&gt;is_arg_stack(k)) {
1088               // The argument global escapes
1089               set_escape_state(arg_ptn, PointsToNode::GlobalEscape);
1090             } else {
</pre>
<hr />
<pre>
1094                 set_fields_escape_state(arg_ptn, PointsToNode::GlobalEscape);
1095               }
1096             }
1097           }
1098         }
1099         if (call_ptn != NULL &amp;&amp; call_ptn-&gt;is_LocalVar()) {
1100           // The call returns arguments.
1101           assert(call_ptn-&gt;edge_count() &gt; 0, &quot;sanity&quot;);
1102           if (!call_analyzer-&gt;is_return_local()) {
1103             // Returns also unknown object.
1104             add_edge(call_ptn, phantom_obj);
1105           }
1106         }
1107         break;
1108       }
1109     }
1110     default: {
1111       // Fall-through here if not a Java method or no analyzer information
1112       // or some other type of call, assume the worst case: all arguments
1113       // globally escape.
<span class="line-modified">1114       const TypeTuple* d = call-&gt;tf()-&gt;domain();</span>
1115       for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
1116         const Type* at = d-&gt;field_at(i);
1117         if (at-&gt;isa_oopptr() != NULL) {
1118           Node* arg = call-&gt;in(i);
1119           if (arg-&gt;is_AddP()) {
1120             arg = get_addp_base(arg);
1121           }
1122           assert(ptnode_adr(arg-&gt;_idx) != NULL, &quot;should be defined already&quot;);
1123           set_escape_state(ptnode_adr(arg-&gt;_idx), PointsToNode::GlobalEscape);
1124         }
1125       }
1126     }
1127   }
1128 }
1129 
1130 
1131 // Finish Graph construction.
1132 bool ConnectionGraph::complete_connection_graph(
1133                          GrowableArray&lt;PointsToNode*&gt;&amp;   ptnodes_worklist,
1134                          GrowableArray&lt;JavaObjectNode*&gt;&amp; non_escaped_worklist,
</pre>
<hr />
<pre>
1615               } else {
1616                 if (!val-&gt;is_LocalVar() || (val-&gt;edge_count() == 0)) {
1617                   tty-&gt;print_cr(&quot;----------init store has invalid value -----&quot;);
1618                   store-&gt;dump();
1619                   val-&gt;dump();
1620                   assert(val-&gt;is_LocalVar() &amp;&amp; (val-&gt;edge_count() &gt; 0), &quot;should be processed already&quot;);
1621                 }
1622                 for (EdgeIterator j(val); j.has_next(); j.next()) {
1623                   PointsToNode* obj = j.get();
1624                   if (obj-&gt;is_JavaObject()) {
1625                     if (!field-&gt;points_to(obj-&gt;as_JavaObject())) {
1626                       missed_obj = obj;
1627                       break;
1628                     }
1629                   }
1630                 }
1631               }
1632               if (missed_obj != NULL) {
1633                 tty-&gt;print_cr(&quot;----------field---------------------------------&quot;);
1634                 field-&gt;dump();
<span class="line-modified">1635                 tty-&gt;print_cr(&quot;----------missed referernce to object-----------&quot;);</span>
1636                 missed_obj-&gt;dump();
<span class="line-modified">1637                 tty-&gt;print_cr(&quot;----------object referernced by init store -----&quot;);</span>
1638                 store-&gt;dump();
1639                 val-&gt;dump();
1640                 assert(!field-&gt;points_to(missed_obj-&gt;as_JavaObject()), &quot;missed JavaObject reference&quot;);
1641               }
1642             }
1643 #endif
1644           } else {
1645             // There could be initializing stores which follow allocation.
1646             // For example, a volatile field store is not collected
1647             // by Initialize node.
1648             //
1649             // Need to check for dependent loads to separate such stores from
1650             // stores which follow loads. For now, add initial value NULL so
1651             // that compare pointers optimization works correctly.
1652           }
1653         }
1654         if (value == NULL) {
1655           // A field&#39;s initializing value was not recorded. Add NULL.
1656           if (add_edge(field, null_obj)) {
1657             // New edge was added
</pre>
<hr />
<pre>
1687       }
1688       // 2. An object is not scalar replaceable if the field into which it is
1689       // stored has multiple bases one of which is null.
1690       if (field-&gt;base_count() &gt; 1) {
1691         for (BaseIterator i(field); i.has_next(); i.next()) {
1692           PointsToNode* base = i.get();
1693           if (base == null_obj) {
1694             jobj-&gt;set_scalar_replaceable(false);
1695             return;
1696           }
1697         }
1698       }
1699     }
1700     assert(use-&gt;is_Field() || use-&gt;is_LocalVar(), &quot;sanity&quot;);
1701     // 3. An object is not scalar replaceable if it is merged with other objects.
1702     for (EdgeIterator j(use); j.has_next(); j.next()) {
1703       PointsToNode* ptn = j.get();
1704       if (ptn-&gt;is_JavaObject() &amp;&amp; ptn != jobj) {
1705         // Mark all objects.
1706         jobj-&gt;set_scalar_replaceable(false);
<span class="line-modified">1707          ptn-&gt;set_scalar_replaceable(false);</span>
1708       }
1709     }
1710     if (!jobj-&gt;scalar_replaceable()) {
1711       return;
1712     }
1713   }
1714 
1715   for (EdgeIterator j(jobj); j.has_next(); j.next()) {
1716     if (j.get()-&gt;is_Arraycopy()) {
1717       continue;
1718     }
1719 
1720     // Non-escaping object node should point only to field nodes.
1721     FieldNode* field = j.get()-&gt;as_Field();
1722     int offset = field-&gt;as_Field()-&gt;offset();
1723 
1724     // 4. An object is not scalar replaceable if it has a field with unknown
1725     // offset (array&#39;s element is accessed in loop).
1726     if (offset == Type::OffsetBot) {
1727       jobj-&gt;set_scalar_replaceable(false);
</pre>
<hr />
<pre>
1850         assert(field-&gt;edge_count() &gt; 0, &quot;sanity&quot;);
1851       }
1852     }
1853   }
1854 }
1855 #endif
1856 
1857 // Optimize ideal graph.
1858 void ConnectionGraph::optimize_ideal_graph(GrowableArray&lt;Node*&gt;&amp; ptr_cmp_worklist,
1859                                            GrowableArray&lt;Node*&gt;&amp; storestore_worklist) {
1860   Compile* C = _compile;
1861   PhaseIterGVN* igvn = _igvn;
1862   if (EliminateLocks) {
1863     // Mark locks before changing ideal graph.
1864     int cnt = C-&gt;macro_count();
1865     for( int i=0; i &lt; cnt; i++ ) {
1866       Node *n = C-&gt;macro_node(i);
1867       if (n-&gt;is_AbstractLock()) { // Lock and Unlock nodes
1868         AbstractLockNode* alock = n-&gt;as_AbstractLock();
1869         if (!alock-&gt;is_non_esc_obj()) {
<span class="line-modified">1870           if (not_global_escape(alock-&gt;obj_node())) {</span>


1871             assert(!alock-&gt;is_eliminated() || alock-&gt;is_coarsened(), &quot;sanity&quot;);
1872             // The lock could be marked eliminated by lock coarsening
1873             // code during first IGVN before EA. Replace coarsened flag
1874             // to eliminate all associated locks/unlocks.
1875 #ifdef ASSERT
1876             alock-&gt;log_lock_optimization(C, &quot;eliminate_lock_set_non_esc3&quot;);
1877 #endif
1878             alock-&gt;set_non_esc_obj();
1879           }
1880         }
1881       }
1882     }
1883   }
1884 
1885   if (OptimizePtrCompare) {
1886     // Add ConI(#CC_GT) and ConI(#CC_EQ).
1887     _pcmp_neq = igvn-&gt;makecon(TypeInt::CC_GT);
1888     _pcmp_eq = igvn-&gt;makecon(TypeInt::CC_EQ);
1889     // Optimize objects compare.
1890     while (ptr_cmp_worklist.length() != 0) {
</pre>
<hr />
<pre>
2053   assert(!src-&gt;is_Field() &amp;&amp; !dst-&gt;is_Field(), &quot;only for JavaObject and LocalVar&quot;);
2054   assert((src != null_obj) &amp;&amp; (dst != null_obj), &quot;not for ConP NULL&quot;);
2055   PointsToNode* ptadr = _nodes.at(n-&gt;_idx);
2056   if (ptadr != NULL) {
2057     assert(ptadr-&gt;is_Arraycopy() &amp;&amp; ptadr-&gt;ideal_node() == n, &quot;sanity&quot;);
2058     return;
2059   }
2060   Compile* C = _compile;
2061   ptadr = new (C-&gt;comp_arena()) ArraycopyNode(this, n, es);
2062   _nodes.at_put(n-&gt;_idx, ptadr);
2063   // Add edge from arraycopy node to source object.
2064   (void)add_edge(ptadr, src);
2065   src-&gt;set_arraycopy_src();
2066   // Add edge from destination object to arraycopy node.
2067   (void)add_edge(dst, ptadr);
2068   dst-&gt;set_arraycopy_dst();
2069 }
2070 
2071 bool ConnectionGraph::is_oop_field(Node* n, int offset, bool* unsafe) {
2072   const Type* adr_type = n-&gt;as_AddP()-&gt;bottom_type();

2073   BasicType bt = T_INT;
<span class="line-modified">2074   if (offset == Type::OffsetBot) {</span>
2075     // Check only oop fields.
2076     if (!adr_type-&gt;isa_aryptr() ||
2077         (adr_type-&gt;isa_aryptr()-&gt;klass() == NULL) ||
2078          adr_type-&gt;isa_aryptr()-&gt;klass()-&gt;is_obj_array_klass()) {
2079       // OffsetBot is used to reference array&#39;s element. Ignore first AddP.
2080       if (find_second_addp(n, n-&gt;in(AddPNode::Base)) == NULL) {
2081         bt = T_OBJECT;
2082       }
2083     }
2084   } else if (offset != oopDesc::klass_offset_in_bytes()) {
2085     if (adr_type-&gt;isa_instptr()) {
<span class="line-modified">2086       ciField* field = _compile-&gt;alias_type(adr_type-&gt;isa_instptr())-&gt;field();</span>
2087       if (field != NULL) {
2088         bt = field-&gt;layout_type();
2089       } else {
2090         // Check for unsafe oop field access
2091         if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2092             n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2093             n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2094             BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2095           bt = T_OBJECT;
2096           (*unsafe) = true;
2097         }
2098       }
2099     } else if (adr_type-&gt;isa_aryptr()) {
2100       if (offset == arrayOopDesc::length_offset_in_bytes()) {
2101         // Ignore array length load.
2102       } else if (find_second_addp(n, n-&gt;in(AddPNode::Base)) != NULL) {
2103         // Ignore first AddP.
2104       } else {
2105         const Type* elemtype = adr_type-&gt;isa_aryptr()-&gt;elem();
<span class="line-modified">2106         bt = elemtype-&gt;array_element_basic_type();</span>






2107       }
2108     } else if (adr_type-&gt;isa_rawptr() || adr_type-&gt;isa_klassptr()) {
2109       // Allocation initialization, ThreadLocal field access, unsafe access
2110       if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2111           n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2112           n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2113           BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2114         bt = T_OBJECT;
2115       }
2116     }
2117   }
2118   // Note: T_NARROWOOP is not classed as a real reference type
2119   return (is_reference_type(bt) || bt == T_NARROWOOP);
2120 }
2121 
2122 // Returns unique pointed java object or NULL.
2123 JavaObjectNode* ConnectionGraph::unique_java_object(Node *n) {
<span class="line-modified">2124   assert(!_collecting, &quot;should not call when contructed graph&quot;);</span>
2125   // If the node was created after the escape computation we can&#39;t answer.
2126   uint idx = n-&gt;_idx;
2127   if (idx &gt;= nodes_size()) {
2128     return NULL;
2129   }
2130   PointsToNode* ptn = ptnode_adr(idx);
2131   if (ptn == NULL) {
2132     return NULL;
2133   }
2134   if (ptn-&gt;is_JavaObject()) {
2135     return ptn-&gt;as_JavaObject();
2136   }
2137   assert(ptn-&gt;is_LocalVar(), &quot;sanity&quot;);
2138   // Check all java objects it points to.
2139   JavaObjectNode* jobj = NULL;
2140   for (EdgeIterator i(ptn); i.has_next(); i.next()) {
2141     PointsToNode* e = i.get();
2142     if (e-&gt;is_JavaObject()) {
2143       if (jobj == NULL) {
2144         jobj = e-&gt;as_JavaObject();
</pre>
<hr />
<pre>
2247     if (i.get() == jobj)
2248       return true;
2249   }
2250   return false;
2251 }
2252 #endif
2253 
2254 int ConnectionGraph::address_offset(Node* adr, PhaseTransform *phase) {
2255   const Type *adr_type = phase-&gt;type(adr);
2256   if (adr-&gt;is_AddP() &amp;&amp; adr_type-&gt;isa_oopptr() == NULL &amp;&amp;
2257       adr-&gt;in(AddPNode::Address)-&gt;is_Proj() &amp;&amp;
2258       adr-&gt;in(AddPNode::Address)-&gt;in(0)-&gt;is_Allocate()) {
2259     // We are computing a raw address for a store captured by an Initialize
2260     // compute an appropriate address type. AddP cases #3 and #5 (see below).
2261     int offs = (int)phase-&gt;find_intptr_t_con(adr-&gt;in(AddPNode::Offset), Type::OffsetBot);
2262     assert(offs != Type::OffsetBot ||
2263            adr-&gt;in(AddPNode::Address)-&gt;in(0)-&gt;is_AllocateArray(),
2264            &quot;offset must be a constant or it is initialization of array&quot;);
2265     return offs;
2266   }
<span class="line-modified">2267   const TypePtr *t_ptr = adr_type-&gt;isa_ptr();</span>
<span class="line-removed">2268   assert(t_ptr != NULL, &quot;must be a pointer type&quot;);</span>
<span class="line-removed">2269   return t_ptr-&gt;offset();</span>
2270 }
2271 
2272 Node* ConnectionGraph::get_addp_base(Node *addp) {
2273   assert(addp-&gt;is_AddP(), &quot;must be AddP&quot;);
2274   //
2275   // AddP cases for Base and Address inputs:
2276   // case #1. Direct object&#39;s field reference:
2277   //     Allocate
2278   //       |
2279   //     Proj #5 ( oop result )
2280   //       |
2281   //     CheckCastPP (cast to instance type)
2282   //      | |
2283   //     AddP  ( base == address )
2284   //
2285   // case #2. Indirect object&#39;s field reference:
2286   //      Phi
2287   //       |
2288   //     CastPP (cast to instance type)
2289   //      | |
</pre>
<hr />
<pre>
2403   }
2404   return NULL;
2405 }
2406 
2407 //
2408 // Adjust the type and inputs of an AddP which computes the
2409 // address of a field of an instance
2410 //
2411 bool ConnectionGraph::split_AddP(Node *addp, Node *base) {
2412   PhaseGVN* igvn = _igvn;
2413   const TypeOopPtr *base_t = igvn-&gt;type(base)-&gt;isa_oopptr();
2414   assert(base_t != NULL &amp;&amp; base_t-&gt;is_known_instance(), &quot;expecting instance oopptr&quot;);
2415   const TypeOopPtr *t = igvn-&gt;type(addp)-&gt;isa_oopptr();
2416   if (t == NULL) {
2417     // We are computing a raw address for a store captured by an Initialize
2418     // compute an appropriate address type (cases #3 and #5).
2419     assert(igvn-&gt;type(addp) == TypeRawPtr::NOTNULL, &quot;must be raw pointer&quot;);
2420     assert(addp-&gt;in(AddPNode::Address)-&gt;is_Proj(), &quot;base of raw address must be result projection from allocation&quot;);
2421     intptr_t offs = (int)igvn-&gt;find_intptr_t_con(addp-&gt;in(AddPNode::Offset), Type::OffsetBot);
2422     assert(offs != Type::OffsetBot, &quot;offset must be a constant&quot;);
<span class="line-modified">2423     t = base_t-&gt;add_offset(offs)-&gt;is_oopptr();</span>







2424   }
<span class="line-modified">2425   int inst_id =  base_t-&gt;instance_id();</span>
2426   assert(!t-&gt;is_known_instance() || t-&gt;instance_id() == inst_id,
2427                              &quot;old type must be non-instance or match new type&quot;);
2428 
2429   // The type &#39;t&#39; could be subclass of &#39;base_t&#39;.
2430   // As result t-&gt;offset() could be large then base_t&#39;s size and it will
2431   // cause the failure in add_offset() with narrow oops since TypeOopPtr()
2432   // constructor verifies correctness of the offset.
2433   //
2434   // It could happened on subclass&#39;s branch (from the type profiling
2435   // inlining) which was not eliminated during parsing since the exactness
2436   // of the allocation type was not propagated to the subclass type check.
2437   //
2438   // Or the type &#39;t&#39; could be not related to &#39;base_t&#39; at all.
<span class="line-modified">2439   // It could happened when CHA type is different from MDO type on a dead path</span>
2440   // (for example, from instanceof check) which is not collapsed during parsing.
2441   //
2442   // Do nothing for such AddP node and don&#39;t process its users since
2443   // this code branch will go away.
2444   //
2445   if (!t-&gt;is_known_instance() &amp;&amp;
2446       !base_t-&gt;klass()-&gt;is_subtype_of(t-&gt;klass())) {
2447      return false; // bail out
2448   }
<span class="line-modified">2449   const TypeOopPtr *tinst = base_t-&gt;add_offset(t-&gt;offset())-&gt;is_oopptr();</span>






2450   // Do NOT remove the next line: ensure a new alias index is allocated
2451   // for the instance type. Note: C++ will not remove it since the call
2452   // has side effect.
2453   int alias_idx = _compile-&gt;get_alias_index(tinst);
2454   igvn-&gt;set_type(addp, tinst);
2455   // record the allocation in the node map
2456   set_map(addp, get_map(base-&gt;_idx));
2457   // Set addp&#39;s Base and Address to &#39;base&#39;.
2458   Node *abase = addp-&gt;in(AddPNode::Base);
2459   Node *adr   = addp-&gt;in(AddPNode::Address);
2460   if (adr-&gt;is_Proj() &amp;&amp; adr-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
2461       adr-&gt;in(0)-&gt;_idx == (uint)inst_id) {
2462     // Skip AddP cases #3 and #5.
2463   } else {
2464     assert(!abase-&gt;is_top(), &quot;sanity&quot;); // AddP case #3
2465     if (abase != base) {
2466       igvn-&gt;hash_delete(addp);
2467       addp-&gt;set_req(AddPNode::Base, base);
2468       if (abase == adr) {
2469         addp-&gt;set_req(AddPNode::Address, base);
</pre>
<hr />
<pre>
3134           igvn-&gt;hash_delete(tn);
3135           igvn-&gt;set_type(tn, tn_type);
3136           tn-&gt;set_type(tn_type);
3137           igvn-&gt;hash_insert(tn);
3138           record_for_optimizer(n);
3139         } else {
3140           assert(tn_type == TypePtr::NULL_PTR ||
3141                  tn_t != NULL &amp;&amp; !tinst-&gt;klass()-&gt;is_subtype_of(tn_t-&gt;klass()),
3142                  &quot;unexpected type&quot;);
3143           continue; // Skip dead path with different type
3144         }
3145       }
3146     } else {
3147       debug_only(n-&gt;dump();)
3148       assert(false, &quot;EA: unexpected node&quot;);
3149       continue;
3150     }
3151     // push allocation&#39;s users on appropriate worklist
3152     for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
3153       Node *use = n-&gt;fast_out(i);
<span class="line-modified">3154       if(use-&gt;is_Mem() &amp;&amp; use-&gt;in(MemNode::Address) == n) {</span>
3155         // Load/store to instance&#39;s field
3156         memnode_worklist.append_if_missing(use);
3157       } else if (use-&gt;is_MemBar()) {
3158         if (use-&gt;in(TypeFunc::Memory) == n) { // Ignore precedent edge
3159           memnode_worklist.append_if_missing(use);
3160         }
3161       } else if (use-&gt;is_AddP() &amp;&amp; use-&gt;outcnt() &gt; 0) { // No dead nodes
3162         Node* addp2 = find_second_addp(use, n);
3163         if (addp2 != NULL) {
3164           alloc_worklist.append_if_missing(addp2);
3165         }
3166         alloc_worklist.append_if_missing(use);
3167       } else if (use-&gt;is_Phi() ||
3168                  use-&gt;is_CheckCastPP() ||
3169                  use-&gt;is_EncodeNarrowPtr() ||
3170                  use-&gt;is_DecodeNarrowPtr() ||
3171                  (use-&gt;is_ConstraintCast() &amp;&amp; use-&gt;Opcode() == Op_CastPP)) {
3172         alloc_worklist.append_if_missing(use);
3173 #ifdef ASSERT
3174       } else if (use-&gt;is_Mem()) {
3175         assert(use-&gt;in(MemNode::Address) != n, &quot;EA: missing allocation reference path&quot;);
3176       } else if (use-&gt;is_MergeMem()) {
3177         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3178       } else if (use-&gt;is_SafePoint()) {
3179         // Look for MergeMem nodes for calls which reference unique allocation
3180         // (through CheckCastPP nodes) even for debug info.
3181         Node* m = use-&gt;in(TypeFunc::Memory);
3182         if (m-&gt;is_MergeMem()) {
3183           assert(_mergemem_worklist.contains(m-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3184         }
3185       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3186         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3187           // EncodeISOArray overwrites destination array
3188           memnode_worklist.append_if_missing(use);
3189         }








3190       } else {
3191         uint op = use-&gt;Opcode();
3192         if ((op == Op_StrCompressedCopy || op == Op_StrInflatedCopy) &amp;&amp;
3193             (use-&gt;in(MemNode::Memory) == n)) {
3194           // They overwrite memory edge corresponding to destination array,
3195           memnode_worklist.append_if_missing(use);
3196         } else if (!(op == Op_CmpP || op == Op_Conv2B ||
3197               op == Op_CastP2X || op == Op_StoreCM ||
3198               op == Op_FastLock || op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3199               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3200               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||
<span class="line-modified">3201               op == Op_SubTypeCheck ||</span>
3202               BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use))) {
3203           n-&gt;dump();
3204           use-&gt;dump();
3205           assert(false, &quot;EA: missing allocation reference path&quot;);
3206         }
3207 #endif
3208       }
3209     }
3210 
3211   }
3212 
3213   // Go over all ArrayCopy nodes and if one of the inputs has a unique
3214   // type, record it in the ArrayCopy node so we know what memory this
3215   // node uses/modified.
3216   for (int next = 0; next &lt; arraycopy_worklist.length(); next++) {
3217     ArrayCopyNode* ac = arraycopy_worklist.at(next);
3218     Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
3219     if (dest-&gt;is_AddP()) {
3220       dest = get_addp_base(dest);
3221     }
</pre>
<hr />
<pre>
3249   //            compute new values for Memory inputs  (the Memory inputs are not
3250   //            actually updated until phase 4.)
3251   if (memnode_worklist.length() == 0)
3252     return;  // nothing to do
3253   while (memnode_worklist.length() != 0) {
3254     Node *n = memnode_worklist.pop();
3255     if (visited.test_set(n-&gt;_idx))
3256       continue;
3257     if (n-&gt;is_Phi() || n-&gt;is_ClearArray()) {
3258       // we don&#39;t need to do anything, but the users must be pushed
3259     } else if (n-&gt;is_MemBar()) { // Initialize, MemBar nodes
3260       // we don&#39;t need to do anything, but the users must be pushed
3261       n = n-&gt;as_MemBar()-&gt;proj_out_or_null(TypeFunc::Memory);
3262       if (n == NULL)
3263         continue;
3264     } else if (n-&gt;Opcode() == Op_StrCompressedCopy ||
3265                n-&gt;Opcode() == Op_EncodeISOArray) {
3266       // get the memory projection
3267       n = n-&gt;find_out_with(Op_SCMemProj);
3268       assert(n != NULL &amp;&amp; n-&gt;Opcode() == Op_SCMemProj, &quot;memory projection required&quot;);



3269     } else {
3270       assert(n-&gt;is_Mem(), &quot;memory node required.&quot;);
3271       Node *addr = n-&gt;in(MemNode::Address);
3272       const Type *addr_t = igvn-&gt;type(addr);
3273       if (addr_t == Type::TOP)
3274         continue;
3275       assert (addr_t-&gt;isa_ptr() != NULL, &quot;pointer type required.&quot;);
3276       int alias_idx = _compile-&gt;get_alias_index(addr_t-&gt;is_ptr());
3277       assert ((uint)alias_idx &lt; new_index_end, &quot;wrong alias index&quot;);
3278       Node *mem = find_inst_mem(n-&gt;in(MemNode::Memory), alias_idx, orig_phis);
3279       if (_compile-&gt;failing()) {
3280         return;
3281       }
3282       if (mem != n-&gt;in(MemNode::Memory)) {
3283         // We delay the memory edge update since we need old one in
3284         // MergeMem code below when instances memory slices are separated.
3285         set_map(n, mem);
3286       }
3287       if (n-&gt;is_Load()) {
3288         continue;  // don&#39;t push users
3289       } else if (n-&gt;is_LoadStore()) {
3290         // get the memory projection
3291         n = n-&gt;find_out_with(Op_SCMemProj);
3292         assert(n != NULL &amp;&amp; n-&gt;Opcode() == Op_SCMemProj, &quot;memory projection required&quot;);
3293       }
3294     }
3295     // push user on appropriate worklist
3296     for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
3297       Node *use = n-&gt;fast_out(i);
3298       if (use-&gt;is_Phi() || use-&gt;is_ClearArray()) {
3299         memnode_worklist.append_if_missing(use);
3300       } else if (use-&gt;is_Mem() &amp;&amp; use-&gt;in(MemNode::Memory) == n) {
3301         if (use-&gt;Opcode() == Op_StoreCM) // Ignore cardmark stores
3302           continue;
3303         memnode_worklist.append_if_missing(use);
3304       } else if (use-&gt;is_MemBar()) {
3305         if (use-&gt;in(TypeFunc::Memory) == n) { // Ignore precedent edge
3306           memnode_worklist.append_if_missing(use);
3307         }
3308 #ifdef ASSERT
<span class="line-modified">3309       } else if(use-&gt;is_Mem()) {</span>
3310         assert(use-&gt;in(MemNode::Memory) != n, &quot;EA: missing memory path&quot;);
3311       } else if (use-&gt;is_MergeMem()) {
3312         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3313       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3314         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3315           // EncodeISOArray overwrites destination array
3316           memnode_worklist.append_if_missing(use);
3317         }




3318       } else {
3319         uint op = use-&gt;Opcode();
3320         if ((use-&gt;in(MemNode::Memory) == n) &amp;&amp;
3321             (op == Op_StrCompressedCopy || op == Op_StrInflatedCopy)) {
3322           // They overwrite memory edge corresponding to destination array,
3323           memnode_worklist.append_if_missing(use);
3324         } else if (!(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use) ||
3325               op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3326               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3327               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {
3328           n-&gt;dump();
3329           use-&gt;dump();
3330           assert(false, &quot;EA: missing memory path&quot;);
3331         }
3332 #endif
3333       }
3334     }
3335   }
3336 
3337   //  Phase 3:  Process MergeMem nodes from mergemem_worklist.
3338   //            Walk each memory slice moving the first node encountered of each
<span class="line-modified">3339   //            instance type to the the input corresponding to its alias index.</span>
3340   uint length = _mergemem_worklist.length();
3341   for( uint next = 0; next &lt; length; ++next ) {
3342     MergeMemNode* nmm = _mergemem_worklist.at(next);
3343     assert(!visited.test_set(nmm-&gt;_idx), &quot;should not be visited before&quot;);
3344     // Note: we don&#39;t want to use MergeMemStream here because we only want to
3345     // scan inputs which exist at the start, not ones we add during processing.
3346     // Note 2: MergeMem may already contains instance memory slices added
3347     // during find_inst_mem() call when memory nodes were processed above.
3348     igvn-&gt;hash_delete(nmm);
3349     uint nslices = MIN2(nmm-&gt;req(), new_index_start);
3350     for (uint i = Compile::AliasIdxRaw+1; i &lt; nslices; i++) {
3351       Node* mem = nmm-&gt;in(i);
3352       Node* cur = NULL;
3353       if (mem == NULL || mem-&gt;is_top())
3354         continue;
3355       // First, update mergemem by moving memory nodes to corresponding slices
3356       // if their type became more precise since this mergemem was created.
3357       while (mem-&gt;is_Mem()) {
3358         const Type *at = igvn-&gt;type(mem-&gt;in(MemNode::Address));
3359         if (at != Type::TOP) {
</pre>
<hr />
<pre>
3391       const TypeOopPtr *tinst = _compile-&gt;get_adr_type(ni)-&gt;isa_oopptr();
3392       Node* result = step_through_mergemem(nmm, ni, tinst);
3393       if (result == nmm-&gt;base_memory()) {
3394         // Didn&#39;t find instance memory, search through general slice recursively.
3395         result = nmm-&gt;memory_at(_compile-&gt;get_general_index(ni));
3396         result = find_inst_mem(result, ni, orig_phis);
3397         if (_compile-&gt;failing()) {
3398           return;
3399         }
3400         nmm-&gt;set_memory_at(ni, result);
3401       }
3402     }
3403     igvn-&gt;hash_insert(nmm);
3404     record_for_optimizer(nmm);
3405   }
3406 
3407   //  Phase 4:  Update the inputs of non-instance memory Phis and
3408   //            the Memory input of memnodes
3409   // First update the inputs of any non-instance Phi&#39;s from
3410   // which we split out an instance Phi.  Note we don&#39;t have
<span class="line-modified">3411   // to recursively process Phi&#39;s encounted on the input memory</span>
<span class="line-modified">3412   // chains as is done in split_memory_phi() since they  will</span>
3413   // also be processed here.
3414   for (int j = 0; j &lt; orig_phis.length(); j++) {
3415     PhiNode *phi = orig_phis.at(j);
3416     int alias_idx = _compile-&gt;get_alias_index(phi-&gt;adr_type());
3417     igvn-&gt;hash_delete(phi);
3418     for (uint i = 1; i &lt; phi-&gt;req(); i++) {
3419       Node *mem = phi-&gt;in(i);
3420       Node *new_mem = find_inst_mem(mem, alias_idx, orig_phis);
3421       if (_compile-&gt;failing()) {
3422         return;
3423       }
3424       if (mem != new_mem) {
3425         phi-&gt;set_req(i, new_mem);
3426       }
3427     }
3428     igvn-&gt;hash_insert(phi);
3429     record_for_optimizer(phi);
3430   }
3431 
3432   // Update the memory inputs of MemNodes with the value we computed
</pre>
</td>
<td>
<hr />
<pre>
 124   GrowableArray&lt;JavaObjectNode*&gt; non_escaped_worklist;
 125   GrowableArray&lt;FieldNode*&gt;      oop_fields_worklist;
 126   DEBUG_ONLY( GrowableArray&lt;Node*&gt; addp_worklist; )
 127 
 128   { Compile::TracePhase tp(&quot;connectionGraph&quot;, &amp;Phase::timers[Phase::_t_connectionGraph]);
 129 
 130   // 1. Populate Connection Graph (CG) with PointsTo nodes.
 131   ideal_nodes.map(C-&gt;live_nodes(), NULL);  // preallocate space
 132   // Initialize worklist
 133   if (C-&gt;root() != NULL) {
 134     ideal_nodes.push(C-&gt;root());
 135   }
 136   // Processed ideal nodes are unique on ideal_nodes list
 137   // but several ideal nodes are mapped to the phantom_obj.
 138   // To avoid duplicated entries on the following worklists
 139   // add the phantom_obj only once to them.
 140   ptnodes_worklist.append(phantom_obj);
 141   java_objects_worklist.append(phantom_obj);
 142   for( uint next = 0; next &lt; ideal_nodes.size(); ++next ) {
 143     Node* n = ideal_nodes.at(next);
<span class="line-added"> 144     if ((n-&gt;Opcode() == Op_LoadX || n-&gt;Opcode() == Op_StoreX) &amp;&amp;</span>
<span class="line-added"> 145         !n-&gt;in(MemNode::Address)-&gt;is_AddP() &amp;&amp;</span>
<span class="line-added"> 146         _igvn-&gt;type(n-&gt;in(MemNode::Address))-&gt;isa_oopptr()) {</span>
<span class="line-added"> 147       // Load/Store at mark work address is at offset 0 so has no AddP which confuses EA</span>
<span class="line-added"> 148       Node* addp = new AddPNode(n-&gt;in(MemNode::Address), n-&gt;in(MemNode::Address), _igvn-&gt;MakeConX(0));</span>
<span class="line-added"> 149       _igvn-&gt;register_new_node_with_optimizer(addp);</span>
<span class="line-added"> 150       _igvn-&gt;replace_input_of(n, MemNode::Address, addp);</span>
<span class="line-added"> 151       ideal_nodes.push(addp);</span>
<span class="line-added"> 152       _nodes.at_put_grow(addp-&gt;_idx, NULL, NULL);</span>
<span class="line-added"> 153     }</span>
 154     // Create PointsTo nodes and add them to Connection Graph. Called
 155     // only once per ideal node since ideal_nodes is Unique_Node list.
 156     add_node_to_connection_graph(n, &amp;delayed_worklist);
 157     PointsToNode* ptn = ptnode_adr(n-&gt;_idx);
 158     if (ptn != NULL &amp;&amp; ptn != phantom_obj) {
 159       ptnodes_worklist.append(ptn);
 160       if (ptn-&gt;is_JavaObject()) {
 161         java_objects_worklist.append(ptn-&gt;as_JavaObject());
 162         if ((n-&gt;is_Allocate() || n-&gt;is_CallStaticJava()) &amp;&amp;
 163             (ptn-&gt;escape_state() &lt; PointsToNode::GlobalEscape)) {
 164           // Only allocations and java static calls results are interesting.
 165           non_escaped_worklist.append(ptn-&gt;as_JavaObject());
 166         }
 167       } else if (ptn-&gt;is_Field() &amp;&amp; ptn-&gt;as_Field()-&gt;is_oop()) {
 168         oop_fields_worklist.append(ptn-&gt;as_Field());
 169       }
 170     }
 171     if (n-&gt;is_MergeMem()) {
 172       // Collect all MergeMem nodes to add memory slices for
 173       // scalar replaceable objects in split_unique_types().
</pre>
<hr />
<pre>
 372       // Put Lock and Unlock nodes on IGVN worklist to process them during
 373       // first IGVN optimization when escape information is still available.
 374       record_for_optimizer(n);
 375     } else if (n-&gt;is_Allocate()) {
 376       add_call_node(n-&gt;as_Call());
 377       record_for_optimizer(n);
 378     } else {
 379       if (n-&gt;is_CallStaticJava()) {
 380         const char* name = n-&gt;as_CallStaticJava()-&gt;_name;
 381         if (name != NULL &amp;&amp; strcmp(name, &quot;uncommon_trap&quot;) == 0)
 382           return; // Skip uncommon traps
 383       }
 384       // Don&#39;t mark as processed since call&#39;s arguments have to be processed.
 385       delayed_worklist-&gt;push(n);
 386       // Check if a call returns an object.
 387       if ((n-&gt;as_Call()-&gt;returns_pointer() &amp;&amp;
 388            n-&gt;as_Call()-&gt;proj_out_or_null(TypeFunc::Parms) != NULL) ||
 389           (n-&gt;is_CallStaticJava() &amp;&amp;
 390            n-&gt;as_CallStaticJava()-&gt;is_boxing_method())) {
 391         add_call_node(n-&gt;as_Call());
<span class="line-added"> 392       } else if (n-&gt;as_Call()-&gt;tf()-&gt;returns_inline_type_as_fields()) {</span>
<span class="line-added"> 393         bool returns_oop = false;</span>
<span class="line-added"> 394         for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax &amp;&amp; !returns_oop; i++) {</span>
<span class="line-added"> 395           ProjNode* pn = n-&gt;fast_out(i)-&gt;as_Proj();</span>
<span class="line-added"> 396           if (pn-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; pn-&gt;bottom_type()-&gt;isa_ptr()) {</span>
<span class="line-added"> 397             returns_oop = true;</span>
<span class="line-added"> 398           }</span>
<span class="line-added"> 399         }</span>
<span class="line-added"> 400         if (returns_oop) {</span>
<span class="line-added"> 401           add_call_node(n-&gt;as_Call());</span>
<span class="line-added"> 402         }</span>
 403       }
 404     }
 405     return;
 406   }
 407   // Put this check here to process call arguments since some call nodes
 408   // point to phantom_obj.
 409   if (n_ptn == phantom_obj || n_ptn == null_obj)
 410     return; // Skip predefined nodes.
 411 
 412   switch (opcode) {
 413     case Op_AddP: {
 414       Node* base = get_addp_base(n);
 415       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 416       // Field nodes are created for all field types. They are used in
 417       // adjust_scalar_replaceable_state() and split_unique_types().
 418       // Note, non-oop fields will have only base edges in Connection
 419       // Graph because such fields are not used for oop loads and stores.
 420       int offset = address_offset(n, igvn);
 421       add_field(n, PointsToNode::NoEscape, offset);
 422       if (ptn_base == NULL) {
 423         delayed_worklist-&gt;push(n); // Process it later.
 424       } else {
 425         n_ptn = ptnode_adr(n_idx);
 426         add_base(n_ptn-&gt;as_Field(), ptn_base);
 427       }
 428       break;
 429     }
 430     case Op_CastX2P: {
 431       map_ideal_node(n, phantom_obj);
 432       break;
 433     }
<span class="line-added"> 434     case Op_InlineTypePtr:</span>
 435     case Op_CastPP:
 436     case Op_CheckCastPP:
 437     case Op_EncodeP:
 438     case Op_DecodeN:
 439     case Op_EncodePKlass:
 440     case Op_DecodeNKlass: {
 441       add_local_var_and_edge(n, PointsToNode::NoEscape,
 442                              n-&gt;in(1), delayed_worklist);
 443       break;
 444     }
 445     case Op_CMoveP: {
 446       add_local_var(n, PointsToNode::NoEscape);
 447       // Do not add edges during first iteration because some could be
 448       // not defined yet.
 449       delayed_worklist-&gt;push(n);
 450       break;
 451     }
 452     case Op_ConP:
 453     case Op_ConN:
 454     case Op_ConNKlass: {
</pre>
<hr />
<pre>
 487     case Op_PartialSubtypeCheck: {
 488       // Produces Null or notNull and is used in only in CmpP so
 489       // phantom_obj could be used.
 490       map_ideal_node(n, phantom_obj); // Result is unknown
 491       break;
 492     }
 493     case Op_Phi: {
 494       // Using isa_ptr() instead of isa_oopptr() for LoadP and Phi because
 495       // ThreadLocal has RawPtr type.
 496       const Type* t = n-&gt;as_Phi()-&gt;type();
 497       if (t-&gt;make_ptr() != NULL) {
 498         add_local_var(n, PointsToNode::NoEscape);
 499         // Do not add edges during first iteration because some could be
 500         // not defined yet.
 501         delayed_worklist-&gt;push(n);
 502       }
 503       break;
 504     }
 505     case Op_Proj: {
 506       // we are only interested in the oop result projection from a call
<span class="line-modified"> 507       if (n-&gt;as_Proj()-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;</span>
<span class="line-modified"> 508           (n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer() || n-&gt;bottom_type()-&gt;isa_ptr())) {</span>
<span class="line-added"> 509         assert((n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) ||</span>
<span class="line-added"> 510                n-&gt;in(0)-&gt;as_Call()-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;what kind of oop return is it?&quot;);</span>
 511         add_local_var_and_edge(n, PointsToNode::NoEscape,
 512                                n-&gt;in(0), delayed_worklist);
 513       }
 514       break;
 515     }
 516     case Op_Rethrow: // Exception object escapes
 517     case Op_Return: {
 518       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 519           igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 520         // Treat Return value as LocalVar with GlobalEscape escape state.
 521         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 522                                n-&gt;in(TypeFunc::Parms), delayed_worklist);
 523       }
 524       break;
 525     }
 526     case Op_CompareAndExchangeP:
 527     case Op_CompareAndExchangeN:
 528     case Op_GetAndSetP:
 529     case Op_GetAndSetN: {
 530       add_objload_to_connection_graph(n, delayed_worklist);
</pre>
<hr />
<pre>
 586   if (n-&gt;is_Call()) {
 587     process_call_arguments(n-&gt;as_Call());
 588     return;
 589   }
 590   assert(n-&gt;is_Store() || n-&gt;is_LoadStore() ||
 591          (n_ptn != NULL) &amp;&amp; (n_ptn-&gt;ideal_node() != NULL),
 592          &quot;node should be registered already&quot;);
 593   int opcode = n-&gt;Opcode();
 594   bool gc_handled = BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_add_final_edges(this, _igvn, n, opcode);
 595   if (gc_handled) {
 596     return; // Ignore node if already handled by GC.
 597   }
 598   switch (opcode) {
 599     case Op_AddP: {
 600       Node* base = get_addp_base(n);
 601       PointsToNode* ptn_base = ptnode_adr(base-&gt;_idx);
 602       assert(ptn_base != NULL, &quot;field&#39;s base should be registered&quot;);
 603       add_base(n_ptn-&gt;as_Field(), ptn_base);
 604       break;
 605     }
<span class="line-added"> 606     case Op_InlineTypePtr:</span>
 607     case Op_CastPP:
 608     case Op_CheckCastPP:
 609     case Op_EncodeP:
 610     case Op_DecodeN:
 611     case Op_EncodePKlass:
 612     case Op_DecodeNKlass: {
 613       add_local_var_and_edge(n, PointsToNode::NoEscape,
 614                              n-&gt;in(1), NULL);
 615       break;
 616     }
 617     case Op_CMoveP: {
 618       for (uint i = CMoveNode::IfFalse; i &lt; n-&gt;req(); i++) {
 619         Node* in = n-&gt;in(i);
 620         if (in == NULL)
 621           continue;  // ignore NULL
 622         Node* uncast_in = in-&gt;uncast();
 623         if (uncast_in-&gt;is_top() || uncast_in == n)
 624           continue;  // ignore top or inputs which go back this node
 625         PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 626         assert(ptn != NULL, &quot;node should be registered&quot;);
</pre>
<hr />
<pre>
 646       // ThreadLocal has RawPtr type.
 647       const Type* t = n-&gt;as_Phi()-&gt;type();
 648       if (t-&gt;make_ptr() != NULL) {
 649         for (uint i = 1; i &lt; n-&gt;req(); i++) {
 650           Node* in = n-&gt;in(i);
 651           if (in == NULL)
 652             continue;  // ignore NULL
 653           Node* uncast_in = in-&gt;uncast();
 654           if (uncast_in-&gt;is_top() || uncast_in == n)
 655             continue;  // ignore top or inputs which go back this node
 656           PointsToNode* ptn = ptnode_adr(in-&gt;_idx);
 657           assert(ptn != NULL, &quot;node should be registered&quot;);
 658           add_edge(n_ptn, ptn);
 659         }
 660         break;
 661       }
 662       ELSE_FAIL(&quot;Op_Phi&quot;);
 663     }
 664     case Op_Proj: {
 665       // we are only interested in the oop result projection from a call
<span class="line-modified"> 666       if (n-&gt;as_Proj()-&gt;_con &gt;= TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;is_Call() &amp;&amp;</span>
<span class="line-modified"> 667           (n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()|| n-&gt;bottom_type()-&gt;isa_ptr())) {</span>
<span class="line-added"> 668         assert((n-&gt;as_Proj()-&gt;_con == TypeFunc::Parms &amp;&amp; n-&gt;in(0)-&gt;as_Call()-&gt;returns_pointer()) ||</span>
<span class="line-added"> 669                n-&gt;in(0)-&gt;as_Call()-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;what kind of oop return is it?&quot;);</span>
 670         add_local_var_and_edge(n, PointsToNode::NoEscape, n-&gt;in(0), NULL);
 671         break;
 672       }
 673       ELSE_FAIL(&quot;Op_Proj&quot;);
 674     }
 675     case Op_Rethrow: // Exception object escapes
 676     case Op_Return: {
 677       if (n-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
 678           _igvn-&gt;type(n-&gt;in(TypeFunc::Parms))-&gt;isa_oopptr()) {
 679         // Treat Return value as LocalVar with GlobalEscape escape state.
 680         add_local_var_and_edge(n, PointsToNode::GlobalEscape,
 681                                n-&gt;in(TypeFunc::Parms), NULL);
 682         break;
 683       }
 684       ELSE_FAIL(&quot;Op_Return&quot;);
 685     }
 686     case Op_StoreP:
 687     case Op_StoreN:
 688     case Op_StoreNKlass:
 689     case Op_StorePConditional:
</pre>
<hr />
<pre>
 811     return true;
 812   } else if ((opcode == Op_StoreP) &amp;&amp; adr_type-&gt;isa_rawptr()) {
 813     // Stored value escapes in unsafe access.
 814     Node* val = n-&gt;in(MemNode::ValueIn);
 815     PointsToNode* ptn = ptnode_adr(val-&gt;_idx);
 816     assert(ptn != NULL, &quot;node should be registered&quot;);
 817     set_escape_state(ptn, PointsToNode::GlobalEscape);
 818     // Add edge to object for unsafe access with offset.
 819     PointsToNode* adr_ptn = ptnode_adr(adr-&gt;_idx);
 820     assert(adr_ptn != NULL, &quot;node should be registered&quot;);
 821     if (adr_ptn-&gt;is_Field()) {
 822       assert(adr_ptn-&gt;as_Field()-&gt;is_oop(), &quot;should be oop field&quot;);
 823       add_edge(adr_ptn, ptn);
 824     }
 825     return true;
 826   }
 827   return false;
 828 }
 829 
 830 void ConnectionGraph::add_call_node(CallNode* call) {
<span class="line-modified"> 831   assert(call-&gt;returns_pointer() || call-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;only for call which returns pointer&quot;);</span>
 832   uint call_idx = call-&gt;_idx;
 833   if (call-&gt;is_Allocate()) {
 834     Node* k = call-&gt;in(AllocateNode::KlassNode);
 835     const TypeKlassPtr* kt = k-&gt;bottom_type()-&gt;isa_klassptr();
 836     assert(kt != NULL, &quot;TypeKlassPtr  required.&quot;);
 837     ciKlass* cik = kt-&gt;klass();
 838     PointsToNode::EscapeState es = PointsToNode::NoEscape;
 839     bool scalar_replaceable = true;
 840     if (call-&gt;is_AllocateArray()) {
 841       if (!cik-&gt;is_array_klass()) { // StressReflectiveCode
 842         es = PointsToNode::GlobalEscape;
 843       } else {
 844         int length = call-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);
 845         if (length &lt; 0 || length &gt; EliminateAllocationArraySizeLimit) {
 846           // Not scalar replaceable if the length is not constant or too big.
 847           scalar_replaceable = false;
 848         }
 849       }
 850     } else {  // Allocate instance
 851       if (cik-&gt;is_subclass_of(_compile-&gt;env()-&gt;Thread_klass()) ||
</pre>
<hr />
<pre>
 898       if (intr == vmIntrinsics::_floatValue || intr == vmIntrinsics::_doubleValue) {
 899         // It does not escape if object is always allocated.
 900         es = PointsToNode::NoEscape;
 901       } else {
 902         // It escapes globally if object could be loaded from cache.
 903         es = PointsToNode::GlobalEscape;
 904       }
 905       add_java_object(call, es);
 906     } else {
 907       BCEscapeAnalyzer* call_analyzer = meth-&gt;get_bcea();
 908       call_analyzer-&gt;copy_dependencies(_compile-&gt;dependencies());
 909       if (call_analyzer-&gt;is_return_allocated()) {
 910         // Returns a newly allocated unescaped object, simply
 911         // update dependency information.
 912         // Mark it as NoEscape so that objects referenced by
 913         // it&#39;s fields will be marked as NoEscape at least.
 914         add_java_object(call, PointsToNode::NoEscape);
 915         ptnode_adr(call_idx)-&gt;set_scalar_replaceable(false);
 916       } else {
 917         // Determine whether any arguments are returned.
<span class="line-modified"> 918         const TypeTuple* d = call-&gt;tf()-&gt;domain_cc();</span>
 919         bool ret_arg = false;
 920         for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
 921           if (d-&gt;field_at(i)-&gt;isa_ptr() != NULL &amp;&amp;
 922               call_analyzer-&gt;is_arg_returned(i - TypeFunc::Parms)) {
 923             ret_arg = true;
 924             break;
 925           }
 926         }
 927         if (ret_arg) {
 928           add_local_var(call, PointsToNode::ArgEscape);
 929         } else {
 930           // Returns unknown object.
 931           map_ideal_node(call, phantom_obj);
 932         }
 933       }
 934     }
 935   } else {
 936     // An other type of call, assume the worst case:
 937     // returned value is unknown and globally escapes.
 938     assert(call-&gt;Opcode() == Op_CallDynamicJava, &quot;add failed case check&quot;);
</pre>
<hr />
<pre>
 945     switch (call-&gt;Opcode()) {
 946 #ifdef ASSERT
 947     case Op_Allocate:
 948     case Op_AllocateArray:
 949     case Op_Lock:
 950     case Op_Unlock:
 951       assert(false, &quot;should be done already&quot;);
 952       break;
 953 #endif
 954     case Op_ArrayCopy:
 955     case Op_CallLeafNoFP:
 956       // Most array copies are ArrayCopy nodes at this point but there
 957       // are still a few direct calls to the copy subroutines (See
 958       // PhaseStringOpts::copy_string())
 959       is_arraycopy = (call-&gt;Opcode() == Op_ArrayCopy) ||
 960         call-&gt;as_CallLeaf()-&gt;is_call_to_arraycopystub();
 961       // fall through
 962     case Op_CallLeaf: {
 963       // Stub calls, objects do not escape but they are not scale replaceable.
 964       // Adjust escape state for outgoing arguments.
<span class="line-modified"> 965       const TypeTuple * d = call-&gt;tf()-&gt;domain_sig();</span>
 966       bool src_has_oops = false;
 967       for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
 968         const Type* at = d-&gt;field_at(i);
 969         Node *arg = call-&gt;in(i);
 970         if (arg == NULL) {
 971           continue;
 972         }
 973         const Type *aat = _igvn-&gt;type(arg);
 974         if (arg-&gt;is_top() || !at-&gt;isa_ptr() || !aat-&gt;isa_ptr())
 975           continue;
 976         if (arg-&gt;is_AddP()) {
 977           //
 978           // The inline_native_clone() case when the arraycopy stub is called
 979           // after the allocation before Initialize and CheckCastPP nodes.
 980           // Or normal arraycopy for object arrays case.
 981           //
 982           // Set AddP&#39;s base (Allocate) as not scalar replaceable since
 983           // pointer to the base (with offset) is passed as argument.
 984           //
 985           arg = get_addp_base(arg);
 986         }
 987         PointsToNode* arg_ptn = ptnode_adr(arg-&gt;_idx);
 988         assert(arg_ptn != NULL, &quot;should be registered&quot;);
 989         PointsToNode::EscapeState arg_esc = arg_ptn-&gt;escape_state();
 990         if (is_arraycopy || arg_esc &lt; PointsToNode::ArgEscape) {
 991           assert(aat == Type::TOP || aat == TypePtr::NULL_PTR ||
 992                  aat-&gt;isa_ptr() != NULL, &quot;expecting an Ptr&quot;);
 993           bool arg_has_oops = aat-&gt;isa_oopptr() &amp;&amp;
 994                               (aat-&gt;isa_oopptr()-&gt;klass() == NULL || aat-&gt;isa_instptr() ||
<span class="line-modified"> 995                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;klass()-&gt;is_obj_array_klass()) ||</span>
<span class="line-added"> 996                                (aat-&gt;isa_aryptr() &amp;&amp; aat-&gt;isa_aryptr()-&gt;elem() != NULL &amp;&amp;</span>
<span class="line-added"> 997                                 aat-&gt;isa_aryptr()-&gt;elem()-&gt;isa_inlinetype() &amp;&amp;</span>
<span class="line-added"> 998                                 aat-&gt;isa_aryptr()-&gt;elem()-&gt;inline_klass()-&gt;contains_oops()));</span>
 999           if (i == TypeFunc::Parms) {
1000             src_has_oops = arg_has_oops;
1001           }
1002           //
1003           // src or dst could be j.l.Object when other is basic type array:
1004           //
1005           //   arraycopy(char[],0,Object*,0,size);
1006           //   arraycopy(Object*,0,char[],0,size);
1007           //
1008           // Don&#39;t add edges in such cases.
1009           //
1010           bool arg_is_arraycopy_dest = src_has_oops &amp;&amp; is_arraycopy &amp;&amp;
1011                                        arg_has_oops &amp;&amp; (i &gt; TypeFunc::Parms);
1012 #ifdef ASSERT
1013           if (!(is_arraycopy ||
1014                 BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(call) ||
1015                 (call-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;
1016                  (strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32&quot;) == 0 ||
1017                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesCRC32C&quot;) == 0 ||
1018                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;updateBytesAdler32&quot;) == 0 ||
1019                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;aescrypt_encryptBlock&quot;) == 0 ||
1020                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;aescrypt_decryptBlock&quot;) == 0 ||
1021                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_encryptAESCrypt&quot;) == 0 ||
1022                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;cipherBlockChaining_decryptAESCrypt&quot;) == 0 ||
1023                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_encryptAESCrypt&quot;) == 0 ||
1024                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;electronicCodeBook_decryptAESCrypt&quot;) == 0 ||
1025                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;counterMode_AESCrypt&quot;) == 0 ||
1026                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;ghash_processBlocks&quot;) == 0 ||
1027                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;encodeBlock&quot;) == 0 ||
1028                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompress&quot;) == 0 ||
1029                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha1_implCompressMB&quot;) == 0 ||
1030                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompress&quot;) == 0 ||
1031                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha256_implCompressMB&quot;) == 0 ||
1032                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompress&quot;) == 0 ||
1033                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;sha512_implCompressMB&quot;) == 0 ||
1034                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;multiplyToLen&quot;) == 0 ||
1035                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;squareToLen&quot;) == 0 ||
1036                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;mulAdd&quot;) == 0 ||
1037                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_multiply&quot;) == 0 ||
1038                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;montgomery_square&quot;) == 0 ||
<span class="line-added">1039                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0 ||</span>
<span class="line-added">1040                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;load_unknown_inline&quot;) == 0 ||</span>
<span class="line-added">1041                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_inline&quot;) == 0 ||</span>
1042                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerRightShiftWorker&quot;) == 0 ||
1043                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;bigIntegerLeftShiftWorker&quot;) == 0 ||
1044                   strcmp(call-&gt;as_CallLeaf()-&gt;_name, &quot;vectorizedMismatch&quot;) == 0)
1045                  ))) {
1046             call-&gt;dump();
1047             fatal(&quot;EA unexpected CallLeaf %s&quot;, call-&gt;as_CallLeaf()-&gt;_name);
1048           }
1049 #endif
1050           // Always process arraycopy&#39;s destination object since
1051           // we need to add all possible edges to references in
1052           // source object.
1053           if (arg_esc &gt;= PointsToNode::ArgEscape &amp;&amp;
1054               !arg_is_arraycopy_dest) {
1055             continue;
1056           }
1057           PointsToNode::EscapeState es = PointsToNode::ArgEscape;
1058           if (call-&gt;is_ArrayCopy()) {
1059             ArrayCopyNode* ac = call-&gt;as_ArrayCopy();
1060             if (ac-&gt;is_clonebasic() ||
1061                 ac-&gt;is_arraycopy_validated() ||
</pre>
<hr />
<pre>
1083           }
1084         }
1085       }
1086       break;
1087     }
1088     case Op_CallStaticJava: {
1089       // For a static call, we know exactly what method is being called.
1090       // Use bytecode estimator to record the call&#39;s escape affects
1091 #ifdef ASSERT
1092       const char* name = call-&gt;as_CallStaticJava()-&gt;_name;
1093       assert((name == NULL || strcmp(name, &quot;uncommon_trap&quot;) != 0), &quot;normal calls only&quot;);
1094 #endif
1095       ciMethod* meth = call-&gt;as_CallJava()-&gt;method();
1096       if ((meth != NULL) &amp;&amp; meth-&gt;is_boxing_method()) {
1097         break; // Boxing methods do not modify any oops.
1098       }
1099       BCEscapeAnalyzer* call_analyzer = (meth !=NULL) ? meth-&gt;get_bcea() : NULL;
1100       // fall-through if not a Java method or no analyzer information
1101       if (call_analyzer != NULL) {
1102         PointsToNode* call_ptn = ptnode_adr(call-&gt;_idx);
<span class="line-modified">1103         const TypeTuple* d = call-&gt;tf()-&gt;domain_cc();</span>
1104         for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
1105           const Type* at = d-&gt;field_at(i);
1106           int k = i - TypeFunc::Parms;
1107           Node* arg = call-&gt;in(i);
1108           PointsToNode* arg_ptn = ptnode_adr(arg-&gt;_idx);
1109           if (at-&gt;isa_ptr() != NULL &amp;&amp;
1110               call_analyzer-&gt;is_arg_returned(k)) {
1111             // The call returns arguments.
1112             if (call_ptn != NULL) { // Is call&#39;s result used?
1113               assert(call_ptn-&gt;is_LocalVar(), &quot;node should be registered&quot;);
1114               assert(arg_ptn != NULL, &quot;node should be registered&quot;);
1115               add_edge(call_ptn, arg_ptn);
1116             }
1117           }
1118           if (at-&gt;isa_oopptr() != NULL &amp;&amp;
1119               arg_ptn-&gt;escape_state() &lt; PointsToNode::GlobalEscape) {
1120             if (!call_analyzer-&gt;is_arg_stack(k)) {
1121               // The argument global escapes
1122               set_escape_state(arg_ptn, PointsToNode::GlobalEscape);
1123             } else {
</pre>
<hr />
<pre>
1127                 set_fields_escape_state(arg_ptn, PointsToNode::GlobalEscape);
1128               }
1129             }
1130           }
1131         }
1132         if (call_ptn != NULL &amp;&amp; call_ptn-&gt;is_LocalVar()) {
1133           // The call returns arguments.
1134           assert(call_ptn-&gt;edge_count() &gt; 0, &quot;sanity&quot;);
1135           if (!call_analyzer-&gt;is_return_local()) {
1136             // Returns also unknown object.
1137             add_edge(call_ptn, phantom_obj);
1138           }
1139         }
1140         break;
1141       }
1142     }
1143     default: {
1144       // Fall-through here if not a Java method or no analyzer information
1145       // or some other type of call, assume the worst case: all arguments
1146       // globally escape.
<span class="line-modified">1147       const TypeTuple* d = call-&gt;tf()-&gt;domain_cc();</span>
1148       for (uint i = TypeFunc::Parms; i &lt; d-&gt;cnt(); i++) {
1149         const Type* at = d-&gt;field_at(i);
1150         if (at-&gt;isa_oopptr() != NULL) {
1151           Node* arg = call-&gt;in(i);
1152           if (arg-&gt;is_AddP()) {
1153             arg = get_addp_base(arg);
1154           }
1155           assert(ptnode_adr(arg-&gt;_idx) != NULL, &quot;should be defined already&quot;);
1156           set_escape_state(ptnode_adr(arg-&gt;_idx), PointsToNode::GlobalEscape);
1157         }
1158       }
1159     }
1160   }
1161 }
1162 
1163 
1164 // Finish Graph construction.
1165 bool ConnectionGraph::complete_connection_graph(
1166                          GrowableArray&lt;PointsToNode*&gt;&amp;   ptnodes_worklist,
1167                          GrowableArray&lt;JavaObjectNode*&gt;&amp; non_escaped_worklist,
</pre>
<hr />
<pre>
1648               } else {
1649                 if (!val-&gt;is_LocalVar() || (val-&gt;edge_count() == 0)) {
1650                   tty-&gt;print_cr(&quot;----------init store has invalid value -----&quot;);
1651                   store-&gt;dump();
1652                   val-&gt;dump();
1653                   assert(val-&gt;is_LocalVar() &amp;&amp; (val-&gt;edge_count() &gt; 0), &quot;should be processed already&quot;);
1654                 }
1655                 for (EdgeIterator j(val); j.has_next(); j.next()) {
1656                   PointsToNode* obj = j.get();
1657                   if (obj-&gt;is_JavaObject()) {
1658                     if (!field-&gt;points_to(obj-&gt;as_JavaObject())) {
1659                       missed_obj = obj;
1660                       break;
1661                     }
1662                   }
1663                 }
1664               }
1665               if (missed_obj != NULL) {
1666                 tty-&gt;print_cr(&quot;----------field---------------------------------&quot;);
1667                 field-&gt;dump();
<span class="line-modified">1668                 tty-&gt;print_cr(&quot;----------missed reference to object------------&quot;);</span>
1669                 missed_obj-&gt;dump();
<span class="line-modified">1670                 tty-&gt;print_cr(&quot;----------object referenced by init store-------&quot;);</span>
1671                 store-&gt;dump();
1672                 val-&gt;dump();
1673                 assert(!field-&gt;points_to(missed_obj-&gt;as_JavaObject()), &quot;missed JavaObject reference&quot;);
1674               }
1675             }
1676 #endif
1677           } else {
1678             // There could be initializing stores which follow allocation.
1679             // For example, a volatile field store is not collected
1680             // by Initialize node.
1681             //
1682             // Need to check for dependent loads to separate such stores from
1683             // stores which follow loads. For now, add initial value NULL so
1684             // that compare pointers optimization works correctly.
1685           }
1686         }
1687         if (value == NULL) {
1688           // A field&#39;s initializing value was not recorded. Add NULL.
1689           if (add_edge(field, null_obj)) {
1690             // New edge was added
</pre>
<hr />
<pre>
1720       }
1721       // 2. An object is not scalar replaceable if the field into which it is
1722       // stored has multiple bases one of which is null.
1723       if (field-&gt;base_count() &gt; 1) {
1724         for (BaseIterator i(field); i.has_next(); i.next()) {
1725           PointsToNode* base = i.get();
1726           if (base == null_obj) {
1727             jobj-&gt;set_scalar_replaceable(false);
1728             return;
1729           }
1730         }
1731       }
1732     }
1733     assert(use-&gt;is_Field() || use-&gt;is_LocalVar(), &quot;sanity&quot;);
1734     // 3. An object is not scalar replaceable if it is merged with other objects.
1735     for (EdgeIterator j(use); j.has_next(); j.next()) {
1736       PointsToNode* ptn = j.get();
1737       if (ptn-&gt;is_JavaObject() &amp;&amp; ptn != jobj) {
1738         // Mark all objects.
1739         jobj-&gt;set_scalar_replaceable(false);
<span class="line-modified">1740         ptn-&gt;set_scalar_replaceable(false);</span>
1741       }
1742     }
1743     if (!jobj-&gt;scalar_replaceable()) {
1744       return;
1745     }
1746   }
1747 
1748   for (EdgeIterator j(jobj); j.has_next(); j.next()) {
1749     if (j.get()-&gt;is_Arraycopy()) {
1750       continue;
1751     }
1752 
1753     // Non-escaping object node should point only to field nodes.
1754     FieldNode* field = j.get()-&gt;as_Field();
1755     int offset = field-&gt;as_Field()-&gt;offset();
1756 
1757     // 4. An object is not scalar replaceable if it has a field with unknown
1758     // offset (array&#39;s element is accessed in loop).
1759     if (offset == Type::OffsetBot) {
1760       jobj-&gt;set_scalar_replaceable(false);
</pre>
<hr />
<pre>
1883         assert(field-&gt;edge_count() &gt; 0, &quot;sanity&quot;);
1884       }
1885     }
1886   }
1887 }
1888 #endif
1889 
1890 // Optimize ideal graph.
1891 void ConnectionGraph::optimize_ideal_graph(GrowableArray&lt;Node*&gt;&amp; ptr_cmp_worklist,
1892                                            GrowableArray&lt;Node*&gt;&amp; storestore_worklist) {
1893   Compile* C = _compile;
1894   PhaseIterGVN* igvn = _igvn;
1895   if (EliminateLocks) {
1896     // Mark locks before changing ideal graph.
1897     int cnt = C-&gt;macro_count();
1898     for( int i=0; i &lt; cnt; i++ ) {
1899       Node *n = C-&gt;macro_node(i);
1900       if (n-&gt;is_AbstractLock()) { // Lock and Unlock nodes
1901         AbstractLockNode* alock = n-&gt;as_AbstractLock();
1902         if (!alock-&gt;is_non_esc_obj()) {
<span class="line-modified">1903           const Type* obj_type = igvn-&gt;type(alock-&gt;obj_node());</span>
<span class="line-added">1904           if (not_global_escape(alock-&gt;obj_node()) &amp;&amp;</span>
<span class="line-added">1905               !obj_type-&gt;isa_inlinetype() &amp;&amp; !obj_type-&gt;is_inlinetypeptr()) {</span>
1906             assert(!alock-&gt;is_eliminated() || alock-&gt;is_coarsened(), &quot;sanity&quot;);
1907             // The lock could be marked eliminated by lock coarsening
1908             // code during first IGVN before EA. Replace coarsened flag
1909             // to eliminate all associated locks/unlocks.
1910 #ifdef ASSERT
1911             alock-&gt;log_lock_optimization(C, &quot;eliminate_lock_set_non_esc3&quot;);
1912 #endif
1913             alock-&gt;set_non_esc_obj();
1914           }
1915         }
1916       }
1917     }
1918   }
1919 
1920   if (OptimizePtrCompare) {
1921     // Add ConI(#CC_GT) and ConI(#CC_EQ).
1922     _pcmp_neq = igvn-&gt;makecon(TypeInt::CC_GT);
1923     _pcmp_eq = igvn-&gt;makecon(TypeInt::CC_EQ);
1924     // Optimize objects compare.
1925     while (ptr_cmp_worklist.length() != 0) {
</pre>
<hr />
<pre>
2088   assert(!src-&gt;is_Field() &amp;&amp; !dst-&gt;is_Field(), &quot;only for JavaObject and LocalVar&quot;);
2089   assert((src != null_obj) &amp;&amp; (dst != null_obj), &quot;not for ConP NULL&quot;);
2090   PointsToNode* ptadr = _nodes.at(n-&gt;_idx);
2091   if (ptadr != NULL) {
2092     assert(ptadr-&gt;is_Arraycopy() &amp;&amp; ptadr-&gt;ideal_node() == n, &quot;sanity&quot;);
2093     return;
2094   }
2095   Compile* C = _compile;
2096   ptadr = new (C-&gt;comp_arena()) ArraycopyNode(this, n, es);
2097   _nodes.at_put(n-&gt;_idx, ptadr);
2098   // Add edge from arraycopy node to source object.
2099   (void)add_edge(ptadr, src);
2100   src-&gt;set_arraycopy_src();
2101   // Add edge from destination object to arraycopy node.
2102   (void)add_edge(dst, ptadr);
2103   dst-&gt;set_arraycopy_dst();
2104 }
2105 
2106 bool ConnectionGraph::is_oop_field(Node* n, int offset, bool* unsafe) {
2107   const Type* adr_type = n-&gt;as_AddP()-&gt;bottom_type();
<span class="line-added">2108   int field_offset = adr_type-&gt;isa_aryptr() ? adr_type-&gt;isa_aryptr()-&gt;field_offset().get() : Type::OffsetBot;</span>
2109   BasicType bt = T_INT;
<span class="line-modified">2110   if (offset == Type::OffsetBot &amp;&amp; field_offset == Type::OffsetBot) {</span>
2111     // Check only oop fields.
2112     if (!adr_type-&gt;isa_aryptr() ||
2113         (adr_type-&gt;isa_aryptr()-&gt;klass() == NULL) ||
2114          adr_type-&gt;isa_aryptr()-&gt;klass()-&gt;is_obj_array_klass()) {
2115       // OffsetBot is used to reference array&#39;s element. Ignore first AddP.
2116       if (find_second_addp(n, n-&gt;in(AddPNode::Base)) == NULL) {
2117         bt = T_OBJECT;
2118       }
2119     }
2120   } else if (offset != oopDesc::klass_offset_in_bytes()) {
2121     if (adr_type-&gt;isa_instptr()) {
<span class="line-modified">2122       ciField* field = _compile-&gt;alias_type(adr_type-&gt;is_ptr())-&gt;field();</span>
2123       if (field != NULL) {
2124         bt = field-&gt;layout_type();
2125       } else {
2126         // Check for unsafe oop field access
2127         if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2128             n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2129             n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2130             BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2131           bt = T_OBJECT;
2132           (*unsafe) = true;
2133         }
2134       }
2135     } else if (adr_type-&gt;isa_aryptr()) {
2136       if (offset == arrayOopDesc::length_offset_in_bytes()) {
2137         // Ignore array length load.
2138       } else if (find_second_addp(n, n-&gt;in(AddPNode::Base)) != NULL) {
2139         // Ignore first AddP.
2140       } else {
2141         const Type* elemtype = adr_type-&gt;isa_aryptr()-&gt;elem();
<span class="line-modified">2142         if (elemtype-&gt;isa_inlinetype() &amp;&amp; field_offset != Type::OffsetBot) {</span>
<span class="line-added">2143           ciInlineKlass* vk = elemtype-&gt;inline_klass();</span>
<span class="line-added">2144           field_offset += vk-&gt;first_field_offset();</span>
<span class="line-added">2145           bt = vk-&gt;get_field_by_offset(field_offset, false)-&gt;layout_type();</span>
<span class="line-added">2146         } else {</span>
<span class="line-added">2147           bt = elemtype-&gt;array_element_basic_type();</span>
<span class="line-added">2148         }</span>
2149       }
2150     } else if (adr_type-&gt;isa_rawptr() || adr_type-&gt;isa_klassptr()) {
2151       // Allocation initialization, ThreadLocal field access, unsafe access
2152       if (n-&gt;has_out_with(Op_StoreP, Op_LoadP, Op_StoreN, Op_LoadN) ||
2153           n-&gt;has_out_with(Op_GetAndSetP, Op_GetAndSetN, Op_CompareAndExchangeP, Op_CompareAndExchangeN) ||
2154           n-&gt;has_out_with(Op_CompareAndSwapP, Op_CompareAndSwapN, Op_WeakCompareAndSwapP, Op_WeakCompareAndSwapN) ||
2155           BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;escape_has_out_with_unsafe_object(n)) {
2156         bt = T_OBJECT;
2157       }
2158     }
2159   }
2160   // Note: T_NARROWOOP is not classed as a real reference type
2161   return (is_reference_type(bt) || bt == T_NARROWOOP);
2162 }
2163 
2164 // Returns unique pointed java object or NULL.
2165 JavaObjectNode* ConnectionGraph::unique_java_object(Node *n) {
<span class="line-modified">2166   assert(!_collecting, &quot;should not call when constructed graph&quot;);</span>
2167   // If the node was created after the escape computation we can&#39;t answer.
2168   uint idx = n-&gt;_idx;
2169   if (idx &gt;= nodes_size()) {
2170     return NULL;
2171   }
2172   PointsToNode* ptn = ptnode_adr(idx);
2173   if (ptn == NULL) {
2174     return NULL;
2175   }
2176   if (ptn-&gt;is_JavaObject()) {
2177     return ptn-&gt;as_JavaObject();
2178   }
2179   assert(ptn-&gt;is_LocalVar(), &quot;sanity&quot;);
2180   // Check all java objects it points to.
2181   JavaObjectNode* jobj = NULL;
2182   for (EdgeIterator i(ptn); i.has_next(); i.next()) {
2183     PointsToNode* e = i.get();
2184     if (e-&gt;is_JavaObject()) {
2185       if (jobj == NULL) {
2186         jobj = e-&gt;as_JavaObject();
</pre>
<hr />
<pre>
2289     if (i.get() == jobj)
2290       return true;
2291   }
2292   return false;
2293 }
2294 #endif
2295 
2296 int ConnectionGraph::address_offset(Node* adr, PhaseTransform *phase) {
2297   const Type *adr_type = phase-&gt;type(adr);
2298   if (adr-&gt;is_AddP() &amp;&amp; adr_type-&gt;isa_oopptr() == NULL &amp;&amp;
2299       adr-&gt;in(AddPNode::Address)-&gt;is_Proj() &amp;&amp;
2300       adr-&gt;in(AddPNode::Address)-&gt;in(0)-&gt;is_Allocate()) {
2301     // We are computing a raw address for a store captured by an Initialize
2302     // compute an appropriate address type. AddP cases #3 and #5 (see below).
2303     int offs = (int)phase-&gt;find_intptr_t_con(adr-&gt;in(AddPNode::Offset), Type::OffsetBot);
2304     assert(offs != Type::OffsetBot ||
2305            adr-&gt;in(AddPNode::Address)-&gt;in(0)-&gt;is_AllocateArray(),
2306            &quot;offset must be a constant or it is initialization of array&quot;);
2307     return offs;
2308   }
<span class="line-modified">2309   return adr_type-&gt;is_ptr()-&gt;flattened_offset();</span>


2310 }
2311 
2312 Node* ConnectionGraph::get_addp_base(Node *addp) {
2313   assert(addp-&gt;is_AddP(), &quot;must be AddP&quot;);
2314   //
2315   // AddP cases for Base and Address inputs:
2316   // case #1. Direct object&#39;s field reference:
2317   //     Allocate
2318   //       |
2319   //     Proj #5 ( oop result )
2320   //       |
2321   //     CheckCastPP (cast to instance type)
2322   //      | |
2323   //     AddP  ( base == address )
2324   //
2325   // case #2. Indirect object&#39;s field reference:
2326   //      Phi
2327   //       |
2328   //     CastPP (cast to instance type)
2329   //      | |
</pre>
<hr />
<pre>
2443   }
2444   return NULL;
2445 }
2446 
2447 //
2448 // Adjust the type and inputs of an AddP which computes the
2449 // address of a field of an instance
2450 //
2451 bool ConnectionGraph::split_AddP(Node *addp, Node *base) {
2452   PhaseGVN* igvn = _igvn;
2453   const TypeOopPtr *base_t = igvn-&gt;type(base)-&gt;isa_oopptr();
2454   assert(base_t != NULL &amp;&amp; base_t-&gt;is_known_instance(), &quot;expecting instance oopptr&quot;);
2455   const TypeOopPtr *t = igvn-&gt;type(addp)-&gt;isa_oopptr();
2456   if (t == NULL) {
2457     // We are computing a raw address for a store captured by an Initialize
2458     // compute an appropriate address type (cases #3 and #5).
2459     assert(igvn-&gt;type(addp) == TypeRawPtr::NOTNULL, &quot;must be raw pointer&quot;);
2460     assert(addp-&gt;in(AddPNode::Address)-&gt;is_Proj(), &quot;base of raw address must be result projection from allocation&quot;);
2461     intptr_t offs = (int)igvn-&gt;find_intptr_t_con(addp-&gt;in(AddPNode::Offset), Type::OffsetBot);
2462     assert(offs != Type::OffsetBot, &quot;offset must be a constant&quot;);
<span class="line-modified">2463     if (base_t-&gt;isa_aryptr() != NULL) {</span>
<span class="line-added">2464       // In the case of a flattened inline type array, each field has its</span>
<span class="line-added">2465       // own slice so we need to extract the field being accessed from</span>
<span class="line-added">2466       // the address computation</span>
<span class="line-added">2467       t = base_t-&gt;isa_aryptr()-&gt;add_field_offset_and_offset(offs)-&gt;is_oopptr();</span>
<span class="line-added">2468     } else {</span>
<span class="line-added">2469       t = base_t-&gt;add_offset(offs)-&gt;is_oopptr();</span>
<span class="line-added">2470     }</span>
2471   }
<span class="line-modified">2472   int inst_id = base_t-&gt;instance_id();</span>
2473   assert(!t-&gt;is_known_instance() || t-&gt;instance_id() == inst_id,
2474                              &quot;old type must be non-instance or match new type&quot;);
2475 
2476   // The type &#39;t&#39; could be subclass of &#39;base_t&#39;.
2477   // As result t-&gt;offset() could be large then base_t&#39;s size and it will
2478   // cause the failure in add_offset() with narrow oops since TypeOopPtr()
2479   // constructor verifies correctness of the offset.
2480   //
2481   // It could happened on subclass&#39;s branch (from the type profiling
2482   // inlining) which was not eliminated during parsing since the exactness
2483   // of the allocation type was not propagated to the subclass type check.
2484   //
2485   // Or the type &#39;t&#39; could be not related to &#39;base_t&#39; at all.
<span class="line-modified">2486   // It could happen when CHA type is different from MDO type on a dead path</span>
2487   // (for example, from instanceof check) which is not collapsed during parsing.
2488   //
2489   // Do nothing for such AddP node and don&#39;t process its users since
2490   // this code branch will go away.
2491   //
2492   if (!t-&gt;is_known_instance() &amp;&amp;
2493       !base_t-&gt;klass()-&gt;is_subtype_of(t-&gt;klass())) {
2494      return false; // bail out
2495   }
<span class="line-modified">2496   const TypePtr* tinst = base_t-&gt;add_offset(t-&gt;offset());</span>
<span class="line-added">2497   if (tinst-&gt;isa_aryptr() &amp;&amp; t-&gt;isa_aryptr()) {</span>
<span class="line-added">2498     // In the case of a flattened inline type array, each field has its</span>
<span class="line-added">2499     // own slice so we need to keep track of the field being accessed.</span>
<span class="line-added">2500     tinst = tinst-&gt;is_aryptr()-&gt;with_field_offset(t-&gt;is_aryptr()-&gt;field_offset().get());</span>
<span class="line-added">2501   }</span>
<span class="line-added">2502 </span>
2503   // Do NOT remove the next line: ensure a new alias index is allocated
2504   // for the instance type. Note: C++ will not remove it since the call
2505   // has side effect.
2506   int alias_idx = _compile-&gt;get_alias_index(tinst);
2507   igvn-&gt;set_type(addp, tinst);
2508   // record the allocation in the node map
2509   set_map(addp, get_map(base-&gt;_idx));
2510   // Set addp&#39;s Base and Address to &#39;base&#39;.
2511   Node *abase = addp-&gt;in(AddPNode::Base);
2512   Node *adr   = addp-&gt;in(AddPNode::Address);
2513   if (adr-&gt;is_Proj() &amp;&amp; adr-&gt;in(0)-&gt;is_Allocate() &amp;&amp;
2514       adr-&gt;in(0)-&gt;_idx == (uint)inst_id) {
2515     // Skip AddP cases #3 and #5.
2516   } else {
2517     assert(!abase-&gt;is_top(), &quot;sanity&quot;); // AddP case #3
2518     if (abase != base) {
2519       igvn-&gt;hash_delete(addp);
2520       addp-&gt;set_req(AddPNode::Base, base);
2521       if (abase == adr) {
2522         addp-&gt;set_req(AddPNode::Address, base);
</pre>
<hr />
<pre>
3187           igvn-&gt;hash_delete(tn);
3188           igvn-&gt;set_type(tn, tn_type);
3189           tn-&gt;set_type(tn_type);
3190           igvn-&gt;hash_insert(tn);
3191           record_for_optimizer(n);
3192         } else {
3193           assert(tn_type == TypePtr::NULL_PTR ||
3194                  tn_t != NULL &amp;&amp; !tinst-&gt;klass()-&gt;is_subtype_of(tn_t-&gt;klass()),
3195                  &quot;unexpected type&quot;);
3196           continue; // Skip dead path with different type
3197         }
3198       }
3199     } else {
3200       debug_only(n-&gt;dump();)
3201       assert(false, &quot;EA: unexpected node&quot;);
3202       continue;
3203     }
3204     // push allocation&#39;s users on appropriate worklist
3205     for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
3206       Node *use = n-&gt;fast_out(i);
<span class="line-modified">3207       if (use-&gt;is_Mem() &amp;&amp; use-&gt;in(MemNode::Address) == n) {</span>
3208         // Load/store to instance&#39;s field
3209         memnode_worklist.append_if_missing(use);
3210       } else if (use-&gt;is_MemBar()) {
3211         if (use-&gt;in(TypeFunc::Memory) == n) { // Ignore precedent edge
3212           memnode_worklist.append_if_missing(use);
3213         }
3214       } else if (use-&gt;is_AddP() &amp;&amp; use-&gt;outcnt() &gt; 0) { // No dead nodes
3215         Node* addp2 = find_second_addp(use, n);
3216         if (addp2 != NULL) {
3217           alloc_worklist.append_if_missing(addp2);
3218         }
3219         alloc_worklist.append_if_missing(use);
3220       } else if (use-&gt;is_Phi() ||
3221                  use-&gt;is_CheckCastPP() ||
3222                  use-&gt;is_EncodeNarrowPtr() ||
3223                  use-&gt;is_DecodeNarrowPtr() ||
3224                  (use-&gt;is_ConstraintCast() &amp;&amp; use-&gt;Opcode() == Op_CastPP)) {
3225         alloc_worklist.append_if_missing(use);
3226 #ifdef ASSERT
3227       } else if (use-&gt;is_Mem()) {
3228         assert(use-&gt;in(MemNode::Address) != n, &quot;EA: missing allocation reference path&quot;);
3229       } else if (use-&gt;is_MergeMem()) {
3230         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3231       } else if (use-&gt;is_SafePoint()) {
3232         // Look for MergeMem nodes for calls which reference unique allocation
3233         // (through CheckCastPP nodes) even for debug info.
3234         Node* m = use-&gt;in(TypeFunc::Memory);
3235         if (m-&gt;is_MergeMem()) {
3236           assert(_mergemem_worklist.contains(m-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3237         }
3238       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3239         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3240           // EncodeISOArray overwrites destination array
3241           memnode_worklist.append_if_missing(use);
3242         }
<span class="line-added">3243       } else if (use-&gt;Opcode() == Op_Return) {</span>
<span class="line-added">3244         assert(_compile-&gt;tf()-&gt;returns_inline_type_as_fields(), &quot;must return an inline type&quot;);</span>
<span class="line-added">3245         // Get InlineKlass by removing the tag bit from the metadata pointer</span>
<span class="line-added">3246         Node* klass = use-&gt;in(TypeFunc::Parms);</span>
<span class="line-added">3247         intptr_t ptr = igvn-&gt;type(klass)-&gt;isa_rawptr()-&gt;get_con();</span>
<span class="line-added">3248         clear_nth_bit(ptr, 0);</span>
<span class="line-added">3249         assert(Metaspace::contains((void*)ptr), &quot;should be klass&quot;);</span>
<span class="line-added">3250         assert(((InlineKlass*)ptr)-&gt;contains_oops(), &quot;returned inline type must contain a reference field&quot;);</span>
3251       } else {
3252         uint op = use-&gt;Opcode();
3253         if ((op == Op_StrCompressedCopy || op == Op_StrInflatedCopy) &amp;&amp;
3254             (use-&gt;in(MemNode::Memory) == n)) {
3255           // They overwrite memory edge corresponding to destination array,
3256           memnode_worklist.append_if_missing(use);
3257         } else if (!(op == Op_CmpP || op == Op_Conv2B ||
3258               op == Op_CastP2X || op == Op_StoreCM ||
3259               op == Op_FastLock || op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3260               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3261               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar ||
<span class="line-modified">3262               op == Op_SubTypeCheck || op == Op_InlineType || op == Op_InlineTypePtr ||</span>
3263               BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use))) {
3264           n-&gt;dump();
3265           use-&gt;dump();
3266           assert(false, &quot;EA: missing allocation reference path&quot;);
3267         }
3268 #endif
3269       }
3270     }
3271 
3272   }
3273 
3274   // Go over all ArrayCopy nodes and if one of the inputs has a unique
3275   // type, record it in the ArrayCopy node so we know what memory this
3276   // node uses/modified.
3277   for (int next = 0; next &lt; arraycopy_worklist.length(); next++) {
3278     ArrayCopyNode* ac = arraycopy_worklist.at(next);
3279     Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
3280     if (dest-&gt;is_AddP()) {
3281       dest = get_addp_base(dest);
3282     }
</pre>
<hr />
<pre>
3310   //            compute new values for Memory inputs  (the Memory inputs are not
3311   //            actually updated until phase 4.)
3312   if (memnode_worklist.length() == 0)
3313     return;  // nothing to do
3314   while (memnode_worklist.length() != 0) {
3315     Node *n = memnode_worklist.pop();
3316     if (visited.test_set(n-&gt;_idx))
3317       continue;
3318     if (n-&gt;is_Phi() || n-&gt;is_ClearArray()) {
3319       // we don&#39;t need to do anything, but the users must be pushed
3320     } else if (n-&gt;is_MemBar()) { // Initialize, MemBar nodes
3321       // we don&#39;t need to do anything, but the users must be pushed
3322       n = n-&gt;as_MemBar()-&gt;proj_out_or_null(TypeFunc::Memory);
3323       if (n == NULL)
3324         continue;
3325     } else if (n-&gt;Opcode() == Op_StrCompressedCopy ||
3326                n-&gt;Opcode() == Op_EncodeISOArray) {
3327       // get the memory projection
3328       n = n-&gt;find_out_with(Op_SCMemProj);
3329       assert(n != NULL &amp;&amp; n-&gt;Opcode() == Op_SCMemProj, &quot;memory projection required&quot;);
<span class="line-added">3330     } else if (n-&gt;is_CallLeaf() &amp;&amp; n-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;</span>
<span class="line-added">3331                strcmp(n-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_inline&quot;) == 0) {</span>
<span class="line-added">3332       n = n-&gt;as_CallLeaf()-&gt;proj_out(TypeFunc::Memory);</span>
3333     } else {
3334       assert(n-&gt;is_Mem(), &quot;memory node required.&quot;);
3335       Node *addr = n-&gt;in(MemNode::Address);
3336       const Type *addr_t = igvn-&gt;type(addr);
3337       if (addr_t == Type::TOP)
3338         continue;
3339       assert (addr_t-&gt;isa_ptr() != NULL, &quot;pointer type required.&quot;);
3340       int alias_idx = _compile-&gt;get_alias_index(addr_t-&gt;is_ptr());
3341       assert ((uint)alias_idx &lt; new_index_end, &quot;wrong alias index&quot;);
3342       Node *mem = find_inst_mem(n-&gt;in(MemNode::Memory), alias_idx, orig_phis);
3343       if (_compile-&gt;failing()) {
3344         return;
3345       }
3346       if (mem != n-&gt;in(MemNode::Memory)) {
3347         // We delay the memory edge update since we need old one in
3348         // MergeMem code below when instances memory slices are separated.
3349         set_map(n, mem);
3350       }
3351       if (n-&gt;is_Load()) {
3352         continue;  // don&#39;t push users
3353       } else if (n-&gt;is_LoadStore()) {
3354         // get the memory projection
3355         n = n-&gt;find_out_with(Op_SCMemProj);
3356         assert(n != NULL &amp;&amp; n-&gt;Opcode() == Op_SCMemProj, &quot;memory projection required&quot;);
3357       }
3358     }
3359     // push user on appropriate worklist
3360     for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
3361       Node *use = n-&gt;fast_out(i);
3362       if (use-&gt;is_Phi() || use-&gt;is_ClearArray()) {
3363         memnode_worklist.append_if_missing(use);
3364       } else if (use-&gt;is_Mem() &amp;&amp; use-&gt;in(MemNode::Memory) == n) {
3365         if (use-&gt;Opcode() == Op_StoreCM) // Ignore cardmark stores
3366           continue;
3367         memnode_worklist.append_if_missing(use);
3368       } else if (use-&gt;is_MemBar()) {
3369         if (use-&gt;in(TypeFunc::Memory) == n) { // Ignore precedent edge
3370           memnode_worklist.append_if_missing(use);
3371         }
3372 #ifdef ASSERT
<span class="line-modified">3373       } else if (use-&gt;is_Mem()) {</span>
3374         assert(use-&gt;in(MemNode::Memory) != n, &quot;EA: missing memory path&quot;);
3375       } else if (use-&gt;is_MergeMem()) {
3376         assert(_mergemem_worklist.contains(use-&gt;as_MergeMem()), &quot;EA: missing MergeMem node in the worklist&quot;);
3377       } else if (use-&gt;Opcode() == Op_EncodeISOArray) {
3378         if (use-&gt;in(MemNode::Memory) == n || use-&gt;in(3) == n) {
3379           // EncodeISOArray overwrites destination array
3380           memnode_worklist.append_if_missing(use);
3381         }
<span class="line-added">3382       } else if (use-&gt;is_CallLeaf() &amp;&amp; use-&gt;as_CallLeaf()-&gt;_name != NULL &amp;&amp;</span>
<span class="line-added">3383                  strcmp(use-&gt;as_CallLeaf()-&gt;_name, &quot;store_unknown_inline&quot;) == 0) {</span>
<span class="line-added">3384         // store_unknown_inline overwrites destination array</span>
<span class="line-added">3385         memnode_worklist.append_if_missing(use);</span>
3386       } else {
3387         uint op = use-&gt;Opcode();
3388         if ((use-&gt;in(MemNode::Memory) == n) &amp;&amp;
3389             (op == Op_StrCompressedCopy || op == Op_StrInflatedCopy)) {
3390           // They overwrite memory edge corresponding to destination array,
3391           memnode_worklist.append_if_missing(use);
3392         } else if (!(BarrierSet::barrier_set()-&gt;barrier_set_c2()-&gt;is_gc_barrier_node(use) ||
3393               op == Op_AryEq || op == Op_StrComp || op == Op_HasNegatives ||
3394               op == Op_StrCompressedCopy || op == Op_StrInflatedCopy ||
3395               op == Op_StrEquals || op == Op_StrIndexOf || op == Op_StrIndexOfChar)) {
3396           n-&gt;dump();
3397           use-&gt;dump();
3398           assert(false, &quot;EA: missing memory path&quot;);
3399         }
3400 #endif
3401       }
3402     }
3403   }
3404 
3405   //  Phase 3:  Process MergeMem nodes from mergemem_worklist.
3406   //            Walk each memory slice moving the first node encountered of each
<span class="line-modified">3407   //            instance type to the input corresponding to its alias index.</span>
3408   uint length = _mergemem_worklist.length();
3409   for( uint next = 0; next &lt; length; ++next ) {
3410     MergeMemNode* nmm = _mergemem_worklist.at(next);
3411     assert(!visited.test_set(nmm-&gt;_idx), &quot;should not be visited before&quot;);
3412     // Note: we don&#39;t want to use MergeMemStream here because we only want to
3413     // scan inputs which exist at the start, not ones we add during processing.
3414     // Note 2: MergeMem may already contains instance memory slices added
3415     // during find_inst_mem() call when memory nodes were processed above.
3416     igvn-&gt;hash_delete(nmm);
3417     uint nslices = MIN2(nmm-&gt;req(), new_index_start);
3418     for (uint i = Compile::AliasIdxRaw+1; i &lt; nslices; i++) {
3419       Node* mem = nmm-&gt;in(i);
3420       Node* cur = NULL;
3421       if (mem == NULL || mem-&gt;is_top())
3422         continue;
3423       // First, update mergemem by moving memory nodes to corresponding slices
3424       // if their type became more precise since this mergemem was created.
3425       while (mem-&gt;is_Mem()) {
3426         const Type *at = igvn-&gt;type(mem-&gt;in(MemNode::Address));
3427         if (at != Type::TOP) {
</pre>
<hr />
<pre>
3459       const TypeOopPtr *tinst = _compile-&gt;get_adr_type(ni)-&gt;isa_oopptr();
3460       Node* result = step_through_mergemem(nmm, ni, tinst);
3461       if (result == nmm-&gt;base_memory()) {
3462         // Didn&#39;t find instance memory, search through general slice recursively.
3463         result = nmm-&gt;memory_at(_compile-&gt;get_general_index(ni));
3464         result = find_inst_mem(result, ni, orig_phis);
3465         if (_compile-&gt;failing()) {
3466           return;
3467         }
3468         nmm-&gt;set_memory_at(ni, result);
3469       }
3470     }
3471     igvn-&gt;hash_insert(nmm);
3472     record_for_optimizer(nmm);
3473   }
3474 
3475   //  Phase 4:  Update the inputs of non-instance memory Phis and
3476   //            the Memory input of memnodes
3477   // First update the inputs of any non-instance Phi&#39;s from
3478   // which we split out an instance Phi.  Note we don&#39;t have
<span class="line-modified">3479   // to recursively process Phi&#39;s encountered on the input memory</span>
<span class="line-modified">3480   // chains as is done in split_memory_phi() since they will</span>
3481   // also be processed here.
3482   for (int j = 0; j &lt; orig_phis.length(); j++) {
3483     PhiNode *phi = orig_phis.at(j);
3484     int alias_idx = _compile-&gt;get_alias_index(phi-&gt;adr_type());
3485     igvn-&gt;hash_delete(phi);
3486     for (uint i = 1; i &lt; phi-&gt;req(); i++) {
3487       Node *mem = phi-&gt;in(i);
3488       Node *new_mem = find_inst_mem(mem, alias_idx, orig_phis);
3489       if (_compile-&gt;failing()) {
3490         return;
3491       }
3492       if (mem != new_mem) {
3493         phi-&gt;set_req(i, new_mem);
3494       }
3495     }
3496     igvn-&gt;hash_insert(phi);
3497     record_for_optimizer(phi);
3498   }
3499 
3500   // Update the memory inputs of MemNodes with the value we computed
</pre>
</td>
</tr>
</table>
<center><a href="compile.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="lcm.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="chaitin.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/compile.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  34 #include &quot;compiler/disassembler.hpp&quot;
  35 #include &quot;compiler/oopMap.hpp&quot;
  36 #include &quot;gc/shared/barrierSet.hpp&quot;
  37 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;memory/resourceArea.hpp&quot;
  40 #include &quot;opto/addnode.hpp&quot;
  41 #include &quot;opto/block.hpp&quot;
  42 #include &quot;opto/c2compiler.hpp&quot;
  43 #include &quot;opto/callGenerator.hpp&quot;
  44 #include &quot;opto/callnode.hpp&quot;
  45 #include &quot;opto/castnode.hpp&quot;
  46 #include &quot;opto/cfgnode.hpp&quot;
  47 #include &quot;opto/chaitin.hpp&quot;
  48 #include &quot;opto/compile.hpp&quot;
  49 #include &quot;opto/connode.hpp&quot;
  50 #include &quot;opto/convertnode.hpp&quot;
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;

  54 #include &quot;opto/loopnode.hpp&quot;
  55 #include &quot;opto/machnode.hpp&quot;
  56 #include &quot;opto/macro.hpp&quot;
  57 #include &quot;opto/matcher.hpp&quot;
  58 #include &quot;opto/mathexactnode.hpp&quot;
  59 #include &quot;opto/memnode.hpp&quot;
  60 #include &quot;opto/mulnode.hpp&quot;
  61 #include &quot;opto/narrowptrnode.hpp&quot;
  62 #include &quot;opto/node.hpp&quot;
  63 #include &quot;opto/opcodes.hpp&quot;
  64 #include &quot;opto/output.hpp&quot;
  65 #include &quot;opto/parse.hpp&quot;
  66 #include &quot;opto/phaseX.hpp&quot;
  67 #include &quot;opto/rootnode.hpp&quot;
  68 #include &quot;opto/runtime.hpp&quot;
  69 #include &quot;opto/stringopts.hpp&quot;
  70 #include &quot;opto/type.hpp&quot;
  71 #include &quot;opto/vectornode.hpp&quot;
  72 #include &quot;runtime/arguments.hpp&quot;
  73 #include &quot;runtime/sharedRuntime.hpp&quot;
</pre>
<hr />
<pre>
 388   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 389     Node* cast = range_check_cast_node(i);
 390     if (!useful.member(cast)) {
 391       remove_range_check_cast(cast);
 392     }
 393   }
 394   // Remove useless expensive nodes
 395   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 396     Node* n = C-&gt;expensive_node(i);
 397     if (!useful.member(n)) {
 398       remove_expensive_node(n);
 399     }
 400   }
 401   // Remove useless Opaque4 nodes
 402   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 403     Node* opaq = opaque4_node(i);
 404     if (!useful.member(opaq)) {
 405       remove_opaque4_node(opaq);
 406     }
 407   }







 408   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 409   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 410   // clean up the late inline lists
 411   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 412   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 413   remove_useless_late_inlines(&amp;_late_inlines, useful);
 414   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 415 }
 416 
 417 // ============================================================================
 418 //------------------------------CompileWrapper---------------------------------
 419 class CompileWrapper : public StackObj {
 420   Compile *const _compile;
 421  public:
 422   CompileWrapper(Compile* compile);
 423 
 424   ~CompileWrapper();
 425 };
 426 
 427 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
</pre>
<hr />
<pre>
 613   // Node list that Iterative GVN will start with
 614   Unique_Node_List for_igvn(comp_arena());
 615   set_for_igvn(&amp;for_igvn);
 616 
 617   // GVN that will be run immediately on new nodes
 618   uint estimated_size = method()-&gt;code_size()*4+64;
 619   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 620   PhaseGVN gvn(node_arena(), estimated_size);
 621   set_initial_gvn(&amp;gvn);
 622 
 623   print_inlining_init();
 624   { // Scope for timing the parser
 625     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 626 
 627     // Put top into the hash table ASAP.
 628     initial_gvn()-&gt;transform_no_reclaim(top());
 629 
 630     // Set up tf(), start(), and find a CallGenerator.
 631     CallGenerator* cg = NULL;
 632     if (is_osr_compilation()) {
<span class="line-modified"> 633       const TypeTuple *domain = StartOSRNode::osr_domain();</span>
<span class="line-modified"> 634       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());</span>
<span class="line-removed"> 635       init_tf(TypeFunc::make(domain, range));</span>
<span class="line-removed"> 636       StartNode* s = new StartOSRNode(root(), domain);</span>
 637       initial_gvn()-&gt;set_type_bottom(s);
 638       init_start(s);
 639       cg = CallGenerator::for_osr(method(), entry_bci());
 640     } else {
 641       // Normal case.
 642       init_tf(TypeFunc::make(method()));
<span class="line-modified"> 643       StartNode* s = new StartNode(root(), tf()-&gt;domain());</span>
 644       initial_gvn()-&gt;set_type_bottom(s);
 645       init_start(s);
 646       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 647         // With java.lang.ref.reference.get() we must go through the
 648         // intrinsic - even when get() is the root
 649         // method of the compile - so that, if necessary, the value in
 650         // the referent field of the reference object gets recorded by
 651         // the pre-barrier code.
 652         cg = find_intrinsic(method(), false);
 653       }
 654       if (cg == NULL) {
 655         float past_uses = method()-&gt;interpreter_invocation_count();
 656         float expected_uses = past_uses;
 657         cg = CallGenerator::for_inline(method(), expected_uses);
 658       }
 659     }
 660     if (failing())  return;
 661     if (cg == NULL) {
 662       record_method_not_compilable(&quot;cannot parse method&quot;);
 663       return;
</pre>
<hr />
<pre>
 748     }
 749   }
 750 #endif
 751 
 752 #ifdef ASSERT
 753   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 754   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 755 #endif
 756 
 757   // Dump compilation data to replay it.
 758   if (directive-&gt;DumpReplayOption) {
 759     env()-&gt;dump_replay_data(_compile_id);
 760   }
 761   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 762     env()-&gt;dump_inline_data(_compile_id);
 763   }
 764 
 765   // Now that we know the size of all the monitors we can add a fixed slot
 766   // for the original deopt pc.
 767   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);




 768   set_fixed_slots(next_slot);
 769 
 770   // Compute when to use implicit null checks. Used by matching trap based
 771   // nodes and NullCheck optimization.
 772   set_allowed_deopt_reasons();
 773 
 774   // Now generate code
 775   Code_Gen();
 776 }
 777 
 778 //------------------------------Compile----------------------------------------
 779 // Compile a runtime stub
 780 Compile::Compile( ciEnv* ci_env,
 781                   TypeFunc_generator generator,
 782                   address stub_function,
 783                   const char *stub_name,
 784                   int is_fancy_jump,
 785                   bool pass_tls,
 786                   bool save_arg_registers,
 787                   bool return_pc,
</pre>
<hr />
<pre>
 901   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 902   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 903   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 904   env()-&gt;set_dependencies(new Dependencies(env()));
 905 
 906   _fixed_slots = 0;
 907   set_has_split_ifs(false);
 908   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 909   set_has_stringbuilder(false);
 910   set_has_boxed_value(false);
 911   _trap_can_recompile = false;  // no traps emitted yet
 912   _major_progress = true; // start out assuming good things will happen
 913   set_has_unsafe_access(false);
 914   set_max_vector_size(0);
 915   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 916   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 917   set_decompile_count(0);
 918 
 919   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 920   _loop_opts_cnt = LoopOptsCount;



 921   set_do_inlining(Inline);
 922   set_max_inline_size(MaxInlineSize);
 923   set_freq_inline_size(FreqInlineSize);
 924   set_do_scheduling(OptoScheduling);
 925   set_do_count_invocations(false);
 926   set_do_method_data_update(false);
 927 
 928   set_do_vector_loop(false);
 929 
 930   if (AllowVectorizeOnDemand) {
 931     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 932       set_do_vector_loop(true);
 933       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 934     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 935                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 936       set_do_vector_loop(true);
 937     }
 938   }
 939   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 940   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
</pre>
<hr />
<pre>
 984   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 985   {
 986     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
 987   }
 988   // Initialize the first few types.
 989   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
 990   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
 991   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
 992   _num_alias_types = AliasIdxRaw+1;
 993   // Zero out the alias type cache.
 994   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
 995   // A NULL adr_type hits in the cache right away.  Preload the right answer.
 996   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
 997 
 998   _intrinsics = NULL;
 999   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1000   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1001   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1002   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1003   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);

1004   register_library_intrinsics();
1005 #ifdef ASSERT
1006   _type_verify_symmetry = true;
1007   _phase_optimize_finished = false;
1008 #endif
1009 }
1010 
1011 //---------------------------init_start----------------------------------------
1012 // Install the StartNode on this compile object.
1013 void Compile::init_start(StartNode* s) {
1014   if (failing())
1015     return; // already failing
1016   assert(s == start(), &quot;&quot;);
1017 }
1018 
1019 /**
1020  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1021  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1022  * the ideal graph.
1023  */
</pre>
<hr />
<pre>
1212 bool Compile::allow_range_check_smearing() const {
1213   // If this method has already thrown a range-check,
1214   // assume it was because we already tried range smearing
1215   // and it failed.
1216   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1217   return !already_trapped;
1218 }
1219 
1220 
1221 //------------------------------flatten_alias_type-----------------------------
1222 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1223   int offset = tj-&gt;offset();
1224   TypePtr::PTR ptr = tj-&gt;ptr();
1225 
1226   // Known instance (scalarizable allocation) alias only with itself.
1227   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1228                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1229 
1230   // Process weird unsafe references.
1231   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<span class="line-modified">1232     assert(InlineUnsafeOps, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>

1233     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1234     tj = TypeOopPtr::BOTTOM;
1235     ptr = tj-&gt;ptr();
1236     offset = tj-&gt;offset();
1237   }
1238 
1239   // Array pointers need some flattening
1240   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1241   if (ta &amp;&amp; ta-&gt;is_stable()) {
1242     // Erase stability property for alias analysis.
1243     tj = ta = ta-&gt;cast_to_stable(false);
1244   }









1245   if( ta &amp;&amp; is_known_inst ) {
1246     if ( offset != Type::OffsetBot &amp;&amp;
1247          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1248       offset = Type::OffsetBot; // Flatten constant access into array body only
<span class="line-modified">1249       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());</span>
1250     }
1251   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1252     // For arrays indexed by constant indices, we flatten the alias
1253     // space to include all of the array body.  Only the header, klass
1254     // and array length can be accessed un-aliased.


1255     if( offset != Type::OffsetBot ) {
1256       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1257         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1258         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1259       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1260         // range is OK as-is.
1261         tj = ta = TypeAryPtr::RANGE;
1262       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1263         tj = TypeInstPtr::KLASS; // all klass loads look alike
1264         ta = TypeAryPtr::RANGE; // generic ignored junk
1265         ptr = TypePtr::BotPTR;
1266       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1267         tj = TypeInstPtr::MARK;
1268         ta = TypeAryPtr::RANGE; // generic ignored junk
1269         ptr = TypePtr::BotPTR;
1270       } else {                  // Random constant offset into array body
1271         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1272         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1273       }
1274     }
1275     // Arrays of fixed size alias with arrays of unknown size.
1276     if (ta-&gt;size() != TypeInt::POS) {
1277       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<span class="line-modified">1278       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);</span>
1279     }
1280     // Arrays of known objects become arrays of unknown objects.
1281     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1282       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<span class="line-modified">1283       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);</span>
1284     }
1285     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1286       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<span class="line-modified">1287       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);</span>





1288     }
1289     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1290     // cannot be distinguished by bytecode alone.
1291     if (ta-&gt;elem() == TypeInt::BOOL) {
1292       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1293       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<span class="line-modified">1294       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);</span>
1295     }
1296     // During the 2nd round of IterGVN, NotNull castings are removed.
1297     // Make sure the Bottom and NotNull variants alias the same.
1298     // Also, make sure exact and non-exact variants alias the same.
1299     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<span class="line-modified">1300       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);</span>
1301     }
1302   }
1303 
1304   // Oop pointers need some flattening
1305   const TypeInstPtr *to = tj-&gt;isa_instptr();
1306   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1307     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1308     if( ptr == TypePtr::Constant ) {
1309       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1310           offset &lt; k-&gt;size_helper() * wordSize) {
1311         // No constant oop pointers (such as Strings); they alias with
1312         // unknown strings.
1313         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1314         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);</span>
1315       }
1316     } else if( is_known_inst ) {
1317       tj = to; // Keep NotNull and klass_is_exact for instance type
1318     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1319       // During the 2nd round of IterGVN, NotNull castings are removed.
1320       // Make sure the Bottom and NotNull variants alias the same.
1321       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1322       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);</span>
1323     }
1324     if (to-&gt;speculative() != NULL) {
<span class="line-modified">1325       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());</span>
1326     }
1327     // Canonicalize the holder of this field
1328     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1329       // First handle header references such as a LoadKlassNode, even if the
1330       // object&#39;s klass is unloaded at compile time (4965979).
1331       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1332         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);</span>
1333       }
1334     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1335       // Static fields are in the space above the normal instance
1336       // fields in the java.lang.Class instance.
1337       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1338         to = NULL;
1339         tj = TypeOopPtr::BOTTOM;
1340         offset = tj-&gt;offset();
1341       }
1342     } else {
1343       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1344       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1345         if( is_known_inst ) {
<span class="line-modified">1346           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());</span>
1347         } else {
<span class="line-modified">1348           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);</span>
1349         }
1350       }
1351     }
1352   }
1353 
1354   // Klass pointers to object array klasses need some flattening
1355   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1356   if( tk ) {
1357     // If we are referencing a field within a Klass, we need
1358     // to assume the worst case of an Object.  Both exact and
1359     // inexact types must flatten to the same alias class so
1360     // use NotNull as the PTR.
1361     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1362 
1363       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1364                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1365                                    offset);</span>

1366     }
1367 
1368     ciKlass* klass = tk-&gt;klass();
<span class="line-modified">1369     if( klass-&gt;is_obj_array_klass() ) {</span>
1370       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1371       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1372         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1373       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );</span>
1374     }
1375 
1376     // Check for precise loads from the primary supertype array and force them
1377     // to the supertype cache alias index.  Check for generic array loads from
1378     // the primary supertype array and also force them to the supertype cache
1379     // alias index.  Since the same load can reach both, we need to merge
1380     // these 2 disparate memories into the same alias class.  Since the
1381     // primary supertype array is read-only, there&#39;s no chance of confusion
1382     // where we bypass an array load and an array store.
1383     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1384     if (offset == Type::OffsetBot ||
1385         (offset &gt;= primary_supers_offset &amp;&amp;
1386          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1387         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1388       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1389       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );</span>
1390     }
1391   }
1392 
1393   // Flatten all Raw pointers together.
1394   if (tj-&gt;base() == Type::RawPtr)
1395     tj = TypeRawPtr::BOTTOM;
1396 
1397   if (tj-&gt;base() == Type::AnyPtr)
1398     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1399 
1400   // Flatten all to bottom for now
1401   switch( _AliasLevel ) {
1402   case 0:
1403     tj = TypePtr::BOTTOM;
1404     break;
1405   case 1:                       // Flatten to: oop, static, field or array
1406     switch (tj-&gt;base()) {
1407     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1408     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1409     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1508   intptr_t key = (intptr_t) adr_type;
1509   key ^= key &gt;&gt; logAliasCacheSize;
1510   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1511 }
1512 
1513 
1514 //-----------------------------grow_alias_types--------------------------------
1515 void Compile::grow_alias_types() {
1516   const int old_ats  = _max_alias_types; // how many before?
1517   const int new_ats  = old_ats;          // how many more?
1518   const int grow_ats = old_ats+new_ats;  // how many now?
1519   _max_alias_types = grow_ats;
1520   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1521   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1522   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1523   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1524 }
1525 
1526 
1527 //--------------------------------find_alias_type------------------------------
<span class="line-modified">1528 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {</span>
1529   if (_AliasLevel == 0)
1530     return alias_type(AliasIdxBot);
1531 
<span class="line-modified">1532   AliasCacheEntry* ace = probe_alias_cache(adr_type);</span>
<span class="line-modified">1533   if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-modified">1534     return alias_type(ace-&gt;_index);</span>



1535   }
1536 
1537   // Handle special cases.
1538   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1539   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1540 
1541   // Do it the slow way.
1542   const TypePtr* flat = flatten_alias_type(adr_type);
1543 
1544 #ifdef ASSERT
1545   {
1546     ResourceMark rm;
1547     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1548            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1549     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1550            Type::str(adr_type));
1551     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1552       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1553       // Scalarizable allocations have exact klass always.
1554       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
</pre>
<hr />
<pre>
1564     if (alias_type(i)-&gt;adr_type() == flat) {
1565       idx = i;
1566       break;
1567     }
1568   }
1569 
1570   if (idx == AliasIdxTop) {
1571     if (no_create)  return NULL;
1572     // Grow the array if necessary.
1573     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1574     // Add a new alias type.
1575     idx = _num_alias_types++;
1576     _alias_types[idx]-&gt;Init(idx, flat);
1577     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1578     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1579     if (flat-&gt;isa_instptr()) {
1580       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1581           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1582         alias_type(idx)-&gt;set_rewritable(false);
1583     }

1584     if (flat-&gt;isa_aryptr()) {
1585 #ifdef ASSERT
1586       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1587       // (T_BYTE has the weakest alignment and size restrictions...)
1588       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1589 #endif

1590       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<span class="line-modified">1591         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());</span>








1592       }
1593     }
1594     if (flat-&gt;isa_klassptr()) {
1595       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1596         alias_type(idx)-&gt;set_rewritable(false);
1597       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1598         alias_type(idx)-&gt;set_rewritable(false);
1599       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1600         alias_type(idx)-&gt;set_rewritable(false);
1601       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1602         alias_type(idx)-&gt;set_rewritable(false);


1603       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1604         alias_type(idx)-&gt;set_rewritable(false);
1605     }
1606     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1607     // but the base pointer type is not distinctive enough to identify
1608     // references into JavaThread.)
1609 
1610     // Check for final fields.
1611     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1612     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
<span class="line-removed">1613       ciField* field;</span>
1614       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1615           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1616           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1617         // static field
1618         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1619         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);




1620       } else {
<span class="line-modified">1621         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1622         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1623       }
<span class="line-modified">1624       assert(field == NULL ||</span>
<span class="line-modified">1625              original_field == NULL ||</span>
<span class="line-modified">1626              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1627               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1628               field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1629       // Set field() and is_rewritable() attributes.</span>
<span class="line-modified">1630       if (field != NULL)  alias_type(idx)-&gt;set_field(field);</span>







1631     }
1632   }
1633 
1634   // Fill the cache for next time.
<span class="line-modified">1635   ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1636   ace-&gt;_index    = idx;</span>
<span class="line-modified">1637   assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>

1638 
<span class="line-modified">1639   // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1640   AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1641   if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1642     face-&gt;_adr_type = flat;</span>
<span class="line-modified">1643     face-&gt;_index    = idx;</span>
<span class="line-modified">1644     assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>

1645   }
1646 
1647   return alias_type(idx);
1648 }
1649 
1650 
1651 Compile::AliasType* Compile::alias_type(ciField* field) {
1652   const TypeOopPtr* t;
1653   if (field-&gt;is_static())
1654     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1655   else
1656     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1657   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1658   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1659   return atp;
1660 }
1661 
1662 
1663 //------------------------------have_alias_type--------------------------------
1664 bool Compile::have_alias_type(const TypePtr* adr_type) {
</pre>
<hr />
<pre>
1786   }
1787   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1788 }
1789 
1790 void Compile::add_opaque4_node(Node* n) {
1791   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1792   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1793   _opaque4_nodes-&gt;append(n);
1794 }
1795 
1796 // Remove all Opaque4 nodes.
1797 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1798   for (int i = opaque4_count(); i &gt; 0; i--) {
1799     Node* opaq = opaque4_node(i-1);
1800     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1801     igvn.replace_node(opaq, opaq-&gt;in(2));
1802   }
1803   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1804 }
1805 




























































































































































































































































































































































1806 // StringOpts and late inlining of string methods
1807 void Compile::inline_string_calls(bool parse_time) {
1808   {
1809     // remove useless nodes to make the usage analysis simpler
1810     ResourceMark rm;
1811     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1812   }
1813 
1814   {
1815     ResourceMark rm;
1816     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1817     PhaseStringOpts pso(initial_gvn(), for_igvn());
1818     print_method(PHASE_AFTER_STRINGOPTS, 3);
1819   }
1820 
1821   // now inline anything that we skipped the first time around
1822   if (!parse_time) {
1823     _late_inlines_pos = _late_inlines.length();
1824   }
1825 
</pre>
<hr />
<pre>
2065   remove_speculative_types(igvn);
2066 
2067   // No more new expensive nodes will be added to the list from here
2068   // so keep only the actual candidates for optimizations.
2069   cleanup_expensive_nodes(igvn);
2070 
2071   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2072     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2073     initial_gvn()-&gt;replace_with(&amp;igvn);
2074     for_igvn()-&gt;clear();
2075     Unique_Node_List new_worklist(C-&gt;comp_arena());
2076     {
2077       ResourceMark rm;
2078       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2079     }
2080     set_for_igvn(&amp;new_worklist);
2081     igvn = PhaseIterGVN(initial_gvn());
2082     igvn.optimize();
2083   }
2084 







2085   // Perform escape analysis
2086   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2087     if (has_loops()) {
2088       // Cleanup graph (remove dead nodes).
2089       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2090       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2091       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2092       if (failing())  return;
2093     }
2094     ConnectionGraph::do_analysis(this, &amp;igvn);
2095 
2096     if (failing())  return;
2097 
2098     // Optimize out fields loads from scalar replaceable allocations.
2099     igvn.optimize();
2100     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2101 
2102     if (failing())  return;
2103 
2104     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2105       TracePhase tp(&quot;macroEliminate&quot;, &amp;timers[_t_macroEliminate]);
2106       PhaseMacroExpand mexp(igvn);
2107       mexp.eliminate_macro_nodes();
2108       igvn.set_delay_transform(false);
2109 
2110       igvn.optimize();
2111       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2112 
2113       if (failing())  return;
2114     }
2115   }
2116 





2117   // Loop transforms on the ideal graph.  Range Check Elimination,
2118   // peeling, unrolling, etc.
2119 
2120   // Set loop opts counter
2121   if((_loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2122     {
2123       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2124       PhaseIdealLoop::optimize(igvn, LoopOptsDefault);
2125       _loop_opts_cnt--;
2126       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2127       if (failing())  return;
2128     }
2129     // Loop opts pass if partial peeling occurred in previous pass
2130     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2131       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2132       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2133       _loop_opts_cnt--;
2134       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2135       if (failing())  return;
2136     }
</pre>
<hr />
<pre>
2739             // Accumulate any precedence edges
2740             if (mem-&gt;in(i) != NULL) {
2741               n-&gt;add_prec(mem-&gt;in(i));
2742             }
2743           }
2744           // Everything above this point has been processed.
2745           done = true;
2746         }
2747         // Eliminate the previous StoreCM
2748         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2749         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
2750         mem-&gt;disconnect_inputs(NULL, this);
2751       } else {
2752         prev = mem;
2753       }
2754       mem = prev-&gt;in(MemNode::Memory);
2755     }
2756   }
2757 }
2758 

2759 //------------------------------final_graph_reshaping_impl----------------------
2760 // Implement items 1-5 from final_graph_reshaping below.
2761 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2762 
2763   if ( n-&gt;outcnt() == 0 ) return; // dead node
2764   uint nop = n-&gt;Opcode();
2765 
2766   // Check for 2-input instruction with &quot;last use&quot; on right input.
2767   // Swap to left input.  Implements item (2).
2768   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2769       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2770       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2771       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2772       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2773     // Check for commutative opcode
2774     switch( nop ) {
2775     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2776     case Op_MaxI:  case Op_MinI:
2777     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2778     case Op_AndL:  case Op_XorL:  case Op_OrL:
</pre>
<hr />
<pre>
3477           // Replace all nodes with identical edges as m with m
3478           k-&gt;subsume_by(m, this);
3479         }
3480       }
3481     }
3482     break;
3483   }
3484   case Op_CmpUL: {
3485     if (!Matcher::has_match_rule(Op_CmpUL)) {
3486       // No support for unsigned long comparisons
3487       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3488       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3489       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3490       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3491       Node* andl = new AndLNode(orl, remove_sign_mask);
3492       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3493       n-&gt;subsume_by(cmp, this);
3494     }
3495     break;
3496   }








3497   default:
3498     assert(!n-&gt;is_Call(), &quot;&quot;);
3499     assert(!n-&gt;is_Mem(), &quot;&quot;);
3500     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3501     break;
3502   }
3503 }
3504 
3505 //------------------------------final_graph_reshaping_walk---------------------
3506 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3507 // requires that the walk visits a node&#39;s inputs before visiting the node.
3508 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3509   Unique_Node_List sfpt;
3510 
3511   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3512   uint cnt = root-&gt;req();
3513   Node *n = root;
3514   uint  i = 0;
3515   while (true) {
3516     if (i &lt; cnt) {
</pre>
<hr />
<pre>
3824   }
3825 }
3826 
3827 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
3828   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
3829 }
3830 
3831 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
3832   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
3833 }
3834 
3835 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
3836   if (holder-&gt;is_initialized()) {
3837     return false;
3838   }
3839   if (holder-&gt;is_being_initialized()) {
3840     if (accessing_method-&gt;holder() == holder) {
3841       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
3842       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
3843       // barrier on the holder klass passed.
<span class="line-modified">3844       if (accessing_method-&gt;is_static_initializer() ||</span>
<span class="line-modified">3845           accessing_method-&gt;is_object_initializer() ||</span>
3846           accessing_method-&gt;is_static()) {
3847         return false;
3848       }
3849     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
3850       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
3851       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
3852       // child class can become fully initialized while its parent class is still being initialized.
<span class="line-modified">3853       if (accessing_method-&gt;is_static_initializer()) {</span>
3854         return false;
3855       }
3856     }
3857     ciMethod* root = method(); // the root method of compilation
3858     if (root != accessing_method) {
3859       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
3860     }
3861   }
3862   return true;
3863 }
3864 
3865 #ifndef PRODUCT
3866 //------------------------------verify_graph_edges---------------------------
3867 // Walk the Graph and verify that there is a one-to-one correspondence
3868 // between Use-Def edges and Def-Use edges in the graph.
3869 void Compile::verify_graph_edges(bool no_dead_code) {
3870   if (VerifyGraphEdges) {
3871     Unique_Node_List visited;
3872     // Call recursive graph walk to check edges
3873     _root-&gt;verify_edges(visited);
</pre>
<hr />
<pre>
3954                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3955   }
3956 
3957   if (VerifyIdealNodeCount) {
3958     Compile::current()-&gt;print_missing_nodes();
3959   }
3960 #endif
3961 
3962   if (_log != NULL) {
3963     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3964   }
3965 }
3966 
3967 //----------------------------static_subtype_check-----------------------------
3968 // Shortcut important common cases when superklass is exact:
3969 // (0) superklass is java.lang.Object (can occur in reflective code)
3970 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3971 // (2) subklass does not overlap with superklass =&gt; always fail
3972 // (3) superklass has NO subtypes and we can check with a simple compare.
3973 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<span class="line-modified">3974   if (StressReflectiveCode) {</span>
3975     return SSC_full_test;       // Let caller generate the general case.
3976   }
3977 
3978   if (superk == env()-&gt;Object_klass()) {
3979     return SSC_always_true;     // (0) this test cannot fail
3980   }
3981 
3982   ciType* superelem = superk;
<span class="line-modified">3983   if (superelem-&gt;is_array_klass())</span>

3984     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();

3985 
3986   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3987     if (subk-&gt;is_subtype_of(superk)) {
3988       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3989     }
3990     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
3991         !superk-&gt;is_subtype_of(subk)) {
3992       return SSC_always_false;
3993     }
3994   }
3995 
3996   // If casting to an instance klass, it must have no subtypes
3997   if (superk-&gt;is_interface()) {
3998     // Cannot trust interfaces yet.
3999     // %%% S.B. superk-&gt;nof_implementors() == 1
4000   } else if (superelem-&gt;is_instance_klass()) {
4001     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4002     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4003       if (!ik-&gt;is_final()) {
4004         // Add a dependency if there is a chance of a later subclass.
</pre>
<hr />
<pre>
4425     for (uint next = 0; next &lt; worklist.size(); ++next) {
4426       Node *n  = worklist.at(next);
4427       const Type* t = igvn.type_or_null(n);
4428       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4429       if (n-&gt;is_Type()) {
4430         t = n-&gt;as_Type()-&gt;type();
4431         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4432       }
4433       uint max = n-&gt;len();
4434       for( uint i = 0; i &lt; max; ++i ) {
4435         Node *m = n-&gt;in(i);
4436         if (not_a_node(m))  continue;
4437         worklist.push(m);
4438       }
4439     }
4440     igvn.check_no_speculative_types();
4441 #endif
4442   }
4443 }
4444 





















4445 // Auxiliary method to support randomized stressing/fuzzing.
4446 //
4447 // This method can be called the arbitrary number of times, with current count
4448 // as the argument. The logic allows selecting a single candidate from the
4449 // running list of candidates as follows:
4450 //    int count = 0;
4451 //    Cand* selected = null;
4452 //    while(cand = cand-&gt;next()) {
4453 //      if (randomized_select(++count)) {
4454 //        selected = cand;
4455 //      }
4456 //    }
4457 //
4458 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4459 // This is useful when we don&#39;t have the complete list of candidates to choose
4460 // from uniformly. In this case, we need to adjust the randomicity of the
4461 // selection, or else we will end up biasing the selection towards the latter
4462 // candidates.
4463 //
4464 // Quick back-envelope calculation shows that for the list of n candidates
</pre>
</td>
<td>
<hr />
<pre>
  34 #include &quot;compiler/disassembler.hpp&quot;
  35 #include &quot;compiler/oopMap.hpp&quot;
  36 #include &quot;gc/shared/barrierSet.hpp&quot;
  37 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
  38 #include &quot;jfr/jfrEvents.hpp&quot;
  39 #include &quot;memory/resourceArea.hpp&quot;
  40 #include &quot;opto/addnode.hpp&quot;
  41 #include &quot;opto/block.hpp&quot;
  42 #include &quot;opto/c2compiler.hpp&quot;
  43 #include &quot;opto/callGenerator.hpp&quot;
  44 #include &quot;opto/callnode.hpp&quot;
  45 #include &quot;opto/castnode.hpp&quot;
  46 #include &quot;opto/cfgnode.hpp&quot;
  47 #include &quot;opto/chaitin.hpp&quot;
  48 #include &quot;opto/compile.hpp&quot;
  49 #include &quot;opto/connode.hpp&quot;
  50 #include &quot;opto/convertnode.hpp&quot;
  51 #include &quot;opto/divnode.hpp&quot;
  52 #include &quot;opto/escape.hpp&quot;
  53 #include &quot;opto/idealGraphPrinter.hpp&quot;
<span class="line-added">  54 #include &quot;opto/inlinetypenode.hpp&quot;</span>
  55 #include &quot;opto/loopnode.hpp&quot;
  56 #include &quot;opto/machnode.hpp&quot;
  57 #include &quot;opto/macro.hpp&quot;
  58 #include &quot;opto/matcher.hpp&quot;
  59 #include &quot;opto/mathexactnode.hpp&quot;
  60 #include &quot;opto/memnode.hpp&quot;
  61 #include &quot;opto/mulnode.hpp&quot;
  62 #include &quot;opto/narrowptrnode.hpp&quot;
  63 #include &quot;opto/node.hpp&quot;
  64 #include &quot;opto/opcodes.hpp&quot;
  65 #include &quot;opto/output.hpp&quot;
  66 #include &quot;opto/parse.hpp&quot;
  67 #include &quot;opto/phaseX.hpp&quot;
  68 #include &quot;opto/rootnode.hpp&quot;
  69 #include &quot;opto/runtime.hpp&quot;
  70 #include &quot;opto/stringopts.hpp&quot;
  71 #include &quot;opto/type.hpp&quot;
  72 #include &quot;opto/vectornode.hpp&quot;
  73 #include &quot;runtime/arguments.hpp&quot;
  74 #include &quot;runtime/sharedRuntime.hpp&quot;
</pre>
<hr />
<pre>
 389   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 390     Node* cast = range_check_cast_node(i);
 391     if (!useful.member(cast)) {
 392       remove_range_check_cast(cast);
 393     }
 394   }
 395   // Remove useless expensive nodes
 396   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 397     Node* n = C-&gt;expensive_node(i);
 398     if (!useful.member(n)) {
 399       remove_expensive_node(n);
 400     }
 401   }
 402   // Remove useless Opaque4 nodes
 403   for (int i = opaque4_count() - 1; i &gt;= 0; i--) {
 404     Node* opaq = opaque4_node(i);
 405     if (!useful.member(opaq)) {
 406       remove_opaque4_node(opaq);
 407     }
 408   }
<span class="line-added"> 409   // Remove useless inline type nodes</span>
<span class="line-added"> 410   for (int i = _inline_type_nodes-&gt;length() - 1; i &gt;= 0; i--) {</span>
<span class="line-added"> 411     Node* vt = _inline_type_nodes-&gt;at(i);</span>
<span class="line-added"> 412     if (!useful.member(vt)) {</span>
<span class="line-added"> 413       _inline_type_nodes-&gt;remove(vt);</span>
<span class="line-added"> 414     }</span>
<span class="line-added"> 415   }</span>
 416   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 417   bs-&gt;eliminate_useless_gc_barriers(useful, this);
 418   // clean up the late inline lists
 419   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 420   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 421   remove_useless_late_inlines(&amp;_late_inlines, useful);
 422   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 423 }
 424 
 425 // ============================================================================
 426 //------------------------------CompileWrapper---------------------------------
 427 class CompileWrapper : public StackObj {
 428   Compile *const _compile;
 429  public:
 430   CompileWrapper(Compile* compile);
 431 
 432   ~CompileWrapper();
 433 };
 434 
 435 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
</pre>
<hr />
<pre>
 621   // Node list that Iterative GVN will start with
 622   Unique_Node_List for_igvn(comp_arena());
 623   set_for_igvn(&amp;for_igvn);
 624 
 625   // GVN that will be run immediately on new nodes
 626   uint estimated_size = method()-&gt;code_size()*4+64;
 627   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 628   PhaseGVN gvn(node_arena(), estimated_size);
 629   set_initial_gvn(&amp;gvn);
 630 
 631   print_inlining_init();
 632   { // Scope for timing the parser
 633     TracePhase tp(&quot;parse&quot;, &amp;timers[_t_parser]);
 634 
 635     // Put top into the hash table ASAP.
 636     initial_gvn()-&gt;transform_no_reclaim(top());
 637 
 638     // Set up tf(), start(), and find a CallGenerator.
 639     CallGenerator* cg = NULL;
 640     if (is_osr_compilation()) {
<span class="line-modified"> 641       init_tf(TypeFunc::make(method(), /* is_osr_compilation = */ true));</span>
<span class="line-modified"> 642       StartNode* s = new StartOSRNode(root(), tf()-&gt;domain_sig());</span>


 643       initial_gvn()-&gt;set_type_bottom(s);
 644       init_start(s);
 645       cg = CallGenerator::for_osr(method(), entry_bci());
 646     } else {
 647       // Normal case.
 648       init_tf(TypeFunc::make(method()));
<span class="line-modified"> 649       StartNode* s = new StartNode(root(), tf()-&gt;domain_cc());</span>
 650       initial_gvn()-&gt;set_type_bottom(s);
 651       init_start(s);
 652       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get) {
 653         // With java.lang.ref.reference.get() we must go through the
 654         // intrinsic - even when get() is the root
 655         // method of the compile - so that, if necessary, the value in
 656         // the referent field of the reference object gets recorded by
 657         // the pre-barrier code.
 658         cg = find_intrinsic(method(), false);
 659       }
 660       if (cg == NULL) {
 661         float past_uses = method()-&gt;interpreter_invocation_count();
 662         float expected_uses = past_uses;
 663         cg = CallGenerator::for_inline(method(), expected_uses);
 664       }
 665     }
 666     if (failing())  return;
 667     if (cg == NULL) {
 668       record_method_not_compilable(&quot;cannot parse method&quot;);
 669       return;
</pre>
<hr />
<pre>
 754     }
 755   }
 756 #endif
 757 
 758 #ifdef ASSERT
 759   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 760   bs-&gt;verify_gc_barriers(this, BarrierSetC2::BeforeCodeGen);
 761 #endif
 762 
 763   // Dump compilation data to replay it.
 764   if (directive-&gt;DumpReplayOption) {
 765     env()-&gt;dump_replay_data(_compile_id);
 766   }
 767   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 768     env()-&gt;dump_inline_data(_compile_id);
 769   }
 770 
 771   // Now that we know the size of all the monitors we can add a fixed slot
 772   // for the original deopt pc.
 773   int next_slot = fixed_slots() + (sizeof(address) / VMRegImpl::stack_slot_size);
<span class="line-added"> 774   if (needs_stack_repair()) {</span>
<span class="line-added"> 775     // One extra slot for the special stack increment value</span>
<span class="line-added"> 776     next_slot += 2;</span>
<span class="line-added"> 777   }</span>
 778   set_fixed_slots(next_slot);
 779 
 780   // Compute when to use implicit null checks. Used by matching trap based
 781   // nodes and NullCheck optimization.
 782   set_allowed_deopt_reasons();
 783 
 784   // Now generate code
 785   Code_Gen();
 786 }
 787 
 788 //------------------------------Compile----------------------------------------
 789 // Compile a runtime stub
 790 Compile::Compile( ciEnv* ci_env,
 791                   TypeFunc_generator generator,
 792                   address stub_function,
 793                   const char *stub_name,
 794                   int is_fancy_jump,
 795                   bool pass_tls,
 796                   bool save_arg_registers,
 797                   bool return_pc,
</pre>
<hr />
<pre>
 911   // Create Debug Information Recorder to record scopes, oopmaps, etc.
 912   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
 913   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
 914   env()-&gt;set_dependencies(new Dependencies(env()));
 915 
 916   _fixed_slots = 0;
 917   set_has_split_ifs(false);
 918   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
 919   set_has_stringbuilder(false);
 920   set_has_boxed_value(false);
 921   _trap_can_recompile = false;  // no traps emitted yet
 922   _major_progress = true; // start out assuming good things will happen
 923   set_has_unsafe_access(false);
 924   set_max_vector_size(0);
 925   set_clear_upper_avx(false);  //false as default for clear upper bits of ymm registers
 926   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
 927   set_decompile_count(0);
 928 
 929   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
 930   _loop_opts_cnt = LoopOptsCount;
<span class="line-added"> 931   _has_flattened_accesses = false;</span>
<span class="line-added"> 932   _flattened_accesses_share_alias = true;</span>
<span class="line-added"> 933 </span>
 934   set_do_inlining(Inline);
 935   set_max_inline_size(MaxInlineSize);
 936   set_freq_inline_size(FreqInlineSize);
 937   set_do_scheduling(OptoScheduling);
 938   set_do_count_invocations(false);
 939   set_do_method_data_update(false);
 940 
 941   set_do_vector_loop(false);
 942 
 943   if (AllowVectorizeOnDemand) {
 944     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
 945       set_do_vector_loop(true);
 946       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print(&quot;Compile::Init: do vectorized loops (SIMD like) for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
 947     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
 948                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
 949       set_do_vector_loop(true);
 950     }
 951   }
 952   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
 953   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print(&quot;Compile::Init: use CMove without profitability tests for method %s\n&quot;,  method()-&gt;name()-&gt;as_quoted_ascii());})
</pre>
<hr />
<pre>
 997   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
 998   {
 999     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1000   }
1001   // Initialize the first few types.
1002   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1003   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1004   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1005   _num_alias_types = AliasIdxRaw+1;
1006   // Zero out the alias type cache.
1007   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1008   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1009   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1010 
1011   _intrinsics = NULL;
1012   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1013   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1014   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1015   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1016   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
<span class="line-added">1017   _inline_type_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);</span>
1018   register_library_intrinsics();
1019 #ifdef ASSERT
1020   _type_verify_symmetry = true;
1021   _phase_optimize_finished = false;
1022 #endif
1023 }
1024 
1025 //---------------------------init_start----------------------------------------
1026 // Install the StartNode on this compile object.
1027 void Compile::init_start(StartNode* s) {
1028   if (failing())
1029     return; // already failing
1030   assert(s == start(), &quot;&quot;);
1031 }
1032 
1033 /**
1034  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1035  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1036  * the ideal graph.
1037  */
</pre>
<hr />
<pre>
1226 bool Compile::allow_range_check_smearing() const {
1227   // If this method has already thrown a range-check,
1228   // assume it was because we already tried range smearing
1229   // and it failed.
1230   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1231   return !already_trapped;
1232 }
1233 
1234 
1235 //------------------------------flatten_alias_type-----------------------------
1236 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1237   int offset = tj-&gt;offset();
1238   TypePtr::PTR ptr = tj-&gt;ptr();
1239 
1240   // Known instance (scalarizable allocation) alias only with itself.
1241   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1242                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1243 
1244   // Process weird unsafe references.
1245   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
<span class="line-modified">1246     bool default_value_load = EnableValhalla &amp;&amp; tj-&gt;is_instptr()-&gt;klass() == ciEnv::current()-&gt;Class_klass();</span>
<span class="line-added">1247     assert(InlineUnsafeOps || default_value_load, &quot;indeterminate pointers come only from unsafe ops&quot;);</span>
1248     assert(!is_known_inst, &quot;scalarizable allocation should not have unsafe references&quot;);
1249     tj = TypeOopPtr::BOTTOM;
1250     ptr = tj-&gt;ptr();
1251     offset = tj-&gt;offset();
1252   }
1253 
1254   // Array pointers need some flattening
1255   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1256   if (ta &amp;&amp; ta-&gt;is_stable()) {
1257     // Erase stability property for alias analysis.
1258     tj = ta = ta-&gt;cast_to_stable(false);
1259   }
<span class="line-added">1260   if (ta &amp;&amp; ta-&gt;is_not_flat()) {</span>
<span class="line-added">1261     // Erase not flat property for alias analysis.</span>
<span class="line-added">1262     tj = ta = ta-&gt;cast_to_not_flat(false);</span>
<span class="line-added">1263   }</span>
<span class="line-added">1264   if (ta &amp;&amp; ta-&gt;is_not_null_free()) {</span>
<span class="line-added">1265     // Erase not null free property for alias analysis.</span>
<span class="line-added">1266     tj = ta = ta-&gt;cast_to_not_null_free(false);</span>
<span class="line-added">1267   }</span>
<span class="line-added">1268 </span>
1269   if( ta &amp;&amp; is_known_inst ) {
1270     if ( offset != Type::OffsetBot &amp;&amp;
1271          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1272       offset = Type::OffsetBot; // Flatten constant access into array body only
<span class="line-modified">1273       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, Type::Offset(offset), ta-&gt;field_offset(), ta-&gt;instance_id());</span>
1274     }
1275   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1276     // For arrays indexed by constant indices, we flatten the alias
1277     // space to include all of the array body.  Only the header, klass
1278     // and array length can be accessed un-aliased.
<span class="line-added">1279     // For flattened inline type array, each field has its own slice so</span>
<span class="line-added">1280     // we must include the field offset.</span>
1281     if( offset != Type::OffsetBot ) {
1282       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1283         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1284         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1285       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1286         // range is OK as-is.
1287         tj = ta = TypeAryPtr::RANGE;
1288       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1289         tj = TypeInstPtr::KLASS; // all klass loads look alike
1290         ta = TypeAryPtr::RANGE; // generic ignored junk
1291         ptr = TypePtr::BotPTR;
1292       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1293         tj = TypeInstPtr::MARK;
1294         ta = TypeAryPtr::RANGE; // generic ignored junk
1295         ptr = TypePtr::BotPTR;
1296       } else {                  // Random constant offset into array body
1297         offset = Type::OffsetBot;   // Flatten constant access into array body
<span class="line-modified">1298         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1299       }
1300     }
1301     // Arrays of fixed size alias with arrays of unknown size.
1302     if (ta-&gt;size() != TypeInt::POS) {
1303       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
<span class="line-modified">1304       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1305     }
1306     // Arrays of known objects become arrays of unknown objects.
1307     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1308       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
<span class="line-modified">1309       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1310     }
1311     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1312       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
<span class="line-modified">1313       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());</span>
<span class="line-added">1314     }</span>
<span class="line-added">1315     // Initially all flattened array accesses share a single slice</span>
<span class="line-added">1316     if (ta-&gt;elem()-&gt;isa_inlinetype() &amp;&amp; ta-&gt;elem() != TypeInlineType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
<span class="line-added">1317       const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta-&gt;size());</span>
<span class="line-added">1318       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));</span>
1319     }
1320     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1321     // cannot be distinguished by bytecode alone.
1322     if (ta-&gt;elem() == TypeInt::BOOL) {
1323       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1324       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
<span class="line-modified">1325       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());</span>
1326     }
1327     // During the 2nd round of IterGVN, NotNull castings are removed.
1328     // Make sure the Bottom and NotNull variants alias the same.
1329     // Also, make sure exact and non-exact variants alias the same.
1330     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
<span class="line-modified">1331       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());</span>
1332     }
1333   }
1334 
1335   // Oop pointers need some flattening
1336   const TypeInstPtr *to = tj-&gt;isa_instptr();
1337   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1338     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1339     if( ptr == TypePtr::Constant ) {
1340       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1341           offset &lt; k-&gt;size_helper() * wordSize) {
1342         // No constant oop pointers (such as Strings); they alias with
1343         // unknown strings.
1344         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1345         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1346       }
1347     } else if( is_known_inst ) {
1348       tj = to; // Keep NotNull and klass_is_exact for instance type
1349     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1350       // During the 2nd round of IterGVN, NotNull castings are removed.
1351       // Make sure the Bottom and NotNull variants alias the same.
1352       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1353       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1354     }
1355     if (to-&gt;speculative() != NULL) {
<span class="line-modified">1356       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),Type::Offset(to-&gt;offset()), to-&gt;klass()-&gt;flatten_array(), to-&gt;instance_id());</span>
1357     }
1358     // Canonicalize the holder of this field
1359     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1360       // First handle header references such as a LoadKlassNode, even if the
1361       // object&#39;s klass is unloaded at compile time (4965979).
1362       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1363         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, Type::Offset(offset), false);</span>
1364       }
1365     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1366       // Static fields are in the space above the normal instance
1367       // fields in the java.lang.Class instance.
1368       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1369         to = NULL;
1370         tj = TypeOopPtr::BOTTOM;
1371         offset = tj-&gt;offset();
1372       }
1373     } else {
1374       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1375       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1376         if( is_known_inst ) {
<span class="line-modified">1377           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array(), to-&gt;instance_id());</span>
1378         } else {
<span class="line-modified">1379           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array());</span>
1380         }
1381       }
1382     }
1383   }
1384 
1385   // Klass pointers to object array klasses need some flattening
1386   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1387   if( tk ) {
1388     // If we are referencing a field within a Klass, we need
1389     // to assume the worst case of an Object.  Both exact and
1390     // inexact types must flatten to the same alias class so
1391     // use NotNull as the PTR.
1392     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1393 
1394       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1395                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1396                                    Type::Offset(offset),</span>
<span class="line-added">1397                                    false);</span>
1398     }
1399 
1400     ciKlass* klass = tk-&gt;klass();
<span class="line-modified">1401     if (klass != NULL &amp;&amp; klass-&gt;is_obj_array_klass()) {</span>
1402       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1403       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1404         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1405       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset), false);</span>
1406     }
1407 
1408     // Check for precise loads from the primary supertype array and force them
1409     // to the supertype cache alias index.  Check for generic array loads from
1410     // the primary supertype array and also force them to the supertype cache
1411     // alias index.  Since the same load can reach both, we need to merge
1412     // these 2 disparate memories into the same alias class.  Since the
1413     // primary supertype array is read-only, there&#39;s no chance of confusion
1414     // where we bypass an array load and an array store.
1415     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1416     if (offset == Type::OffsetBot ||
1417         (offset &gt;= primary_supers_offset &amp;&amp;
1418          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1419         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1420       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1421       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk-&gt;klass(), Type::Offset(offset), tk-&gt;flat_array());</span>
1422     }
1423   }
1424 
1425   // Flatten all Raw pointers together.
1426   if (tj-&gt;base() == Type::RawPtr)
1427     tj = TypeRawPtr::BOTTOM;
1428 
1429   if (tj-&gt;base() == Type::AnyPtr)
1430     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1431 
1432   // Flatten all to bottom for now
1433   switch( _AliasLevel ) {
1434   case 0:
1435     tj = TypePtr::BOTTOM;
1436     break;
1437   case 1:                       // Flatten to: oop, static, field or array
1438     switch (tj-&gt;base()) {
1439     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1440     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1441     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1540   intptr_t key = (intptr_t) adr_type;
1541   key ^= key &gt;&gt; logAliasCacheSize;
1542   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1543 }
1544 
1545 
1546 //-----------------------------grow_alias_types--------------------------------
1547 void Compile::grow_alias_types() {
1548   const int old_ats  = _max_alias_types; // how many before?
1549   const int new_ats  = old_ats;          // how many more?
1550   const int grow_ats = old_ats+new_ats;  // how many now?
1551   _max_alias_types = grow_ats;
1552   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1553   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1554   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1555   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1556 }
1557 
1558 
1559 //--------------------------------find_alias_type------------------------------
<span class="line-modified">1560 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field, bool uncached) {</span>
1561   if (_AliasLevel == 0)
1562     return alias_type(AliasIdxBot);
1563 
<span class="line-modified">1564   AliasCacheEntry* ace = NULL;</span>
<span class="line-modified">1565   if (!uncached) {</span>
<span class="line-modified">1566     ace = probe_alias_cache(adr_type);</span>
<span class="line-added">1567     if (ace-&gt;_adr_type == adr_type) {</span>
<span class="line-added">1568       return alias_type(ace-&gt;_index);</span>
<span class="line-added">1569     }</span>
1570   }
1571 
1572   // Handle special cases.
1573   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1574   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1575 
1576   // Do it the slow way.
1577   const TypePtr* flat = flatten_alias_type(adr_type);
1578 
1579 #ifdef ASSERT
1580   {
1581     ResourceMark rm;
1582     assert(flat == flatten_alias_type(flat), &quot;not idempotent: adr_type = %s; flat = %s =&gt; %s&quot;,
1583            Type::str(adr_type), Type::str(flat), Type::str(flatten_alias_type(flat)));
1584     assert(flat != TypePtr::BOTTOM, &quot;cannot alias-analyze an untyped ptr: adr_type = %s&quot;,
1585            Type::str(adr_type));
1586     if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1587       const TypeOopPtr* foop = flat-&gt;is_oopptr();
1588       // Scalarizable allocations have exact klass always.
1589       bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
</pre>
<hr />
<pre>
1599     if (alias_type(i)-&gt;adr_type() == flat) {
1600       idx = i;
1601       break;
1602     }
1603   }
1604 
1605   if (idx == AliasIdxTop) {
1606     if (no_create)  return NULL;
1607     // Grow the array if necessary.
1608     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1609     // Add a new alias type.
1610     idx = _num_alias_types++;
1611     _alias_types[idx]-&gt;Init(idx, flat);
1612     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1613     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1614     if (flat-&gt;isa_instptr()) {
1615       if (flat-&gt;offset() == java_lang_Class::klass_offset()
1616           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1617         alias_type(idx)-&gt;set_rewritable(false);
1618     }
<span class="line-added">1619     ciField* field = NULL;</span>
1620     if (flat-&gt;isa_aryptr()) {
1621 #ifdef ASSERT
1622       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1623       // (T_BYTE has the weakest alignment and size restrictions...)
1624       assert(flat-&gt;offset() &lt; header_size_min, &quot;array body reference must be OffsetBot&quot;);
1625 #endif
<span class="line-added">1626       const Type* elemtype = flat-&gt;is_aryptr()-&gt;elem();</span>
1627       if (flat-&gt;offset() == TypePtr::OffsetBot) {
<span class="line-modified">1628         alias_type(idx)-&gt;set_element(elemtype);</span>
<span class="line-added">1629       }</span>
<span class="line-added">1630       int field_offset = flat-&gt;is_aryptr()-&gt;field_offset().get();</span>
<span class="line-added">1631       if (elemtype-&gt;isa_inlinetype() &amp;&amp;</span>
<span class="line-added">1632           elemtype-&gt;inline_klass() != NULL &amp;&amp;</span>
<span class="line-added">1633           field_offset != Type::OffsetBot) {</span>
<span class="line-added">1634         ciInlineKlass* vk = elemtype-&gt;inline_klass();</span>
<span class="line-added">1635         field_offset += vk-&gt;first_field_offset();</span>
<span class="line-added">1636         field = vk-&gt;get_field_by_offset(field_offset, false);</span>
1637       }
1638     }
1639     if (flat-&gt;isa_klassptr()) {
1640       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1641         alias_type(idx)-&gt;set_rewritable(false);
1642       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1643         alias_type(idx)-&gt;set_rewritable(false);
1644       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1645         alias_type(idx)-&gt;set_rewritable(false);
1646       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1647         alias_type(idx)-&gt;set_rewritable(false);
<span class="line-added">1648       if (flat-&gt;offset() == in_bytes(Klass::layout_helper_offset()))</span>
<span class="line-added">1649         alias_type(idx)-&gt;set_rewritable(false);</span>
1650       if (flat-&gt;offset() == in_bytes(Klass::secondary_super_cache_offset()))
1651         alias_type(idx)-&gt;set_rewritable(false);
1652     }
1653     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1654     // but the base pointer type is not distinctive enough to identify
1655     // references into JavaThread.)
1656 
1657     // Check for final fields.
1658     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1659     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {

1660       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1661           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1662           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1663         // static field
1664         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1665         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
<span class="line-added">1666       } else if (tinst-&gt;klass()-&gt;is_inlinetype()) {</span>
<span class="line-added">1667         // Inline type field</span>
<span class="line-added">1668         ciInlineKlass* vk = tinst-&gt;inline_klass();</span>
<span class="line-added">1669         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);</span>
1670       } else {
<span class="line-modified">1671         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();</span>
1672         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1673       }
<span class="line-modified">1674     }</span>
<span class="line-modified">1675     assert(field == NULL ||</span>
<span class="line-modified">1676            original_field == NULL ||</span>
<span class="line-modified">1677            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;</span>
<span class="line-modified">1678             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;</span>
<span class="line-modified">1679             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);</span>
<span class="line-modified">1680     // Set field() and is_rewritable() attributes.</span>
<span class="line-added">1681     if (field != NULL) {</span>
<span class="line-added">1682       alias_type(idx)-&gt;set_field(field);</span>
<span class="line-added">1683       if (flat-&gt;isa_aryptr()) {</span>
<span class="line-added">1684         // Fields of flat arrays are rewritable although they are declared final</span>
<span class="line-added">1685         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype(), &quot;must be a flat array&quot;);</span>
<span class="line-added">1686         alias_type(idx)-&gt;set_rewritable(true);</span>
<span class="line-added">1687       }</span>
1688     }
1689   }
1690 
1691   // Fill the cache for next time.
<span class="line-modified">1692   if (!uncached) {</span>
<span class="line-modified">1693     ace-&gt;_adr_type = adr_type;</span>
<span class="line-modified">1694     ace-&gt;_index    = idx;</span>
<span class="line-added">1695     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);</span>
1696 
<span class="line-modified">1697     // Might as well try to fill the cache for the flattened version, too.</span>
<span class="line-modified">1698     AliasCacheEntry* face = probe_alias_cache(flat);</span>
<span class="line-modified">1699     if (face-&gt;_adr_type == NULL) {</span>
<span class="line-modified">1700       face-&gt;_adr_type = flat;</span>
<span class="line-modified">1701       face-&gt;_index    = idx;</span>
<span class="line-modified">1702       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);</span>
<span class="line-added">1703     }</span>
1704   }
1705 
1706   return alias_type(idx);
1707 }
1708 
1709 
1710 Compile::AliasType* Compile::alias_type(ciField* field) {
1711   const TypeOopPtr* t;
1712   if (field-&gt;is_static())
1713     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1714   else
1715     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1716   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1717   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), &quot;must get the rewritable bits correct&quot;);
1718   return atp;
1719 }
1720 
1721 
1722 //------------------------------have_alias_type--------------------------------
1723 bool Compile::have_alias_type(const TypePtr* adr_type) {
</pre>
<hr />
<pre>
1845   }
1846   assert(range_check_cast_count() == 0, &quot;should be empty&quot;);
1847 }
1848 
1849 void Compile::add_opaque4_node(Node* n) {
1850   assert(n-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1851   assert(!_opaque4_nodes-&gt;contains(n), &quot;duplicate entry in Opaque4 list&quot;);
1852   _opaque4_nodes-&gt;append(n);
1853 }
1854 
1855 // Remove all Opaque4 nodes.
1856 void Compile::remove_opaque4_nodes(PhaseIterGVN &amp;igvn) {
1857   for (int i = opaque4_count(); i &gt; 0; i--) {
1858     Node* opaq = opaque4_node(i-1);
1859     assert(opaq-&gt;Opcode() == Op_Opaque4, &quot;Opaque4 only&quot;);
1860     igvn.replace_node(opaq, opaq-&gt;in(2));
1861   }
1862   assert(opaque4_count() == 0, &quot;should be empty&quot;);
1863 }
1864 
<span class="line-added">1865 void Compile::add_inline_type(Node* n) {</span>
<span class="line-added">1866   assert(n-&gt;is_InlineTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1867   if (_inline_type_nodes != NULL) {</span>
<span class="line-added">1868     _inline_type_nodes-&gt;push(n);</span>
<span class="line-added">1869   }</span>
<span class="line-added">1870 }</span>
<span class="line-added">1871 </span>
<span class="line-added">1872 void Compile::remove_inline_type(Node* n) {</span>
<span class="line-added">1873   assert(n-&gt;is_InlineTypeBase(), &quot;unexpected node&quot;);</span>
<span class="line-added">1874   if (_inline_type_nodes != NULL &amp;&amp; _inline_type_nodes-&gt;contains(n)) {</span>
<span class="line-added">1875     _inline_type_nodes-&gt;remove(n);</span>
<span class="line-added">1876   }</span>
<span class="line-added">1877 }</span>
<span class="line-added">1878 </span>
<span class="line-added">1879 // Does the return value keep otherwise useless inline type allocations alive?</span>
<span class="line-added">1880 static bool return_val_keeps_allocations_alive(Node* ret_val) {</span>
<span class="line-added">1881   ResourceMark rm;</span>
<span class="line-added">1882   Unique_Node_List wq;</span>
<span class="line-added">1883   wq.push(ret_val);</span>
<span class="line-added">1884   bool some_allocations = false;</span>
<span class="line-added">1885   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1886     Node* n = wq.at(i);</span>
<span class="line-added">1887     assert(!n-&gt;is_InlineType(), &quot;chain of inline type nodes&quot;);</span>
<span class="line-added">1888     if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">1889       // Some other use for the allocation</span>
<span class="line-added">1890       return false;</span>
<span class="line-added">1891     } else if (n-&gt;is_InlineTypePtr()) {</span>
<span class="line-added">1892       wq.push(n-&gt;in(1));</span>
<span class="line-added">1893     } else if (n-&gt;is_Phi()) {</span>
<span class="line-added">1894       for (uint j = 1; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1895         wq.push(n-&gt;in(j));</span>
<span class="line-added">1896       }</span>
<span class="line-added">1897     } else if (n-&gt;is_CheckCastPP() &amp;&amp;</span>
<span class="line-added">1898                n-&gt;in(1)-&gt;is_Proj() &amp;&amp;</span>
<span class="line-added">1899                n-&gt;in(1)-&gt;in(0)-&gt;is_Allocate()) {</span>
<span class="line-added">1900       some_allocations = true;</span>
<span class="line-added">1901     }</span>
<span class="line-added">1902   }</span>
<span class="line-added">1903   return some_allocations;</span>
<span class="line-added">1904 }</span>
<span class="line-added">1905 </span>
<span class="line-added">1906 void Compile::process_inline_types(PhaseIterGVN &amp;igvn, bool post_ea) {</span>
<span class="line-added">1907   // Make inline types scalar in safepoints</span>
<span class="line-added">1908   for (int i = _inline_type_nodes-&gt;length()-1; i &gt;= 0; i--) {</span>
<span class="line-added">1909     InlineTypeBaseNode* vt = _inline_type_nodes-&gt;at(i)-&gt;as_InlineTypeBase();</span>
<span class="line-added">1910     vt-&gt;make_scalar_in_safepoints(&amp;igvn);</span>
<span class="line-added">1911   }</span>
<span class="line-added">1912   // Remove InlineTypePtr nodes only after EA to give scalar replacement a chance</span>
<span class="line-added">1913   // to remove buffer allocations. InlineType nodes are kept until loop opts and</span>
<span class="line-added">1914   // removed via InlineTypeNode::remove_redundant_allocations.</span>
<span class="line-added">1915   if (post_ea) {</span>
<span class="line-added">1916     while (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-added">1917       InlineTypeBaseNode* vt = _inline_type_nodes-&gt;pop()-&gt;as_InlineTypeBase();</span>
<span class="line-added">1918       if (vt-&gt;is_InlineTypePtr()) {</span>
<span class="line-added">1919         igvn.replace_node(vt, vt-&gt;get_oop());</span>
<span class="line-added">1920       }</span>
<span class="line-added">1921     }</span>
<span class="line-added">1922   }</span>
<span class="line-added">1923   // Make sure that the return value does not keep an unused allocation alive</span>
<span class="line-added">1924   if (tf()-&gt;returns_inline_type_as_fields()) {</span>
<span class="line-added">1925     Node* ret = NULL;</span>
<span class="line-added">1926     for (uint i = 1; i &lt; root()-&gt;req(); i++){</span>
<span class="line-added">1927       Node* in = root()-&gt;in(i);</span>
<span class="line-added">1928       if (in-&gt;Opcode() == Op_Return) {</span>
<span class="line-added">1929         assert(ret == NULL, &quot;only one return&quot;);</span>
<span class="line-added">1930         ret = in;</span>
<span class="line-added">1931       }</span>
<span class="line-added">1932     }</span>
<span class="line-added">1933     if (ret != NULL) {</span>
<span class="line-added">1934       Node* ret_val = ret-&gt;in(TypeFunc::Parms);</span>
<span class="line-added">1935       if (igvn.type(ret_val)-&gt;isa_oopptr() &amp;&amp;</span>
<span class="line-added">1936           return_val_keeps_allocations_alive(ret_val)) {</span>
<span class="line-added">1937         igvn.replace_input_of(ret, TypeFunc::Parms, InlineTypeNode::tagged_klass(igvn.type(ret_val)-&gt;inline_klass(), igvn));</span>
<span class="line-added">1938         assert(ret_val-&gt;outcnt() == 0, &quot;should be dead now&quot;);</span>
<span class="line-added">1939         igvn.remove_dead_node(ret_val);</span>
<span class="line-added">1940       }</span>
<span class="line-added">1941     }</span>
<span class="line-added">1942   }</span>
<span class="line-added">1943   igvn.optimize();</span>
<span class="line-added">1944 }</span>
<span class="line-added">1945 </span>
<span class="line-added">1946 void Compile::adjust_flattened_array_access_aliases(PhaseIterGVN&amp; igvn) {</span>
<span class="line-added">1947   if (!_has_flattened_accesses) {</span>
<span class="line-added">1948     return;</span>
<span class="line-added">1949   }</span>
<span class="line-added">1950   // Initially, all flattened array accesses share the same slice to</span>
<span class="line-added">1951   // keep dependencies with Object[] array accesses (that could be</span>
<span class="line-added">1952   // to a flattened array) correct. We&#39;re done with parsing so we</span>
<span class="line-added">1953   // now know all flattened array accesses in this compile</span>
<span class="line-added">1954   // unit. Let&#39;s move flattened array accesses to their own slice,</span>
<span class="line-added">1955   // one per element field. This should help memory access</span>
<span class="line-added">1956   // optimizations.</span>
<span class="line-added">1957   ResourceMark rm;</span>
<span class="line-added">1958   Unique_Node_List wq;</span>
<span class="line-added">1959   wq.push(root());</span>
<span class="line-added">1960 </span>
<span class="line-added">1961   Node_List mergememnodes;</span>
<span class="line-added">1962   Node_List memnodes;</span>
<span class="line-added">1963 </span>
<span class="line-added">1964   // Alias index currently shared by all flattened memory accesses</span>
<span class="line-added">1965   int index = get_alias_index(TypeAryPtr::INLINES);</span>
<span class="line-added">1966 </span>
<span class="line-added">1967   // Find MergeMem nodes and flattened array accesses</span>
<span class="line-added">1968   for (uint i = 0; i &lt; wq.size(); i++) {</span>
<span class="line-added">1969     Node* n = wq.at(i);</span>
<span class="line-added">1970     if (n-&gt;is_Mem()) {</span>
<span class="line-added">1971       const TypePtr* adr_type = NULL;</span>
<span class="line-added">1972       if (n-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">1973         adr_type = get_adr_type(get_alias_index(n-&gt;in(MemNode::OopStore)-&gt;adr_type()));</span>
<span class="line-added">1974       } else {</span>
<span class="line-added">1975         adr_type = get_adr_type(get_alias_index(n-&gt;adr_type()));</span>
<span class="line-added">1976       }</span>
<span class="line-added">1977       if (adr_type == TypeAryPtr::INLINES) {</span>
<span class="line-added">1978         memnodes.push(n);</span>
<span class="line-added">1979       }</span>
<span class="line-added">1980     } else if (n-&gt;is_MergeMem()) {</span>
<span class="line-added">1981       MergeMemNode* mm = n-&gt;as_MergeMem();</span>
<span class="line-added">1982       if (mm-&gt;memory_at(index) != mm-&gt;base_memory()) {</span>
<span class="line-added">1983         mergememnodes.push(n);</span>
<span class="line-added">1984       }</span>
<span class="line-added">1985     }</span>
<span class="line-added">1986     for (uint j = 0; j &lt; n-&gt;req(); j++) {</span>
<span class="line-added">1987       Node* m = n-&gt;in(j);</span>
<span class="line-added">1988       if (m != NULL) {</span>
<span class="line-added">1989         wq.push(m);</span>
<span class="line-added">1990       }</span>
<span class="line-added">1991     }</span>
<span class="line-added">1992   }</span>
<span class="line-added">1993 </span>
<span class="line-added">1994   if (memnodes.size() &gt; 0) {</span>
<span class="line-added">1995     _flattened_accesses_share_alias = false;</span>
<span class="line-added">1996 </span>
<span class="line-added">1997     // We are going to change the slice for the flattened array</span>
<span class="line-added">1998     // accesses so we need to clear the cache entries that refer to</span>
<span class="line-added">1999     // them.</span>
<span class="line-added">2000     for (uint i = 0; i &lt; AliasCacheSize; i++) {</span>
<span class="line-added">2001       AliasCacheEntry* ace = &amp;_alias_cache[i];</span>
<span class="line-added">2002       if (ace-&gt;_adr_type != NULL &amp;&amp;</span>
<span class="line-added">2003           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;</span>
<span class="line-added">2004           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2005         ace-&gt;_adr_type = NULL;</span>
<span class="line-added">2006         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop</span>
<span class="line-added">2007       }</span>
<span class="line-added">2008     }</span>
<span class="line-added">2009 </span>
<span class="line-added">2010     // Find what aliases we are going to add</span>
<span class="line-added">2011     int start_alias = num_alias_types()-1;</span>
<span class="line-added">2012     int stop_alias = 0;</span>
<span class="line-added">2013 </span>
<span class="line-added">2014     for (uint i = 0; i &lt; memnodes.size(); i++) {</span>
<span class="line-added">2015       Node* m = memnodes.at(i);</span>
<span class="line-added">2016       const TypePtr* adr_type = NULL;</span>
<span class="line-added">2017       if (m-&gt;Opcode() == Op_StoreCM) {</span>
<span class="line-added">2018         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();</span>
<span class="line-added">2019         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),</span>
<span class="line-added">2020                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),</span>
<span class="line-added">2021                                       get_alias_index(adr_type));</span>
<span class="line-added">2022         igvn.register_new_node_with_optimizer(clone);</span>
<span class="line-added">2023         igvn.replace_node(m, clone);</span>
<span class="line-added">2024       } else {</span>
<span class="line-added">2025         adr_type = m-&gt;adr_type();</span>
<span class="line-added">2026 #ifdef ASSERT</span>
<span class="line-added">2027         m-&gt;as_Mem()-&gt;set_adr_type(adr_type);</span>
<span class="line-added">2028 #endif</span>
<span class="line-added">2029       }</span>
<span class="line-added">2030       int idx = get_alias_index(adr_type);</span>
<span class="line-added">2031       start_alias = MIN2(start_alias, idx);</span>
<span class="line-added">2032       stop_alias = MAX2(stop_alias, idx);</span>
<span class="line-added">2033     }</span>
<span class="line-added">2034 </span>
<span class="line-added">2035     assert(stop_alias &gt;= start_alias, &quot;should have expanded aliases&quot;);</span>
<span class="line-added">2036 </span>
<span class="line-added">2037     Node_Stack stack(0);</span>
<span class="line-added">2038 #ifdef ASSERT</span>
<span class="line-added">2039     VectorSet seen(Thread::current()-&gt;resource_area());</span>
<span class="line-added">2040 #endif</span>
<span class="line-added">2041     // Now let&#39;s fix the memory graph so each flattened array access</span>
<span class="line-added">2042     // is moved to the right slice. Start from the MergeMem nodes.</span>
<span class="line-added">2043     uint last = unique();</span>
<span class="line-added">2044     for (uint i = 0; i &lt; mergememnodes.size(); i++) {</span>
<span class="line-added">2045       MergeMemNode* current = mergememnodes.at(i)-&gt;as_MergeMem();</span>
<span class="line-added">2046       Node* n = current-&gt;memory_at(index);</span>
<span class="line-added">2047       MergeMemNode* mm = NULL;</span>
<span class="line-added">2048       do {</span>
<span class="line-added">2049         // Follow memory edges through memory accesses, phis and</span>
<span class="line-added">2050         // narrow membars and push nodes on the stack. Once we hit</span>
<span class="line-added">2051         // bottom memory, we pop element off the stack one at a</span>
<span class="line-added">2052         // time, in reverse order, and move them to the right slice</span>
<span class="line-added">2053         // by changing their memory edges.</span>
<span class="line-added">2054         if ((n-&gt;is_Phi() &amp;&amp; n-&gt;adr_type() != TypePtr::BOTTOM) || n-&gt;is_Mem() || n-&gt;adr_type() == TypeAryPtr::INLINES) {</span>
<span class="line-added">2055           assert(!seen.test_set(n-&gt;_idx), &quot;&quot;);</span>
<span class="line-added">2056           // Uses (a load for instance) will need to be moved to the</span>
<span class="line-added">2057           // right slice as well and will get a new memory state</span>
<span class="line-added">2058           // that we don&#39;t know yet. The use could also be the</span>
<span class="line-added">2059           // backedge of a loop. We put a place holder node between</span>
<span class="line-added">2060           // the memory node and its uses. We replace that place</span>
<span class="line-added">2061           // holder with the correct memory state once we know it,</span>
<span class="line-added">2062           // i.e. when nodes are popped off the stack. Using the</span>
<span class="line-added">2063           // place holder make the logic work in the presence of</span>
<span class="line-added">2064           // loops.</span>
<span class="line-added">2065           if (n-&gt;outcnt() &gt; 1) {</span>
<span class="line-added">2066             Node* place_holder = NULL;</span>
<span class="line-added">2067             assert(!n-&gt;has_out_with(Op_Node), &quot;&quot;);</span>
<span class="line-added">2068             for (DUIterator k = n-&gt;outs(); n-&gt;has_out(k); k++) {</span>
<span class="line-added">2069               Node* u = n-&gt;out(k);</span>
<span class="line-added">2070               if (u != current &amp;&amp; u-&gt;_idx &lt; last) {</span>
<span class="line-added">2071                 bool success = false;</span>
<span class="line-added">2072                 for (uint l = 0; l &lt; u-&gt;req(); l++) {</span>
<span class="line-added">2073                   if (!stack.is_empty() &amp;&amp; u == stack.node() &amp;&amp; l == stack.index()) {</span>
<span class="line-added">2074                     continue;</span>
<span class="line-added">2075                   }</span>
<span class="line-added">2076                   Node* in = u-&gt;in(l);</span>
<span class="line-added">2077                   if (in == n) {</span>
<span class="line-added">2078                     if (place_holder == NULL) {</span>
<span class="line-added">2079                       place_holder = new Node(1);</span>
<span class="line-added">2080                       place_holder-&gt;init_req(0, n);</span>
<span class="line-added">2081                     }</span>
<span class="line-added">2082                     igvn.replace_input_of(u, l, place_holder);</span>
<span class="line-added">2083                     success = true;</span>
<span class="line-added">2084                   }</span>
<span class="line-added">2085                 }</span>
<span class="line-added">2086                 if (success) {</span>
<span class="line-added">2087                   --k;</span>
<span class="line-added">2088                 }</span>
<span class="line-added">2089               }</span>
<span class="line-added">2090             }</span>
<span class="line-added">2091           }</span>
<span class="line-added">2092           if (n-&gt;is_Phi()) {</span>
<span class="line-added">2093             stack.push(n, 1);</span>
<span class="line-added">2094             n = n-&gt;in(1);</span>
<span class="line-added">2095           } else if (n-&gt;is_Mem()) {</span>
<span class="line-added">2096             stack.push(n, n-&gt;req());</span>
<span class="line-added">2097             n = n-&gt;in(MemNode::Memory);</span>
<span class="line-added">2098           } else {</span>
<span class="line-added">2099             assert(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;Opcode() == Op_MemBarCPUOrder, &quot;&quot;);</span>
<span class="line-added">2100             stack.push(n, n-&gt;req());</span>
<span class="line-added">2101             n = n-&gt;in(0)-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">2102           }</span>
<span class="line-added">2103         } else {</span>
<span class="line-added">2104           assert(n-&gt;adr_type() == TypePtr::BOTTOM || (n-&gt;Opcode() == Op_Node &amp;&amp; n-&gt;_idx &gt;= last) || (n-&gt;is_Proj() &amp;&amp; n-&gt;in(0)-&gt;is_Initialize()), &quot;&quot;);</span>
<span class="line-added">2105           // Build a new MergeMem node to carry the new memory state</span>
<span class="line-added">2106           // as we build it. IGVN should fold extraneous MergeMem</span>
<span class="line-added">2107           // nodes.</span>
<span class="line-added">2108           mm = MergeMemNode::make(n);</span>
<span class="line-added">2109           igvn.register_new_node_with_optimizer(mm);</span>
<span class="line-added">2110           while (stack.size() &gt; 0) {</span>
<span class="line-added">2111             Node* m = stack.node();</span>
<span class="line-added">2112             uint idx = stack.index();</span>
<span class="line-added">2113             if (m-&gt;is_Mem()) {</span>
<span class="line-added">2114               // Move memory node to its new slice</span>
<span class="line-added">2115               const TypePtr* adr_type = m-&gt;adr_type();</span>
<span class="line-added">2116               int alias = get_alias_index(adr_type);</span>
<span class="line-added">2117               Node* prev = mm-&gt;memory_at(alias);</span>
<span class="line-added">2118               igvn.replace_input_of(m, MemNode::Memory, prev);</span>
<span class="line-added">2119               mm-&gt;set_memory_at(alias, m);</span>
<span class="line-added">2120             } else if (m-&gt;is_Phi()) {</span>
<span class="line-added">2121               // We need as many new phis as there are new aliases</span>
<span class="line-added">2122               igvn.replace_input_of(m, idx, mm);</span>
<span class="line-added">2123               if (idx == m-&gt;req()-1) {</span>
<span class="line-added">2124                 Node* r = m-&gt;in(0);</span>
<span class="line-added">2125                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2126                   const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2127                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2128                     continue;</span>
<span class="line-added">2129                   }</span>
<span class="line-added">2130                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));</span>
<span class="line-added">2131                   igvn.register_new_node_with_optimizer(phi);</span>
<span class="line-added">2132                   for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2133                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));</span>
<span class="line-added">2134                   }</span>
<span class="line-added">2135                   mm-&gt;set_memory_at(j, phi);</span>
<span class="line-added">2136                 }</span>
<span class="line-added">2137                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);</span>
<span class="line-added">2138                 igvn.register_new_node_with_optimizer(base_phi);</span>
<span class="line-added">2139                 for (uint k = 1; k &lt; m-&gt;req(); k++) {</span>
<span class="line-added">2140                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());</span>
<span class="line-added">2141                 }</span>
<span class="line-added">2142                 mm-&gt;set_base_memory(base_phi);</span>
<span class="line-added">2143               }</span>
<span class="line-added">2144             } else {</span>
<span class="line-added">2145               // This is a MemBarCPUOrder node from</span>
<span class="line-added">2146               // Parse::array_load()/Parse::array_store(), in the</span>
<span class="line-added">2147               // branch that handles flattened arrays hidden under</span>
<span class="line-added">2148               // an Object[] array. We also need one new membar per</span>
<span class="line-added">2149               // new alias to keep the unknown access that the</span>
<span class="line-added">2150               // membars protect properly ordered with accesses to</span>
<span class="line-added">2151               // known flattened array.</span>
<span class="line-added">2152               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);</span>
<span class="line-added">2153               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);</span>
<span class="line-added">2154               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());</span>
<span class="line-added">2155               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2156                 const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2157                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2158                   continue;</span>
<span class="line-added">2159                 }</span>
<span class="line-added">2160                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);</span>
<span class="line-added">2161                 igvn.register_new_node_with_optimizer(mb);</span>
<span class="line-added">2162                 Node* mem = mm-&gt;memory_at(j);</span>
<span class="line-added">2163                 mb-&gt;init_req(TypeFunc::Control, ctrl);</span>
<span class="line-added">2164                 mb-&gt;init_req(TypeFunc::Memory, mem);</span>
<span class="line-added">2165                 ctrl = new ProjNode(mb, TypeFunc::Control);</span>
<span class="line-added">2166                 igvn.register_new_node_with_optimizer(ctrl);</span>
<span class="line-added">2167                 mem = new ProjNode(mb, TypeFunc::Memory);</span>
<span class="line-added">2168                 igvn.register_new_node_with_optimizer(mem);</span>
<span class="line-added">2169                 mm-&gt;set_memory_at(j, mem);</span>
<span class="line-added">2170               }</span>
<span class="line-added">2171               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);</span>
<span class="line-added">2172             }</span>
<span class="line-added">2173             if (idx &lt; m-&gt;req()-1) {</span>
<span class="line-added">2174               idx += 1;</span>
<span class="line-added">2175               stack.set_index(idx);</span>
<span class="line-added">2176               n = m-&gt;in(idx);</span>
<span class="line-added">2177               break;</span>
<span class="line-added">2178             }</span>
<span class="line-added">2179             // Take care of place holder nodes</span>
<span class="line-added">2180             if (m-&gt;has_out_with(Op_Node)) {</span>
<span class="line-added">2181               Node* place_holder = m-&gt;find_out_with(Op_Node);</span>
<span class="line-added">2182               if (place_holder != NULL) {</span>
<span class="line-added">2183                 Node* mm_clone = mm-&gt;clone();</span>
<span class="line-added">2184                 igvn.register_new_node_with_optimizer(mm_clone);</span>
<span class="line-added">2185                 Node* hook = new Node(1);</span>
<span class="line-added">2186                 hook-&gt;init_req(0, mm);</span>
<span class="line-added">2187                 igvn.replace_node(place_holder, mm_clone);</span>
<span class="line-added">2188                 hook-&gt;destruct();</span>
<span class="line-added">2189               }</span>
<span class="line-added">2190               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);</span>
<span class="line-added">2191             }</span>
<span class="line-added">2192             stack.pop();</span>
<span class="line-added">2193           }</span>
<span class="line-added">2194         }</span>
<span class="line-added">2195       } while(stack.size() &gt; 0);</span>
<span class="line-added">2196       // Fix the memory state at the MergeMem we started from</span>
<span class="line-added">2197       igvn.rehash_node_delayed(current);</span>
<span class="line-added">2198       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {</span>
<span class="line-added">2199         const Type* adr_type = get_adr_type(j);</span>
<span class="line-added">2200         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
<span class="line-added">2201           continue;</span>
<span class="line-added">2202         }</span>
<span class="line-added">2203         current-&gt;set_memory_at(j, mm);</span>
<span class="line-added">2204       }</span>
<span class="line-added">2205       current-&gt;set_memory_at(index, current-&gt;base_memory());</span>
<span class="line-added">2206     }</span>
<span class="line-added">2207     igvn.optimize();</span>
<span class="line-added">2208   }</span>
<span class="line-added">2209   print_method(PHASE_SPLIT_INLINES_ARRAY, 2);</span>
<span class="line-added">2210 }</span>
<span class="line-added">2211 </span>
<span class="line-added">2212 </span>
2213 // StringOpts and late inlining of string methods
2214 void Compile::inline_string_calls(bool parse_time) {
2215   {
2216     // remove useless nodes to make the usage analysis simpler
2217     ResourceMark rm;
2218     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2219   }
2220 
2221   {
2222     ResourceMark rm;
2223     print_method(PHASE_BEFORE_STRINGOPTS, 3);
2224     PhaseStringOpts pso(initial_gvn(), for_igvn());
2225     print_method(PHASE_AFTER_STRINGOPTS, 3);
2226   }
2227 
2228   // now inline anything that we skipped the first time around
2229   if (!parse_time) {
2230     _late_inlines_pos = _late_inlines.length();
2231   }
2232 
</pre>
<hr />
<pre>
2472   remove_speculative_types(igvn);
2473 
2474   // No more new expensive nodes will be added to the list from here
2475   // so keep only the actual candidates for optimizations.
2476   cleanup_expensive_nodes(igvn);
2477 
2478   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2479     Compile::TracePhase tp(&quot;&quot;, &amp;timers[_t_renumberLive]);
2480     initial_gvn()-&gt;replace_with(&amp;igvn);
2481     for_igvn()-&gt;clear();
2482     Unique_Node_List new_worklist(C-&gt;comp_arena());
2483     {
2484       ResourceMark rm;
2485       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2486     }
2487     set_for_igvn(&amp;new_worklist);
2488     igvn = PhaseIterGVN(initial_gvn());
2489     igvn.optimize();
2490   }
2491 
<span class="line-added">2492   if (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-added">2493     // Do this once all inlining is over to avoid getting inconsistent debug info</span>
<span class="line-added">2494     process_inline_types(igvn);</span>
<span class="line-added">2495   }</span>
<span class="line-added">2496 </span>
<span class="line-added">2497   adjust_flattened_array_access_aliases(igvn);</span>
<span class="line-added">2498 </span>
2499   // Perform escape analysis
2500   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2501     if (has_loops()) {
2502       // Cleanup graph (remove dead nodes).
2503       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2504       PhaseIdealLoop::optimize(igvn, LoopOptsMaxUnroll);
2505       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2506       if (failing())  return;
2507     }
2508     ConnectionGraph::do_analysis(this, &amp;igvn);
2509 
2510     if (failing())  return;
2511 
2512     // Optimize out fields loads from scalar replaceable allocations.
2513     igvn.optimize();
2514     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2515 
2516     if (failing())  return;
2517 
2518     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2519       TracePhase tp(&quot;macroEliminate&quot;, &amp;timers[_t_macroEliminate]);
2520       PhaseMacroExpand mexp(igvn);
2521       mexp.eliminate_macro_nodes();
2522       igvn.set_delay_transform(false);
2523 
2524       igvn.optimize();
2525       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2526 
2527       if (failing())  return;
2528     }
2529   }
2530 
<span class="line-added">2531   if (_inline_type_nodes-&gt;length() &gt; 0) {</span>
<span class="line-added">2532     // Process inline types again now that EA might have simplified the graph</span>
<span class="line-added">2533     process_inline_types(igvn, /* post_ea= */ true);</span>
<span class="line-added">2534   }</span>
<span class="line-added">2535 </span>
2536   // Loop transforms on the ideal graph.  Range Check Elimination,
2537   // peeling, unrolling, etc.
2538 
2539   // Set loop opts counter
2540   if((_loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2541     {
2542       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2543       PhaseIdealLoop::optimize(igvn, LoopOptsDefault);
2544       _loop_opts_cnt--;
2545       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2546       if (failing())  return;
2547     }
2548     // Loop opts pass if partial peeling occurred in previous pass
2549     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (_loop_opts_cnt &gt; 0)) {
2550       TracePhase tp(&quot;idealLoop&quot;, &amp;timers[_t_idealLoop]);
2551       PhaseIdealLoop::optimize(igvn, LoopOptsSkipSplitIf);
2552       _loop_opts_cnt--;
2553       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2554       if (failing())  return;
2555     }
</pre>
<hr />
<pre>
3158             // Accumulate any precedence edges
3159             if (mem-&gt;in(i) != NULL) {
3160               n-&gt;add_prec(mem-&gt;in(i));
3161             }
3162           }
3163           // Everything above this point has been processed.
3164           done = true;
3165         }
3166         // Eliminate the previous StoreCM
3167         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
3168         assert(mem-&gt;outcnt() == 0, &quot;should be dead&quot;);
3169         mem-&gt;disconnect_inputs(NULL, this);
3170       } else {
3171         prev = mem;
3172       }
3173       mem = prev-&gt;in(MemNode::Memory);
3174     }
3175   }
3176 }
3177 
<span class="line-added">3178 </span>
3179 //------------------------------final_graph_reshaping_impl----------------------
3180 // Implement items 1-5 from final_graph_reshaping below.
3181 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
3182 
3183   if ( n-&gt;outcnt() == 0 ) return; // dead node
3184   uint nop = n-&gt;Opcode();
3185 
3186   // Check for 2-input instruction with &quot;last use&quot; on right input.
3187   // Swap to left input.  Implements item (2).
3188   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
3189       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
3190       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
3191       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
3192       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
3193     // Check for commutative opcode
3194     switch( nop ) {
3195     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
3196     case Op_MaxI:  case Op_MinI:
3197     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
3198     case Op_AndL:  case Op_XorL:  case Op_OrL:
</pre>
<hr />
<pre>
3897           // Replace all nodes with identical edges as m with m
3898           k-&gt;subsume_by(m, this);
3899         }
3900       }
3901     }
3902     break;
3903   }
3904   case Op_CmpUL: {
3905     if (!Matcher::has_match_rule(Op_CmpUL)) {
3906       // No support for unsigned long comparisons
3907       ConINode* sign_pos = new ConINode(TypeInt::make(BitsPerLong - 1));
3908       Node* sign_bit_mask = new RShiftLNode(n-&gt;in(1), sign_pos);
3909       Node* orl = new OrLNode(n-&gt;in(1), sign_bit_mask);
3910       ConLNode* remove_sign_mask = new ConLNode(TypeLong::make(max_jlong));
3911       Node* andl = new AndLNode(orl, remove_sign_mask);
3912       Node* cmp = new CmpLNode(andl, n-&gt;in(2));
3913       n-&gt;subsume_by(cmp, this);
3914     }
3915     break;
3916   }
<span class="line-added">3917 #ifdef ASSERT</span>
<span class="line-added">3918   case Op_InlineTypePtr:</span>
<span class="line-added">3919   case Op_InlineType: {</span>
<span class="line-added">3920     n-&gt;dump(-1);</span>
<span class="line-added">3921     assert(false, &quot;inline type node was not removed&quot;);</span>
<span class="line-added">3922     break;</span>
<span class="line-added">3923   }</span>
<span class="line-added">3924 #endif</span>
3925   default:
3926     assert(!n-&gt;is_Call(), &quot;&quot;);
3927     assert(!n-&gt;is_Mem(), &quot;&quot;);
3928     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3929     break;
3930   }
3931 }
3932 
3933 //------------------------------final_graph_reshaping_walk---------------------
3934 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3935 // requires that the walk visits a node&#39;s inputs before visiting the node.
3936 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3937   Unique_Node_List sfpt;
3938 
3939   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3940   uint cnt = root-&gt;req();
3941   Node *n = root;
3942   uint  i = 0;
3943   while (true) {
3944     if (i &lt; cnt) {
</pre>
<hr />
<pre>
4252   }
4253 }
4254 
4255 bool Compile::needs_clinit_barrier(ciMethod* method, ciMethod* accessing_method) {
4256   return method-&gt;is_static() &amp;&amp; needs_clinit_barrier(method-&gt;holder(), accessing_method);
4257 }
4258 
4259 bool Compile::needs_clinit_barrier(ciField* field, ciMethod* accessing_method) {
4260   return field-&gt;is_static() &amp;&amp; needs_clinit_barrier(field-&gt;holder(), accessing_method);
4261 }
4262 
4263 bool Compile::needs_clinit_barrier(ciInstanceKlass* holder, ciMethod* accessing_method) {
4264   if (holder-&gt;is_initialized()) {
4265     return false;
4266   }
4267   if (holder-&gt;is_being_initialized()) {
4268     if (accessing_method-&gt;holder() == holder) {
4269       // Access inside a class. The barrier can be elided when access happens in &lt;clinit&gt;,
4270       // &lt;init&gt;, or a static method. In all those cases, there was an initialization
4271       // barrier on the holder klass passed.
<span class="line-modified">4272       if (accessing_method-&gt;is_class_initializer() ||</span>
<span class="line-modified">4273           accessing_method-&gt;is_object_constructor() ||</span>
4274           accessing_method-&gt;is_static()) {
4275         return false;
4276       }
4277     } else if (accessing_method-&gt;holder()-&gt;is_subclass_of(holder)) {
4278       // Access from a subclass. The barrier can be elided only when access happens in &lt;clinit&gt;.
4279       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
4280       // child class can become fully initialized while its parent class is still being initialized.
<span class="line-modified">4281       if (accessing_method-&gt;is_class_initializer()) {</span>
4282         return false;
4283       }
4284     }
4285     ciMethod* root = method(); // the root method of compilation
4286     if (root != accessing_method) {
4287       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
4288     }
4289   }
4290   return true;
4291 }
4292 
4293 #ifndef PRODUCT
4294 //------------------------------verify_graph_edges---------------------------
4295 // Walk the Graph and verify that there is a one-to-one correspondence
4296 // between Use-Def edges and Def-Use edges in the graph.
4297 void Compile::verify_graph_edges(bool no_dead_code) {
4298   if (VerifyGraphEdges) {
4299     Unique_Node_List visited;
4300     // Call recursive graph walk to check edges
4301     _root-&gt;verify_edges(visited);
</pre>
<hr />
<pre>
4382                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
4383   }
4384 
4385   if (VerifyIdealNodeCount) {
4386     Compile::current()-&gt;print_missing_nodes();
4387   }
4388 #endif
4389 
4390   if (_log != NULL) {
4391     _log-&gt;done(&quot;phase name=&#39;%s&#39; nodes=&#39;%d&#39; live=&#39;%d&#39;&quot;, _phase_name, C-&gt;unique(), C-&gt;live_nodes());
4392   }
4393 }
4394 
4395 //----------------------------static_subtype_check-----------------------------
4396 // Shortcut important common cases when superklass is exact:
4397 // (0) superklass is java.lang.Object (can occur in reflective code)
4398 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
4399 // (2) subklass does not overlap with superklass =&gt; always fail
4400 // (3) superklass has NO subtypes and we can check with a simple compare.
4401 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
<span class="line-modified">4402   if (StressReflectiveCode || superk == NULL || subk == NULL) {</span>
4403     return SSC_full_test;       // Let caller generate the general case.
4404   }
4405 
4406   if (superk == env()-&gt;Object_klass()) {
4407     return SSC_always_true;     // (0) this test cannot fail
4408   }
4409 
4410   ciType* superelem = superk;
<span class="line-modified">4411   if (superelem-&gt;is_array_klass()) {</span>
<span class="line-added">4412     ciArrayKlass* ak = superelem-&gt;as_array_klass();</span>
4413     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
<span class="line-added">4414   }</span>
4415 
4416   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
4417     if (subk-&gt;is_subtype_of(superk)) {
4418       return SSC_always_true;   // (1) false path dead; no dynamic test needed
4419     }
4420     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
4421         !superk-&gt;is_subtype_of(subk)) {
4422       return SSC_always_false;
4423     }
4424   }
4425 
4426   // If casting to an instance klass, it must have no subtypes
4427   if (superk-&gt;is_interface()) {
4428     // Cannot trust interfaces yet.
4429     // %%% S.B. superk-&gt;nof_implementors() == 1
4430   } else if (superelem-&gt;is_instance_klass()) {
4431     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4432     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4433       if (!ik-&gt;is_final()) {
4434         // Add a dependency if there is a chance of a later subclass.
</pre>
<hr />
<pre>
4855     for (uint next = 0; next &lt; worklist.size(); ++next) {
4856       Node *n  = worklist.at(next);
4857       const Type* t = igvn.type_or_null(n);
4858       assert((t == NULL) || (t == t-&gt;remove_speculative()), &quot;no more speculative types&quot;);
4859       if (n-&gt;is_Type()) {
4860         t = n-&gt;as_Type()-&gt;type();
4861         assert(t == t-&gt;remove_speculative(), &quot;no more speculative types&quot;);
4862       }
4863       uint max = n-&gt;len();
4864       for( uint i = 0; i &lt; max; ++i ) {
4865         Node *m = n-&gt;in(i);
4866         if (not_a_node(m))  continue;
4867         worklist.push(m);
4868       }
4869     }
4870     igvn.check_no_speculative_types();
4871 #endif
4872   }
4873 }
4874 
<span class="line-added">4875 Node* Compile::optimize_acmp(PhaseGVN* phase, Node* a, Node* b) {</span>
<span class="line-added">4876   const TypeInstPtr* ta = phase-&gt;type(a)-&gt;isa_instptr();</span>
<span class="line-added">4877   const TypeInstPtr* tb = phase-&gt;type(b)-&gt;isa_instptr();</span>
<span class="line-added">4878   if (!EnableValhalla || ta == NULL || tb == NULL ||</span>
<span class="line-added">4879       ta-&gt;is_zero_type() || tb-&gt;is_zero_type() ||</span>
<span class="line-added">4880       !ta-&gt;can_be_inline_type() || !tb-&gt;can_be_inline_type()) {</span>
<span class="line-added">4881     // Use old acmp if one operand is null or not an inline type</span>
<span class="line-added">4882     return new CmpPNode(a, b);</span>
<span class="line-added">4883   } else if (ta-&gt;is_inlinetypeptr() || tb-&gt;is_inlinetypeptr()) {</span>
<span class="line-added">4884     // We know that one operand is an inline type. Therefore,</span>
<span class="line-added">4885     // new acmp will only return true if both operands are NULL.</span>
<span class="line-added">4886     // Check if both operands are null by or&#39;ing the oops.</span>
<span class="line-added">4887     a = phase-&gt;transform(new CastP2XNode(NULL, a));</span>
<span class="line-added">4888     b = phase-&gt;transform(new CastP2XNode(NULL, b));</span>
<span class="line-added">4889     a = phase-&gt;transform(new OrXNode(a, b));</span>
<span class="line-added">4890     return new CmpXNode(a, phase-&gt;MakeConX(0));</span>
<span class="line-added">4891   }</span>
<span class="line-added">4892   // Use new acmp</span>
<span class="line-added">4893   return NULL;</span>
<span class="line-added">4894 }</span>
<span class="line-added">4895 </span>
4896 // Auxiliary method to support randomized stressing/fuzzing.
4897 //
4898 // This method can be called the arbitrary number of times, with current count
4899 // as the argument. The logic allows selecting a single candidate from the
4900 // running list of candidates as follows:
4901 //    int count = 0;
4902 //    Cand* selected = null;
4903 //    while(cand = cand-&gt;next()) {
4904 //      if (randomized_select(++count)) {
4905 //        selected = cand;
4906 //      }
4907 //    }
4908 //
4909 // Including count equalizes the chances any candidate is &quot;selected&quot;.
4910 // This is useful when we don&#39;t have the complete list of candidates to choose
4911 // from uniformly. In this case, we need to adjust the randomicity of the
4912 // selection, or else we will end up biasing the selection towards the latter
4913 // candidates.
4914 //
4915 // Quick back-envelope calculation shows that for the list of n candidates
</pre>
</td>
</tr>
</table>
<center><a href="chaitin.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/phaseX.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="node.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stringopts.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/phaseX.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1175     }
1176   }
1177   NOT_PRODUCT(verify_PhaseIterGVN();)
1178 }
1179 
1180 
1181 /**
1182  * Register a new node with the optimizer.  Update the types array, the def-use
1183  * info.  Put on worklist.
1184  */
1185 Node* PhaseIterGVN::register_new_node_with_optimizer(Node* n, Node* orig) {
1186   set_type_bottom(n);
1187   _worklist.push(n);
1188   if (orig != NULL)  C-&gt;copy_node_notes_to(n, orig);
1189   return n;
1190 }
1191 
1192 //------------------------------transform--------------------------------------
1193 // Non-recursive: idealize Node &#39;n&#39; with respect to its inputs and its value
1194 Node *PhaseIterGVN::transform( Node *n ) {
<span class="line-removed">1195   if (_delay_transform) {</span>
<span class="line-removed">1196     // Register the node but don&#39;t optimize for now</span>
<span class="line-removed">1197     register_new_node_with_optimizer(n);</span>
<span class="line-removed">1198     return n;</span>
<span class="line-removed">1199   }</span>
<span class="line-removed">1200 </span>
1201   // If brand new node, make space in type array, and give it a type.
1202   ensure_type_or_null(n);
1203   if (type_or_null(n) == NULL) {
1204     set_type_bottom(n);
1205   }
1206 






1207   return transform_old(n);
1208 }
1209 
1210 Node *PhaseIterGVN::transform_old(Node* n) {
1211   DEBUG_ONLY(uint loop_count = 0;);
1212   NOT_PRODUCT(set_transforms());
1213 
1214   // Remove &#39;n&#39; from hash table in case it gets modified
1215   _table.hash_delete(n);
1216   if (VerifyIterativeGVN) {
1217    assert(!_table.find_index(n-&gt;_idx), &quot;found duplicate entry in table&quot;);
1218   }
1219 
1220   // Apply the Ideal call in a loop until it no longer applies
1221   Node* k = n;
1222   DEBUG_ONLY(dead_loop_check(k);)
1223   DEBUG_ONLY(bool is_new = (k-&gt;outcnt() == 0);)
1224   C-&gt;remove_modified_node(k);
1225   Node* i = apply_ideal(k, /*can_reshape=*/true);
1226   assert(i != k || is_new || i-&gt;outcnt() &gt; 0, &quot;don&#39;t return dead nodes&quot;);
</pre>
<hr />
<pre>
1407       // root is usually dead. However, sometimes reshaping walk makes
1408       // it reachable by adding use edges. So, we will NOT count Con nodes
1409       // as dead to be conservative about the dead node count at any
1410       // given time.
1411       if (!dead-&gt;is_Con()) {
1412         C-&gt;record_dead_node(dead-&gt;_idx);
1413       }
1414       if (dead-&gt;is_macro()) {
1415         C-&gt;remove_macro_node(dead);
1416       }
1417       if (dead-&gt;is_expensive()) {
1418         C-&gt;remove_expensive_node(dead);
1419       }
1420       CastIINode* cast = dead-&gt;isa_CastII();
1421       if (cast != NULL &amp;&amp; cast-&gt;has_range_check()) {
1422         C-&gt;remove_range_check_cast(cast);
1423       }
1424       if (dead-&gt;Opcode() == Op_Opaque4) {
1425         C-&gt;remove_opaque4_node(dead);
1426       }



1427       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1428       bs-&gt;unregister_potential_barrier_node(dead);
1429     }
1430   } // while (_stack.is_nonempty())
1431 }
1432 
1433 //------------------------------subsume_node-----------------------------------
1434 // Remove users from node &#39;old&#39; and add them to node &#39;nn&#39;.
1435 void PhaseIterGVN::subsume_node( Node *old, Node *nn ) {
1436   if (old-&gt;Opcode() == Op_SafePoint) {
1437     old-&gt;as_SafePoint()-&gt;disconnect_from_root(this);
1438   }
1439   assert( old != hash_find(old), &quot;should already been removed&quot; );
1440   assert( old != C-&gt;top(), &quot;cannot subsume top node&quot;);
1441   // Copy debug or profile information to the new version:
1442   C-&gt;copy_node_notes_to(nn, old);
1443   // Move users of node &#39;old&#39; to node &#39;nn&#39;
1444   for (DUIterator_Last imin, i = old-&gt;last_outs(imin); i &gt;= imin; ) {
1445     Node* use = old-&gt;last_out(i);  // for each use...
1446     // use might need re-hashing (but it won&#39;t if it&#39;s a new node)
</pre>
<hr />
<pre>
1471     }
1472   }
1473 
1474   // Smash all inputs to &#39;old&#39;, isolating him completely
1475   Node *temp = new Node(1);
1476   temp-&gt;init_req(0,nn);     // Add a use to nn to prevent him from dying
1477   remove_dead_node( old );
1478   temp-&gt;del_req(0);         // Yank bogus edge
1479 #ifndef PRODUCT
1480   if( VerifyIterativeGVN ) {
1481     for ( int i = 0; i &lt; _verify_window_size; i++ ) {
1482       if ( _verify_window[i] == old )
1483         _verify_window[i] = nn;
1484     }
1485   }
1486 #endif
1487   _worklist.remove(temp);   // this can be necessary
1488   temp-&gt;destruct();         // reuse the _idx of this little guy
1489 }
1490 













1491 //------------------------------add_users_to_worklist--------------------------
1492 void PhaseIterGVN::add_users_to_worklist0( Node *n ) {
1493   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
1494     _worklist.push(n-&gt;fast_out(i));  // Push on worklist
1495   }
1496 }
1497 
1498 // Return counted loop Phi if as a counted loop exit condition, cmp
1499 // compares the the induction variable with n
1500 static PhiNode* countedloop_phi_from_cmp(CmpINode* cmp, Node* n) {
1501   for (DUIterator_Fast imax, i = cmp-&gt;fast_outs(imax); i &lt; imax; i++) {
1502     Node* bol = cmp-&gt;fast_out(i);
1503     for (DUIterator_Fast i2max, i2 = bol-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1504       Node* iff = bol-&gt;fast_out(i2);
1505       if (iff-&gt;is_CountedLoopEnd()) {
1506         CountedLoopEndNode* cle = iff-&gt;as_CountedLoopEnd();
1507         if (cle-&gt;limit() == n) {
1508           PhiNode* phi = cle-&gt;phi();
1509           if (phi != NULL) {
1510             return phi;
</pre>
<hr />
<pre>
1571         }
1572         Node* in1 = use-&gt;in(1);
1573         for (uint i = 0; i &lt; in1-&gt;outcnt(); i++) {
1574           if (in1-&gt;raw_out(i)-&gt;Opcode() == Op_CastII) {
1575             Node* castii = in1-&gt;raw_out(i);
1576             if (castii-&gt;in(0) != NULL &amp;&amp; castii-&gt;in(0)-&gt;in(0) != NULL &amp;&amp; castii-&gt;in(0)-&gt;in(0)-&gt;is_If()) {
1577               Node* ifnode = castii-&gt;in(0)-&gt;in(0);
1578               if (ifnode-&gt;in(1) != NULL &amp;&amp; ifnode-&gt;in(1)-&gt;is_Bool() &amp;&amp; ifnode-&gt;in(1)-&gt;in(1) == use) {
1579                 // Reprocess a CastII node that may depend on an
1580                 // opaque node value when the opaque node is
1581                 // removed. In case it carries a dependency we can do
1582                 // a better job of computing its type.
1583                 _worklist.push(castii);
1584               }
1585             }
1586           }
1587         }
1588       }
1589     }
1590 









1591     // If changed Cast input, check Phi users for simple cycles
1592     if (use-&gt;is_ConstraintCast()) {
1593       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1594         Node* u = use-&gt;fast_out(i2);
1595         if (u-&gt;is_Phi())
1596           _worklist.push(u);
1597       }
1598     }
1599     // If changed LShift inputs, check RShift users for useless sign-ext
1600     if( use_op == Op_LShiftI ) {
1601       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1602         Node* u = use-&gt;fast_out(i2);
1603         if (u-&gt;Opcode() == Op_RShiftI)
1604           _worklist.push(u);
1605       }
1606     }
1607     // If changed AddI/SubI inputs, check CmpU for range check optimization.
1608     if (use_op == Op_AddI || use_op == Op_SubI) {
1609       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1610         Node* u = use-&gt;fast_out(i2);
</pre>
<hr />
<pre>
1616     // If changed AddP inputs, check Stores for loop invariant
1617     if( use_op == Op_AddP ) {
1618       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1619         Node* u = use-&gt;fast_out(i2);
1620         if (u-&gt;is_Mem())
1621           _worklist.push(u);
1622       }
1623     }
1624     // If changed initialization activity, check dependent Stores
1625     if (use_op == Op_Allocate || use_op == Op_AllocateArray) {
1626       InitializeNode* init = use-&gt;as_Allocate()-&gt;initialization();
1627       if (init != NULL) {
1628         Node* imem = init-&gt;proj_out_or_null(TypeFunc::Memory);
1629         if (imem != NULL)  add_users_to_worklist0(imem);
1630       }
1631     }
1632     if (use_op == Op_Initialize) {
1633       Node* imem = use-&gt;as_Initialize()-&gt;proj_out_or_null(TypeFunc::Memory);
1634       if (imem != NULL)  add_users_to_worklist0(imem);
1635     }








1636     // Loading the java mirror from a Klass requires two loads and the type
1637     // of the mirror load depends on the type of &#39;n&#39;. See LoadNode::Value().
1638     //   LoadBarrier?(LoadP(LoadP(AddP(foo:Klass, #java_mirror))))
1639     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1640     bool has_load_barrier_nodes = bs-&gt;has_load_barrier_nodes();
1641 
1642     if (use_op == Op_LoadP &amp;&amp; use-&gt;bottom_type()-&gt;isa_rawptr()) {
1643       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1644         Node* u = use-&gt;fast_out(i2);
1645         const Type* ut = u-&gt;bottom_type();
1646         if (u-&gt;Opcode() == Op_LoadP &amp;&amp; ut-&gt;isa_instptr()) {
1647           if (has_load_barrier_nodes) {
1648             // Search for load barriers behind the load
1649             for (DUIterator_Fast i3max, i3 = u-&gt;fast_outs(i3max); i3 &lt; i3max; i3++) {
1650               Node* b = u-&gt;fast_out(i3);
1651               if (bs-&gt;is_gc_barrier_node(b)) {
1652                 _worklist.push(b);
1653               }
1654             }
1655           }
1656           _worklist.push(u);
1657         }
1658       }
1659     }











1660   }
1661 }
1662 
1663 /**
1664  * Remove the speculative part of all types that we know of
1665  */
1666 void PhaseIterGVN::remove_speculative_types()  {
1667   assert(UseTypeSpeculation, &quot;speculation is off&quot;);
1668   for (uint i = 0; i &lt; _types.Size(); i++)  {
1669     const Type* t = _types.fast_lookup(i);
1670     if (t != NULL) {
1671       _types.map(i, t-&gt;remove_speculative());
1672     }
1673   }
1674   _table.check_no_speculative_types();
1675 }
1676 
1677 //=============================================================================
1678 #ifndef PRODUCT
1679 uint PhaseCCP::_total_invokes   = 0;
</pre>
<hr />
<pre>
1778         if (m_op == Op_AddI || m_op == Op_SubI) {
1779           for (DUIterator_Fast i2max, i2 = m-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1780             Node* p = m-&gt;fast_out(i2); // Propagate changes to uses
1781             if (p-&gt;Opcode() == Op_CmpU) {
1782               // Got a CmpU which might need the new type information from node n.
1783               if(p-&gt;bottom_type() != type(p)) { // If not already bottomed out
1784                 worklist.push(p); // Propagate change to user
1785               }
1786             }
1787           }
1788         }
1789         // If n is used in a counted loop exit condition then the type
1790         // of the counted loop&#39;s Phi depends on the type of n. See
1791         // PhiNode::Value().
1792         if (m_op == Op_CmpI) {
1793           PhiNode* phi = countedloop_phi_from_cmp((CmpINode*)m, n);
1794           if (phi != NULL) {
1795             worklist.push(phi);
1796           }
1797         }








1798         // Loading the java mirror from a Klass requires two loads and the type
1799         // of the mirror load depends on the type of &#39;n&#39;. See LoadNode::Value().
1800         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1801         bool has_load_barrier_nodes = bs-&gt;has_load_barrier_nodes();
1802 
1803         if (m_op == Op_LoadP &amp;&amp; m-&gt;bottom_type()-&gt;isa_rawptr()) {
1804           for (DUIterator_Fast i2max, i2 = m-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1805             Node* u = m-&gt;fast_out(i2);
1806             const Type* ut = u-&gt;bottom_type();
1807             if (u-&gt;Opcode() == Op_LoadP &amp;&amp; ut-&gt;isa_instptr() &amp;&amp; ut != type(u)) {
1808               if (has_load_barrier_nodes) {
1809                 // Search for load barriers behind the load
1810                 for (DUIterator_Fast i3max, i3 = u-&gt;fast_outs(i3max); i3 &lt; i3max; i3++) {
1811                   Node* b = u-&gt;fast_out(i3);
1812                   if (bs-&gt;is_gc_barrier_node(b)) {
1813                     worklist.push(b);
1814                   }
1815                 }
1816               }
1817               worklist.push(u);
</pre>
</td>
<td>
<hr />
<pre>
1175     }
1176   }
1177   NOT_PRODUCT(verify_PhaseIterGVN();)
1178 }
1179 
1180 
1181 /**
1182  * Register a new node with the optimizer.  Update the types array, the def-use
1183  * info.  Put on worklist.
1184  */
1185 Node* PhaseIterGVN::register_new_node_with_optimizer(Node* n, Node* orig) {
1186   set_type_bottom(n);
1187   _worklist.push(n);
1188   if (orig != NULL)  C-&gt;copy_node_notes_to(n, orig);
1189   return n;
1190 }
1191 
1192 //------------------------------transform--------------------------------------
1193 // Non-recursive: idealize Node &#39;n&#39; with respect to its inputs and its value
1194 Node *PhaseIterGVN::transform( Node *n ) {






1195   // If brand new node, make space in type array, and give it a type.
1196   ensure_type_or_null(n);
1197   if (type_or_null(n) == NULL) {
1198     set_type_bottom(n);
1199   }
1200 
<span class="line-added">1201   if (_delay_transform) {</span>
<span class="line-added">1202     // Add the node to the worklist but don&#39;t optimize for now</span>
<span class="line-added">1203     _worklist.push(n);</span>
<span class="line-added">1204     return n;</span>
<span class="line-added">1205   }</span>
<span class="line-added">1206 </span>
1207   return transform_old(n);
1208 }
1209 
1210 Node *PhaseIterGVN::transform_old(Node* n) {
1211   DEBUG_ONLY(uint loop_count = 0;);
1212   NOT_PRODUCT(set_transforms());
1213 
1214   // Remove &#39;n&#39; from hash table in case it gets modified
1215   _table.hash_delete(n);
1216   if (VerifyIterativeGVN) {
1217    assert(!_table.find_index(n-&gt;_idx), &quot;found duplicate entry in table&quot;);
1218   }
1219 
1220   // Apply the Ideal call in a loop until it no longer applies
1221   Node* k = n;
1222   DEBUG_ONLY(dead_loop_check(k);)
1223   DEBUG_ONLY(bool is_new = (k-&gt;outcnt() == 0);)
1224   C-&gt;remove_modified_node(k);
1225   Node* i = apply_ideal(k, /*can_reshape=*/true);
1226   assert(i != k || is_new || i-&gt;outcnt() &gt; 0, &quot;don&#39;t return dead nodes&quot;);
</pre>
<hr />
<pre>
1407       // root is usually dead. However, sometimes reshaping walk makes
1408       // it reachable by adding use edges. So, we will NOT count Con nodes
1409       // as dead to be conservative about the dead node count at any
1410       // given time.
1411       if (!dead-&gt;is_Con()) {
1412         C-&gt;record_dead_node(dead-&gt;_idx);
1413       }
1414       if (dead-&gt;is_macro()) {
1415         C-&gt;remove_macro_node(dead);
1416       }
1417       if (dead-&gt;is_expensive()) {
1418         C-&gt;remove_expensive_node(dead);
1419       }
1420       CastIINode* cast = dead-&gt;isa_CastII();
1421       if (cast != NULL &amp;&amp; cast-&gt;has_range_check()) {
1422         C-&gt;remove_range_check_cast(cast);
1423       }
1424       if (dead-&gt;Opcode() == Op_Opaque4) {
1425         C-&gt;remove_opaque4_node(dead);
1426       }
<span class="line-added">1427       if (dead-&gt;is_InlineTypeBase()) {</span>
<span class="line-added">1428         C-&gt;remove_inline_type(dead);</span>
<span class="line-added">1429       }</span>
1430       BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1431       bs-&gt;unregister_potential_barrier_node(dead);
1432     }
1433   } // while (_stack.is_nonempty())
1434 }
1435 
1436 //------------------------------subsume_node-----------------------------------
1437 // Remove users from node &#39;old&#39; and add them to node &#39;nn&#39;.
1438 void PhaseIterGVN::subsume_node( Node *old, Node *nn ) {
1439   if (old-&gt;Opcode() == Op_SafePoint) {
1440     old-&gt;as_SafePoint()-&gt;disconnect_from_root(this);
1441   }
1442   assert( old != hash_find(old), &quot;should already been removed&quot; );
1443   assert( old != C-&gt;top(), &quot;cannot subsume top node&quot;);
1444   // Copy debug or profile information to the new version:
1445   C-&gt;copy_node_notes_to(nn, old);
1446   // Move users of node &#39;old&#39; to node &#39;nn&#39;
1447   for (DUIterator_Last imin, i = old-&gt;last_outs(imin); i &gt;= imin; ) {
1448     Node* use = old-&gt;last_out(i);  // for each use...
1449     // use might need re-hashing (but it won&#39;t if it&#39;s a new node)
</pre>
<hr />
<pre>
1474     }
1475   }
1476 
1477   // Smash all inputs to &#39;old&#39;, isolating him completely
1478   Node *temp = new Node(1);
1479   temp-&gt;init_req(0,nn);     // Add a use to nn to prevent him from dying
1480   remove_dead_node( old );
1481   temp-&gt;del_req(0);         // Yank bogus edge
1482 #ifndef PRODUCT
1483   if( VerifyIterativeGVN ) {
1484     for ( int i = 0; i &lt; _verify_window_size; i++ ) {
1485       if ( _verify_window[i] == old )
1486         _verify_window[i] = nn;
1487     }
1488   }
1489 #endif
1490   _worklist.remove(temp);   // this can be necessary
1491   temp-&gt;destruct();         // reuse the _idx of this little guy
1492 }
1493 
<span class="line-added">1494 void PhaseIterGVN::replace_in_uses(Node* n, Node* m) {</span>
<span class="line-added">1495   assert(n != NULL, &quot;sanity&quot;);</span>
<span class="line-added">1496   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1497     Node* u = n-&gt;fast_out(i);</span>
<span class="line-added">1498     if (u != n) {</span>
<span class="line-added">1499       rehash_node_delayed(u);</span>
<span class="line-added">1500       int nb = u-&gt;replace_edge(n, m);</span>
<span class="line-added">1501       --i, imax -= nb;</span>
<span class="line-added">1502     }</span>
<span class="line-added">1503   }</span>
<span class="line-added">1504   assert(n-&gt;outcnt() == 0, &quot;all uses must be deleted&quot;);</span>
<span class="line-added">1505 }</span>
<span class="line-added">1506 </span>
1507 //------------------------------add_users_to_worklist--------------------------
1508 void PhaseIterGVN::add_users_to_worklist0( Node *n ) {
1509   for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {
1510     _worklist.push(n-&gt;fast_out(i));  // Push on worklist
1511   }
1512 }
1513 
1514 // Return counted loop Phi if as a counted loop exit condition, cmp
1515 // compares the the induction variable with n
1516 static PhiNode* countedloop_phi_from_cmp(CmpINode* cmp, Node* n) {
1517   for (DUIterator_Fast imax, i = cmp-&gt;fast_outs(imax); i &lt; imax; i++) {
1518     Node* bol = cmp-&gt;fast_out(i);
1519     for (DUIterator_Fast i2max, i2 = bol-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1520       Node* iff = bol-&gt;fast_out(i2);
1521       if (iff-&gt;is_CountedLoopEnd()) {
1522         CountedLoopEndNode* cle = iff-&gt;as_CountedLoopEnd();
1523         if (cle-&gt;limit() == n) {
1524           PhiNode* phi = cle-&gt;phi();
1525           if (phi != NULL) {
1526             return phi;
</pre>
<hr />
<pre>
1587         }
1588         Node* in1 = use-&gt;in(1);
1589         for (uint i = 0; i &lt; in1-&gt;outcnt(); i++) {
1590           if (in1-&gt;raw_out(i)-&gt;Opcode() == Op_CastII) {
1591             Node* castii = in1-&gt;raw_out(i);
1592             if (castii-&gt;in(0) != NULL &amp;&amp; castii-&gt;in(0)-&gt;in(0) != NULL &amp;&amp; castii-&gt;in(0)-&gt;in(0)-&gt;is_If()) {
1593               Node* ifnode = castii-&gt;in(0)-&gt;in(0);
1594               if (ifnode-&gt;in(1) != NULL &amp;&amp; ifnode-&gt;in(1)-&gt;is_Bool() &amp;&amp; ifnode-&gt;in(1)-&gt;in(1) == use) {
1595                 // Reprocess a CastII node that may depend on an
1596                 // opaque node value when the opaque node is
1597                 // removed. In case it carries a dependency we can do
1598                 // a better job of computing its type.
1599                 _worklist.push(castii);
1600               }
1601             }
1602           }
1603         }
1604       }
1605     }
1606 
<span class="line-added">1607     // Inline type nodes can have other inline types as users. If an input gets</span>
<span class="line-added">1608     // updated, make sure that inline type users get a chance for optimization.</span>
<span class="line-added">1609     if (use-&gt;is_InlineTypeBase()) {</span>
<span class="line-added">1610       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {</span>
<span class="line-added">1611         Node* u = use-&gt;fast_out(i2);</span>
<span class="line-added">1612         if (u-&gt;is_InlineTypeBase())</span>
<span class="line-added">1613           _worklist.push(u);</span>
<span class="line-added">1614       }</span>
<span class="line-added">1615     }</span>
1616     // If changed Cast input, check Phi users for simple cycles
1617     if (use-&gt;is_ConstraintCast()) {
1618       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1619         Node* u = use-&gt;fast_out(i2);
1620         if (u-&gt;is_Phi())
1621           _worklist.push(u);
1622       }
1623     }
1624     // If changed LShift inputs, check RShift users for useless sign-ext
1625     if( use_op == Op_LShiftI ) {
1626       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1627         Node* u = use-&gt;fast_out(i2);
1628         if (u-&gt;Opcode() == Op_RShiftI)
1629           _worklist.push(u);
1630       }
1631     }
1632     // If changed AddI/SubI inputs, check CmpU for range check optimization.
1633     if (use_op == Op_AddI || use_op == Op_SubI) {
1634       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1635         Node* u = use-&gt;fast_out(i2);
</pre>
<hr />
<pre>
1641     // If changed AddP inputs, check Stores for loop invariant
1642     if( use_op == Op_AddP ) {
1643       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1644         Node* u = use-&gt;fast_out(i2);
1645         if (u-&gt;is_Mem())
1646           _worklist.push(u);
1647       }
1648     }
1649     // If changed initialization activity, check dependent Stores
1650     if (use_op == Op_Allocate || use_op == Op_AllocateArray) {
1651       InitializeNode* init = use-&gt;as_Allocate()-&gt;initialization();
1652       if (init != NULL) {
1653         Node* imem = init-&gt;proj_out_or_null(TypeFunc::Memory);
1654         if (imem != NULL)  add_users_to_worklist0(imem);
1655       }
1656     }
1657     if (use_op == Op_Initialize) {
1658       Node* imem = use-&gt;as_Initialize()-&gt;proj_out_or_null(TypeFunc::Memory);
1659       if (imem != NULL)  add_users_to_worklist0(imem);
1660     }
<span class="line-added">1661     if (use_op == Op_CastP2X) {</span>
<span class="line-added">1662       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {</span>
<span class="line-added">1663         Node* u = use-&gt;fast_out(i2);</span>
<span class="line-added">1664         if (u-&gt;Opcode() == Op_AndX) {</span>
<span class="line-added">1665           _worklist.push(u);</span>
<span class="line-added">1666         }</span>
<span class="line-added">1667       }</span>
<span class="line-added">1668     }</span>
1669     // Loading the java mirror from a Klass requires two loads and the type
1670     // of the mirror load depends on the type of &#39;n&#39;. See LoadNode::Value().
1671     //   LoadBarrier?(LoadP(LoadP(AddP(foo:Klass, #java_mirror))))
1672     BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1673     bool has_load_barrier_nodes = bs-&gt;has_load_barrier_nodes();
1674 
1675     if (use_op == Op_LoadP &amp;&amp; use-&gt;bottom_type()-&gt;isa_rawptr()) {
1676       for (DUIterator_Fast i2max, i2 = use-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1677         Node* u = use-&gt;fast_out(i2);
1678         const Type* ut = u-&gt;bottom_type();
1679         if (u-&gt;Opcode() == Op_LoadP &amp;&amp; ut-&gt;isa_instptr()) {
1680           if (has_load_barrier_nodes) {
1681             // Search for load barriers behind the load
1682             for (DUIterator_Fast i3max, i3 = u-&gt;fast_outs(i3max); i3 &lt; i3max; i3++) {
1683               Node* b = u-&gt;fast_out(i3);
1684               if (bs-&gt;is_gc_barrier_node(b)) {
1685                 _worklist.push(b);
1686               }
1687             }
1688           }
1689           _worklist.push(u);
1690         }
1691       }
1692     }
<span class="line-added">1693 </span>
<span class="line-added">1694     // Give CallStaticJavaNode::remove_useless_allocation a chance to run</span>
<span class="line-added">1695     if (use-&gt;is_Region()) {</span>
<span class="line-added">1696       Node* c = use;</span>
<span class="line-added">1697       do {</span>
<span class="line-added">1698         c = c-&gt;unique_ctrl_out();</span>
<span class="line-added">1699       } while (c != NULL &amp;&amp; c-&gt;is_Region());</span>
<span class="line-added">1700       if (c != NULL &amp;&amp; c-&gt;is_CallStaticJava() &amp;&amp; c-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() != 0) {</span>
<span class="line-added">1701         _worklist.push(c);</span>
<span class="line-added">1702       }</span>
<span class="line-added">1703     }</span>
1704   }
1705 }
1706 
1707 /**
1708  * Remove the speculative part of all types that we know of
1709  */
1710 void PhaseIterGVN::remove_speculative_types()  {
1711   assert(UseTypeSpeculation, &quot;speculation is off&quot;);
1712   for (uint i = 0; i &lt; _types.Size(); i++)  {
1713     const Type* t = _types.fast_lookup(i);
1714     if (t != NULL) {
1715       _types.map(i, t-&gt;remove_speculative());
1716     }
1717   }
1718   _table.check_no_speculative_types();
1719 }
1720 
1721 //=============================================================================
1722 #ifndef PRODUCT
1723 uint PhaseCCP::_total_invokes   = 0;
</pre>
<hr />
<pre>
1822         if (m_op == Op_AddI || m_op == Op_SubI) {
1823           for (DUIterator_Fast i2max, i2 = m-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1824             Node* p = m-&gt;fast_out(i2); // Propagate changes to uses
1825             if (p-&gt;Opcode() == Op_CmpU) {
1826               // Got a CmpU which might need the new type information from node n.
1827               if(p-&gt;bottom_type() != type(p)) { // If not already bottomed out
1828                 worklist.push(p); // Propagate change to user
1829               }
1830             }
1831           }
1832         }
1833         // If n is used in a counted loop exit condition then the type
1834         // of the counted loop&#39;s Phi depends on the type of n. See
1835         // PhiNode::Value().
1836         if (m_op == Op_CmpI) {
1837           PhiNode* phi = countedloop_phi_from_cmp((CmpINode*)m, n);
1838           if (phi != NULL) {
1839             worklist.push(phi);
1840           }
1841         }
<span class="line-added">1842         if (m_op == Op_CastP2X) {</span>
<span class="line-added">1843           for (DUIterator_Fast i2max, i2 = m-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {</span>
<span class="line-added">1844             Node* u = m-&gt;fast_out(i2);</span>
<span class="line-added">1845             if (u-&gt;Opcode() == Op_AndX) {</span>
<span class="line-added">1846               worklist.push(u);</span>
<span class="line-added">1847             }</span>
<span class="line-added">1848           }</span>
<span class="line-added">1849         }</span>
1850         // Loading the java mirror from a Klass requires two loads and the type
1851         // of the mirror load depends on the type of &#39;n&#39;. See LoadNode::Value().
1852         BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
1853         bool has_load_barrier_nodes = bs-&gt;has_load_barrier_nodes();
1854 
1855         if (m_op == Op_LoadP &amp;&amp; m-&gt;bottom_type()-&gt;isa_rawptr()) {
1856           for (DUIterator_Fast i2max, i2 = m-&gt;fast_outs(i2max); i2 &lt; i2max; i2++) {
1857             Node* u = m-&gt;fast_out(i2);
1858             const Type* ut = u-&gt;bottom_type();
1859             if (u-&gt;Opcode() == Op_LoadP &amp;&amp; ut-&gt;isa_instptr() &amp;&amp; ut != type(u)) {
1860               if (has_load_barrier_nodes) {
1861                 // Search for load barriers behind the load
1862                 for (DUIterator_Fast i3max, i3 = u-&gt;fast_outs(i3max); i3 &lt; i3max; i3++) {
1863                   Node* b = u-&gt;fast_out(i3);
1864                   if (bs-&gt;is_gc_barrier_node(b)) {
1865                     worklist.push(b);
1866                   }
1867                 }
1868               }
1869               worklist.push(u);
</pre>
</td>
</tr>
</table>
<center><a href="node.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stringopts.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -646,5 +646,7 @@
 93813843680bbe1b7efbca56c03fd137f20a2c31 jdk-16+0
 93813843680bbe1b7efbca56c03fd137f20a2c31 jdk-15+27
 4a485c89d5a08b495961835f5308a96038678aeb jdk-16+1
 06c9f89459daba98395fad726100feb44f89ba71 jdk-15+28
 bcbe7b8a77b8971bc221c0be1bd2abb6fb68c2d0 jdk-16+2
+b58fc60580550a4a587cab729d8fd87223ad6932 jdk-15+29
+76810b3a88c8c641ae3850a8dfd7c40c984aea9d jdk-16+3
diff a/make/test/BuildMicrobenchmark.gmk b/make/test/BuildMicrobenchmark.gmk
--- a/make/test/BuildMicrobenchmark.gmk
+++ b/make/test/BuildMicrobenchmark.gmk
@@ -88,15 +88,15 @@
 # Build microbenchmark suite for the current JDK
 $(eval $(call SetupJavaCompilation, BUILD_JDK_MICROBENCHMARK, \
     TARGET_RELEASE := $(TARGET_RELEASE_NEWJDK_UPGRADED), \
     SMALL_JAVA := false, \
     CLASSPATH := $(MICROBENCHMARK_CLASSPATH), \
-    DISABLED_WARNINGS := processing rawtypes unchecked cast serial deprecation, \
-    JAVAC_FLAGS := -XDallowWithFieldOperator, \
+    DISABLED_WARNINGS := processing rawtypes unchecked cast serial preview deprecation, \
     SRC := $(MICROBENCHMARK_SRC), \
     BIN := $(MICROBENCHMARK_CLASSES), \
     JAVA_FLAGS := --add-modules jdk.unsupported --limit-modules java.management, \
+    JAVAC_FLAGS := -XDallowWithFieldOperator --enable-preview, \
 ))
 
 $(BUILD_JDK_MICROBENCHMARK): $(JMH_COMPILE_JARS)
 
 # Run Indify
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -2177,24 +2177,38 @@
   for (int reg = 0; reg <= 31; reg++) {
     if (1 & bitset)
       regs[count++] = reg;
     bitset >>= 1;
   }
-  regs[count++] = zr->encoding_nocheck();
-  count &= ~1;  // Only push an even number of regs
-
-  // Always pushing full 128 bit registers.
-  if (count) {
-    stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -count * wordSize * 2)));
+
+  if (count == 0) {
     words_pushed += 2;
   }
-  for (int i = 2; i < count; i += 2) {
+
+  if (count == 1) {
+    strq(as_FloatRegister(regs[0]), Address(pre(stack, -wordSize * 2)));
+    return 1;
+  }
+
+  bool odd = (count & 1) == 1;
+  int push_slots = count + (odd ? 1 : 0);
+
+  // Always pushing full 128 bit registers.
+  stpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(pre(stack, -push_slots * wordSize * 2)));
+  words_pushed += 2;
+
+  for (int i = 2; i + 1 < count; i += 2) {
     stpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
     words_pushed += 2;
   }
 
-  assert(words_pushed == count, "oops, pushed != count");
+  if (odd) {
+    strq(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize * 2));
+    words_pushed++;
+  }
+
+  assert(words_pushed == count, "oops, pushed(%d) != count(%d)", words_pushed, count);
   return count;
 }
 
 int MacroAssembler::pop_fp(unsigned int bitset, Register stack) {
   int words_pushed = 0;
@@ -2205,23 +2219,37 @@
   for (int reg = 0; reg <= 31; reg++) {
     if (1 & bitset)
       regs[count++] = reg;
     bitset >>= 1;
   }
-  regs[count++] = zr->encoding_nocheck();
-  count &= ~1;
-
-  for (int i = 2; i < count; i += 2) {
-    ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
+
+  if (count == 0) {
     words_pushed += 2;
   }
-  if (count) {
-    ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, count * wordSize * 2)));
+
+  if (count == 1) {
+    ldrq(as_FloatRegister(regs[0]), Address(post(stack, wordSize * 2)));
+    return 1;
+  }
+
+  bool odd = (count & 1) == 1;
+  int push_slots = count + (odd ? 1 : 0);
+
+  if (odd) {
+    ldrq(as_FloatRegister(regs[count - 1]), Address(stack, (count - 1) * wordSize * 2));
+    words_pushed++;
+  }
+
+  for (int i = 2; i + 1 < count; i += 2) {
+    ldpq(as_FloatRegister(regs[i]), as_FloatRegister(regs[i+1]), Address(stack, i * wordSize * 2));
     words_pushed += 2;
   }
 
-  assert(words_pushed == count, "oops, pushed != count");
+  ldpq(as_FloatRegister(regs[0]), as_FloatRegister(regs[1]), Address(post(stack, push_slots * wordSize * 2)));
+  words_pushed += 2;
+
+  assert(words_pushed == count, "oops, pushed(%d) != count(%d)", words_pushed, count);
 
   return count;
 }
 
 #ifdef ASSERT
diff a/src/hotspot/cpu/ppc/interp_masm_ppc_64.cpp b/src/hotspot/cpu/ppc/interp_masm_ppc_64.cpp
--- a/src/hotspot/cpu/ppc/interp_masm_ppc_64.cpp
+++ b/src/hotspot/cpu/ppc/interp_masm_ppc_64.cpp
@@ -1,8 +1,8 @@
 /*
  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2012, 2018 SAP SE. All rights reserved.
+ * Copyright (c) 2012, 2020 SAP SE. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -28,10 +28,11 @@
 #include "asm/macroAssembler.inline.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "gc/shared/barrierSetAssembler.hpp"
 #include "interp_masm_ppc.hpp"
 #include "interpreter/interpreterRuntime.hpp"
+#include "oops/methodData.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/safepointMechanism.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/powerOfTwo.hpp"
diff a/src/hotspot/cpu/s390/interp_masm_s390.cpp b/src/hotspot/cpu/s390/interp_masm_s390.cpp
--- a/src/hotspot/cpu/s390/interp_masm_s390.cpp
+++ b/src/hotspot/cpu/s390/interp_masm_s390.cpp
@@ -1,8 +1,8 @@
 /*
  * Copyright (c) 2016, 2020, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2016, 2019 SAP SE. All rights reserved.
+ * Copyright (c) 2016, 2020 SAP SE. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -32,10 +32,11 @@
 #include "interp_masm_s390.hpp"
 #include "interpreter/interpreter.hpp"
 #include "interpreter/interpreterRuntime.hpp"
 #include "oops/arrayOop.hpp"
 #include "oops/markWord.hpp"
+#include "oops/methodData.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "runtime/basicLock.hpp"
 #include "runtime/biasedLocking.hpp"
 #include "runtime/frame.inline.hpp"
diff a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
--- a/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
+++ b/src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp
@@ -3948,17 +3948,14 @@
 //------------------------------Montgomery multiplication------------------------
 //
 
 #ifndef _WINDOWS
 
-#define ASM_SUBTRACT
-
-#ifdef ASM_SUBTRACT
-// Subtract 0:b from carry:a.  Return carry.
-static unsigned long
-sub(unsigned long a[], unsigned long b[], unsigned long carry, long len) {
-  long i = 0, cnt = len;
+// Subtract 0:b from carry:a.  Return carry.
+static julong
+sub(julong a[], julong b[], julong carry, long len) {
+  long long i = 0, cnt = len;
   unsigned long tmp;
   asm volatile("clc; "
                "0: ; "
                "mov (%[b], %[i], 8), %[tmp]; "
                "sbb %[tmp], (%[a], %[i], 8); "
@@ -3968,28 +3965,10 @@
                : [i]"+r"(i), [cnt]"+r"(cnt), [tmp]"=&r"(tmp)
                : [a]"r"(a), [b]"r"(b), [carry]"r"(carry)
                : "memory");
   return tmp;
 }
-#else // ASM_SUBTRACT
-typedef int __attribute__((mode(TI))) int128;
-
-// Subtract 0:b from carry:a.  Return carry.
-static unsigned long
-sub(unsigned long a[], unsigned long b[], unsigned long carry, int len) {
-  int128 tmp = 0;
-  int i;
-  for (i = 0; i < len; i++) {
-    tmp += a[i];
-    tmp -= b[i];
-    a[i] = tmp;
-    tmp >>= 64;
-    assert(-1 <= tmp && tmp <= 0, "invariant");
-  }
-  return tmp + carry;
-}
-#endif // ! ASM_SUBTRACT
 
 // Multiply (unsigned) Long A by Long B, accumulating the double-
 // length result into the accumulator formed of T0, T1, and T2.
 #define MACC(A, B, T0, T1, T2)                                  \
 do {                                                            \
@@ -4008,21 +3987,63 @@
            "add %%rax, %2; adc %%rdx, %3; adc $0, %4"           \
            : "=&d"(hi), "=a"(lo), "+r"(T0), "+r"(T1), "+g"(T2)  \
            : "r"(A), "a"(B) : "cc");                            \
  } while(0)
 
+#else //_WINDOWS
+
+static julong
+sub(julong a[], julong b[], julong carry, long len) {
+  long i;
+  julong tmp;
+  unsigned char c = 1;
+  for (i = 0; i < len; i++) {
+    c = _addcarry_u64(c, a[i], ~b[i], &tmp);
+    a[i] = tmp;
+  }
+  c = _addcarry_u64(c, carry, ~0, &tmp);
+  return tmp;
+}
+
+// Multiply (unsigned) Long A by Long B, accumulating the double-
+// length result into the accumulator formed of T0, T1, and T2.
+#define MACC(A, B, T0, T1, T2)                          \
+do {                                                    \
+  julong hi, lo;                            \
+  lo = _umul128(A, B, &hi);                             \
+  unsigned char c = _addcarry_u64(0, lo, T0, &T0);      \
+  c = _addcarry_u64(c, hi, T1, &T1);                    \
+  _addcarry_u64(c, T2, 0, &T2);                         \
+ } while(0)
+
+// As above, but add twice the double-length result into the
+// accumulator.
+#define MACC2(A, B, T0, T1, T2)                         \
+do {                                                    \
+  julong hi, lo;                            \
+  lo = _umul128(A, B, &hi);                             \
+  unsigned char c = _addcarry_u64(0, lo, T0, &T0);      \
+  c = _addcarry_u64(c, hi, T1, &T1);                    \
+  _addcarry_u64(c, T2, 0, &T2);                         \
+  c = _addcarry_u64(0, lo, T0, &T0);                    \
+  c = _addcarry_u64(c, hi, T1, &T1);                    \
+  _addcarry_u64(c, T2, 0, &T2);                         \
+ } while(0)
+
+#endif //_WINDOWS
+
 // Fast Montgomery multiplication.  The derivation of the algorithm is
 // in  A Cryptographic Library for the Motorola DSP56000,
 // Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.
 
-static void __attribute__((noinline))
-montgomery_multiply(unsigned long a[], unsigned long b[], unsigned long n[],
-                    unsigned long m[], unsigned long inv, int len) {
-  unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
+static void NOINLINE
+montgomery_multiply(julong a[], julong b[], julong n[],
+                    julong m[], julong inv, int len) {
+  julong t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
   int i;
 
-  assert(inv * n[0] == -1UL, "broken inverse in Montgomery multiply");
+  assert(inv * n[0] == ULLONG_MAX, "broken inverse in Montgomery multiply");
 
   for (i = 0; i < len; i++) {
     int j;
     for (j = 0; j < i; j++) {
       MACC(a[j], b[i-j], t0, t1, t2);
@@ -4054,17 +4075,17 @@
 // Fast Montgomery squaring.  This uses asymptotically 25% fewer
 // multiplies so it should be up to 25% faster than Montgomery
 // multiplication.  However, its loop control is more complex and it
 // may actually run slower on some machines.
 
-static void __attribute__((noinline))
-montgomery_square(unsigned long a[], unsigned long n[],
-                  unsigned long m[], unsigned long inv, int len) {
-  unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
+static void NOINLINE
+montgomery_square(julong a[], julong n[],
+                  julong m[], julong inv, int len) {
+  julong t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
   int i;
 
-  assert(inv * n[0] == -1UL, "broken inverse in Montgomery multiply");
+  assert(inv * n[0] == ULLONG_MAX, "broken inverse in Montgomery square");
 
   for (i = 0; i < len; i++) {
     int j;
     int end = (i+1)/2;
     for (j = 0; j < end; j++) {
@@ -4106,17 +4127,17 @@
   while (t0)
     t0 = sub(m, n, t0, len);
 }
 
 // Swap words in a longword.
-static unsigned long swap(unsigned long x) {
+static julong swap(julong x) {
   return (x << 32) | (x >> 32);
 }
 
 // Copy len longwords from s to d, word-swapping as we go.  The
 // destination array is reversed.
-static void reverse_words(unsigned long *s, unsigned long *d, int len) {
+static void reverse_words(julong *s, julong *d, int len) {
   d += len;
   while(len-- > 0) {
     d--;
     *d = swap(*s);
     s++;
@@ -4134,28 +4155,28 @@
   int longwords = len/2;
 
   // Make very sure we don't use so much space that the stack might
   // overflow.  512 jints corresponds to an 16384-bit integer and
   // will use here a total of 8k bytes of stack space.
-  int total_allocation = longwords * sizeof (unsigned long) * 4;
+  int total_allocation = longwords * sizeof (julong) * 4;
   guarantee(total_allocation <= 8192, "must be");
-  unsigned long *scratch = (unsigned long *)alloca(total_allocation);
+  julong *scratch = (julong *)alloca(total_allocation);
 
   // Local scratch arrays
-  unsigned long
+  julong
     *a = scratch + 0 * longwords,
     *b = scratch + 1 * longwords,
     *n = scratch + 2 * longwords,
     *m = scratch + 3 * longwords;
 
-  reverse_words((unsigned long *)a_ints, a, longwords);
-  reverse_words((unsigned long *)b_ints, b, longwords);
-  reverse_words((unsigned long *)n_ints, n, longwords);
+  reverse_words((julong *)a_ints, a, longwords);
+  reverse_words((julong *)b_ints, b, longwords);
+  reverse_words((julong *)n_ints, n, longwords);
 
-  ::montgomery_multiply(a, b, n, m, (unsigned long)inv, longwords);
+  ::montgomery_multiply(a, b, n, m, (julong)inv, longwords);
 
-  reverse_words(m, (unsigned long *)m_ints, longwords);
+  reverse_words(m, (julong *)m_ints, longwords);
 }
 
 void SharedRuntime::montgomery_square(jint *a_ints, jint *n_ints,
                                       jint len, jlong inv,
                                       jint *m_ints) {
@@ -4163,34 +4184,32 @@
   int longwords = len/2;
 
   // Make very sure we don't use so much space that the stack might
   // overflow.  512 jints corresponds to an 16384-bit integer and
   // will use here a total of 6k bytes of stack space.
-  int total_allocation = longwords * sizeof (unsigned long) * 3;
+  int total_allocation = longwords * sizeof (julong) * 3;
   guarantee(total_allocation <= 8192, "must be");
-  unsigned long *scratch = (unsigned long *)alloca(total_allocation);
+  julong *scratch = (julong *)alloca(total_allocation);
 
   // Local scratch arrays
-  unsigned long
+  julong
     *a = scratch + 0 * longwords,
     *n = scratch + 1 * longwords,
     *m = scratch + 2 * longwords;
 
-  reverse_words((unsigned long *)a_ints, a, longwords);
-  reverse_words((unsigned long *)n_ints, n, longwords);
+  reverse_words((julong *)a_ints, a, longwords);
+  reverse_words((julong *)n_ints, n, longwords);
 
   if (len >= MONTGOMERY_SQUARING_THRESHOLD) {
-    ::montgomery_square(a, n, m, (unsigned long)inv, longwords);
+    ::montgomery_square(a, n, m, (julong)inv, longwords);
   } else {
-    ::montgomery_multiply(a, a, n, m, (unsigned long)inv, longwords);
+    ::montgomery_multiply(a, a, n, m, (julong)inv, longwords);
   }
 
-  reverse_words(m, (unsigned long *)m_ints, longwords);
+  reverse_words(m, (julong *)m_ints, longwords);
 }
 
-#endif // WINDOWS
-
 #ifdef COMPILER2
 // This is here instead of runtime_x86_64.cpp because it uses SimpleRuntimeFrame
 //
 //------------------------------generate_exception_blob---------------------------
 // creates exception blob at the end
diff a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
--- a/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
+++ b/src/hotspot/cpu/x86/stubGenerator_x86_64.cpp
@@ -6722,20 +6722,18 @@
     }
     if (VM_Version::supports_avx512_vbmi2()) {
       StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();
       StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();
     }
-#ifndef _WINDOWS
     if (UseMontgomeryMultiplyIntrinsic) {
       StubRoutines::_montgomeryMultiply
         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);
     }
     if (UseMontgomerySquareIntrinsic) {
       StubRoutines::_montgomerySquare
         = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);
     }
-#endif // WINDOWS
 #endif // COMPILER2
 
     if (UseVectorizedMismatchIntrinsic) {
       StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();
     }
diff a/src/hotspot/cpu/x86/vm_version_x86.cpp b/src/hotspot/cpu/x86/vm_version_x86.cpp
--- a/src/hotspot/cpu/x86/vm_version_x86.cpp
+++ b/src/hotspot/cpu/x86/vm_version_x86.cpp
@@ -1505,10 +1505,16 @@
 #ifdef COMPILER2
   if (FLAG_IS_DEFAULT(AlignVector)) {
     // Modern processors allow misaligned memory operations for vectors.
     AlignVector = !UseUnalignedLoadStores;
   }
+  if (FLAG_IS_DEFAULT(OptimizeFill)) {
+    // 8247307: On x86, the auto-vectorized loop array fill code shows
+    // better performance than the array fill stubs. We should reenable
+    // this after the x86 stubs get improved.
+    OptimizeFill = false;
+  }
 #endif // COMPILER2
 
   if (FLAG_IS_DEFAULT(AllocatePrefetchInstr)) {
     if (AllocatePrefetchInstr == 3 && !supports_3dnow_prefetch()) {
       FLAG_SET_DEFAULT(AllocatePrefetchInstr, 0);
diff a/src/hotspot/share/code/compiledIC.cpp b/src/hotspot/share/code/compiledIC.cpp
--- a/src/hotspot/share/code/compiledIC.cpp
+++ b/src/hotspot/share/code/compiledIC.cpp
@@ -769,9 +769,11 @@
          !old_method->method_holder()->is_loader_alive() ||
          old_method->is_old(),  // may be race patching deoptimized nmethod due to redefinition.
          "a) MT-unsafe modification of inline cache");
 
   address destination = jump->jump_destination();
-  assert(destination == (address)-1 || destination == entry,
+  assert(destination == (address)-1 || destination == entry
+         || old_method == NULL || !old_method->method_holder()->is_loader_alive() // may have a race due to class unloading.
+         || old_method->is_old(),  // may be race patching deoptimized nmethod due to redefinition.
          "b) MT-unsafe modification of inline cache");
 }
 #endif // !PRODUCT
diff a/src/hotspot/share/compiler/compileBroker.cpp b/src/hotspot/share/compiler/compileBroker.cpp
--- a/src/hotspot/share/compiler/compileBroker.cpp
+++ b/src/hotspot/share/compiler/compileBroker.cpp
@@ -1653,11 +1653,11 @@
 
   methodHandle method(thread, task->method());
   bool free_task;
 #if INCLUDE_JVMCI
   AbstractCompiler* comp = compiler(task->comp_level());
-  if (comp->is_jvmci() && !task->should_wait_for_compilation()) {
+  if (!UseJVMCINativeLibrary && comp->is_jvmci() && !task->should_wait_for_compilation()) {
     // It may return before compilation is completed.
     free_task = wait_for_jvmci_completion((JVMCICompiler*) comp, task, thread);
   } else
 #endif
   {
diff a/src/hotspot/share/gc/g1/c2/g1BarrierSetC2.cpp b/src/hotspot/share/gc/g1/c2/g1BarrierSetC2.cpp
--- a/src/hotspot/share/gc/g1/c2/g1BarrierSetC2.cpp
+++ b/src/hotspot/share/gc/g1/c2/g1BarrierSetC2.cpp
@@ -783,13 +783,12 @@
     return;
   }
   // Verify G1 pre-barriers
   const int marking_offset = in_bytes(G1ThreadLocalData::satb_mark_queue_active_offset());
 
-  ResourceArea *area = Thread::current()->resource_area();
-  Unique_Node_List visited(area);
-  Node_List worklist(area);
+  Unique_Node_List visited;
+  Node_List worklist;
   // We're going to walk control flow backwards starting from the Root
   worklist.push(compile->root());
   while (worklist.size() > 0) {
     Node* x = worklist.pop();
     if (x == NULL || x == compile->top()) continue;
diff a/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp b/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
--- a/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
+++ b/src/hotspot/share/gc/shenandoah/c2/shenandoahBarrierSetC2.cpp
@@ -984,13 +984,12 @@
     ShenandoahBarrierC2Support::verify(Compile::current()->root());
   } else if (phase == BarrierSetC2::BeforeCodeGen) {
     // Verify G1 pre-barriers
     const int marking_offset = in_bytes(ShenandoahThreadLocalData::satb_mark_queue_active_offset());
 
-    ResourceArea *area = Thread::current()->resource_area();
-    Unique_Node_List visited(area);
-    Node_List worklist(area);
+    Unique_Node_List visited;
+    Node_List worklist;
     // We're going to walk control flow backwards starting from the Root
     worklist.push(compile->root());
     while (worklist.size() > 0) {
       Node *x = worklist.pop();
       if (x == NULL || x == compile->top()) continue;
diff a/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp b/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
--- a/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
+++ b/src/hotspot/share/gc/shenandoah/c2/shenandoahSupport.cpp
@@ -277,11 +277,11 @@
   ResourceMark rm;
   Unique_Node_List wq;
   GrowableArray<Node*> barriers;
   Unique_Node_List barriers_used;
   Node_Stack phis(0);
-  VectorSet visited(Thread::current()->resource_area());
+  VectorSet visited;
   const bool trace = false;
   const bool verify_no_useless_barrier = false;
 
   wq.push(root);
   for (uint next = 0; next < wq.size(); next++) {
@@ -764,11 +764,11 @@
   return iffproj;
 }
 
 Node* ShenandoahBarrierC2Support::dom_mem(Node* mem, Node* ctrl, int alias, Node*& mem_ctrl, PhaseIdealLoop* phase) {
   ResourceMark rm;
-  VectorSet wq(Thread::current()->resource_area());
+  VectorSet wq;
   wq.set(mem->_idx);
   mem_ctrl = phase->ctrl_or_self(mem);
   while (!phase->is_dominator(mem_ctrl, ctrl) || mem_ctrl == ctrl) {
     mem = next_mem(mem, alias);
     if (wq.test_set(mem->_idx)) {
@@ -1080,15 +1080,16 @@
   Unique_Node_List uses;
   for (int i = 0; i < state->enqueue_barriers_count(); i++) {
     Node* barrier = state->enqueue_barrier(i);
     Node* ctrl = phase->get_ctrl(barrier);
     IdealLoopTree* loop = phase->get_loop(ctrl);
-    if (loop->_head->is_OuterStripMinedLoop()) {
+    Node* head = loop->head();
+    if (head->is_OuterStripMinedLoop()) {
       // Expanding a barrier here will break loop strip mining
       // verification. Transform the loop so the loop nest doesn't
       // appear as strip mined.
-      OuterStripMinedLoopNode* outer = loop->_head->as_OuterStripMinedLoop();
+      OuterStripMinedLoopNode* outer = head->as_OuterStripMinedLoop();
       hide_strip_mined_loop(outer, outer->unique_ctrl_out()->as_CountedLoop(), phase);
     }
   }
 
   Node_Stack stack(0);
@@ -1179,11 +1180,11 @@
         } while(stack.size() > 0);
         continue;
       }
       CallProjections* projs = call->extract_projections(false, false);
 #ifdef ASSERT
-      VectorSet cloned(Thread::current()->resource_area());
+      VectorSet cloned;
 #endif
       Node* lrb_clone = lrb->clone();
       phase->register_new_node(lrb_clone, projs->catchall_catchproj);
       phase->set_ctrl(lrb, projs->fallthrough_catchproj);
 
@@ -1284,15 +1285,16 @@
 
   for (int i = 0; i < state->load_reference_barriers_count(); i++) {
     ShenandoahLoadReferenceBarrierNode* lrb = state->load_reference_barrier(i);
     Node* ctrl = phase->get_ctrl(lrb);
     IdealLoopTree* loop = phase->get_loop(ctrl);
-    if (loop->_head->is_OuterStripMinedLoop()) {
+    Node* head = loop->head();
+    if (head->is_OuterStripMinedLoop()) {
       // Expanding a barrier here will break loop strip mining
       // verification. Transform the loop so the loop nest doesn't
       // appear as strip mined.
-      OuterStripMinedLoopNode* outer = loop->_head->as_OuterStripMinedLoop();
+      OuterStripMinedLoopNode* outer = head->as_OuterStripMinedLoop();
       hide_strip_mined_loop(outer, outer->unique_ctrl_out()->as_CountedLoop(), phase);
     }
   }
 
   // Expand load-reference-barriers
@@ -1349,11 +1351,11 @@
     // Call lrb-stub and wire up that path in slots 4
     Node* result_mem = NULL;
 
     Node* addr;
     if (ShenandoahSelfFixing) {
-      VectorSet visited(Thread::current()->resource_area());
+      VectorSet visited;
       addr = get_load_addr(phase, visited, lrb);
     } else {
       addr = phase->igvn().zerocon(T_OBJECT);
     }
     if (addr->Opcode() == Op_AddP) {
@@ -1813,11 +1815,11 @@
     assert(is_heap_stable_test(n), "only evacuation test");
     merge_back_to_back_tests(n, phase);
   }
 
   if (!phase->C->major_progress()) {
-    VectorSet seen(Thread::current()->resource_area());
+    VectorSet seen;
     for (uint i = 0; i < heap_stable_tests.size(); i++) {
       Node* n = heap_stable_tests.at(i);
       IdealLoopTree* loop = phase->get_loop(n);
       if (loop != phase->ltree_root() &&
           loop->_child == NULL &&
@@ -2085,11 +2087,11 @@
 }
 #endif
 
 void MemoryGraphFixer::collect_memory_nodes() {
   Node_Stack stack(0);
-  VectorSet visited(Thread::current()->resource_area());
+  VectorSet visited;
   Node_List regions;
 
   // Walk the raw memory graph and create a mapping from CFG node to
   // memory node. Exclude phis for now.
   stack.push(_phase->C->root(), 1);
diff a/src/hotspot/share/interpreter/bytecodes.cpp b/src/hotspot/share/interpreter/bytecodes.cpp
--- a/src/hotspot/share/interpreter/bytecodes.cpp
+++ b/src/hotspot/share/interpreter/bytecodes.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2017, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -534,12 +534,12 @@
   def(_fast_aldc_w         , "fast_aldc_w"         , "bJJ"  , NULL    , T_OBJECT,   1, true,  _ldc_w );
 
   def(_nofast_getfield     , "nofast_getfield"     , "bJJ"  , NULL    , T_ILLEGAL,  0, true,  _getfield       );
   def(_nofast_putfield     , "nofast_putfield"     , "bJJ"  , NULL    , T_ILLEGAL, -2, true , _putfield       );
 
-  def(_nofast_aload_0      , "nofast_aload_0"      , "b"    , NULL    , T_ILLEGAL,  1, true , _aload_0        );
-  def(_nofast_iload        , "nofast_iload"        , "bi"   , NULL    , T_ILLEGAL,  1, false, _iload          );
+  def(_nofast_aload_0      , "nofast_aload_0"      , "b"    , NULL    , T_OBJECT,   1, true , _aload_0        );
+  def(_nofast_iload        , "nofast_iload"        , "bi"   , NULL    , T_INT,      1, false, _iload          );
 
   def(_shouldnotreachhere  , "_shouldnotreachhere" , "b"    , NULL    , T_VOID   ,  0, false);
 
   // compare can_trap information for each bytecode with the
   // can_trap information for the corresponding base bytecode
diff a/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp b/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp
--- a/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp
+++ b/src/hotspot/share/jvmci/jvmciCompilerToVM.cpp
@@ -1023,20 +1023,20 @@
   HandleMark hm;
 
   JVMCIObject nmethod_mirror = JVMCIENV->wrap(hs_nmethod);
   nmethodLocker locker;
   nmethod* nm = JVMCIENV->get_nmethod(nmethod_mirror, locker);
-  if (nm == NULL) {
+  if (nm == NULL || !nm->is_in_use()) {
     JVMCI_THROW_NULL(InvalidInstalledCodeException);
   }
   methodHandle mh(THREAD, nm->method());
   Symbol* signature = mh->signature();
   JavaCallArguments jca(mh->size_of_parameters());
 
   JavaArgumentUnboxer jap(signature, &jca, (arrayOop) JNIHandles::resolve(args), mh->is_static());
   JavaValue result(jap.return_type());
-  jca.set_alternative_target(nm);
+  jca.set_alternative_target(Handle(THREAD, JNIHandles::resolve(nmethod_mirror.as_jobject())));
   JavaCalls::call(&result, mh, &jca, CHECK_NULL);
 
   if (jap.return_type() == T_VOID) {
     return NULL;
   } else if (is_reference_type(jap.return_type())) {
@@ -1766,10 +1766,22 @@
     InstanceKlass* k = InstanceKlass::cast(klass);
     k->initialize(CHECK);
   }
 C2V_END
 
+C2V_VMENTRY(void, ensureLinked, (JNIEnv* env, jobject, jobject jvmci_type))
+  if (jvmci_type == NULL) {
+    JVMCI_THROW(NullPointerException);
+  }
+
+  Klass* klass = JVMCIENV->asKlass(jvmci_type);
+  if (klass != NULL && klass->is_instance_klass()) {
+    InstanceKlass* k = InstanceKlass::cast(klass);
+    k->link_class(CHECK);
+  }
+C2V_END
+
 C2V_VMENTRY_0(jint, interpreterFrameSize, (JNIEnv* env, jobject, jobject bytecode_frame_handle))
   if (bytecode_frame_handle == NULL) {
     JVMCI_THROW_0(NullPointerException);
   }
 
@@ -2773,10 +2785,11 @@
   {CC "getFlagValue",                                 CC "(" STRING ")" OBJECT,                                                             FN_PTR(getFlagValue)},
   {CC "getObjectAtAddress",                           CC "(J)" OBJECT,                                                                      FN_PTR(getObjectAtAddress)},
   {CC "getInterfaces",                                CC "(" HS_RESOLVED_KLASS ")[" HS_RESOLVED_KLASS,                                      FN_PTR(getInterfaces)},
   {CC "getComponentType",                             CC "(" HS_RESOLVED_KLASS ")" HS_RESOLVED_TYPE,                                        FN_PTR(getComponentType)},
   {CC "ensureInitialized",                            CC "(" HS_RESOLVED_KLASS ")V",                                                        FN_PTR(ensureInitialized)},
+  {CC "ensureLinked",                                 CC "(" HS_RESOLVED_KLASS ")V",                                                        FN_PTR(ensureLinked)},
   {CC "getIdentityHashCode",                          CC "(" OBJECTCONSTANT ")I",                                                           FN_PTR(getIdentityHashCode)},
   {CC "isInternedString",                             CC "(" OBJECTCONSTANT ")Z",                                                           FN_PTR(isInternedString)},
   {CC "unboxPrimitive",                               CC "(" OBJECTCONSTANT ")" OBJECT,                                                     FN_PTR(unboxPrimitive)},
   {CC "boxPrimitive",                                 CC "(" OBJECT ")" OBJECTCONSTANT,                                                     FN_PTR(boxPrimitive)},
   {CC "getDeclaredConstructors",                      CC "(" HS_RESOLVED_KLASS ")[" RESOLVED_METHOD,                                        FN_PTR(getDeclaredConstructors)},
diff a/src/hotspot/share/jvmci/vmStructs_jvmci.cpp b/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
--- a/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
+++ b/src/hotspot/share/jvmci/vmStructs_jvmci.cpp
@@ -553,11 +553,18 @@
   declare_constant(FieldInfo::field_slots)                                \
                                                                           \
   declare_constant(InstanceKlass::linked)                                 \
   declare_constant(InstanceKlass::being_initialized)                      \
   declare_constant(InstanceKlass::fully_initialized)                      \
+                                                                          \
+  /*********************************/                                     \
+  /* InstanceKlass _misc_flags */                                         \
+  /*********************************/                                     \
+                                                                          \
   declare_constant(InstanceKlass::_misc_is_unsafe_anonymous)              \
+  declare_constant(InstanceKlass::_misc_has_nonstatic_concrete_methods)   \
+  declare_constant(InstanceKlass::_misc_declares_nonstatic_concrete_methods) \
                                                                           \
   declare_constant(JumpData::taken_off_set)                               \
   declare_constant(JumpData::displacement_off_set)                        \
                                                                           \
   declare_preprocessor_constant("JVMCI::ok",                   JVMCI::ok)                      \
diff a/src/hotspot/share/memory/metaspaceShared.cpp b/src/hotspot/share/memory/metaspaceShared.cpp
--- a/src/hotspot/share/memory/metaspaceShared.cpp
+++ b/src/hotspot/share/memory/metaspaceShared.cpp
@@ -663,10 +663,37 @@
       }
     }
   }
 };
 
+// Global object for holding symbols that created during class loading. See SymbolTable::new_symbol
+static GrowableArray<Symbol*>* _global_symbol_objects = NULL;
+
+static int compare_symbols_by_address(Symbol** a, Symbol** b) {
+  if (a[0] < b[0]) {
+    return -1;
+  } else if (a[0] == b[0]) {
+    ResourceMark rm;
+    log_warning(cds)("Duplicated symbol %s unexpected", (*a)->as_C_string());
+    return 0;
+  } else {
+    return 1;
+  }
+}
+
+void MetaspaceShared::add_symbol(Symbol* sym) {
+  MutexLocker ml(CDSAddSymbol_lock, Mutex::_no_safepoint_check_flag);
+  if (_global_symbol_objects == NULL) {
+    _global_symbol_objects = new (ResourceObj::C_HEAP, mtSymbol) GrowableArray<Symbol*>(2048, mtSymbol);
+  }
+  _global_symbol_objects->append(sym);
+}
+
+GrowableArray<Symbol*>* MetaspaceShared::collected_symbols() {
+  return _global_symbol_objects;
+}
+
 static void remove_unshareable_in_classes() {
   for (int i = 0; i < _global_klass_objects->length(); i++) {
     Klass* k = _global_klass_objects->at(i);
     if (!k->is_objArray_klass()) {
       // InstanceKlass and TypeArrayKlass will in turn call remove_unshareable_info
@@ -1242,38 +1269,10 @@
   VMOp_Type type() const { return VMOp_PopulateDumpSharedSpace; }
   void doit();   // outline because gdb sucks
   bool allow_nested_vm_operations() const { return true; }
 }; // class VM_PopulateDumpSharedSpace
 
-class SortedSymbolClosure: public SymbolClosure {
-  GrowableArray<Symbol*> _symbols;
-  virtual void do_symbol(Symbol** sym) {
-    assert((*sym)->is_permanent(), "archived symbols must be permanent");
-    _symbols.append(*sym);
-  }
-  static int compare_symbols_by_address(Symbol** a, Symbol** b) {
-    if (a[0] < b[0]) {
-      return -1;
-    } else if (a[0] == b[0]) {
-      ResourceMark rm;
-      log_warning(cds)("Duplicated symbol %s unexpected", (*a)->as_C_string());
-      return 0;
-    } else {
-      return 1;
-    }
-  }
-
-public:
-  SortedSymbolClosure() {
-    SymbolTable::symbols_do(this);
-    _symbols.sort(compare_symbols_by_address);
-  }
-  GrowableArray<Symbol*>* get_sorted_symbols() {
-    return &_symbols;
-  }
-};
-
 // ArchiveCompactor --
 //
 // This class is the central piece of shared archive compaction -- all metaspace data are
 // initially allocated outside of the shared regions. ArchiveCompactor copies the
 // metaspace data into their final location in the shared regions.
@@ -1281,11 +1280,10 @@
 class ArchiveCompactor : AllStatic {
   static const int INITIAL_TABLE_SIZE = 8087;
   static const int MAX_TABLE_SIZE     = 1000000;
 
   static DumpAllocStats* _alloc_stats;
-  static SortedSymbolClosure* _ssc;
 
   typedef KVHashtable<address, address, mtInternal> RelocationTable;
   static RelocationTable* _new_loc_table;
 
 public:
@@ -1439,12 +1437,10 @@
 #endif
 
 public:
   static void copy_and_compact() {
     ResourceMark rm;
-    SortedSymbolClosure the_ssc; // StackObj
-    _ssc = &the_ssc;
 
     log_info(cds)("Scanning all metaspace objects ... ");
     {
       // allocate and shallow-copy RW objects, immediately following the MC region
       log_info(cds)("Allocating RW objects ... ");
@@ -1476,27 +1472,25 @@
       iterate_roots(&ext_reloc);
     }
     {
       log_info(cds)("Fixing symbol identity hash ... ");
       os::init_random(0x12345678);
-      GrowableArray<Symbol*>* symbols = _ssc->get_sorted_symbols();
-      for (int i=0; i<symbols->length(); i++) {
-        symbols->at(i)->update_identity_hash();
+      GrowableArray<Symbol*>* all_symbols = MetaspaceShared::collected_symbols();
+      all_symbols->sort(compare_symbols_by_address);
+      for (int i = 0; i < all_symbols->length(); i++) {
+        assert(all_symbols->at(i)->is_permanent(), "archived symbols must be permanent");
+        all_symbols->at(i)->update_identity_hash();
       }
     }
 #ifdef ASSERT
     {
       log_info(cds)("Verifying external roots ... ");
       ResourceMark rm;
       IsRefInArchiveChecker checker;
       iterate_roots(&checker);
     }
 #endif
-
-
-    // cleanup
-    _ssc = NULL;
   }
 
   // We must relocate the System::_well_known_klasses only after we have copied the
   // java objects in during dump_java_heap_objects(): during the object copy, we operate on
   // old objects which assert that their klass is the original klass.
@@ -1528,12 +1522,12 @@
     // the SharedClassListFile (class loading is done in a single thread and the JIT
     // is disabled). Also, Symbols are allocated in monotonically increasing addresses
     // (see Symbol::operator new(size_t, int)). So if we iterate the Symbols by
     // ascending address order, we ensure that all Symbols are copied into deterministic
     // locations in the archive.
-    GrowableArray<Symbol*>* symbols = _ssc->get_sorted_symbols();
-    for (int i=0; i<symbols->length(); i++) {
+    GrowableArray<Symbol*>* symbols = _global_symbol_objects;
+    for (int i = 0; i < symbols->length(); i++) {
       it->push(symbols->adr_at(i));
     }
     if (_global_klass_objects != NULL) {
       // Need to fix up the pointers
       for (int i = 0; i < _global_klass_objects->length(); i++) {
@@ -1559,11 +1553,10 @@
     return klass;
   }
 };
 
 DumpAllocStats* ArchiveCompactor::_alloc_stats;
-SortedSymbolClosure* ArchiveCompactor::_ssc;
 ArchiveCompactor::RelocationTable* ArchiveCompactor::_new_loc_table;
 
 void VM_PopulateDumpSharedSpace::dump_symbols() {
   log_info(cds)("Dumping symbol table ...");
 
diff a/src/hotspot/share/oops/method.cpp b/src/hotspot/share/oops/method.cpp
--- a/src/hotspot/share/oops/method.cpp
+++ b/src/hotspot/share/oops/method.cpp
@@ -367,10 +367,11 @@
 // where they should point in a new JVM.  Further initialize some
 // entries now in order allow them to be write protected later.
 
 void Method::remove_unshareable_info() {
   unlink_method();
+  JFR_ONLY(REMOVE_METHOD_ID(this);)
 }
 
 void Method::set_vtable_index(int index) {
   if (is_shared() && !MetaspaceShared::remapped_readwrite()) {
     // At runtime initialize_vtable is rerun as part of link_class_impl()
diff a/src/hotspot/share/opto/c2_globals.hpp b/src/hotspot/share/opto/c2_globals.hpp
--- a/src/hotspot/share/opto/c2_globals.hpp
+++ b/src/hotspot/share/opto/c2_globals.hpp
@@ -244,13 +244,10 @@
           "Use Conservative Copy Coalescing in the Register Allocator")     \
                                                                             \
   develop(bool, UseUniqueSubclasses, true,                                  \
           "Narrow an abstract reference to the unique concrete subclass")   \
                                                                             \
-  develop(bool, UseExactTypes, true,                                        \
-          "Use exact types to eliminate array store checks and v-calls")    \
-                                                                            \
   product(intx, TrackedInitializationLimit, 50,                             \
           "When initializing fields, track up to this many words")          \
           range(0, 65535)                                                   \
                                                                             \
   product(bool, ReduceFieldZeroing, true,                                   \
diff a/src/hotspot/share/opto/cfgnode.cpp b/src/hotspot/share/opto/cfgnode.cpp
--- a/src/hotspot/share/opto/cfgnode.cpp
+++ b/src/hotspot/share/opto/cfgnode.cpp
@@ -345,13 +345,12 @@
     return false; // An unsafe case was NOT found - don't need graph walk.
 
   // Unsafe case - check if the Region node is reachable from root.
   ResourceMark rm;
 
-  Arena *a = Thread::current()->resource_area();
-  Node_List nstack(a);
-  VectorSet visited(a);
+  Node_List nstack;
+  VectorSet visited;
 
   // Mark all control nodes reachable from root outputs
   Node *n = (Node*)phase->C->root();
   nstack.push(n);
   visited.set(n->_idx);
@@ -1045,11 +1044,11 @@
 
   assert(_adr_type == flatten_phi_adr_type(_adr_type),
          "Phi::adr_type must be pre-normalized");
 
   if (recursive) {
-    VectorSet visited(Thread::current()->resource_area());
+    VectorSet visited;
     verify_adr_type(visited, _adr_type);
   }
 }
 #endif
 
@@ -1788,13 +1787,12 @@
   // Unsafe case when we should go through data graph to determine
   // if the phi references itself.
 
   ResourceMark rm;
 
-  Arena *a = Thread::current()->resource_area();
-  Node_List nstack(a);
-  VectorSet visited(a);
+  Node_List nstack;
+  VectorSet visited;
 
   nstack.push(in); // Start with unique input.
   visited.set(in->_idx);
   while (nstack.size() != 0) {
     Node* n = nstack.pop();
diff a/src/hotspot/share/opto/chaitin.cpp b/src/hotspot/share/opto/chaitin.cpp
--- a/src/hotspot/share/opto/chaitin.cpp
+++ b/src/hotspot/share/opto/chaitin.cpp
@@ -200,12 +200,10 @@
 #else
        NULL
 #endif
        )
   , _live(0)
-  , _spilled_once(Thread::current()->resource_area())
-  , _spilled_twice(Thread::current()->resource_area())
   , _lo_degree(0), _lo_stk_degree(0), _hi_degree(0), _simplified(0)
   , _oldphi(unique)
 #ifndef PRODUCT
   , _trace_spilling(C->directive()->TraceSpillingOption)
 #endif
diff a/src/hotspot/share/opto/compile.cpp b/src/hotspot/share/opto/compile.cpp
--- a/src/hotspot/share/opto/compile.cpp
+++ b/src/hotspot/share/opto/compile.cpp
@@ -1016,10 +1016,11 @@
   _opaque4_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   _inline_type_nodes = new(comp_arena()) GrowableArray<Node*>(comp_arena(), 8,  0, NULL);
   register_library_intrinsics();
 #ifdef ASSERT
   _type_verify_symmetry = true;
+  _phase_optimize_finished = false;
 #endif
 }
 
 //---------------------------init_start----------------------------------------
 // Install the StartNode on this compile object.
@@ -2657,10 +2658,11 @@
      return;
    }
  }
 
  print_method(PHASE_OPTIMIZE_FINISHED, 2);
+ DEBUG_ONLY(set_phase_optimize_finished();)
 }
 
 //---------------------------- Bitwise operation packing optimization ---------------------------
 
 static bool is_vector_unary_bitwise_op(Node* n) {
@@ -3101,12 +3103,11 @@
   VectorSet _visited;           // Visitation flags
   Node_List _tests;             // Set of IfNodes & PCTableNodes
 
   Final_Reshape_Counts() :
     _call_count(0), _float_count(0), _double_count(0),
-    _java_call_count(0), _inner_loop_count(0),
-    _visited( Thread::current()->resource_area() ) { }
+    _java_call_count(0), _inner_loop_count(0) { }
 
   void inc_call_count  () { _call_count  ++; }
   void inc_float_count () { _float_count ++; }
   void inc_double_count() { _double_count++; }
   void inc_java_call_count() { _java_call_count++; }
@@ -3931,12 +3932,11 @@
 
 //------------------------------final_graph_reshaping_walk---------------------
 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
 // requires that the walk visits a node's inputs before visiting the node.
 void Compile::final_graph_reshaping_walk( Node_Stack &nstack, Node *root, Final_Reshape_Counts &frc ) {
-  ResourceArea *area = Thread::current()->resource_area();
-  Unique_Node_List sfpt(area);
+  Unique_Node_List sfpt;
 
   frc._visited.set(root->_idx); // first, mark node as visited
   uint cnt = root->req();
   Node *n = root;
   uint  i = 0;
@@ -4294,18 +4294,17 @@
 //------------------------------verify_graph_edges---------------------------
 // Walk the Graph and verify that there is a one-to-one correspondence
 // between Use-Def edges and Def-Use edges in the graph.
 void Compile::verify_graph_edges(bool no_dead_code) {
   if (VerifyGraphEdges) {
-    ResourceArea *area = Thread::current()->resource_area();
-    Unique_Node_List visited(area);
+    Unique_Node_List visited;
     // Call recursive graph walk to check edges
     _root->verify_edges(visited);
     if (no_dead_code) {
       // Now make sure that no visited node is used by an unvisited node.
       bool dead_nodes = false;
-      Unique_Node_List checked(area);
+      Unique_Node_List checked;
       while (visited.size() > 0) {
         Node* n = visited.pop();
         checked.push(n);
         for (uint i = 0; i < n->outcnt(); i++) {
           Node* use = n->raw_out(i);
diff a/src/hotspot/share/opto/compile.hpp b/src/hotspot/share/opto/compile.hpp
--- a/src/hotspot/share/opto/compile.hpp
+++ b/src/hotspot/share/opto/compile.hpp
@@ -330,11 +330,12 @@
   // Node management
   uint                  _unique;                // Counter for unique Node indices
   VectorSet             _dead_node_list;        // Set of dead nodes
   uint                  _dead_node_count;       // Number of dead nodes; VectorSet::Size() is O(N).
                                                 // So use this to keep count and make the call O(1).
-  DEBUG_ONLY( Unique_Node_List* _modified_nodes; )  // List of nodes which inputs were modified
+  DEBUG_ONLY(Unique_Node_List* _modified_nodes;)   // List of nodes which inputs were modified
+  DEBUG_ONLY(bool       _phase_optimize_finished;) // Used for live node verification while creating new nodes
 
   debug_only(static int _debug_idx;)            // Monotonic counter (not reset), use -XX:BreakAtNode=<idx>
   Arena                 _node_arena;            // Arena for new-space Nodes
   Arena                 _old_arena;             // Arena for old-space Nodes, lifetime during xform
   RootNode*             _root;                  // Unique root of compilation, or NULL after bail-out.
@@ -801,10 +802,12 @@
     int  val = _unique - _dead_node_count;
     assert (val >= 0, "number of tracked dead nodes %d more than created nodes %d", _unique, _dead_node_count);
             return (uint) val;
                                            }
 #ifdef ASSERT
+  void         set_phase_optimize_finished() { _phase_optimize_finished = true; }
+  bool         phase_optimize_finished() const { return _phase_optimize_finished; }
   uint         count_live_nodes_by_graph_walk();
   void         print_missing_nodes();
 #endif
 
   // Record modified nodes to check that they are put on IGVN worklist
diff a/src/hotspot/share/opto/escape.cpp b/src/hotspot/share/opto/escape.cpp
--- a/src/hotspot/share/opto/escape.cpp
+++ b/src/hotspot/share/opto/escape.cpp
@@ -2986,12 +2986,11 @@
 void ConnectionGraph::split_unique_types(GrowableArray<Node *>  &alloc_worklist, GrowableArray<ArrayCopyNode*> &arraycopy_worklist) {
   GrowableArray<Node *>  memnode_worklist;
   GrowableArray<PhiNode *>  orig_phis;
   PhaseIterGVN  *igvn = _igvn;
   uint new_index_start = (uint) _compile->num_alias_types();
-  Arena* arena = Thread::current()->resource_area();
-  VectorSet visited(arena);
+  VectorSet visited;
   ideal_nodes.clear(); // Reset for use with set_map/get_map.
   uint unique_old = _compile->unique();
 
   //  Phase 1:  Process possible allocations from alloc_worklist.
   //  Create instance types for the CheckCastPP for allocations where possible.
diff a/src/hotspot/share/opto/lcm.cpp b/src/hotspot/share/opto/lcm.cpp
--- a/src/hotspot/share/opto/lcm.cpp
+++ b/src/hotspot/share/opto/lcm.cpp
@@ -151,11 +151,10 @@
   assert(!is_decoden || (val->in(0) == NULL) && val->is_Mach() &&
          (val->as_Mach()->ideal_Opcode() == Op_DecodeN), "sanity");
 
   // Search the successor block for a load or store who's base value is also
   // the tested value.  There may be several.
-  Node_List *out = new Node_List(Thread::current()->resource_area());
   MachNode *best = NULL;        // Best found so far
   for (DUIterator i = val->outs(); val->has_out(i); i++) {
     Node *m = val->out(i);
     if( !m->is_Mach() ) continue;
     MachNode *mach = m->as_Mach();
@@ -1254,11 +1253,11 @@
       break;
 
   if( j == def_blk->_num_succs ) {
     // Block at same level in dom-tree is not a successor.  It needs a
     // PhiNode, the PhiNode uses from the def and IT's uses need fixup.
-    Node_Array inputs = new Node_List(Thread::current()->resource_area());
+    Node_Array inputs = new Node_List();
     for(uint k = 1; k < use_blk->num_preds(); k++) {
       Block* block = get_block_for_node(use_blk->pred(k));
       inputs.map(k, catch_cleanup_find_cloned_def(block, def, def_blk, n_clone_idx));
     }
 
@@ -1365,11 +1364,11 @@
   // Fixup edges.  Check the def-use info per cloned Node
   for(uint i2 = beg; i2 < end; i2++ ) {
     uint n_clone_idx = i2-beg+1; // Index of clone of n in each successor block
     Node *n = block->get_node(i2);        // Node that got cloned
     // Need DU safe iterator because of edge manipulation in calls.
-    Unique_Node_List *out = new Unique_Node_List(Thread::current()->resource_area());
+    Unique_Node_List* out = new Unique_Node_List();
     for (DUIterator_Fast j1max, j1 = n->fast_outs(j1max); j1 < j1max; j1++) {
       out->push(n->fast_out(j1));
     }
     uint max = out->size();
     for (uint j = 0; j < max; j++) {// For all users
diff a/src/hotspot/share/opto/live.cpp b/src/hotspot/share/opto/live.cpp
--- a/src/hotspot/share/opto/live.cpp
+++ b/src/hotspot/share/opto/live.cpp
@@ -87,11 +87,11 @@
   memset(_deltas, 0, sizeof(IndexSet*)* _cfg.number_of_blocks());
 
   _free_IndexSet = NULL;
 
   // Blocks having done pass-1
-  VectorSet first_pass(Thread::current()->resource_area());
+  VectorSet first_pass;
 
   // Outer loop: must compute local live-in sets and push into predecessors.
   for (uint j = _cfg.number_of_blocks(); j > 0; j--) {
     Block* block = _cfg.get_block(j - 1);
 
diff a/src/hotspot/share/opto/loopnode.hpp b/src/hotspot/share/opto/loopnode.hpp
--- a/src/hotspot/share/opto/loopnode.hpp
+++ b/src/hotspot/share/opto/loopnode.hpp
@@ -486,10 +486,11 @@
   // we peel off the multiple backedges,  merging all edges at the bottom and
   // ensuring that one proper backedge flow into the loop.
   Node *_head;                  // Head of loop
   Node *_tail;                  // Tail of loop
   inline Node *tail();          // Handle lazy update of _tail field
+  inline Node *head();          // Handle lazy update of _head field
   PhaseIdealLoop* _phase;
   int _local_loop_unroll_limit;
   int _local_loop_unroll_factor;
 
   Node_List _body;              // Loop body for inner loops
@@ -1407,18 +1408,21 @@
   // introduced since the last 'begin'. Always check that we have not exceeded
   // the maximum node limit.
   void require_nodes_final(uint live_at_begin, bool check_estimate) {
     assert(_nodes_required < UINT_MAX, "Bad state (final).");
 
+#ifdef ASSERT
     if (check_estimate) {
-      // Assert that the node budget request was not off by too much (x2).
+      // Check that the node budget request was not off by too much (x2).
       // Should this be the case we _surely_ need to improve the estimates
       // used in our budget calculations.
-      assert(C->live_nodes() - live_at_begin <= 2 * _nodes_required,
-             "Bad node estimate: actual = %d >> request = %d",
-             C->live_nodes() - live_at_begin, _nodes_required);
+      if (C->live_nodes() - live_at_begin > 2 * _nodes_required) {
+        log_info(compilation)("Bad node estimate: actual = %d >> request = %d",
+                              C->live_nodes() - live_at_begin, _nodes_required);
+      }
     }
+#endif
     // Assert that we have stayed within the node budget limit.
     assert(C->live_nodes() < C->max_node_limit(),
            "Exceeding node budget limit: %d + %d > %d (request = %d)",
            C->live_nodes() - live_at_begin, live_at_begin,
            C->max_node_limit(), _nodes_required);
@@ -1578,10 +1582,17 @@
     _tail = _phase->get_ctrl(_tail);
   }
   return _tail;
 }
 
+inline Node* IdealLoopTree::head() {
+  // Handle lazy update of _head field.
+  if (_head->in(0) == NULL) {
+    _head = _phase->get_ctrl(_head);
+  }
+  return _head;
+}
 
 // Iterate over the loop tree using a preorder, left-to-right traversal.
 //
 // Example that visits all counted loops from within PhaseIdealLoop
 //
diff a/src/hotspot/share/opto/loopopts.cpp b/src/hotspot/share/opto/loopopts.cpp
--- a/src/hotspot/share/opto/loopopts.cpp
+++ b/src/hotspot/share/opto/loopopts.cpp
@@ -1853,26 +1853,23 @@
       // Loop predicate If node connects to Bool node through Opaque1 node.
       if (use->is_If() || use->is_CMove() || C->is_predicate_opaq(use) || use->Opcode() == Op_Opaque4) {
         // Since this code is highly unlikely, we lazily build the worklist
         // of such Nodes to go split.
         if (!split_if_set) {
-          ResourceArea *area = Thread::current()->resource_area();
-          split_if_set = new Node_List(area);
+          split_if_set = new Node_List();
         }
         split_if_set->push(use);
       }
       if (use->is_Bool()) {
         if (!split_bool_set) {
-          ResourceArea *area = Thread::current()->resource_area();
-          split_bool_set = new Node_List(area);
+          split_bool_set = new Node_List();
         }
         split_bool_set->push(use);
       }
       if (use->Opcode() == Op_CreateEx) {
         if (!split_cex_set) {
-          ResourceArea *area = Thread::current()->resource_area();
-          split_cex_set = new Node_List(area);
+          split_cex_set = new Node_List();
         }
         split_cex_set->push(use);
       }
 
 
@@ -2201,20 +2198,19 @@
         }
     }
     _igvn.hash_find_insert(nnn);
   }
 
-  ResourceArea *area = Thread::current()->resource_area();
-  Node_List extra_data_nodes(area); // data nodes in the outer strip mined loop
+  Node_List extra_data_nodes; // data nodes in the outer strip mined loop
   clone_outer_loop(head, mode, loop, outer_loop, dd, old_new, extra_data_nodes);
 
   // Step 3: Now fix control uses.  Loop varying control uses have already
   // been fixed up (as part of all input edges in Step 2).  Loop invariant
   // control uses must be either an IfFalse or an IfTrue.  Make a merge
   // point to merge the old and new IfFalse/IfTrue nodes; make the use
   // refer to this.
-  Node_List worklist(area);
+  Node_List worklist;
   uint new_counter = C->unique();
   for( i = 0; i < loop->_body.size(); i++ ) {
     Node* old = loop->_body.at(i);
     if( !old->is_CFG() ) continue;
 
@@ -2691,13 +2687,12 @@
 // followed by the nodes that have an immediate input dependence
 // on a node in "sched".
 void PhaseIdealLoop::scheduled_nodelist( IdealLoopTree *loop, VectorSet& member, Node_List &sched ) {
 
   assert(member.test(loop->_head->_idx), "loop head must be in member set");
-  Arena *a = Thread::current()->resource_area();
-  VectorSet visited(a);
-  Node_Stack nstack(a, loop->_body.size());
+  VectorSet visited;
+  Node_Stack nstack(loop->_body.size());
 
   Node* n  = loop->_head;  // top of stack is cached in "n"
   uint idx = 0;
   visited.set(n->_idx);
 
@@ -3269,16 +3264,15 @@
       tt->dump();
       if (tt == last_peel) tty->print_cr("-- cut --");
     }
   }
 #endif
-  ResourceArea *area = Thread::current()->resource_area();
-  VectorSet peel(area);
-  VectorSet not_peel(area);
-  Node_List peel_list(area);
-  Node_List worklist(area);
-  Node_List sink_list(area);
+  VectorSet peel;
+  VectorSet not_peel;
+  Node_List peel_list;
+  Node_List worklist;
+  Node_List sink_list;
 
   uint estimate = loop->est_loop_clone_sz(1);
   if (exceeding_node_budget(estimate)) {
     return false;
   }
@@ -3541,11 +3535,11 @@
   loop->record_for_igvn();
 
 #ifndef PRODUCT
   if (TracePartialPeeling) {
     tty->print_cr("\nafter partial peel one iteration");
-    Node_List wl(area);
+    Node_List wl;
     Node* t = last_peel;
     while (true) {
       wl.push(t);
       if (t == head_clone) break;
       t = idom(t);
diff a/src/hotspot/share/opto/macro.cpp b/src/hotspot/share/opto/macro.cpp
--- a/src/hotspot/share/opto/macro.cpp
+++ b/src/hotspot/share/opto/macro.cpp
@@ -499,12 +499,11 @@
   assert((uint)instance_id == alloc->_idx, "wrong allocation");
 
   int alias_idx = C->get_alias_index(adr_t);
   int offset = adr_t->flattened_offset();
   Node *start_mem = C->start()->proj_out_or_null(TypeFunc::Memory);
-  Node *alloc_mem = alloc->in(TypeFunc::Memory);
-  Arena *a = Thread::current()->resource_area();
+  Node *alloc_mem = alloc->in(TypeFunc::Memory);
   VectorSet visited(a);
 
   bool done = sfpt_mem == alloc_mem;
   Node *mem = sfpt_mem;
   while (!done) {
@@ -570,12 +569,12 @@
       BarrierSetC2* bs = BarrierSet::barrier_set()->barrier_set_c2();
       n = bs->step_over_gc_barrier(n);
       return n;
     } else if (mem->is_Phi()) {
       // attempt to produce a Phi reflecting the values on the input paths of the Phi
-      Node_Stack value_phis(a, 8);
-      Node * phi = value_from_mem_phi(mem, ft, ftype, adr_t, alloc, &value_phis, ValueSearchLimit);
+      Node_Stack value_phis(8);
+      Node* phi = value_from_mem_phi(mem, ft, ftype, adr_t, alloc, &value_phis, ValueSearchLimit);
       if (phi != NULL) {
         return phi;
       } else {
         // Kill all new Phis
         while(value_phis.is_nonempty()) {
diff a/src/hotspot/share/opto/matcher.cpp b/src/hotspot/share/opto/matcher.cpp
--- a/src/hotspot/share/opto/matcher.cpp
+++ b/src/hotspot/share/opto/matcher.cpp
@@ -151,11 +151,11 @@
 #ifdef ASSERT
 void Matcher::verify_new_nodes_only(Node* xroot) {
   // Make sure that the new graph only references new nodes
   ResourceMark rm;
   Unique_Node_List worklist;
-  VectorSet visited(Thread::current()->resource_area());
+  VectorSet visited;
   worklist.push(xroot);
   while (worklist.size() > 0) {
     Node* n = worklist.pop();
     visited.set(n->_idx);
     assert(C->node_arena()->contains(n), "dead node");
@@ -2024,11 +2024,11 @@
 // Set bits if Node is shared or otherwise a root
 void Matcher::find_shared(Node* n) {
   // Allocate stack of size C->live_nodes() * 2 to avoid frequent realloc
   MStack mstack(C->live_nodes() * 2);
   // Mark nodes as address_visited if they are inputs to an address expression
-  VectorSet address_visited(Thread::current()->resource_area());
+  VectorSet address_visited;
   mstack.push(n, Visit);     // Don't need to pre-visit root node
   while (mstack.is_nonempty()) {
     n = mstack.node();       // Leave node on stack
     Node_State nstate = mstack.state();
     uint nop = n->Opcode();
diff a/src/hotspot/share/opto/memnode.cpp b/src/hotspot/share/opto/memnode.cpp
--- a/src/hotspot/share/opto/memnode.cpp
+++ b/src/hotspot/share/opto/memnode.cpp
@@ -460,13 +460,12 @@
 
   {
     // Check all control edges of 'dom'.
 
     ResourceMark rm;
-    Arena* arena = Thread::current()->resource_area();
-    Node_List nlist(arena);
-    Unique_Node_List dom_list(arena);
+    Node_List nlist;
+    Unique_Node_List dom_list;
 
     dom_list.push(dom);
     bool only_dominating_controls = false;
 
     for (uint next = 0; next < dom_list.size(); next++) {
@@ -2265,11 +2264,10 @@
         return TypeKlassPtr::make(ik);
       }
       // See if we can become precise: no subklasses and no interface
       // (Note:  We need to support verified interfaces.)
       if (!ik->is_interface() && !ik->has_subklass()) {
-        //assert(!UseExactTypes, "this code should be useless with exact types");
         // Add a dependence; if any subclass added we need to recompile
         if (!ik->is_final()) {
           // %%% should use stronger assert_unique_concrete_subtype instead
           phase->C->dependencies()->assert_leaf_type(ik);
         }
@@ -2299,11 +2297,10 @@
         ciKlass *base_k = ak->as_obj_array_klass()->base_element_klass();
         if (base_k->is_loaded() && base_k->is_instance_klass()) {
           ciInstanceKlass *ik = base_k->as_instance_klass();
           // See if we can become precise: no subklasses and no interface
           if (!ik->is_interface() && !ik->has_subklass()) {
-            //assert(!UseExactTypes, "this code should be useless with exact types");
             // Add a dependence; if any subclass added we need to recompile
             if (!ik->is_final()) {
               phase->C->dependencies()->assert_leaf_type(ik);
             }
             // Return precise array klass
@@ -3358,11 +3355,11 @@
 }
 
 MemBarNode* MemBarNode::trailing_membar() const {
   ResourceMark rm;
   Node* trailing = (Node*)this;
-  VectorSet seen(Thread::current()->resource_area());
+  VectorSet seen;
   Node_Stack multis(0);
   do {
     Node* c = trailing;
     uint i = 0;
     do {
@@ -3402,11 +3399,11 @@
   return mb;
 }
 
 MemBarNode* MemBarNode::leading_membar() const {
   ResourceMark rm;
-  VectorSet seen(Thread::current()->resource_area());
+  VectorSet seen;
   Node_Stack regions(0);
   Node* leading = in(0);
   while (leading != NULL && (!leading->is_MemBar() || !leading->as_MemBar()->leading())) {
     while (leading == NULL || leading->is_top() || seen.test_set(leading->_idx)) {
       leading = NULL;
diff a/src/hotspot/share/opto/node.cpp b/src/hotspot/share/opto/node.cpp
--- a/src/hotspot/share/opto/node.cpp
+++ b/src/hotspot/share/opto/node.cpp
@@ -65,25 +65,31 @@
 //-------------------------- construct_node------------------------------------
 // Set a breakpoint here to identify where a particular node index is built.
 void Node::verify_construction() {
   _debug_orig = NULL;
   int old_debug_idx = Compile::debug_idx();
-  int new_debug_idx = old_debug_idx+1;
+  int new_debug_idx = old_debug_idx + 1;
   if (new_debug_idx > 0) {
     // Arrange that the lowest five decimal digits of _debug_idx
     // will repeat those of _idx. In case this is somehow pathological,
     // we continue to assign negative numbers (!) consecutively.
     const int mod = 100000;
     int bump = (int)(_idx - new_debug_idx) % mod;
-    if (bump < 0)  bump += mod;
+    if (bump < 0) {
+      bump += mod;
+    }
     assert(bump >= 0 && bump < mod, "");
     new_debug_idx += bump;
   }
   Compile::set_debug_idx(new_debug_idx);
-  set_debug_idx( new_debug_idx );
-  assert(Compile::current()->unique() < (INT_MAX - 1), "Node limit exceeded INT_MAX");
-  assert(Compile::current()->live_nodes() < Compile::current()->max_node_limit(), "Live Node limit exceeded limit");
+  set_debug_idx(new_debug_idx);
+  Compile* C = Compile::current();
+  assert(C->unique() < (INT_MAX - 1), "Node limit exceeded INT_MAX");
+  if (!C->phase_optimize_finished()) {
+    // Only check assert during parsing and optimization phase. Skip it while generating code.
+    assert(C->live_nodes() <= C->max_node_limit(), "Live Node limit exceeded limit");
+  }
   if (BreakAtNode != 0 && (_debug_idx == BreakAtNode || (int)_idx == BreakAtNode)) {
     tty->print_cr("BreakAtNode: _idx=%d _debug_idx=%d", _idx, _debug_idx);
     BREAKPOINT;
   }
 #if OPTO_DU_ITERATOR_ASSERT
@@ -1332,11 +1338,11 @@
 static void kill_dead_code( Node *dead, PhaseIterGVN *igvn ) {
   // Con's are a popular node to re-hit in the hash table again.
   if( dead->is_Con() ) return;
 
   ResourceMark rm;
-  Node_List  nstack(Thread::current()->resource_area());
+  Node_List nstack;
 
   Node *top = igvn->C->top();
   nstack.push(dead);
   bool has_irreducible_loop = igvn->C->has_irreducible_loop();
 
@@ -1608,24 +1614,22 @@
   return Compile::current()->root()->find(idx);
 }
 
 //------------------------------find-------------------------------------------
 Node* Node::find(int idx) const {
-  ResourceArea *area = Thread::current()->resource_area();
-  VectorSet old_space(area), new_space(area);
+  VectorSet old_space, new_space;
   Node* result = NULL;
-  find_recur(Compile::current(), result, (Node*) this, idx, false, &old_space, &new_space );
+  find_recur(Compile::current(), result, (Node*) this, idx, false, &old_space, &new_space);
   return result;
 }
 
 //------------------------------find_ctrl--------------------------------------
 // Find an ancestor to this node in the control history with given _idx
 Node* Node::find_ctrl(int idx) const {
-  ResourceArea *area = Thread::current()->resource_area();
-  VectorSet old_space(area), new_space(area);
+  VectorSet old_space, new_space;
   Node* result = NULL;
-  find_recur(Compile::current(), result, (Node*) this, idx, true, &old_space, &new_space );
+  find_recur(Compile::current(), result, (Node*)this, idx, true, &old_space, &new_space);
   return result;
 }
 #endif
 
 
@@ -2165,14 +2169,13 @@
 
 // Verify all nodes if verify_depth is negative
 void Node::verify(Node* n, int verify_depth) {
   assert(verify_depth != 0, "depth should not be 0");
   ResourceMark rm;
-  ResourceArea* area = Thread::current()->resource_area();
-  VectorSet old_space(area);
-  VectorSet new_space(area);
-  Node_List worklist(area);
+  VectorSet old_space;
+  VectorSet new_space;
+  Node_List worklist;
   worklist.push(n);
   Compile* C = Compile::current();
   uint last_index_on_current_depth = 0;
   verify_depth--; // Visiting the first node on depth 1
   // Only add nodes to worklist if verify_depth is negative (visit all nodes) or greater than 0
@@ -2237,11 +2240,11 @@
 #endif
 
 //------------------------------walk-------------------------------------------
 // Graph walk, with both pre-order and post-order functions
 void Node::walk(NFunc pre, NFunc post, void *env) {
-  VectorSet visited(Thread::current()->resource_area()); // Setup for local walk
+  VectorSet visited; // Setup for local walk
   walk_(pre, post, env, visited);
 }
 
 void Node::walk_(NFunc pre, NFunc post, void *env, VectorSet &visited) {
   if( visited.test_set(_idx) ) return;
diff a/src/hotspot/share/opto/node.hpp b/src/hotspot/share/opto/node.hpp
--- a/src/hotspot/share/opto/node.hpp
+++ b/src/hotspot/share/opto/node.hpp
@@ -1533,11 +1533,11 @@
 class Unique_Node_List : public Node_List {
   friend class VMStructs;
   VectorSet _in_worklist;
   uint _clock_index;            // Index in list where to pop from next
 public:
-  Unique_Node_List() : Node_List(), _in_worklist(Thread::current()->resource_area()), _clock_index(0) {}
+  Unique_Node_List() : Node_List(), _clock_index(0) {}
   Unique_Node_List(Arena *a) : Node_List(a), _in_worklist(a), _clock_index(0) {}
 
   void remove( Node *n );
   bool member( Node *n ) { return _in_worklist.test(n->_idx) != 0; }
   VectorSet& member_set(){ return _in_worklist; }
diff a/src/hotspot/share/opto/phaseX.cpp b/src/hotspot/share/opto/phaseX.cpp
--- a/src/hotspot/share/opto/phaseX.cpp
+++ b/src/hotspot/share/opto/phaseX.cpp
@@ -399,12 +399,11 @@
 
 
 //=============================================================================
 //------------------------------PhaseRemoveUseless-----------------------------
 // 1) Use a breadthfirst walk to collect useful nodes reachable from root.
-PhaseRemoveUseless::PhaseRemoveUseless(PhaseGVN *gvn, Unique_Node_List *worklist, PhaseNumber phase_num) : Phase(phase_num),
-  _useful(Thread::current()->resource_area()) {
+PhaseRemoveUseless::PhaseRemoveUseless(PhaseGVN* gvn, Unique_Node_List* worklist, PhaseNumber phase_num) : Phase(phase_num) {
 
   // Implementation requires 'UseLoopSafepoints == true' and an edge from root
   // to each SafePointNode at a backward branch.  Inserted in add_safepoint().
   if( !UseLoopSafepoints || !OptoRemoveUseless ) return;
 
@@ -454,11 +453,10 @@
                                      Unique_Node_List* worklist, Unique_Node_List* new_worklist,
                                      PhaseNumber phase_num) :
   PhaseRemoveUseless(gvn, worklist, Remove_Useless_And_Renumber_Live),
   _new_type_array(C->comp_arena()),
   _old2new_map(C->unique(), C->unique(), -1),
-  _delayed(Thread::current()->resource_area()),
   _is_pass_finished(false),
   _live_node_count(C->live_nodes())
 {
   assert(RenumberLiveNodes, "RenumberLiveNodes must be set to true for node renumbering to take place");
   assert(C->live_nodes() == _useful.size(), "the number of live nodes must match the number of useful nodes");
@@ -660,13 +658,13 @@
 void PhaseTransform::dump_types( ) const {
   _types.dump();
 }
 
 //------------------------------dump_nodes_and_types---------------------------
-void PhaseTransform::dump_nodes_and_types(const Node *root, uint depth, bool only_ctrl) {
-  VectorSet visited(Thread::current()->resource_area());
-  dump_nodes_and_types_recur( root, depth, only_ctrl, visited );
+void PhaseTransform::dump_nodes_and_types(const Node* root, uint depth, bool only_ctrl) {
+  VectorSet visited;
+  dump_nodes_and_types_recur(root, depth, only_ctrl, visited);
 }
 
 //------------------------------dump_nodes_and_types_recur---------------------
 void PhaseTransform::dump_nodes_and_types_recur( const Node *n, uint depth, bool only_ctrl, VectorSet &visited) {
   if( !n ) return;
diff a/src/hotspot/share/opto/stringopts.cpp b/src/hotspot/share/opto/stringopts.cpp
--- a/src/hotspot/share/opto/stringopts.cpp
+++ b/src/hotspot/share/opto/stringopts.cpp
@@ -582,12 +582,11 @@
 }
 
 
 PhaseStringOpts::PhaseStringOpts(PhaseGVN* gvn, Unique_Node_List*):
   Phase(StringOpts),
-  _gvn(gvn),
-  _visited(Thread::current()->resource_area()) {
+  _gvn(gvn) {
 
   assert(OptimizeStringConcat, "shouldn't be here");
 
   size_table_field = C->env()->Integer_klass()->get_field_by_name(ciSymbol::make("sizeTable"),
                                                                   ciSymbol::make("[I"), true);
diff a/src/hotspot/share/opto/subnode.cpp b/src/hotspot/share/opto/subnode.cpp
--- a/src/hotspot/share/opto/subnode.cpp
+++ b/src/hotspot/share/opto/subnode.cpp
@@ -1444,10 +1444,38 @@
     cmp->swap_edges(1, 2);
     cmp = phase->transform( cmp );
     return new BoolNode( cmp, _test.commute() );
   }
 
+  // Change "bool eq/ne (cmp (and X 16) 16)" into "bool ne/eq (cmp (and X 16) 0)".
+  if (cop == Op_CmpI &&
+      (_test._test == BoolTest::eq || _test._test == BoolTest::ne) &&
+      cmp1->Opcode() == Op_AndI && cmp2->Opcode() == Op_ConI &&
+      cmp1->in(2)->Opcode() == Op_ConI) {
+    const TypeInt *t12 = phase->type(cmp2)->isa_int();
+    const TypeInt *t112 = phase->type(cmp1->in(2))->isa_int();
+    if (t12 && t12->is_con() && t112 && t112->is_con() &&
+        t12->get_con() == t112->get_con() && is_power_of_2(t12->get_con())) {
+      Node *ncmp = phase->transform(new CmpINode(cmp1, phase->intcon(0)));
+      return new BoolNode(ncmp, _test.negate());
+    }
+  }
+
+  // Same for long type: change "bool eq/ne (cmp (and X 16) 16)" into "bool ne/eq (cmp (and X 16) 0)".
+  if (cop == Op_CmpL &&
+      (_test._test == BoolTest::eq || _test._test == BoolTest::ne) &&
+      cmp1->Opcode() == Op_AndL && cmp2->Opcode() == Op_ConL &&
+      cmp1->in(2)->Opcode() == Op_ConL) {
+    const TypeLong *t12 = phase->type(cmp2)->isa_long();
+    const TypeLong *t112 = phase->type(cmp1->in(2))->isa_long();
+    if (t12 && t12->is_con() && t112 && t112->is_con() &&
+        t12->get_con() == t112->get_con() && is_power_of_2(t12->get_con())) {
+      Node *ncmp = phase->transform(new CmpLNode(cmp1, phase->longcon(0)));
+      return new BoolNode(ncmp, _test.negate());
+    }
+  }
+
   // Change "bool eq/ne (cmp (xor X 1) 0)" into "bool ne/eq (cmp X 0)".
   // The XOR-1 is an idiom used to flip the sense of a bool.  We flip the
   // test instead.
   int cmp1_op = cmp1->Opcode();
   const TypeInt* cmp2_type = phase->type(cmp2)->isa_int();
diff a/src/hotspot/share/opto/type.cpp b/src/hotspot/share/opto/type.cpp
--- a/src/hotspot/share/opto/type.cpp
+++ b/src/hotspot/share/opto/type.cpp
@@ -2362,11 +2362,10 @@
   return _elem->empty() || _size->empty();
 }
 
 //--------------------------ary_must_be_exact----------------------------------
 bool TypeAry::ary_must_be_exact() const {
-  if (!UseExactTypes)       return false;
   // This logic looks at the element type of an array, and returns true
   // if the element type is either a primitive or a final instance class.
   // In such cases, an array built on this ary must have no subclasses.
   if (_elem == BOTTOM)      return false;  // general array not exact
   if (_elem == TOP   )      return false;  // inverted general array not exact
@@ -3453,17 +3452,15 @@
           deps->assert_abstract_with_unique_concrete_subtype(ik, sub);
           klass = ik = sub;
           klass_is_exact = sub->is_final();
         }
       }
-      if (!klass_is_exact && try_for_exact
-          && deps != NULL && UseExactTypes) {
-        if (!ik->is_interface() && !ik->has_subklass()) {
-          // Add a dependence; if concrete subclass added we need to recompile
-          deps->assert_leaf_type(ik);
-          klass_is_exact = true;
-        }
+      if (!klass_is_exact && try_for_exact && deps != NULL &&
+          !ik->is_interface() && !ik->has_subklass()) {
+        // Add a dependence; if concrete subclass added we need to recompile
+        deps->assert_leaf_type(ik);
+        klass_is_exact = true;
       }
     }
     return TypeInstPtr::make(TypePtr::BotPTR, klass, klass_is_exact, NULL, Offset(0), klass->flatten_array());
   } else if (klass->is_obj_array_klass()) {
     // Element is an object or inline type array. Recursively call ourself.
@@ -3803,12 +3800,11 @@
   assert( (!o && ptr != Constant) || (o && ptr == Constant),
           "constant pointers must have a value supplied" );
   // Ptr is never Null
   assert( ptr != Null, "NULL pointers are not typed" );
 
-  assert(instance_id <= 0 || xk || !UseExactTypes, "instances are always exactly typed");
-  if (!UseExactTypes)  xk = false;
+  assert(instance_id <= 0 || xk, "instances are always exactly typed");
   if (ptr == Constant) {
     // Note:  This case includes meta-object constants, such as methods.
     xk = true;
   } else if (k->is_loaded()) {
     ciInstanceKlass* ik = k->as_instance_klass();
@@ -3856,11 +3852,10 @@
 
 
 //-----------------------------cast_to_exactness-------------------------------
 const Type *TypeInstPtr::cast_to_exactness(bool klass_is_exact) const {
   if( klass_is_exact == _klass_is_exact ) return this;
-  if (!UseExactTypes)  return this;
   if (!_klass->is_loaded())  return this;
   ciInstanceKlass* ik = _klass->as_instance_klass();
   if( (ik->is_final() || _const_oop) )  return this;  // cannot clear xk
   if( ik->is_interface() )              return this;  // cannot set xk
   return make(ptr(), klass(), klass_is_exact, const_oop(), _offset, _flat_array, _instance_id, _speculative, _inline_depth);
@@ -4407,13 +4402,12 @@
 //------------------------------make-------------------------------------------
 const TypeAryPtr* TypeAryPtr::make(PTR ptr, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,
                                    int instance_id, const TypePtr* speculative, int inline_depth) {
   assert(!(k == NULL && ary->_elem->isa_int()),
          "integral arrays must be pre-equipped with a class");
-  if (!xk) xk = ary->ary_must_be_exact();
-  assert(instance_id <= 0 || xk || !UseExactTypes, "instances are always exactly typed");
-  if (!UseExactTypes)  xk = (ptr == Constant);
+  if (!xk)  xk = ary->ary_must_be_exact();
+  assert(instance_id <= 0 || xk, "instances are always exactly typed");
   return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, field_offset, instance_id, false, speculative, inline_depth))->hashcons();
 }
 
 //------------------------------make-------------------------------------------
 const TypeAryPtr* TypeAryPtr::make(PTR ptr, ciObject* o, const TypeAry *ary, ciKlass* k, bool xk, Offset offset, Offset field_offset,
@@ -4421,12 +4415,11 @@
                                    bool is_autobox_cache) {
   assert(!(k == NULL && ary->_elem->isa_int()),
          "integral arrays must be pre-equipped with a class");
   assert( (ptr==Constant && o) || (ptr!=Constant && !o), "" );
   if (!xk)  xk = (o != NULL) || ary->ary_must_be_exact();
-  assert(instance_id <= 0 || xk || !UseExactTypes, "instances are always exactly typed");
-  if (!UseExactTypes)  xk = (ptr == Constant);
+  assert(instance_id <= 0 || xk, "instances are always exactly typed");
   return (TypeAryPtr*)(new TypeAryPtr(ptr, o, ary, k, xk, offset, field_offset, instance_id, is_autobox_cache, speculative, inline_depth))->hashcons();
 }
 
 //------------------------------cast_to_ptr_type-------------------------------
 const Type *TypeAryPtr::cast_to_ptr_type(PTR ptr) const {
@@ -4436,11 +4429,10 @@
 
 
 //-----------------------------cast_to_exactness-------------------------------
 const Type *TypeAryPtr::cast_to_exactness(bool klass_is_exact) const {
   if( klass_is_exact == _klass_is_exact ) return this;
-  if (!UseExactTypes)  return this;
   if (_ary->ary_must_be_exact())  return this;  // cannot clear xk
 
   const TypeAry* new_ary = _ary;
   if (klass() != NULL && klass()->is_obj_array_klass() && klass_is_exact) {
     // An object array can't be flat or null-free if the klass is exact
@@ -5514,11 +5506,10 @@
 
 
 //-----------------------------cast_to_exactness-------------------------------
 const Type *TypeKlassPtr::cast_to_exactness(bool klass_is_exact) const {
   if( klass_is_exact == _klass_is_exact ) return this;
-  if (!UseExactTypes)  return this;
   return make(klass_is_exact ? Constant : NotNull, _klass, _offset, _flat_array);
 }
 
 
 //-----------------------------as_instance_type--------------------------------
diff a/src/hotspot/share/prims/jvm.cpp b/src/hotspot/share/prims/jvm.cpp
--- a/src/hotspot/share/prims/jvm.cpp
+++ b/src/hotspot/share/prims/jvm.cpp
@@ -492,15 +492,10 @@
 
 
 JVM_ENTRY_NO_ENV(void, JVM_GC(void))
   JVMWrapper("JVM_GC");
   if (!DisableExplicitGC) {
-    if (AsyncDeflateIdleMonitors) {
-      // AsyncDeflateIdleMonitors needs to know when System.gc() is
-      // called so any special deflation can be done at a safepoint.
-      ObjectSynchronizer::set_is_special_deflation_requested(true);
-    }
     Universe::heap()->collect(GCCause::_java_lang_system_gc);
   }
 JVM_END
 
 
diff a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -1211,12 +1211,12 @@
   if (java_thread == calling_thread) {
     err = get_owned_monitors(calling_thread, java_thread, owned_monitors_list);
   } else {
     // get owned monitors info with handshake
     GetOwnedMonitorInfoClosure op(calling_thread, this, owned_monitors_list);
-    Handshake::execute_direct(&op, java_thread);
-    err = op.result();
+    bool executed = Handshake::execute_direct(&op, java_thread);
+    err = executed ? op.result() : JVMTI_ERROR_THREAD_NOT_ALIVE;
   }
   jint owned_monitor_count = owned_monitors_list->length();
   if (err == JVMTI_ERROR_NONE) {
     if ((err = allocate(owned_monitor_count * sizeof(jobject *),
                       (unsigned char**)owned_monitors_ptr)) == JVMTI_ERROR_NONE) {
@@ -1256,12 +1256,12 @@
   if (java_thread == calling_thread) {
     err = get_owned_monitors(calling_thread, java_thread, owned_monitors_list);
   } else {
     // get owned monitors info with handshake
     GetOwnedMonitorInfoClosure op(calling_thread, this, owned_monitors_list);
-    Handshake::execute_direct(&op, java_thread);
-    err = op.result();
+    bool executed = Handshake::execute_direct(&op, java_thread);
+    err = executed ? op.result() : JVMTI_ERROR_THREAD_NOT_ALIVE;
   }
 
   jint owned_monitor_count = owned_monitors_list->length();
   if (err == JVMTI_ERROR_NONE) {
     if ((err = allocate(owned_monitor_count * sizeof(jvmtiMonitorStackDepthInfo),
@@ -1300,12 +1300,12 @@
   if (java_thread == calling_thread) {
     err = get_current_contended_monitor(calling_thread, java_thread, monitor_ptr);
   } else {
     // get contended monitor information with handshake
     GetCurrentContendedMonitorClosure op(calling_thread, this, monitor_ptr);
-    Handshake::execute_direct(&op, java_thread);
-    err = op.result();
+    bool executed = Handshake::execute_direct(&op, java_thread);
+    err = executed ? op.result() : JVMTI_ERROR_THREAD_NOT_ALIVE;
   }
   return err;
 } /* end GetCurrentContendedMonitor */
 
 
diff a/src/hotspot/share/prims/jvmtiEnvBase.cpp b/src/hotspot/share/prims/jvmtiEnvBase.cpp
--- a/src/hotspot/share/prims/jvmtiEnvBase.cpp
+++ b/src/hotspot/share/prims/jvmtiEnvBase.cpp
@@ -1510,16 +1510,28 @@
   }
 }
 
 void
 GetOwnedMonitorInfoClosure::do_thread(Thread *target) {
-  _result = ((JvmtiEnvBase *)_env)->get_owned_monitors(_calling_thread, (JavaThread *)target, _owned_monitors_list);
+  assert(target->is_Java_thread(), "just checking");
+  JavaThread *jt = (JavaThread *)target;
+  if (!jt->is_exiting() && (jt->threadObj() != NULL)) {
+    _result = ((JvmtiEnvBase *)_env)->get_owned_monitors(_calling_thread,
+                                                         jt,
+                                                         _owned_monitors_list);
+  }
 }
 
 void
 GetCurrentContendedMonitorClosure::do_thread(Thread *target) {
-  _result = ((JvmtiEnvBase *)_env)->get_current_contended_monitor(_calling_thread, (JavaThread *)target, _owned_monitor_ptr);
+  assert(target->is_Java_thread(), "just checking");
+  JavaThread *jt = (JavaThread *)target;
+  if (!jt->is_exiting() && (jt->threadObj() != NULL)) {
+    _result = ((JvmtiEnvBase *)_env)->get_current_contended_monitor(_calling_thread,
+                                                                    jt,
+                                                                    _owned_monitor_ptr);
+  }
 }
 
 void
 VM_GetStackTrace::doit() {
   _result = JVMTI_ERROR_THREAD_NOT_ALIVE;
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -1089,10 +1089,11 @@
   return ret == e;
 } UNSAFE_END
 
 UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSetInt(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jint e, jint x)) {
   oop p = JNIHandles::resolve(obj);
+  GuardUnsafeAccess guard(thread);
   if (p == NULL) {
     volatile jint* addr = (volatile jint*)index_oop_from_field_offset_long(p, offset);
     return RawAccess<>::atomic_cmpxchg(addr, e, x) == e;
   } else {
     assert_field_offset_sane(p, offset);
@@ -1100,10 +1101,11 @@
   }
 } UNSAFE_END
 
 UNSAFE_ENTRY(jboolean, Unsafe_CompareAndSetLong(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jlong e, jlong x)) {
   oop p = JNIHandles::resolve(obj);
+  GuardUnsafeAccess guard(thread);
   if (p == NULL) {
     volatile jlong* addr = (volatile jlong*)index_oop_from_field_offset_long(p, offset);
     return RawAccess<>::atomic_cmpxchg(addr, e, x) == e;
   } else {
     assert_field_offset_sane(p, offset);
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -39,10 +39,11 @@
 #include "gc/shared/concurrentGCBreakpoints.hpp"
 #include "gc/shared/gcConfig.hpp"
 #include "gc/shared/genArguments.hpp"
 #include "gc/shared/genCollectedHeap.hpp"
 #include "jvmtifiles/jvmtiEnv.hpp"
+#include "logging/log.hpp"
 #include "memory/filemap.hpp"
 #include "memory/heapShared.inline.hpp"
 #include "memory/metaspaceShared.hpp"
 #include "memory/metadataFactory.hpp"
 #include "memory/iterator.inline.hpp"
@@ -478,16 +479,10 @@
   THROW_MSG_0(vmSymbols::java_lang_UnsupportedOperationException(), "WB_G1InConcurrentMark: G1 GC is not enabled");
 WB_END
 
 WB_ENTRY(jboolean, WB_G1StartMarkCycle(JNIEnv* env, jobject o))
   if (UseG1GC) {
-    if (AsyncDeflateIdleMonitors) {
-      // AsyncDeflateIdleMonitors needs to know when System.gc() or
-      // the equivalent is called so any special clean up can be done
-      // at a safepoint, e.g., TestHumongousClassLoader.java.
-      ObjectSynchronizer::set_is_special_deflation_requested(true);
-    }
     G1CollectedHeap* g1h = G1CollectedHeap::heap();
     if (!g1h->concurrent_mark()->cm_thread()->during_cycle()) {
       g1h->collect(GCCause::_wb_conc_mark);
       return true;
     }
@@ -1455,16 +1450,10 @@
   jchar* name = java_lang_String::as_unicode_string(JNIHandles::resolve(javaString), len, CHECK_false);
   return (StringTable::lookup(name, len) != NULL);
 WB_END
 
 WB_ENTRY(void, WB_FullGC(JNIEnv* env, jobject o))
-  if (AsyncDeflateIdleMonitors) {
-    // AsyncDeflateIdleMonitors needs to know when System.gc() or
-    // the equivalent is called so any special clean up can be done
-    // at a safepoint, e.g., TestHumongousClassLoader.java.
-    ObjectSynchronizer::set_is_special_deflation_requested(true);
-  }
   Universe::heap()->soft_ref_policy()->set_should_clear_all_soft_refs(true);
   Universe::heap()->collect(GCCause::_wb_full_gc);
 #if INCLUDE_G1GC
   if (UseG1GC) {
     // Needs to be cleared explicitly for G1
@@ -1809,18 +1798,16 @@
 WB_ENTRY(jboolean, WB_IsMonitorInflated(JNIEnv* env, jobject wb, jobject obj))
   oop obj_oop = JNIHandles::resolve(obj);
   return (jboolean) obj_oop->mark().has_monitor();
 WB_END
 
+WB_ENTRY(jboolean, WB_DeflateIdleMonitors(JNIEnv* env, jobject wb))
+  log_info(monitorinflation)("WhiteBox initiated DeflateIdleMonitors");
+  return ObjectSynchronizer::request_deflate_idle_monitors();
+WB_END
+
 WB_ENTRY(void, WB_ForceSafepoint(JNIEnv* env, jobject wb))
-  if (AsyncDeflateIdleMonitors) {
-    // AsyncDeflateIdleMonitors needs to know when System.gc() or
-    // the equivalent is called so any special clean up can be done
-    // at a safepoint, e.g., TestRTMTotalCountIncrRate.java or
-    // TestUseRTMForStackLocks.java.
-    ObjectSynchronizer::set_is_special_deflation_requested(true);
-  }
   VM_ForceSafepoint force_safepoint_op;
   VMThread::execute(&force_safepoint_op);
 WB_END
 
 WB_ENTRY(jlong, WB_GetConstantPool(JNIEnv* env, jobject wb, jclass klass))
@@ -2572,10 +2559,11 @@
                                                       (void*)&WB_AddModuleExportsToAllUnnamed },
   {CC"AddModuleExportsToAll", CC"(Ljava/lang/Object;Ljava/lang/String;)V",
                                                       (void*)&WB_AddModuleExportsToAll },
   {CC"assertMatchingSafepointCalls", CC"(ZZ)V",       (void*)&WB_AssertMatchingSafepointCalls },
   {CC"assertSpecialLock",  CC"(ZZ)V",                 (void*)&WB_AssertSpecialLock },
+  {CC"deflateIdleMonitors", CC"()Z",                  (void*)&WB_DeflateIdleMonitors },
   {CC"isMonitorInflated0", CC"(Ljava/lang/Object;)Z", (void*)&WB_IsMonitorInflated  },
   {CC"forceSafepoint",     CC"()V",                   (void*)&WB_ForceSafepoint     },
   {CC"getConstantPool0",   CC"(Ljava/lang/Class;)J",  (void*)&WB_GetConstantPool    },
   {CC"getConstantPoolCacheIndexTag0", CC"()I",  (void*)&WB_GetConstantPoolCacheIndexTag},
   {CC"getConstantPoolCacheLength0", CC"(Ljava/lang/Class;)I",  (void*)&WB_GetConstantPoolCacheLength},
diff a/src/hotspot/share/runtime/javaCalls.cpp b/src/hotspot/share/runtime/javaCalls.cpp
--- a/src/hotspot/share/runtime/javaCalls.cpp
+++ b/src/hotspot/share/runtime/javaCalls.cpp
@@ -28,10 +28,13 @@
 #include "code/nmethod.hpp"
 #include "compiler/compilationPolicy.hpp"
 #include "compiler/compileBroker.hpp"
 #include "interpreter/interpreter.hpp"
 #include "interpreter/linkResolver.hpp"
+#if INCLUDE_JVMCI
+#include "jvmci/jvmciJavaClasses.hpp"
+#endif
 #include "memory/universe.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/inlineKlass.hpp"
 #include "prims/jniCheck.hpp"
@@ -360,31 +363,19 @@
   assert(thread->is_Java_thread(), "must be called by a java thread");
   assert(method.not_null(), "must have a method to call");
   assert(!SafepointSynchronize::is_at_safepoint(), "call to Java code during VM operation");
   assert(!thread->handle_area()->no_handle_mark_active(), "cannot call out to Java here");
 
-#if INCLUDE_JVMCI
-  // Gets the nmethod (if any) that should be called instead of normal target
-  nmethod* alternative_target = args->alternative_target();
-  if (alternative_target == NULL) {
-#endif
-// Verify the arguments
-
-  if (CheckJNICalls)  {
+  // Verify the arguments
+  if (JVMCI_ONLY(args->alternative_target().is_null() &&) (DEBUG_ONLY(true ||) CheckJNICalls)) {
     args->verify(method, result->get_type());
   }
-  else debug_only(args->verify(method, result->get_type()));
-#if INCLUDE_JVMCI
-  }
-#else
-
-  // Ignore call if method is empty
+  // Ignore call if method is empty
   if (method->is_empty_method()) {
     assert(result->get_type() == T_VOID, "an empty method must return a void value");
     return;
   }
-#endif
 
 #ifdef ASSERT
   { InstanceKlass* holder = method->method_holder();
     // A klass might not be initialized since JavaCall's might be used during the executing of
     // the <clinit>. For example, a Thread.start might start executing on an object that is
@@ -428,21 +419,10 @@
   } else {
     // Touch pages checked if the OS needs them to be touched to be mapped.
     os::map_stack_shadow_pages(sp);
   }
 
-#if INCLUDE_JVMCI
-  if (alternative_target != NULL) {
-    if (alternative_target->is_alive() && !alternative_target->is_unloading()) {
-      thread->set_jvmci_alternate_call_target(alternative_target->verified_entry_point());
-      entry_point = method->adapter()->get_i2c_entry();
-    } else {
-      THROW(vmSymbols::jdk_vm_ci_code_InvalidInstalledCodeException());
-    }
-  }
-#endif
-
   jobject value_buffer = NULL;
   if (InlineTypeReturnedAsFields && result->get_type() == T_INLINE_TYPE) {
     // Pre allocate a buffered inline type in case the result is returned
     // flattened by compiled code
     InlineKlass* vk = method->returned_inline_type(thread);
@@ -459,11 +439,24 @@
 
       // NOTE: if we move the computation of the result_val_address inside
       // the call to call_stub, the optimizer produces wrong code.
       intptr_t* result_val_address = (intptr_t*)(result->get_value_addr());
       intptr_t* parameter_address = args->parameters();
-
+#if INCLUDE_JVMCI
+      // Gets the alternative target (if any) that should be called
+      Handle alternative_target = args->alternative_target();
+      if (!alternative_target.is_null()) {
+        // Must extract verified entry point from HotSpotNmethod after VM to Java
+        // transition in JavaCallWrapper constructor so that it is safe with
+        // respect to nmethod sweeping.
+        address verified_entry_point = (address) HotSpotJVMCI::InstalledCode::entryPoint(NULL, alternative_target());
+        if (verified_entry_point != NULL) {
+          thread->set_jvmci_alternate_call_target(verified_entry_point);
+          entry_point = method->adapter()->get_i2c_entry();
+        }
+      }
+#endif
       StubRoutines::call_stub()(
         (address)&link,
         // (intptr_t*)&(result->_value), // see NOTE above (compiler problem)
         result_val_address,          // see NOTE above (compiler problem)
         result_type,
diff a/src/hotspot/share/runtime/synchronizer.cpp b/src/hotspot/share/runtime/synchronizer.cpp
--- a/src/hotspot/share/runtime/synchronizer.cpp
+++ b/src/hotspot/share/runtime/synchronizer.cpp
@@ -119,11 +119,10 @@
 static volatile intptr_t gInflationLocks[NINFLATIONLOCKS];
 
 // global list of blocks of monitors
 PaddedObjectMonitor* ObjectSynchronizer::g_block_list = NULL;
 bool volatile ObjectSynchronizer::_is_async_deflation_requested = false;
-bool volatile ObjectSynchronizer::_is_special_deflation_requested = false;
 jlong ObjectSynchronizer::_last_async_deflation_time_ns = 0;
 
 struct ObjectMonitorListGlobals {
   char         _pad_prefix[OM_CACHE_LINE_SIZE];
   // These are highly shared list related variables.
@@ -1335,34 +1334,64 @@
       monitors_used_above_threshold()) {
     // It's been longer than our specified deflate interval and there
     // are too many monitors in use. We don't deflate more frequently
     // than AsyncDeflationInterval (unless is_async_deflation_requested)
     // in order to not swamp the ServiceThread.
-    _last_async_deflation_time_ns = os::javaTimeNanos();
     return true;
   }
   return false;
 }
 
 bool ObjectSynchronizer::is_safepoint_deflation_needed() {
-  if (!AsyncDeflateIdleMonitors) {
-    if (monitors_used_above_threshold()) {
-      // Too many monitors in use.
-      return true;
+  return !AsyncDeflateIdleMonitors &&
+         monitors_used_above_threshold();  // Too many monitors in use.
+}
+
+bool ObjectSynchronizer::request_deflate_idle_monitors() {
+  bool is_JavaThread = Thread::current()->is_Java_thread();
+  bool ret_code = false;
+
+  if (AsyncDeflateIdleMonitors) {
+    jlong last_time = last_async_deflation_time_ns();
+    set_is_async_deflation_requested(true);
+    {
+      MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);
+      ml.notify_all();
     }
-    return false;
-  }
-  if (is_special_deflation_requested()) {
-    // For AsyncDeflateIdleMonitors only do a safepoint deflation
-    // if there is a special deflation request.
-    return true;
+    const int N_CHECKS = 5;
+    for (int i = 0; i < N_CHECKS; i++) {  // sleep for at most 5 seconds
+      if (last_async_deflation_time_ns() > last_time) {
+        log_info(monitorinflation)("Async Deflation happened after %d check(s).", i);
+        ret_code = true;
+        break;
+      }
+      if (is_JavaThread) {
+        // JavaThread has to honor the blocking protocol.
+        ThreadBlockInVM tbivm(JavaThread::current());
+        os::naked_short_sleep(999);  // sleep for almost 1 second
+      } else {
+        os::naked_short_sleep(999);  // sleep for almost 1 second
+      }
+    }
+    if (!ret_code) {
+      log_info(monitorinflation)("Async Deflation DID NOT happen after %d checks.", N_CHECKS);
+    }
+  } else {
+    // Only need to force this safepoint if we are not using async
+    // deflation. The VMThread won't call this function before the
+    // final safepoint if we are not using async deflation so we
+    // don't have to reason about the VMThread executing a VM-op here.
+    VM_ForceSafepoint force_safepoint_op;
+    VMThread::execute(&force_safepoint_op);
+    ret_code = true;
   }
-  return false;
+
+  return ret_code;
 }
 
 jlong ObjectSynchronizer::time_since_last_async_deflation_ms() {
-  return (os::javaTimeNanos() - _last_async_deflation_time_ns) / (NANOUNITS / MILLIUNITS);
+  return (os::javaTimeNanos() - last_async_deflation_time_ns()) / (NANOUNITS / MILLIUNITS);
 }
 
 void ObjectSynchronizer::oops_do(OopClosure* f) {
   // We only scan the global used list here (for moribund threads), and
   // the thread-local monitors in Thread::oops_do().
@@ -2047,13 +2076,12 @@
   assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
 
   // The per-thread in-use lists are handled in
   // ParallelSPCleanupThreadClosure::do_thread().
 
-  if (!AsyncDeflateIdleMonitors || is_special_deflation_requested()) {
-    // Use the older mechanism for the global in-use list or if a
-    // special deflation has been requested before the safepoint.
+  if (!AsyncDeflateIdleMonitors) {
+    // Use the older mechanism for the global in-use list.
     ObjectSynchronizer::deflate_idle_monitors(counters);
     return;
   }
 
   log_debug(monitorinflation)("requesting async deflation of idle monitors.");
@@ -2468,14 +2496,12 @@
 void ObjectSynchronizer::deflate_idle_monitors(DeflateMonitorCounters* counters) {
   assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
 
   if (AsyncDeflateIdleMonitors) {
     // Nothing to do when global idle ObjectMonitors are deflated using
-    // a JavaThread unless a special deflation has been requested.
-    if (!is_special_deflation_requested()) {
-      return;
-    }
+    // a JavaThread.
+    return;
   }
 
   bool deflated = false;
 
   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
@@ -2564,10 +2590,11 @@
                              Atomic::load(&om_list_globals._in_use_count),
                              Atomic::load(&om_list_globals._free_count),
                              Atomic::load(&om_list_globals._wait_count));
 
   // The ServiceThread's async deflation request has been processed.
+  _last_async_deflation_time_ns = os::javaTimeNanos();
   set_is_async_deflation_requested(false);
 
   if (Atomic::load(&om_list_globals._wait_count) > 0) {
     // There are deflated ObjectMonitors waiting for a handshake
     // (or a safepoint) for safety.
@@ -2639,20 +2666,10 @@
   } else {
     OM_PERFDATA_OP(MonExtant, inc(Atomic::load(&target->om_in_use_count)));
   }
 
   do {
-    if (saved_mid_in_use_p != NULL) {
-      // We looped around because deflate_monitor_list_using_JT()
-      // detected a pending safepoint. Honoring the safepoint is good,
-      // but as long as is_special_deflation_requested() is supported,
-      // we can't safely restart using saved_mid_in_use_p. That saved
-      // ObjectMonitor could have been deflated by safepoint based
-      // deflation and would no longer be on the in-use list where we
-      // originally found it.
-      saved_mid_in_use_p = NULL;
-    }
     int local_deflated_count;
     if (is_global) {
       local_deflated_count =
           deflate_monitor_list_using_JT(&om_list_globals._in_use_list,
                                         &om_list_globals._in_use_count,
@@ -2731,14 +2748,13 @@
   // monitors. Note: if the work is split among more than one
   // worker thread, then the reported time will likely be more
   // than a beginning to end measurement of the phase.
   log_info(safepoint, cleanup)("deflating per-thread idle monitors, %3.7f secs, monitors=%d", counters->per_thread_times, counters->per_thread_scavenged);
 
-  bool needs_special_deflation = is_special_deflation_requested();
-  if (AsyncDeflateIdleMonitors && !needs_special_deflation) {
+  if (AsyncDeflateIdleMonitors) {
     // Nothing to do when idle ObjectMonitors are deflated using
-    // a JavaThread unless a special deflation has been requested.
+    // a JavaThread.
     return;
   }
 
   if (log_is_enabled(Debug, monitorinflation)) {
     // exit_globals()'s call to audit_and_print_stats() is done
@@ -2759,21 +2775,18 @@
   OM_PERFDATA_OP(Deflations, inc(counters->n_scavenged));
   OM_PERFDATA_OP(MonExtant, set_value(counters->n_in_circulation));
 
   GVars.stw_random = os::random();
   GVars.stw_cycle++;
-
-  if (needs_special_deflation) {
-    set_is_special_deflation_requested(false);  // special deflation is done
-  }
 }
 
 void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {
   assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
 
-  if (AsyncDeflateIdleMonitors && !is_special_deflation_requested()) {
-    // Nothing to do if a special deflation has NOT been requested.
+  if (AsyncDeflateIdleMonitors) {
+    // Nothing to do when per-thread idle ObjectMonitors are deflated
+    // using a JavaThread.
     return;
   }
 
   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
   ObjectMonitor* free_tail_p = NULL;
diff a/src/hotspot/share/runtime/synchronizer.hpp b/src/hotspot/share/runtime/synchronizer.hpp
--- a/src/hotspot/share/runtime/synchronizer.hpp
+++ b/src/hotspot/share/runtime/synchronizer.hpp
@@ -159,13 +159,13 @@
                                        ObjectMonitor** free_head_p,
                                        ObjectMonitor** free_tail_p);
   static bool is_async_deflation_needed();
   static bool is_safepoint_deflation_needed();
   static bool is_async_deflation_requested() { return _is_async_deflation_requested; }
-  static bool is_special_deflation_requested() { return _is_special_deflation_requested; }
+  static jlong last_async_deflation_time_ns() { return _last_async_deflation_time_ns; }
+  static bool request_deflate_idle_monitors();  // for whitebox test support and VM exit logging
   static void set_is_async_deflation_requested(bool new_value) { _is_async_deflation_requested = new_value; }
-  static void set_is_special_deflation_requested(bool new_value) { _is_special_deflation_requested = new_value; }
   static jlong time_since_last_async_deflation_ms();
   static void oops_do(OopClosure* f);
   // Process oops in thread local used monitors
   static void thread_local_used_oops_do(Thread* thread, OopClosure* f);
 
@@ -198,11 +198,10 @@
 
   enum { _BLOCKSIZE = 128 };
   // global list of blocks of monitors
   static PaddedObjectMonitor* g_block_list;
   static volatile bool _is_async_deflation_requested;
-  static volatile bool _is_special_deflation_requested;
   static jlong         _last_async_deflation_time_ns;
 
   // Function to prepend new blocks to the appropriate lists:
   static void prepend_block_to_lists(PaddedObjectMonitor* new_blk);
 
diff a/src/hotspot/share/runtime/vmOperations.cpp b/src/hotspot/share/runtime/vmOperations.cpp
--- a/src/hotspot/share/runtime/vmOperations.cpp
+++ b/src/hotspot/share/runtime/vmOperations.cpp
@@ -430,15 +430,14 @@
   }
 }
 
 bool VM_Exit::doit_prologue() {
   if (AsyncDeflateIdleMonitors && log_is_enabled(Info, monitorinflation)) {
-    // AsyncDeflateIdleMonitors does a special deflation at the VM_Exit
-    // safepoint in order to reduce the in-use monitor population that
-    // is reported by ObjectSynchronizer::log_in_use_monitor_details()
-    // at VM exit.
-    ObjectSynchronizer::set_is_special_deflation_requested(true);
+    // AsyncDeflateIdleMonitors does a special deflation in order
+    // to reduce the in-use monitor population that is reported by
+    // ObjectSynchronizer::log_in_use_monitor_details() at VM exit.
+    ObjectSynchronizer::request_deflate_idle_monitors();
   }
   return true;
 }
 
 void VM_Exit::doit() {
diff a/src/java.base/share/classes/java/io/ObjectStreamClass.java b/src/java.base/share/classes/java/io/ObjectStreamClass.java
--- a/src/java.base/share/classes/java/io/ObjectStreamClass.java
+++ b/src/java.base/share/classes/java/io/ObjectStreamClass.java
@@ -25,10 +25,11 @@
 
 package java.io;
 
 import java.lang.invoke.MethodHandle;
 import java.lang.invoke.MethodHandles;
+import java.lang.invoke.MethodType;
 import java.lang.ref.Reference;
 import java.lang.ref.ReferenceQueue;
 import java.lang.ref.SoftReference;
 import java.lang.ref.WeakReference;
 import java.lang.reflect.Constructor;
@@ -54,10 +55,11 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.Comparator;
 import java.util.HashSet;
+import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 import jdk.internal.misc.Unsafe;
 import jdk.internal.reflect.CallerSensitive;
@@ -190,12 +192,18 @@
     /** data layout of serialized objects described by this class desc */
     private volatile ClassDataSlot[] dataLayout;
 
     /** serialization-appropriate constructor, or null if none */
     private Constructor<?> cons;
-    /** record canonical constructor, or null */
+    /** record canonical constructor (shared among OSCs for same class), or null */
     private MethodHandle canonicalCtr;
+    /** cache of record deserialization constructors per unique set of stream fields
+     * (shared among OSCs for same class), or null */
+    private DeserializationConstructorsCache deserializationCtrs;
+    /** session-cache of record deserialization constructor
+     * (in de-serialized OSC only), or null */
+    private MethodHandle deserializationCtr;
     /** protection domains that need to be checked when calling the constructor */
     private ProtectionDomain[] domains;
 
     /** class-defined writeObject method, or null if none */
     private Method writeObjectMethod;
@@ -525,10 +533,11 @@
                         fields = NO_FIELDS;
                     }
 
                     if (isRecord) {
                         canonicalCtr = canonicalRecordCtr(cl);
+                        deserializationCtrs = new DeserializationConstructorsCache();
                     } else if (externalizable) {
                         cons = getExternalizableConstructor(cl);
                     } else {
                         cons = getSerializableConstructor(cl);
                         writeObjectMethod = getPrivateMethod(cl, "writeObject",
@@ -740,14 +749,10 @@
                 }
             }
         }
 
         this.cl = cl;
-        if (cl != null) {
-            this.isRecord = isRecord(cl);
-            this.canonicalCtr = osc.canonicalCtr;
-        }
         this.resolveEx = resolveEx;
         this.superDesc = superDesc;
         name = model.name;
         this.suid = suid;
         isProxy = false;
@@ -760,10 +765,15 @@
         primDataSize = model.primDataSize;
         numObjFields = model.numObjFields;
 
         if (osc != null) {
             localDesc = osc;
+            isRecord = localDesc.isRecord;
+            // canonical record constructor is shared
+            canonicalCtr = localDesc.canonicalCtr;
+            // cache of deserialization constructors is shared
+            deserializationCtrs = localDesc.deserializationCtrs;
             writeObjectMethod = localDesc.writeObjectMethod;
             readObjectMethod = localDesc.readObjectMethod;
             readObjectNoDataMethod = localDesc.readObjectNoDataMethod;
             writeReplaceMethod = localDesc.writeReplaceMethod;
             readResolveMethod = localDesc.readResolveMethod;
@@ -2532,36 +2542,178 @@
                 return false;
             }
         }
     }
 
+    // a LRA cache of record deserialization constructors
+    @SuppressWarnings("serial")
+    private static final class DeserializationConstructorsCache
+        extends ConcurrentHashMap<DeserializationConstructorsCache.Key, MethodHandle>  {
+
+        // keep max. 10 cached entries - when the 11th element is inserted the oldest
+        // is removed and 10 remains - 11 is the biggest map size where internal
+        // table of 16 elements is sufficient (inserting 12th element would resize it to 32)
+        private static final int MAX_SIZE = 10;
+        private Key.Impl first, last; // first and last in FIFO queue
+
+        DeserializationConstructorsCache() {
+            // start small - if there is more than one shape of ObjectStreamClass
+            // deserialized, there will typically be two (current version and previous version)
+            super(2);
+        }
+
+        MethodHandle get(ObjectStreamField[] fields) {
+            return get(new Key.Lookup(fields));
+        }
+
+        synchronized MethodHandle putIfAbsentAndGet(ObjectStreamField[] fields, MethodHandle mh) {
+            Key.Impl key = new Key.Impl(fields);
+            var oldMh = putIfAbsent(key, mh);
+            if (oldMh != null) return oldMh;
+            // else we did insert new entry -> link the new key as last
+            if (last == null) {
+                last = first = key;
+            } else {
+                last = (last.next = key);
+            }
+            // may need to remove first
+            if (size() > MAX_SIZE) {
+                assert first != null;
+                remove(first);
+                first = first.next;
+                if (first == null) {
+                    last = null;
+                }
+            }
+            return mh;
+        }
+
+        // a key composed of ObjectStreamField[] names and types
+        static abstract class Key {
+            abstract int length();
+            abstract String fieldName(int i);
+            abstract Class<?> fieldType(int i);
+
+            @Override
+            public final int hashCode() {
+                int n = length();
+                int h = 0;
+                for (int i = 0; i < n; i++) h = h * 31 + fieldType(i).hashCode();
+                for (int i = 0; i < n; i++) h = h * 31 + fieldName(i).hashCode();
+                return h;
+            }
+
+            @Override
+            public final boolean equals(Object obj) {
+                if (!(obj instanceof Key)) return false;
+                Key other = (Key) obj;
+                int n = length();
+                if (n != other.length()) return false;
+                for (int i = 0; i < n; i++) if (fieldType(i) != other.fieldType(i)) return false;
+                for (int i = 0; i < n; i++) if (!fieldName(i).equals(other.fieldName(i))) return false;
+                return true;
+            }
+
+            // lookup key - just wraps ObjectStreamField[]
+            static final class Lookup extends Key {
+                final ObjectStreamField[] fields;
+
+                Lookup(ObjectStreamField[] fields) { this.fields = fields; }
+
+                @Override
+                int length() { return fields.length; }
+
+                @Override
+                String fieldName(int i) { return fields[i].getName(); }
+
+                @Override
+                Class<?> fieldType(int i) { return fields[i].getType(); }
+            }
+
+            // real key - copies field names and types and forms FIFO queue in cache
+            static final class Impl extends Key {
+                Impl next;
+                final String[] fieldNames;
+                final Class<?>[] fieldTypes;
+
+                Impl(ObjectStreamField[] fields) {
+                    this.fieldNames = new String[fields.length];
+                    this.fieldTypes = new Class<?>[fields.length];
+                    for (int i = 0; i < fields.length; i++) {
+                        fieldNames[i] = fields[i].getName();
+                        fieldTypes[i] = fields[i].getType();
+                    }
+                }
+
+                @Override
+                int length() { return fieldNames.length; }
+
+                @Override
+                String fieldName(int i) { return fieldNames[i]; }
+
+                @Override
+                Class<?> fieldType(int i) { return fieldTypes[i]; }
+            }
+        }
+    }
+
     /** Record specific support for retrieving and binding stream field values. */
     static final class RecordSupport {
-
-        /** Binds the given stream field values to the given method handle. */
+        /**
+         * Returns canonical record constructor adapted to take two arguments:
+         * {@code (byte[] primValues, Object[] objValues)}
+         * and return
+         * {@code Object}
+         */
         @SuppressWarnings("preview")
-        static MethodHandle bindCtrValues(MethodHandle ctrMH,
-                                          ObjectStreamClass desc,
-                                          ObjectInputStream.FieldValues fieldValues) {
+        static MethodHandle deserializationCtr(ObjectStreamClass desc) {
+            // check the cached value 1st
+            MethodHandle mh = desc.deserializationCtr;
+            if (mh != null) return mh;
+            mh = desc.deserializationCtrs.get(desc.getFields(false));
+            if (mh != null) return desc.deserializationCtr = mh;
+
+            // retrieve record components
             RecordComponent[] recordComponents;
             try {
                 Class<?> cls = desc.forClass();
                 PrivilegedExceptionAction<RecordComponent[]> pa = cls::getRecordComponents;
                 recordComponents = AccessController.doPrivileged(pa);
             } catch (PrivilegedActionException e) {
                 throw new InternalError(e.getCause());
             }
 
-            Object[] args = new Object[recordComponents.length];
-            for (int i = 0; i < recordComponents.length; i++) {
+            // retrieve the canonical constructor
+            // (T1, T2, ..., Tn):TR
+            mh = desc.getRecordConstructor();
+
+            // change return type to Object
+            // (T1, T2, ..., Tn):TR -> (T1, T2, ..., Tn):Object
+            mh = mh.asType(mh.type().changeReturnType(Object.class));
+
+            // drop last 2 arguments representing primValues and objValues arrays
+            // (T1, T2, ..., Tn):Object -> (T1, T2, ..., Tn, byte[], Object[]):Object
+            mh = MethodHandles.dropArguments(mh, mh.type().parameterCount(), byte[].class, Object[].class);
+
+            for (int i = recordComponents.length-1; i >= 0; i--) {
                 String name = recordComponents[i].getName();
-                Class<?> type= recordComponents[i].getType();
-                Object o = streamFieldValue(name, type, desc, fieldValues);
-                args[i] = o;
+                Class<?> type = recordComponents[i].getType();
+                // obtain stream field extractor that extracts argument at
+                // position i (Ti+1) from primValues and objValues arrays
+                // (byte[], Object[]):Ti+1
+                MethodHandle combiner = streamFieldExtractor(name, type, desc);
+                // fold byte[] privValues and Object[] objValues into argument at position i (Ti+1)
+                // (..., Ti, Ti+1, byte[], Object[]):Object -> (..., Ti, byte[], Object[]):Object
+                mh = MethodHandles.foldArguments(mh, i, combiner);
             }
+            // what we are left with is a MethodHandle taking just the primValues
+            // and objValues arrays and returning the constructed record instance
+            // (byte[], Object[]):Object
 
-            return MethodHandles.insertArguments(ctrMH, 0, args);
+            // store it into cache and return the 1st value stored
+            return desc.deserializationCtr =
+                desc.deserializationCtrs.putIfAbsentAndGet(desc.getFields(false), mh);
         }
 
         /** Returns the number of primitive fields for the given descriptor. */
         private static int numberPrimValues(ObjectStreamClass desc) {
             ObjectStreamField[] fields = desc.getFields();
@@ -2573,41 +2725,19 @@
                     break;  // can be no more
             }
             return primValueCount;
         }
 
-        /** Returns the default value for the given type. */
-        private static Object defaultValueFor(Class<?> pType) {
-            if (pType == Integer.TYPE)
-                return 0;
-            else if (pType == Byte.TYPE)
-                return (byte)0;
-            else if (pType == Long.TYPE)
-                return 0L;
-            else if (pType == Float.TYPE)
-                return 0.0f;
-            else if (pType == Double.TYPE)
-                return 0.0d;
-            else if (pType == Short.TYPE)
-                return (short)0;
-            else if (pType == Character.TYPE)
-                return '\u0000';
-            else if (pType == Boolean.TYPE)
-                return false;
-            else
-                return null;
-        }
-
-        /**
-         * Returns the stream field value for the given name. The default value
+        /**
+         * Returns extractor MethodHandle taking the primValues and objValues arrays
+         * and extracting the argument of canonical constructor with given name and type
          * for the given type is returned if the field value is absent.
          */
-        private static Object streamFieldValue(String pName,
-                                               Class<?> pType,
-                                               ObjectStreamClass desc,
-                                               ObjectInputStream.FieldValues fieldValues) {
-            ObjectStreamField[] fields = desc.getFields();
+        private static MethodHandle streamFieldExtractor(String pName,
+                                                         Class<?> pType,
+                                                         ObjectStreamClass desc) {
+            ObjectStreamField[] fields = desc.getFields(false);
 
             for (int i = 0; i < fields.length; i++) {
                 ObjectStreamField f = fields[i];
                 String fName = f.getName();
                 if (!fName.equals(pName))
@@ -2616,32 +2746,64 @@
                 Class<?> fType = f.getField().getType();
                 if (!pType.isAssignableFrom(fType))
                     throw new InternalError(fName + " unassignable, pType:" + pType + ", fType:" + fType);
 
                 if (f.isPrimitive()) {
-                    if (pType == Integer.TYPE)
-                        return Bits.getInt(fieldValues.primValues, f.getOffset());
-                    else if (fType == Byte.TYPE)
-                        return fieldValues.primValues[f.getOffset()];
-                    else if (fType == Long.TYPE)
-                        return Bits.getLong(fieldValues.primValues, f.getOffset());
-                    else if (fType == Float.TYPE)
-                        return Bits.getFloat(fieldValues.primValues, f.getOffset());
-                    else if (fType == Double.TYPE)
-                        return Bits.getDouble(fieldValues.primValues, f.getOffset());
-                    else if (fType == Short.TYPE)
-                        return Bits.getShort(fieldValues.primValues, f.getOffset());
-                    else if (fType == Character.TYPE)
-                        return Bits.getChar(fieldValues.primValues, f.getOffset());
-                    else if (fType == Boolean.TYPE)
-                        return Bits.getBoolean(fieldValues.primValues, f.getOffset());
-                    else
+                    // (byte[], int):fType
+                    MethodHandle mh = PRIM_VALUE_EXTRACTORS.get(fType);
+                    if (mh == null) {
                         throw new InternalError("Unexpected type: " + fType);
+                    }
+                    // bind offset
+                    // (byte[], int):fType -> (byte[]):fType
+                    mh = MethodHandles.insertArguments(mh, 1, f.getOffset());
+                    // drop objValues argument
+                    // (byte[]):fType -> (byte[], Object[]):fType
+                    mh = MethodHandles.dropArguments(mh, 1, Object[].class);
+                    // adapt return type to pType
+                    // (byte[], Object[]):fType -> (byte[], Object[]):pType
+                    if (pType != fType) {
+                        mh = mh.asType(mh.type().changeReturnType(pType));
+                    }
+                    return mh;
                 } else { // reference
-                    return fieldValues.objValues[i - numberPrimValues(desc)];
+                    // (Object[], int):Object
+                    MethodHandle mh = MethodHandles.arrayElementGetter(Object[].class);
+                    // bind index
+                    // (Object[], int):Object -> (Object[]):Object
+                    mh = MethodHandles.insertArguments(mh, 1, i - numberPrimValues(desc));
+                    // drop primValues argument
+                    // (Object[]):Object -> (byte[], Object[]):Object
+                    mh = MethodHandles.dropArguments(mh, 0, byte[].class);
+                    // adapt return type to pType
+                    // (byte[], Object[]):Object -> (byte[], Object[]):pType
+                    if (pType != Object.class) {
+                        mh = mh.asType(mh.type().changeReturnType(pType));
+                    }
+                    return mh;
                 }
             }
 
-            return defaultValueFor(pType);
+            // return default value extractor if no field matches pName
+            return MethodHandles.empty(MethodType.methodType(pType, byte[].class, Object[].class));
+        }
+
+        private static final Map<Class<?>, MethodHandle> PRIM_VALUE_EXTRACTORS;
+        static {
+            var lkp = MethodHandles.lookup();
+            try {
+                PRIM_VALUE_EXTRACTORS = Map.of(
+                    byte.class, MethodHandles.arrayElementGetter(byte[].class),
+                    short.class, lkp.findStatic(Bits.class, "getShort", MethodType.methodType(short.class, byte[].class, int.class)),
+                    int.class, lkp.findStatic(Bits.class, "getInt", MethodType.methodType(int.class, byte[].class, int.class)),
+                    long.class, lkp.findStatic(Bits.class, "getLong", MethodType.methodType(long.class, byte[].class, int.class)),
+                    float.class, lkp.findStatic(Bits.class, "getFloat", MethodType.methodType(float.class, byte[].class, int.class)),
+                    double.class, lkp.findStatic(Bits.class, "getDouble", MethodType.methodType(double.class, byte[].class, int.class)),
+                    char.class, lkp.findStatic(Bits.class, "getChar", MethodType.methodType(char.class, byte[].class, int.class)),
+                    boolean.class, lkp.findStatic(Bits.class, "getBoolean", MethodType.methodType(boolean.class, byte[].class, int.class))
+                );
+            } catch (NoSuchMethodException | IllegalAccessException e) {
+                throw new InternalError("Can't lookup Bits.getXXX", e);
+            }
         }
     }
 }
diff a/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java b/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
@@ -34,10 +34,11 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Objects;
 import java.util.Set;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
 
 import javax.lang.model.element.Element;
@@ -112,10 +113,11 @@
     boolean foundReturn = false;
     boolean hasNonWhitespaceText = false;
 
     public enum Flag {
         TABLE_HAS_CAPTION,
+        TABLE_IS_PRESENTATION,
         HAS_ELEMENT,
         HAS_HEADING,
         HAS_INLINE_TAG,
         HAS_TEXT,
         REPORTED_BAD_INLINE
@@ -528,11 +530,12 @@
             while (!tagStack.isEmpty()) {
                 TagStackItem top = tagStack.peek();
                 if (t == top.tag) {
                     switch (t) {
                         case TABLE:
-                            if (!top.attrs.contains(HtmlTag.Attr.SUMMARY)
+                            if (!top.flags.contains(Flag.TABLE_IS_PRESENTATION)
+                                    && !top.attrs.contains(HtmlTag.Attr.SUMMARY)
                                     && !top.flags.contains(Flag.TABLE_HAS_CAPTION)) {
                                 env.messages.error(ACCESSIBILITY, tree,
                                         "dc.no.summary.or.caption.for.table");
                             }
                             break;
@@ -685,10 +688,19 @@
                             } catch (NumberFormatException ex) {
                                 env.messages.error(HTML, tree, "dc.attr.table.border.html5", attr);
                             }
                         }
                         break;
+
+                    case ROLE:
+                        if (currTag == HtmlTag.TABLE) {
+                            String v = getAttrValue(tree);
+                            if (Objects.equals(v, "presentation")) {
+                                tagStack.peek().flags.add(Flag.TABLE_IS_PRESENTATION);
+                            }
+                        }
+                        break;
                 }
             }
         }
 
         // TODO: basic check on value
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/code/Symtab.java
@@ -218,10 +218,11 @@
     public final Type repeatableType;
     public final Type documentedType;
     public final Type elementTypeType;
     public final Type functionalInterfaceType;
     public final Type previewFeatureType;
+    public final Type previewFeatureInternalType;
     public final Type typeDescriptorType;
     public final Type recordType;
     public final Type valueBasedType;
     public final Type identityObjectType;
 
@@ -593,10 +594,11 @@
         lambdaMetafactory = enterClass("java.lang.invoke.LambdaMetafactory");
         stringConcatFactory = enterClass("java.lang.invoke.StringConcatFactory");
         valueBootstrapMethods = enterClass("java.lang.invoke.ValueBootstrapMethods");
         functionalInterfaceType = enterClass("java.lang.FunctionalInterface");
         previewFeatureType = enterClass("jdk.internal.PreviewFeature");
+        previewFeatureInternalType = enterSyntheticAnnotation("jdk.internal.PreviewFeature+Annotation");
         typeDescriptorType = enterClass("java.lang.invoke.TypeDescriptor");
         recordType = enterClass("java.lang.Record");
         valueBasedType = enterClass("java.lang.ValueBased");
         identityObjectType = enterClass("java.lang.IdentityObject");
 
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/ClassReader.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/ClassReader.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/ClassReader.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/jvm/ClassReader.java
@@ -1468,10 +1468,13 @@
                                 sym.flags_field |= NOT_IN_PROFILE;
                             }
                         }
                     }
                 }
+            } else if (proxy.type.tsym.flatName() == syms.previewFeatureInternalType.tsym.flatName()) {
+                sym.flags_field |= PREVIEW_API;
+                setFlagIfAttributeTrue(proxy, sym, names.essentialAPI, PREVIEW_ESSENTIAL_API);
             } else {
                 if (proxy.type.tsym == syms.annotationTargetType.tsym) {
                     target = proxy;
                 } else if (proxy.type.tsym == syms.repeatableType.tsym) {
                     repeatable = proxy;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/parser/JavacParser.java
@@ -3891,19 +3891,11 @@
         List<JCExpression> implementing = List.nil();
         if (token.kind == IMPLEMENTS) {
             nextToken();
             implementing = typeList();
         }
-        List<JCExpression> permitting = List.nil();
-        if (allowSealedTypes && token.kind == IDENTIFIER && token.name() == names.permits) {
-            checkSourceLevel(Feature.SEALED_CLASSES);
-            if ((mods.flags & Flags.SEALED) == 0) {
-                log.error(token.pos, Errors.InvalidPermitsClause(Fragments.ClassIsNotSealed("class")));
-            }
-            nextToken();
-            permitting = qualidentList(false);
-        }
+        List<JCExpression> permitting = permitsClause(mods, "class");
         List<JCTree> defs = classInterfaceOrRecordBody(name, false, false);
         JCClassDecl result = toP(F.at(pos).ClassDef(
             mods, name, typarams, extending, implementing, permitting, defs));
         attach(result, dc);
         return result;
@@ -3979,27 +3971,31 @@
         List<JCExpression> extending = List.nil();
         if (token.kind == EXTENDS) {
             nextToken();
             extending = typeList();
         }
-        List<JCExpression> permitting = List.nil();
-        if (allowSealedTypes && token.kind == IDENTIFIER && token.name() == names.permits) {
-            checkSourceLevel(Feature.SEALED_CLASSES);
-            if ((mods.flags & Flags.SEALED) == 0) {
-                log.error(token.pos, Errors.InvalidPermitsClause(Fragments.ClassIsNotSealed("interface")));
-            }
-            nextToken();
-            permitting = typeList();
-        }
+        List<JCExpression> permitting = permitsClause(mods, "interface");
         List<JCTree> defs;
         defs = classInterfaceOrRecordBody(name, true, false);
         JCClassDecl result = toP(F.at(pos).ClassDef(
             mods, name, typarams, null, extending, permitting, defs));
         attach(result, dc);
         return result;
     }
 
+    List<JCExpression> permitsClause(JCModifiers mods, String classOrInterface) {
+        if (allowSealedTypes && token.kind == IDENTIFIER && token.name() == names.permits) {
+            checkSourceLevel(Feature.SEALED_CLASSES);
+            if ((mods.flags & Flags.SEALED) == 0) {
+                log.error(token.pos, Errors.InvalidPermitsClause(Fragments.ClassIsNotSealed(classOrInterface)));
+            }
+            nextToken();
+            return qualidentList(false);
+        }
+        return List.nil();
+    }
+
     /** EnumDeclaration = ENUM Ident [IMPLEMENTS TypeList] EnumBody
      *  @param mods    The modifiers starting the enum declaration
      *  @param dc       The documentation comment for the enum, or null.
      */
     protected JCClassDecl enumDeclaration(JCModifiers mods, Comment dc) {
diff a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/interpreter/Bytecodes.java b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/interpreter/Bytecodes.java
--- a/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/interpreter/Bytecodes.java
+++ b/src/jdk.hotspot.agent/share/classes/sun/jvm/hotspot/interpreter/Bytecodes.java
@@ -820,12 +820,12 @@
     def(_invokehandle        , "invokehandle"        , "bJJ"  , null    , BasicType.getTIllegal(), -1, true, _invokevirtual   );
 
     // CDS specific. Bytecodes rewritten at CDS dump time
     def(_nofast_getfield     , "_nofast_getfield"    , "bJJ"  , null    , BasicType.getTIllegal() , 0, true,  _getfield );
     def(_nofast_putfield     , "_nofast_putfield"    , "bJJ"  , null    , BasicType.getTIllegal() ,-2, true,  _putfield );
-    def(_nofast_aload_0      , "_nofast_aload_0"     , "b"    , null    , BasicType.getTIllegal() , 1, true,  _aload_0  );
-    def(_nofast_iload        , "_nofast_iload"       , "bi"   , null    , BasicType.getTIllegal() , 1, false, _iload    );
+    def(_nofast_aload_0      , "_nofast_aload_0"     , "b"    , null    , BasicType.getTObject()  , 1, true,  _aload_0  );
+    def(_nofast_iload        , "_nofast_iload"       , "bi"   , null    , BasicType.getTInt()     , 1, false, _iload    );
 
     def(_shouldnotreachhere  , "_shouldnotreachhere" , "b"    , null    , BasicType.getTVoid()   ,  0, false);
 
     if (Assert.ASSERTS_ENABLED) {
       // compare can_trap information for each bytecode with the
diff a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
--- a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
+++ b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotResolvedObjectTypeImpl.java
@@ -371,10 +371,31 @@
     @Override
     public boolean isLinked() {
         return isArray() ? true : getInitState() >= config().instanceKlassStateLinked;
     }
 
+    @Override
+    public void link() {
+        if (!isLinked()) {
+            runtime().compilerToVm.ensureLinked(this);
+        }
+    }
+
+    @Override
+    public boolean hasDefaultMethods() {
+        HotSpotVMConfig config = config();
+        int miscFlags = UNSAFE.getChar(getMetaspaceKlass() + config.instanceKlassMiscFlagsOffset);
+        return (miscFlags & config.jvmMiscFlagsHasDefaultMethods) != 0;
+    }
+
+    @Override
+    public boolean declaresDefaultMethods() {
+        HotSpotVMConfig config = config();
+        int miscFlags = UNSAFE.getChar(getMetaspaceKlass() + config.instanceKlassMiscFlagsOffset);
+        return (miscFlags & config.jvmMiscFlagsDeclaresDefaultMethods) != 0;
+    }
+
     /**
      * Returns the value of the state field {@code InstanceKlass::_init_state} of the metaspace
      * klass.
      *
      * @return state field value of this type
diff a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java
--- a/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java
+++ b/src/jdk.internal.vm.ci/share/classes/jdk.vm.ci.hotspot/src/jdk/vm/ci/hotspot/HotSpotVMConfig.java
@@ -135,10 +135,13 @@
     final int jvmAccBridge = getConstant("JVM_ACC_BRIDGE", Integer.class);
     final int jvmAccVarargs = getConstant("JVM_ACC_VARARGS", Integer.class);
     final int jvmAccEnum = getConstant("JVM_ACC_ENUM", Integer.class);
     final int jvmAccInterface = getConstant("JVM_ACC_INTERFACE", Integer.class);
 
+    final int jvmMiscFlagsHasDefaultMethods = getConstant("InstanceKlass::_misc_has_nonstatic_concrete_methods", Integer.class);
+    final int jvmMiscFlagsDeclaresDefaultMethods = getConstant("InstanceKlass::_misc_declares_nonstatic_concrete_methods", Integer.class);
+
     // This is only valid on AMD64.
     final int runtimeCallStackSize = getConstant("frame::arg_reg_save_area_bytes", Integer.class, osArch.equals("amd64") ? null : 0);
 
     private final int markWordNoHashInPlace = getConstant("markWord::no_hash_in_place", Integer.class);
     private final int markWordNoLockInPlace = getConstant("markWord::no_lock_in_place", Integer.class);
diff a/test/hotspot/jtreg/ProblemList.txt b/test/hotspot/jtreg/ProblemList.txt
--- a/test/hotspot/jtreg/ProblemList.txt
+++ b/test/hotspot/jtreg/ProblemList.txt
@@ -199,10 +199,11 @@
 serviceability/sa/TestInstanceKlassSizeForInterface.java 8230664 linux-ppc64le,linux-ppc64
 serviceability/sa/TestRevPtrsForInvokeDynamic.java 8241235 generic-all
 
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatIntervalTest.java 8214032 generic-all
 serviceability/jvmti/HeapMonitor/MyPackage/HeapMonitorStatArrayCorrectnessTest.java 8224150 generic-all
+serviceability/jvmti/ModuleAwareAgents/ThreadStart/MAAThreadStart.java 8225354 windows-all
 
 # Valhalla TODO:
 serviceability/sa/ClhsdbCDSCore.java 8190936 generic-all
 serviceability/sa/ClhsdbCDSJstackPrintAll.java 8190936 generic-all
 serviceability/sa/ClhsdbFindPC.java 8190936 generic-all
diff a/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java b/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
--- a/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
+++ b/test/hotspot/jtreg/compiler/jvmci/jdk.vm.ci.runtime.test/src/jdk/vm/ci/runtime/test/TestResolvedJavaType.java
@@ -47,10 +47,13 @@
 import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
 
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.io.InputStream;
 import java.lang.annotation.Annotation;
 import java.lang.invoke.MethodHandles.Lookup;
 import java.lang.reflect.AccessibleObject;
 import java.lang.reflect.Constructor;
 import java.lang.reflect.Field;
@@ -62,10 +65,11 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Map;
 import java.util.Set;
 
+import org.junit.Assert;
 import org.junit.Test;
 
 import jdk.internal.org.objectweb.asm.*;
 import jdk.internal.reflect.ConstantPool;
 import jdk.vm.ci.common.JVMCIError;
@@ -335,10 +339,115 @@
                 }
             }
         }
     }
 
+    @Test
+    public void linkTest() {
+        for (Class<?> c : classes) {
+            ResolvedJavaType type = metaAccess.lookupJavaType(c);
+            type.link();
+        }
+    }
+
+    private class HidingClassLoader extends ClassLoader {
+        @Override
+        protected Class<?> findClass(final String name) throws ClassNotFoundException {
+            if (name.endsWith("MissingInterface")) {
+                throw new ClassNotFoundException("missing");
+            }
+            byte[] classData = null;
+            try {
+                InputStream is = HidingClassLoader.class.getResourceAsStream("/" + name.replace('.', '/') + ".class");
+                classData = new byte[is.available()];
+                new DataInputStream(is).readFully(classData);
+            } catch (IOException e) {
+                Assert.fail("can't access class: " + name);
+            }
+
+            return defineClass(null, classData, 0, classData.length);
+        }
+
+        ResolvedJavaType lookupJavaType(String name) throws ClassNotFoundException {
+            return metaAccess.lookupJavaType(loadClass(name));
+        }
+
+        HidingClassLoader() {
+            super(null);
+        }
+
+    }
+
+    interface MissingInterface {
+    }
+
+    static class MissingInterfaceImpl implements MissingInterface {
+    }
+
+    interface SomeInterface {
+        default MissingInterface someMethod() {
+            return new MissingInterfaceImpl();
+        }
+    }
+
+    static class Wrapper implements SomeInterface {
+    }
+
+    @Test
+    public void linkExceptionTest() throws ClassNotFoundException {
+        HidingClassLoader cl = new HidingClassLoader();
+        ResolvedJavaType inner = cl.lookupJavaType(Wrapper.class.getName());
+        assertTrue("expected default methods", inner.hasDefaultMethods());
+        try {
+            inner.link();
+            assertFalse("link should throw an exception", true);
+        } catch (NoClassDefFoundError e) {
+        }
+    }
+
+    @Test
+    public void hasDefaultMethodsTest() {
+        for (Class<?> c : classes) {
+            ResolvedJavaType type = metaAccess.lookupJavaType(c);
+            assertEquals(hasDefaultMethods(type), type.hasDefaultMethods());
+        }
+    }
+
+    @Test
+    public void declaresDefaultMethodsTest() {
+        for (Class<?> c : classes) {
+            ResolvedJavaType type = metaAccess.lookupJavaType(c);
+            assertEquals(declaresDefaultMethods(type), type.declaresDefaultMethods());
+        }
+    }
+
+    private static boolean hasDefaultMethods(ResolvedJavaType type) {
+        if (!type.isInterface() && type.getSuperclass() != null && hasDefaultMethods(type.getSuperclass())) {
+            return true;
+        }
+        for (ResolvedJavaType iface : type.getInterfaces()) {
+            if (hasDefaultMethods(iface)) {
+                return true;
+            }
+        }
+        return declaresDefaultMethods(type);
+    }
+
+    static boolean declaresDefaultMethods(ResolvedJavaType type) {
+        if (!type.isInterface()) {
+            /* Only interfaces can declare default methods. */
+            return false;
+        }
+        for (ResolvedJavaMethod method : type.getDeclaredMethods()) {
+            if (method.isDefault()) {
+                assert !Modifier.isStatic(method.getModifiers()) : "Default method that is static?";
+                return true;
+            }
+        }
+        return false;
+    }
+
     private static class Base {
     }
 
     abstract static class Abstract1 extends Base {
     }
diff a/test/lib/sun/hotspot/WhiteBox.java b/test/lib/sun/hotspot/WhiteBox.java
--- a/test/lib/sun/hotspot/WhiteBox.java
+++ b/test/lib/sun/hotspot/WhiteBox.java
@@ -108,10 +108,12 @@
     return countAliveClasses(name) != 0;
   }
 
   public  native int getSymbolRefcount(String name);
 
+  public native boolean deflateIdleMonitors();
+
   private native boolean isMonitorInflated0(Object obj);
   public         boolean isMonitorInflated(Object obj) {
     Objects.requireNonNull(obj);
     return isMonitorInflated0(obj);
   }

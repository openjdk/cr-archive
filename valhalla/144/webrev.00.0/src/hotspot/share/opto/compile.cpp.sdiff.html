<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="chaitin.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/compile.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1001   // Initialize the first few types.
1002   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1003   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1004   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1005   _num_alias_types = AliasIdxRaw+1;
1006   // Zero out the alias type cache.
1007   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1008   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1009   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1010 
1011   _intrinsics = NULL;
1012   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1013   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1014   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1015   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1016   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1017   _inline_type_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1018   register_library_intrinsics();
1019 #ifdef ASSERT
1020   _type_verify_symmetry = true;

1021 #endif
1022 }
1023 
1024 //---------------------------init_start----------------------------------------
1025 // Install the StartNode on this compile object.
1026 void Compile::init_start(StartNode* s) {
1027   if (failing())
1028     return; // already failing
1029   assert(s == start(), &quot;&quot;);
1030 }
1031 
1032 /**
1033  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1034  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1035  * the ideal graph.
1036  */
1037 StartNode* Compile::start() const {
1038   assert (!failing(), &quot;Must not have pending failure. Reason is: %s&quot;, failure_reason());
1039   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1040     Node* start = root()-&gt;fast_out(i);
</pre>
<hr />
<pre>
2642 
2643   if (C-&gt;max_vector_size() &gt; 0) {
2644     C-&gt;optimize_logic_cones(igvn);
2645     igvn.optimize();
2646   }
2647 
2648   DEBUG_ONLY( _modified_nodes = NULL; )
2649  } // (End scope of igvn; run destructor if necessary for asserts.)
2650 
2651  process_print_inlining();
2652  // A method with only infinite loops has no edges entering loops from root
2653  {
2654    TracePhase tp(&quot;graphReshape&quot;, &amp;timers[_t_graphReshaping]);
2655    if (final_graph_reshaping()) {
2656      assert(failing(), &quot;must bail out w/ explicit message&quot;);
2657      return;
2658    }
2659  }
2660 
2661  print_method(PHASE_OPTIMIZE_FINISHED, 2);

2662 }
2663 
2664 //---------------------------- Bitwise operation packing optimization ---------------------------
2665 
2666 static bool is_vector_unary_bitwise_op(Node* n) {
2667   return n-&gt;Opcode() == Op_XorV &amp;&amp;
2668          VectorNode::is_vector_bitwise_not_pattern(n);
2669 }
2670 
2671 static bool is_vector_binary_bitwise_op(Node* n) {
2672   switch (n-&gt;Opcode()) {
2673     case Op_AndV:
2674     case Op_OrV:
2675       return true;
2676 
2677     case Op_XorV:
2678       return !is_vector_unary_bitwise_op(n);
2679 
2680     default:
2681       return false;
</pre>
<hr />
<pre>
3086 
3087   // He&#39;s dead, Jim.
3088   _cfg     = (PhaseCFG*)((intptr_t)0xdeadbeef);
3089   _regalloc = (PhaseChaitin*)((intptr_t)0xdeadbeef);
3090 }
3091 
3092 //------------------------------Final_Reshape_Counts---------------------------
3093 // This class defines counters to help identify when a method
3094 // may/must be executed using hardware with only 24-bit precision.
3095 struct Final_Reshape_Counts : public StackObj {
3096   int  _call_count;             // count non-inlined &#39;common&#39; calls
3097   int  _float_count;            // count float ops requiring 24-bit precision
3098   int  _double_count;           // count double ops requiring more precision
3099   int  _java_call_count;        // count non-inlined &#39;java&#39; calls
3100   int  _inner_loop_count;       // count loops which need alignment
3101   VectorSet _visited;           // Visitation flags
3102   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
3103 
3104   Final_Reshape_Counts() :
3105     _call_count(0), _float_count(0), _double_count(0),
<span class="line-modified">3106     _java_call_count(0), _inner_loop_count(0),</span>
<span class="line-removed">3107     _visited( Thread::current()-&gt;resource_area() ) { }</span>
3108 
3109   void inc_call_count  () { _call_count  ++; }
3110   void inc_float_count () { _float_count ++; }
3111   void inc_double_count() { _double_count++; }
3112   void inc_java_call_count() { _java_call_count++; }
3113   void inc_inner_loop_count() { _inner_loop_count++; }
3114 
3115   int  get_call_count  () const { return _call_count  ; }
3116   int  get_float_count () const { return _float_count ; }
3117   int  get_double_count() const { return _double_count; }
3118   int  get_java_call_count() const { return _java_call_count; }
3119   int  get_inner_loop_count() const { return _inner_loop_count; }
3120 };
3121 
3122 #ifdef ASSERT
3123 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
3124   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
3125   // Make sure the offset goes inside the instance layout.
3126   return k-&gt;contains_field_offset(tp-&gt;offset());
3127   // Note that OffsetBot and OffsetTop are very negative.
</pre>
<hr />
<pre>
3916 #ifdef ASSERT
3917   case Op_InlineTypePtr:
3918   case Op_InlineType: {
3919     n-&gt;dump(-1);
3920     assert(false, &quot;inline type node was not removed&quot;);
3921     break;
3922   }
3923 #endif
3924   default:
3925     assert(!n-&gt;is_Call(), &quot;&quot;);
3926     assert(!n-&gt;is_Mem(), &quot;&quot;);
3927     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3928     break;
3929   }
3930 }
3931 
3932 //------------------------------final_graph_reshaping_walk---------------------
3933 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3934 // requires that the walk visits a node&#39;s inputs before visiting the node.
3935 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
<span class="line-modified">3936   ResourceArea *area = Thread::current()-&gt;resource_area();</span>
<span class="line-removed">3937   Unique_Node_List sfpt(area);</span>
3938 
3939   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3940   uint cnt = root-&gt;req();
3941   Node *n = root;
3942   uint  i = 0;
3943   while (true) {
3944     if (i &lt; cnt) {
3945       // Place all non-visited non-null inputs onto stack
3946       Node* m = n-&gt;in(i);
3947       ++i;
3948       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3949         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3950           // compute worst case interpreter size in case of a deoptimization
3951           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3952 
3953           sfpt.push(m);
3954         }
3955         cnt = m-&gt;req();
3956         nstack.push(n, i); // put on stack parent and next input&#39;s index
3957         n = m;
</pre>
<hr />
<pre>
4279       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
4280       // child class can become fully initialized while its parent class is still being initialized.
4281       if (accessing_method-&gt;is_class_initializer()) {
4282         return false;
4283       }
4284     }
4285     ciMethod* root = method(); // the root method of compilation
4286     if (root != accessing_method) {
4287       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
4288     }
4289   }
4290   return true;
4291 }
4292 
4293 #ifndef PRODUCT
4294 //------------------------------verify_graph_edges---------------------------
4295 // Walk the Graph and verify that there is a one-to-one correspondence
4296 // between Use-Def edges and Def-Use edges in the graph.
4297 void Compile::verify_graph_edges(bool no_dead_code) {
4298   if (VerifyGraphEdges) {
<span class="line-modified">4299     ResourceArea *area = Thread::current()-&gt;resource_area();</span>
<span class="line-removed">4300     Unique_Node_List visited(area);</span>
4301     // Call recursive graph walk to check edges
4302     _root-&gt;verify_edges(visited);
4303     if (no_dead_code) {
4304       // Now make sure that no visited node is used by an unvisited node.
4305       bool dead_nodes = false;
<span class="line-modified">4306       Unique_Node_List checked(area);</span>
4307       while (visited.size() &gt; 0) {
4308         Node* n = visited.pop();
4309         checked.push(n);
4310         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
4311           Node* use = n-&gt;raw_out(i);
4312           if (checked.member(use))  continue;  // already checked
4313           if (visited.member(use))  continue;  // already in the graph
4314           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
4315           // At this point, we have found a dead node which is DU-reachable.
4316           if (!dead_nodes) {
4317             tty-&gt;print_cr(&quot;*** Dead nodes reachable via DU edges:&quot;);
4318             dead_nodes = true;
4319           }
4320           use-&gt;dump(2);
4321           tty-&gt;print_cr(&quot;---&quot;);
4322           checked.push(use);  // No repeats; pretend it is now checked.
4323         }
4324       }
4325       assert(!dead_nodes, &quot;using nodes must be reachable from root&quot;);
4326     }
</pre>
</td>
<td>
<hr />
<pre>
1001   // Initialize the first few types.
1002   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1003   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1004   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1005   _num_alias_types = AliasIdxRaw+1;
1006   // Zero out the alias type cache.
1007   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1008   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1009   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1010 
1011   _intrinsics = NULL;
1012   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1013   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1014   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1015   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1016   _opaque4_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1017   _inline_type_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1018   register_library_intrinsics();
1019 #ifdef ASSERT
1020   _type_verify_symmetry = true;
<span class="line-added">1021   _phase_optimize_finished = false;</span>
1022 #endif
1023 }
1024 
1025 //---------------------------init_start----------------------------------------
1026 // Install the StartNode on this compile object.
1027 void Compile::init_start(StartNode* s) {
1028   if (failing())
1029     return; // already failing
1030   assert(s == start(), &quot;&quot;);
1031 }
1032 
1033 /**
1034  * Return the &#39;StartNode&#39;. We must not have a pending failure, since the ideal graph
1035  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1036  * the ideal graph.
1037  */
1038 StartNode* Compile::start() const {
1039   assert (!failing(), &quot;Must not have pending failure. Reason is: %s&quot;, failure_reason());
1040   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1041     Node* start = root()-&gt;fast_out(i);
</pre>
<hr />
<pre>
2643 
2644   if (C-&gt;max_vector_size() &gt; 0) {
2645     C-&gt;optimize_logic_cones(igvn);
2646     igvn.optimize();
2647   }
2648 
2649   DEBUG_ONLY( _modified_nodes = NULL; )
2650  } // (End scope of igvn; run destructor if necessary for asserts.)
2651 
2652  process_print_inlining();
2653  // A method with only infinite loops has no edges entering loops from root
2654  {
2655    TracePhase tp(&quot;graphReshape&quot;, &amp;timers[_t_graphReshaping]);
2656    if (final_graph_reshaping()) {
2657      assert(failing(), &quot;must bail out w/ explicit message&quot;);
2658      return;
2659    }
2660  }
2661 
2662  print_method(PHASE_OPTIMIZE_FINISHED, 2);
<span class="line-added">2663  DEBUG_ONLY(set_phase_optimize_finished();)</span>
2664 }
2665 
2666 //---------------------------- Bitwise operation packing optimization ---------------------------
2667 
2668 static bool is_vector_unary_bitwise_op(Node* n) {
2669   return n-&gt;Opcode() == Op_XorV &amp;&amp;
2670          VectorNode::is_vector_bitwise_not_pattern(n);
2671 }
2672 
2673 static bool is_vector_binary_bitwise_op(Node* n) {
2674   switch (n-&gt;Opcode()) {
2675     case Op_AndV:
2676     case Op_OrV:
2677       return true;
2678 
2679     case Op_XorV:
2680       return !is_vector_unary_bitwise_op(n);
2681 
2682     default:
2683       return false;
</pre>
<hr />
<pre>
3088 
3089   // He&#39;s dead, Jim.
3090   _cfg     = (PhaseCFG*)((intptr_t)0xdeadbeef);
3091   _regalloc = (PhaseChaitin*)((intptr_t)0xdeadbeef);
3092 }
3093 
3094 //------------------------------Final_Reshape_Counts---------------------------
3095 // This class defines counters to help identify when a method
3096 // may/must be executed using hardware with only 24-bit precision.
3097 struct Final_Reshape_Counts : public StackObj {
3098   int  _call_count;             // count non-inlined &#39;common&#39; calls
3099   int  _float_count;            // count float ops requiring 24-bit precision
3100   int  _double_count;           // count double ops requiring more precision
3101   int  _java_call_count;        // count non-inlined &#39;java&#39; calls
3102   int  _inner_loop_count;       // count loops which need alignment
3103   VectorSet _visited;           // Visitation flags
3104   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
3105 
3106   Final_Reshape_Counts() :
3107     _call_count(0), _float_count(0), _double_count(0),
<span class="line-modified">3108     _java_call_count(0), _inner_loop_count(0) { }</span>

3109 
3110   void inc_call_count  () { _call_count  ++; }
3111   void inc_float_count () { _float_count ++; }
3112   void inc_double_count() { _double_count++; }
3113   void inc_java_call_count() { _java_call_count++; }
3114   void inc_inner_loop_count() { _inner_loop_count++; }
3115 
3116   int  get_call_count  () const { return _call_count  ; }
3117   int  get_float_count () const { return _float_count ; }
3118   int  get_double_count() const { return _double_count; }
3119   int  get_java_call_count() const { return _java_call_count; }
3120   int  get_inner_loop_count() const { return _inner_loop_count; }
3121 };
3122 
3123 #ifdef ASSERT
3124 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
3125   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
3126   // Make sure the offset goes inside the instance layout.
3127   return k-&gt;contains_field_offset(tp-&gt;offset());
3128   // Note that OffsetBot and OffsetTop are very negative.
</pre>
<hr />
<pre>
3917 #ifdef ASSERT
3918   case Op_InlineTypePtr:
3919   case Op_InlineType: {
3920     n-&gt;dump(-1);
3921     assert(false, &quot;inline type node was not removed&quot;);
3922     break;
3923   }
3924 #endif
3925   default:
3926     assert(!n-&gt;is_Call(), &quot;&quot;);
3927     assert(!n-&gt;is_Mem(), &quot;&quot;);
3928     assert(nop != Op_ProfileBoolean, &quot;should be eliminated during IGVN&quot;);
3929     break;
3930   }
3931 }
3932 
3933 //------------------------------final_graph_reshaping_walk---------------------
3934 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3935 // requires that the walk visits a node&#39;s inputs before visiting the node.
3936 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
<span class="line-modified">3937   Unique_Node_List sfpt;</span>

3938 
3939   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3940   uint cnt = root-&gt;req();
3941   Node *n = root;
3942   uint  i = 0;
3943   while (true) {
3944     if (i &lt; cnt) {
3945       // Place all non-visited non-null inputs onto stack
3946       Node* m = n-&gt;in(i);
3947       ++i;
3948       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3949         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3950           // compute worst case interpreter size in case of a deoptimization
3951           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3952 
3953           sfpt.push(m);
3954         }
3955         cnt = m-&gt;req();
3956         nstack.push(n, i); // put on stack parent and next input&#39;s index
3957         n = m;
</pre>
<hr />
<pre>
4279       // In case of &lt;init&gt; or a static method, the barrier is on the subclass is not enough:
4280       // child class can become fully initialized while its parent class is still being initialized.
4281       if (accessing_method-&gt;is_class_initializer()) {
4282         return false;
4283       }
4284     }
4285     ciMethod* root = method(); // the root method of compilation
4286     if (root != accessing_method) {
4287       return needs_clinit_barrier(holder, root); // check access in the context of compilation root
4288     }
4289   }
4290   return true;
4291 }
4292 
4293 #ifndef PRODUCT
4294 //------------------------------verify_graph_edges---------------------------
4295 // Walk the Graph and verify that there is a one-to-one correspondence
4296 // between Use-Def edges and Def-Use edges in the graph.
4297 void Compile::verify_graph_edges(bool no_dead_code) {
4298   if (VerifyGraphEdges) {
<span class="line-modified">4299     Unique_Node_List visited;</span>

4300     // Call recursive graph walk to check edges
4301     _root-&gt;verify_edges(visited);
4302     if (no_dead_code) {
4303       // Now make sure that no visited node is used by an unvisited node.
4304       bool dead_nodes = false;
<span class="line-modified">4305       Unique_Node_List checked;</span>
4306       while (visited.size() &gt; 0) {
4307         Node* n = visited.pop();
4308         checked.push(n);
4309         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
4310           Node* use = n-&gt;raw_out(i);
4311           if (checked.member(use))  continue;  // already checked
4312           if (visited.member(use))  continue;  // already in the graph
4313           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
4314           // At this point, we have found a dead node which is DU-reachable.
4315           if (!dead_nodes) {
4316             tty-&gt;print_cr(&quot;*** Dead nodes reachable via DU edges:&quot;);
4317             dead_nodes = true;
4318           }
4319           use-&gt;dump(2);
4320           tty-&gt;print_cr(&quot;---&quot;);
4321           checked.push(use);  // No repeats; pretend it is now checked.
4322         }
4323       }
4324       assert(!dead_nodes, &quot;using nodes must be reachable from root&quot;);
4325     }
</pre>
</td>
</tr>
</table>
<center><a href="chaitin.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="compile.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
diff a/src/hotspot/cpu/x86/vm_version_x86.cpp b/src/hotspot/cpu/x86/vm_version_x86.cpp
--- a/src/hotspot/cpu/x86/vm_version_x86.cpp
+++ b/src/hotspot/cpu/x86/vm_version_x86.cpp
@@ -1505,10 +1505,16 @@
 #ifdef COMPILER2
   if (FLAG_IS_DEFAULT(AlignVector)) {
     // Modern processors allow misaligned memory operations for vectors.
     AlignVector = !UseUnalignedLoadStores;
   }
+  if (FLAG_IS_DEFAULT(OptimizeFill)) {
+    // 8247307: On x86, the auto-vectorized loop array fill code shows
+    // better performance than the array fill stubs. We should reenable
+    // this after the x86 stubs get improved.
+    OptimizeFill = false;
+  }
 #endif // COMPILER2
 
   if (FLAG_IS_DEFAULT(AllocatePrefetchInstr)) {
     if (AllocatePrefetchInstr == 3 && !supports_3dnow_prefetch()) {
       FLAG_SET_DEFAULT(AllocatePrefetchInstr, 0);

diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -644,5 +644,7 @@
 90b266a84c06f1b3dc0ed8767856793e8c1c357e jdk-15+25
 0a32396f7a690015d22ca3328ac441a358295d90 jdk-15+26
 93813843680bbe1b7efbca56c03fd137f20a2c31 jdk-16+0
 93813843680bbe1b7efbca56c03fd137f20a2c31 jdk-15+27
 4a485c89d5a08b495961835f5308a96038678aeb jdk-16+1
+06c9f89459daba98395fad726100feb44f89ba71 jdk-15+28
+bcbe7b8a77b8971bc221c0be1bd2abb6fb68c2d0 jdk-16+2
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -13930,11 +13930,11 @@
 // clearing of an array
 
 instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)
 %{
   match(Set dummy (ClearArray (Binary cnt base) val));
-  effect(USE_KILL cnt, USE_KILL base);
+  effect(USE_KILL cnt, USE_KILL base, KILL cr);
 
   ins_cost(4 * INSN_COST);
   format %{ "ClearArray $cnt, $base, $val" %}
 
   ins_encode %{
diff a/src/hotspot/cpu/aarch64/frame_aarch64.cpp b/src/hotspot/cpu/aarch64/frame_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/frame_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/frame_aarch64.cpp
@@ -748,11 +748,11 @@
 }
 
 extern "C" void pf(unsigned long sp, unsigned long fp, unsigned long pc,
                    unsigned long bcx, unsigned long thread) {
   if (!reg_map) {
-    reg_map = NEW_C_HEAP_OBJ(RegisterMap, mtNone);
+    reg_map = NEW_C_HEAP_OBJ(RegisterMap, mtInternal);
     ::new (reg_map) RegisterMap((JavaThread*)thread, false);
   } else {
     *reg_map = RegisterMap((JavaThread*)thread, false);
   }
 
diff a/src/hotspot/share/ci/ciField.cpp b/src/hotspot/share/ci/ciField.cpp
--- a/src/hotspot/share/ci/ciField.cpp
+++ b/src/hotspot/share/ci/ciField.cpp
@@ -255,10 +255,13 @@
   if (holder->is_inlinetype())
     return true;
   // Trust final fields in all boxed classes
   if (holder->is_box_klass())
     return true;
+  // Trust final fields in records
+  if (holder->is_record())
+    return true;
   // Trust final fields in String
   if (holder->name() == ciSymbol::java_lang_String())
     return true;
   // Trust Atomic*FieldUpdaters: they are very important for performance, and make up one
   // more reason not to use Unsafe, if their final fields are trusted. See more in JDK-8140483.
diff a/src/hotspot/share/ci/ciInstanceKlass.cpp b/src/hotspot/share/ci/ciInstanceKlass.cpp
--- a/src/hotspot/share/ci/ciInstanceKlass.cpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.cpp
@@ -64,10 +64,11 @@
   _nonstatic_field_size = ik->nonstatic_field_size();
   _has_nonstatic_fields = ik->has_nonstatic_fields();
   _has_nonstatic_concrete_methods = ik->has_nonstatic_concrete_methods();
   _is_unsafe_anonymous = ik->is_unsafe_anonymous();
   _is_hidden = ik->is_hidden();
+  _is_record = ik->is_record();
   _nonstatic_fields = NULL; // initialized lazily by compute_nonstatic_fields:
   _has_injected_fields = -1;
   _implementor = NULL; // we will fill these lazily
 
   // Ensure that the metadata wrapped by the ciMetadata is kept alive by GC.
@@ -126,10 +127,11 @@
   _has_nonstatic_fields = false;
   _nonstatic_fields = NULL;            // initialized lazily by compute_nonstatic_fields
   _has_injected_fields = -1;
   _is_unsafe_anonymous = false;
   _is_hidden = false;
+  _is_record = false;
   _loader = loader;
   _protection_domain = protection_domain;
   _is_shared = false;
   _super = NULL;
   _java_mirror = NULL;
diff a/src/hotspot/share/ci/ciInstanceKlass.hpp b/src/hotspot/share/ci/ciInstanceKlass.hpp
--- a/src/hotspot/share/ci/ciInstanceKlass.hpp
+++ b/src/hotspot/share/ci/ciInstanceKlass.hpp
@@ -55,10 +55,11 @@
   SubklassValue          _has_subklass;
   bool                   _has_nonstatic_fields;
   bool                   _has_nonstatic_concrete_methods;
   bool                   _is_unsafe_anonymous;
   bool                   _is_hidden;
+  bool                   _is_record;
 
   ciFlags                _flags;
   jint                   _nonstatic_field_size;
   jint                   _nonstatic_oop_map_size;
 
@@ -199,10 +200,14 @@
 
   bool is_hidden() const {
     return _is_hidden;
   }
 
+  bool is_record() const {
+    return _is_record;
+  }
+
   ciInstanceKlass* get_canonical_holder(int offset);
   ciField* get_field_by_offset(int field_offset, bool is_static);
   ciField* get_field_by_name(ciSymbol* name, ciSymbol* signature, bool is_static);
   // get field descriptor at field_offset ignoring flattening
   ciField* get_non_flattened_field_by_offset(int field_offset);
diff a/src/hotspot/share/classfile/classLoaderData.cpp b/src/hotspot/share/classfile/classLoaderData.cpp
--- a/src/hotspot/share/classfile/classLoaderData.cpp
+++ b/src/hotspot/share/classfile/classLoaderData.cpp
@@ -53,10 +53,11 @@
 #include "classfile/javaClasses.hpp"
 #include "classfile/moduleEntry.hpp"
 #include "classfile/packageEntry.hpp"
 #include "classfile/symbolTable.hpp"
 #include "classfile/systemDictionary.hpp"
+#include "gc/shared/oopStorageSet.hpp"
 #include "logging/log.hpp"
 #include "logging/logStream.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/metadataFactory.hpp"
 #include "memory/resourceArea.hpp"
@@ -496,11 +497,11 @@
 }
 
 void ClassLoaderData::initialize_holder(Handle loader_or_mirror) {
   if (loader_or_mirror() != NULL) {
     assert(_holder.is_null(), "never replace holders");
-    _holder = WeakHandle<vm_weak_data>::create(loader_or_mirror);
+    _holder = WeakHandle(OopStorageSet::vm_weak(), loader_or_mirror);
   }
 }
 
 // Remove a klass from the _klasses list for scratch_class during redefinition
 // or parsed class in the case of an error.
@@ -665,11 +666,11 @@
 
   ClassLoaderDataGraph::dec_array_classes(cl.array_class_released());
   ClassLoaderDataGraph::dec_instance_classes(cl.instance_class_released());
 
   // Release the WeakHandle
-  _holder.release();
+  _holder.release(OopStorageSet::vm_weak());
 
   // Release C heap allocated hashtable for all the packages.
   if (_packages != NULL) {
     // Destroy the table itself
     delete _packages;
diff a/src/hotspot/share/classfile/classLoaderData.hpp b/src/hotspot/share/classfile/classLoaderData.hpp
--- a/src/hotspot/share/classfile/classLoaderData.hpp
+++ b/src/hotspot/share/classfile/classLoaderData.hpp
@@ -107,13 +107,13 @@
   friend class MetaDataFactory;
   friend class Method;
 
   static ClassLoaderData * _the_null_class_loader_data;
 
-  WeakHandle<vm_weak_data> _holder; // The oop that determines lifetime of this class loader
-  OopHandle _class_loader;          // The instance of java/lang/ClassLoader associated with
-                                    // this ClassLoaderData
+  WeakHandle _holder;       // The oop that determines lifetime of this class loader
+  OopHandle  _class_loader; // The instance of java/lang/ClassLoader associated with
+                            // this ClassLoaderData
 
   ClassLoaderMetaspace * volatile _metaspace;  // Meta-space where meta-data defined by the
                                     // classes in the class loader are allocated.
   Mutex* _metaspace_lock;  // Locks the metaspace for allocations and setup.
   bool _unloading;         // true if this class loader goes away
diff a/src/hotspot/share/classfile/javaClasses.cpp b/src/hotspot/share/classfile/javaClasses.cpp
--- a/src/hotspot/share/classfile/javaClasses.cpp
+++ b/src/hotspot/share/classfile/javaClasses.cpp
@@ -3204,19 +3204,21 @@
 int java_lang_reflect_Field::_clazz_offset;
 int java_lang_reflect_Field::_name_offset;
 int java_lang_reflect_Field::_type_offset;
 int java_lang_reflect_Field::_slot_offset;
 int java_lang_reflect_Field::_modifiers_offset;
+int java_lang_reflect_Field::_trusted_final_offset;
 int java_lang_reflect_Field::_signature_offset;
 int java_lang_reflect_Field::_annotations_offset;
 
 #define FIELD_FIELDS_DO(macro) \
   macro(_clazz_offset,     k, vmSymbols::clazz_name(),     class_signature,  false); \
   macro(_name_offset,      k, vmSymbols::name_name(),      string_signature, false); \
   macro(_type_offset,      k, vmSymbols::type_name(),      class_signature,  false); \
   macro(_slot_offset,      k, vmSymbols::slot_name(),      int_signature,    false); \
   macro(_modifiers_offset, k, vmSymbols::modifiers_name(), int_signature,    false); \
+  macro(_trusted_final_offset,    k, vmSymbols::trusted_final_name(),    bool_signature,       false); \
   macro(_signature_offset,        k, vmSymbols::signature_name(),        string_signature,     false); \
   macro(_annotations_offset,      k, vmSymbols::annotations_name(),      byte_array_signature, false);
 
 void java_lang_reflect_Field::compute_offsets() {
   InstanceKlass* k = SystemDictionary::reflect_Field_klass();
@@ -3277,10 +3279,14 @@
 
 void java_lang_reflect_Field::set_modifiers(oop field, int value) {
   field->int_field_put(_modifiers_offset, value);
 }
 
+void java_lang_reflect_Field::set_trusted_final(oop field) {
+  field->bool_field_put(_trusted_final_offset, true);
+}
+
 void java_lang_reflect_Field::set_signature(oop field, oop value) {
   field->obj_field_put(_signature_offset, value);
 }
 
 void java_lang_reflect_Field::set_annotations(oop field, oop value) {
diff a/src/hotspot/share/classfile/javaClasses.hpp b/src/hotspot/share/classfile/javaClasses.hpp
--- a/src/hotspot/share/classfile/javaClasses.hpp
+++ b/src/hotspot/share/classfile/javaClasses.hpp
@@ -704,10 +704,11 @@
   static int _clazz_offset;
   static int _name_offset;
   static int _type_offset;
   static int _slot_offset;
   static int _modifiers_offset;
+  static int _trusted_final_offset;
   static int _signature_offset;
   static int _annotations_offset;
 
   static void compute_offsets();
 
@@ -731,10 +732,12 @@
   static void set_slot(oop reflect, int value);
 
   static int modifiers(oop field);
   static void set_modifiers(oop field, int value);
 
+  static void set_trusted_final(oop field);
+
   static void set_signature(oop constructor, oop value);
   static void set_annotations(oop constructor, oop value);
   static void set_parameter_annotations(oop method, oop value);
   static void set_annotation_default(oop method, oop value);
 
@@ -1129,10 +1132,11 @@
     MN_IS_METHOD             = 0x00010000, // method (not constructor)
     MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, // constructor
     MN_IS_FIELD              = 0x00040000, // field
     MN_IS_TYPE               = 0x00080000, // nested type
     MN_CALLER_SENSITIVE      = 0x00100000, // @CallerSensitive annotation detected
+    MN_TRUSTED_FINAL         = 0x00200000, // trusted final field
     MN_REFERENCE_KIND_SHIFT  = 24, // refKind
     MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,
     // The SEARCH_* bits are not for MN.flags but for the matchFlags argument of MHN.getMembers:
     MN_SEARCH_SUPERCLASSES   = 0x00100000, // walk super classes
     MN_SEARCH_INTERFACES     = 0x00200000, // walk implemented interfaces
diff a/src/hotspot/share/classfile/systemDictionary.cpp b/src/hotspot/share/classfile/systemDictionary.cpp
--- a/src/hotspot/share/classfile/systemDictionary.cpp
+++ b/src/hotspot/share/classfile/systemDictionary.cpp
@@ -44,10 +44,11 @@
 #include "classfile/systemDictionary.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "code/codeCache.hpp"
 #include "compiler/compileBroker.hpp"
 #include "gc/shared/gcTraceTime.inline.hpp"
+#include "gc/shared/oopStorageSet.hpp"
 #include "interpreter/bytecodeStream.hpp"
 #include "interpreter/interpreter.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "logging/log.hpp"
 #include "logging/logStream.hpp"
@@ -176,19 +177,19 @@
                          class_loader_klass,
                          vmSymbols::getSystemClassLoader_name(),
                          vmSymbols::void_classloader_signature(),
                          CHECK);
 
-  _java_system_loader = OopHandle::create((oop)result.get_jobject());
+  _java_system_loader = OopHandle(OopStorageSet::vm_global(), (oop)result.get_jobject());
 
   JavaCalls::call_static(&result,
                          class_loader_klass,
                          vmSymbols::getPlatformClassLoader_name(),
                          vmSymbols::void_classloader_signature(),
                          CHECK);
 
-  _java_platform_loader = OopHandle::create((oop)result.get_jobject());
+  _java_platform_loader = OopHandle(OopStorageSet::vm_global(), (oop)result.get_jobject());
 }
 
 ClassLoaderData* SystemDictionary::register_loader(Handle class_loader, bool create_mirror_cld) {
   if (create_mirror_cld) {
     // Add a new class loader data to the graph.
@@ -1289,108 +1290,96 @@
     return load_shared_class(ik, Handle(), Handle(), NULL, pkg_entry, THREAD);
   }
   return NULL;
 }
 
-// Check if a shared class can be loaded by the specific classloader:
-//
-// NULL classloader:
-//   - Module class from "modules" jimage. ModuleEntry must be defined in the classloader.
-//   - Class from -Xbootclasspath/a. The class has no defined PackageEntry, or must
-//     be defined in an unnamed module.
+// Check if a shared class can be loaded by the specific classloader.
 bool SystemDictionary::is_shared_class_visible(Symbol* class_name,
                                                InstanceKlass* ik,
                                                PackageEntry* pkg_entry,
                                                Handle class_loader, TRAPS) {
   assert(!ModuleEntryTable::javabase_moduleEntry()->is_patched(),
          "Cannot use sharing if java.base is patched");
-  if (ik->shared_classpath_index() < 0) {
-    // path_index < 0 indicates that the class is intended for a custom loader
-    // and should not be loaded by boot/platform/app loaders
-    if (is_builtin_class_loader(class_loader())) {
+
+  // (1) Check if we are loading into the same loader as in dump time.
+
+  if (ik->is_shared_boot_class()) {
+    if (class_loader() != NULL) {
+      return false;
+    }
+  } else if (ik->is_shared_platform_class()) {
+    if (class_loader() != java_platform_loader()) {
+      return false;
+    }
+  } else if (ik->is_shared_app_class()) {
+    if (class_loader() != java_system_loader()) {
+      return false;
+    }
+  } else {
+    // ik was loaded by a custom loader during dump time
+    if (class_loader_data(class_loader)->is_builtin_class_loader_data()) {
       return false;
     } else {
       return true;
     }
   }
 
-  // skip class visibility check
+  // (2) Check if we are loading into the same module from the same location as in dump time.
+
   if (MetaspaceShared::use_optimized_module_handling()) {
-    assert(SystemDictionary::is_shared_class_visible_impl(class_name, ik, pkg_entry, class_loader, THREAD), "Optimizing module handling failed.");
+    // Class visibility has not changed between dump time and run time, so a class
+    // that was visible (and thus archived) during dump time is always visible during runtime.
+    assert(SystemDictionary::is_shared_class_visible_impl(class_name, ik, pkg_entry, class_loader, THREAD),
+           "visibility cannot change between dump time and runtime");
     return true;
   }
   return is_shared_class_visible_impl(class_name, ik, pkg_entry, class_loader, THREAD);
 }
 
 bool SystemDictionary::is_shared_class_visible_impl(Symbol* class_name,
-                                               InstanceKlass* ik,
-                                               PackageEntry* pkg_entry,
-                                               Handle class_loader, TRAPS) {
-  int path_index = ik->shared_classpath_index();
-  ClassLoaderData* loader_data = class_loader_data(class_loader);
-  SharedClassPathEntry* ent =
-            (SharedClassPathEntry*)FileMapInfo::shared_path(path_index);
+                                                    InstanceKlass* ik,
+                                                    PackageEntry* pkg_entry,
+                                                    Handle class_loader, TRAPS) {
+  int scp_index = ik->shared_classpath_index();
+  assert(!ik->is_shared_unregistered_class(), "this function should be called for built-in classes only");
+  assert(scp_index >= 0, "must be");
+  SharedClassPathEntry* scp_entry = FileMapInfo::shared_path(scp_index);
   if (!Universe::is_module_initialized()) {
-    assert(ent != NULL && ent->is_modules_image(),
+    assert(scp_entry != NULL && scp_entry->is_modules_image(),
            "Loading non-bootstrap classes before the module system is initialized");
     assert(class_loader.is_null(), "sanity");
     return true;
   }
-  // Get the pkg_entry from the classloader
-  ModuleEntry* mod_entry = NULL;
-  TempNewSymbol pkg_name = pkg_entry != NULL ? pkg_entry->name() :
-                                               ClassLoader::package_from_class_name(class_name);
-  if (pkg_name != NULL) {
-    if (loader_data != NULL) {
-      if (pkg_entry != NULL) {
-        mod_entry = pkg_entry->module();
-        // If the archived class is from a module that has been patched at runtime,
-        // the class cannot be loaded from the archive.
-        if (mod_entry != NULL && mod_entry->is_patched()) {
-          return false;
-        }
-      }
-    }
-  }
-
-  if (class_loader.is_null()) {
-    assert(ent != NULL, "Shared class for NULL classloader must have valid SharedClassPathEntry");
-    // The NULL classloader can load archived class originated from the
-    // "modules" jimage and the -Xbootclasspath/a. For class from the
-    // "modules" jimage, the PackageEntry/ModuleEntry must be defined
-    // by the NULL classloader.
-    if (mod_entry != NULL) {
-      // PackageEntry/ModuleEntry is found in the classloader. Check if the
-      // ModuleEntry's location agrees with the archived class' origination.
-      if (ent->is_modules_image() && mod_entry->location()->starts_with("jrt:")) {
-        return true; // Module class from the "module" jimage
-      }
+
+  ModuleEntry* mod_entry = (pkg_entry == NULL) ? NULL : pkg_entry->module();
+  bool should_be_in_named_module = (mod_entry != NULL && mod_entry->is_named());
+  bool was_archived_from_named_module = scp_entry->in_named_module();
     }
 
-    // If the archived class is not from the "module" jimage, the class can be
-    // loaded by the NULL classloader if
-    //
-    // 1. the class is from the unamed package
-    // 2. or, the class is not from a module defined in the NULL classloader
-    // 3. or, the class is from an unamed module
-    if (!ent->is_modules_image() && ik->is_shared_boot_class()) {
-      // the class is from the -Xbootclasspath/a
-      if (pkg_name == NULL ||
-          pkg_entry == NULL ||
-          pkg_entry->in_unnamed_module()) {
-        assert(mod_entry == NULL ||
-               mod_entry == loader_data->unnamed_module(),
-               "the unnamed module is not defined in the classloader");
-        return true;
+  if (was_archived_from_named_module) {
+    if (should_be_in_named_module) {
+      // Is the module loaded from the same location as during dump time?
+      visible = mod_entry->shared_path_index() == scp_index;
+      if (visible) {
+        assert(!mod_entry->is_patched(), "cannot load archived classes for patched module");
       }
+    } else {
+      // During dump time, this class was in a named module, but at run time, this class should be
+      // in an unnamed module.
+      visible = false;
     }
-    return false;
-  } else {
-    bool res = SystemDictionaryShared::is_shared_class_visible_for_classloader(
-              ik, class_loader, pkg_name, pkg_entry, mod_entry, CHECK_(false));
+  } else {
+    if (should_be_in_named_module) {
+      // During dump time, this class was in an unnamed, but at run time, this class should be
+      // in a named module.
+      visible = false;
+    } else {
+      visible = true;
     return res;
   }
+
+  return visible;
 }
 
 bool SystemDictionary::check_shared_class_super_type(InstanceKlass* child, InstanceKlass* super_type,
                                                      Handle class_loader,  Handle protection_domain,
                                                      bool is_superclass, TRAPS) {
@@ -2130,11 +2119,11 @@
   _invoke_method_table = new SymbolPropertyTable(_invoke_method_size);
   _pd_cache_table = new ProtectionDomainCacheTable(defaultProtectionDomainCacheSize);
 
   // Allocate private object used as system class loader lock
   oop lock_obj = oopFactory::new_intArray(0, CHECK);
-  _system_loader_lock_obj = OopHandle::create(lock_obj);
+  _system_loader_lock_obj = OopHandle(OopStorageSet::vm_global(), lock_obj);
 
   // Initialize basic classes
   resolve_well_known_classes(CHECK);
 }
 
diff a/src/hotspot/share/classfile/vmSymbols.cpp b/src/hotspot/share/classfile/vmSymbols.cpp
--- a/src/hotspot/share/classfile/vmSymbols.cpp
+++ b/src/hotspot/share/classfile/vmSymbols.cpp
@@ -30,10 +30,11 @@
 #include "memory/allocation.inline.hpp"
 #include "memory/oopFactory.hpp"
 #include "memory/metaspaceClosure.hpp"
 #include "oops/oop.inline.hpp"
 #include "runtime/handles.inline.hpp"
+#include "utilities/tribool.hpp"
 #include "utilities/xmlstream.hpp"
 
 
 Symbol* vmSymbols::_symbols[vmSymbols::SID_LIMIT];
 
@@ -458,47 +459,11 @@
   default:
     return 0;
   }
 }
 
-bool vmIntrinsics::is_intrinsic_available(vmIntrinsics::ID id) {
-  return !vmIntrinsics::is_intrinsic_disabled(id) &&
-    !vmIntrinsics::is_disabled_by_flags(id);
-}
-
-bool vmIntrinsics::is_intrinsic_disabled(vmIntrinsics::ID id) {
-  assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
-
-  // Canonicalize DisableIntrinsic to contain only ',' as a separator.
-  // Note, DirectiveSet may not be created at this point yet since this code
-  // is called from initial stub geenration code.
-  char* local_list = (char*)DirectiveSet::canonicalize_disableintrinsic(DisableIntrinsic);
-  char* save_ptr;
-  bool found = false;
-
-  char* token = strtok_r(local_list, ",", &save_ptr);
-  while (token != NULL) {
-    if (strcmp(token, vmIntrinsics::name_at(id)) == 0) {
-      found = true;
-      break;
-    } else {
-      token = strtok_r(NULL, ",", &save_ptr);
-    }
-  }
-
-  FREE_C_HEAP_ARRAY(char, local_list);
-  return found;
-}
-
-
-bool vmIntrinsics::is_disabled_by_flags(const methodHandle& method) {
-  vmIntrinsics::ID id = method->intrinsic_id();
-  assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
-  return is_disabled_by_flags(id);
-}
-
-bool vmIntrinsics::is_disabled_by_flags(vmIntrinsics::ID id) {
+bool vmIntrinsics::disabled_by_jvm_flags(vmIntrinsics::ID id) {
   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 
   // -XX:-InlineNatives disables nearly all intrinsics except the ones listed in
   // the following switch statement.
   if (!InlineNatives) {
@@ -863,29 +828,90 @@
 static const char* vm_intrinsic_name_bodies =
   VM_INTRINSICS_DO(VM_INTRINSIC_INITIALIZE,
                    VM_SYMBOL_IGNORE, VM_SYMBOL_IGNORE, VM_SYMBOL_IGNORE, VM_ALIAS_IGNORE);
 
 static const char* vm_intrinsic_name_table[vmIntrinsics::ID_LIMIT];
+static TriBoolArray<vmIntrinsics::ID_LIMIT, int> vm_intrinsic_control_words;
+
+static void init_vm_intrinsic_name_table() {
+  const char** nt = &vm_intrinsic_name_table[0];
+  char* string = (char*) &vm_intrinsic_name_bodies[0];
+  for (int index = vmIntrinsics::FIRST_ID; index < vmIntrinsics::ID_LIMIT; index++) {
+    nt[index] = string;
+    string += strlen(string); // skip string body
+    string += 1;              // skip trailing null
+  }
+  assert(!strcmp(nt[vmIntrinsics::_hashCode], "_hashCode"), "lined up");
+  nt[vmIntrinsics::_none] = "_none";
+}
 
 const char* vmIntrinsics::name_at(vmIntrinsics::ID id) {
   const char** nt = &vm_intrinsic_name_table[0];
   if (nt[_none] == NULL) {
-    char* string = (char*) &vm_intrinsic_name_bodies[0];
-    for (int index = FIRST_ID; index < ID_LIMIT; index++) {
-      nt[index] = string;
-      string += strlen(string); // skip string body
-      string += 1;              // skip trailing null
-    }
-    assert(!strcmp(nt[_hashCode], "_hashCode"), "lined up");
-    nt[_none] = "_none";
+    init_vm_intrinsic_name_table();
   }
+
   if ((uint)id < (uint)ID_LIMIT)
     return vm_intrinsic_name_table[(uint)id];
   else
     return "(unknown intrinsic)";
 }
 
+vmIntrinsics::ID vmIntrinsics::find_id(const char* name) {
+  const char** nt = &vm_intrinsic_name_table[0];
+  if (nt[_none] == NULL) {
+    init_vm_intrinsic_name_table();
+  }
+
+  for (int index = FIRST_ID; index < ID_LIMIT; ++index) {
+    if (0 == strcmp(name, nt[index])) {
+      return ID_from(index);
+    }
+  }
+
+  return _none;
+}
+
+bool vmIntrinsics::is_disabled_by_flags(const methodHandle& method) {
+  vmIntrinsics::ID id = method->intrinsic_id();
+  return is_disabled_by_flags(id);
+}
+
+bool vmIntrinsics::is_disabled_by_flags(vmIntrinsics::ID id) {
+  assert(id > _none && id < ID_LIMIT, "must be a VM intrinsic");
+
+  // not initialized yet, process Control/DisableIntrinsic
+  if (vm_intrinsic_control_words[_none].is_default()) {
+    for (ControlIntrinsicIter iter(ControlIntrinsic); *iter != NULL; ++iter) {
+      vmIntrinsics::ID id = vmIntrinsics::find_id(*iter);
+
+      if (id != vmIntrinsics::_none) {
+        vm_intrinsic_control_words[id] = iter.is_enabled() && !disabled_by_jvm_flags(id);
+      }
+    }
+
+    // Order matters, DisableIntrinsic can overwrite ControlIntrinsic
+    for (ControlIntrinsicIter iter(DisableIntrinsic, true/*disable_all*/); *iter != NULL; ++iter) {
+      vmIntrinsics::ID id = vmIntrinsics::find_id(*iter);
+
+      if (id != vmIntrinsics::_none) {
+        vm_intrinsic_control_words[id] = false;
+      }
+    }
+
+    vm_intrinsic_control_words[_none] = true;
+  }
+
+  TriBool b = vm_intrinsic_control_words[id];
+  if (b.is_default()) {
+    // unknown yet, query and cache it
+    b = vm_intrinsic_control_words[id] = !disabled_by_jvm_flags(id);
+  }
+
+  return !b;
+}
+
 // These are flag-matching functions:
 inline bool match_F_R(jshort flags) {
   const int req = 0;
   const int neg = JVM_ACC_STATIC | JVM_ACC_SYNCHRONIZED;
   return (flags & (req | neg)) == req;
diff a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -266,10 +266,11 @@
   template(invoke_name,                               "invoke")                                   \
   template(parameterTypes_name,                       "parameterTypes")                           \
   template(returnType_name,                           "returnType")                               \
   template(signature_name,                            "signature")                                \
   template(slot_name,                                 "slot")                                     \
+  template(trusted_final_name,                        "trustedFinal")                             \
                                                                                                   \
   /* Support for annotations (JDK 1.5 and above) */                                               \
                                                                                                   \
   template(annotations_name,                          "annotations")                              \
   template(index_name,                                "index")                                    \
@@ -1653,11 +1654,14 @@
   static ID find_id_impl(vmSymbols::SID holder,
                          vmSymbols::SID name,
                          vmSymbols::SID sig,
                          jshort flags);
 
+  // check if the intrinsic is disabled by course-grained flags.
+  static bool disabled_by_jvm_flags(vmIntrinsics::ID id);
 public:
+  static ID find_id(const char* name);
   // Given a method's class, name, signature, and access flags, report its ID.
   static ID find_id(vmSymbols::SID holder,
                     vmSymbols::SID name,
                     vmSymbols::SID sig,
                     jshort flags) {
@@ -1703,14 +1707,27 @@
   static bool does_virtual_dispatch(vmIntrinsics::ID id);
   // A return value larger than 0 indicates that the intrinsic for method
   // 'method' requires predicated logic.
   static int predicates_needed(vmIntrinsics::ID id);
 
-  // Returns true if a compiler intrinsic is disabled by command-line flags
-  // and false otherwise.
-  static bool is_disabled_by_flags(const methodHandle& method);
+  // There are 2 kinds of JVM options to control intrinsics.
+  // 1. Disable/Control Intrinsic accepts a list of intrinsic IDs.
+  //    ControlIntrinsic is recommended. DisableIntrinic will be deprecated.
+  //    Currently, the DisableIntrinsic list prevails if an intrinsic appears on
+  //    both lists.
+  //
+  // 2. Explicit UseXXXIntrinsics options. eg. UseAESIntrinsics, UseCRC32Intrinsics etc.
+  //    Each option can control a group of intrinsics. The user can specify them but
+  //    their final values are subject to hardware inspection (VM_Version::initialize).
+  //    Stub generators are controlled by them.
+  //
+  // An intrinsic is enabled if and only if neither the fine-grained control(1) nor
+  // the corresponding coarse-grained control(2) disables it.
   static bool is_disabled_by_flags(vmIntrinsics::ID id);
-  static bool is_intrinsic_disabled(vmIntrinsics::ID id);
-  static bool is_intrinsic_available(vmIntrinsics::ID id);
+
+  static bool is_disabled_by_flags(const methodHandle& method);
+  static bool is_intrinsic_available(vmIntrinsics::ID id) {
+    return !is_disabled_by_flags(id);
+  }
 };
 
 #endif // SHARE_CLASSFILE_VMSYMBOLS_HPP
diff a/src/hotspot/share/code/nmethod.hpp b/src/hotspot/share/code/nmethod.hpp
--- a/src/hotspot/share/code/nmethod.hpp
+++ b/src/hotspot/share/code/nmethod.hpp
@@ -262,11 +262,11 @@
   volatile long _stack_traversal_mark;
 
   // The _hotness_counter indicates the hotness of a method. The higher
   // the value the hotter the method. The hotness counter of a nmethod is
   // set to [(ReservedCodeCacheSize / (1024 * 1024)) * 2] each time the method
-  // is active while stack scanning (mark_active_nmethods()). The hotness
+  // is active while stack scanning (do_stack_scanning()). The hotness
   // counter is decreased (by 1) while sweeping.
   int _hotness_counter;
 
   // Local state used to keep track of whether unloading is happening or not
   volatile uint8_t _is_unloading_state;
diff a/src/hotspot/share/gc/parallel/psParallelCompact.cpp b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
--- a/src/hotspot/share/gc/parallel/psParallelCompact.cpp
+++ b/src/hotspot/share/gc/parallel/psParallelCompact.cpp
@@ -48,11 +48,12 @@
 #include "gc/shared/gcTimer.hpp"
 #include "gc/shared/gcTrace.hpp"
 #include "gc/shared/gcTraceTime.inline.hpp"
 #include "gc/shared/isGCActiveMark.hpp"
 #include "gc/shared/oopStorage.inline.hpp"
-#include "gc/shared/oopStorageSet.hpp"
+#include "gc/shared/oopStorageSet.inline.hpp"
+#include "gc/shared/oopStorageSetParState.inline.hpp"
 #include "gc/shared/referencePolicy.hpp"
 #include "gc/shared/referenceProcessor.hpp"
 #include "gc/shared/referenceProcessorPhaseTimes.hpp"
 #include "gc/shared/spaceDecorator.inline.hpp"
 #include "gc/shared/taskTerminator.hpp"
@@ -2014,14 +2015,10 @@
   switch (root_type) {
     case ParallelRootType::universe:
       Universe::oops_do(&mark_and_push_closure);
       break;
 
-    case ParallelRootType::jni_handles:
-      JNIHandles::oops_do(&mark_and_push_closure);
-      break;
-
     case ParallelRootType::object_synchronizer:
       ObjectSynchronizer::oops_do(&mark_and_push_closure);
       break;
 
     case ParallelRootType::management:
@@ -2030,14 +2027,10 @@
 
     case ParallelRootType::jvmti:
       JvmtiExport::oops_do(&mark_and_push_closure);
       break;
 
-    case ParallelRootType::vm_global:
-      OopStorageSet::vm_global()->oops_do(&mark_and_push_closure);
-      break;
-
     case ParallelRootType::class_loader_data:
       {
         CLDToOopClosure cld_closure(&mark_and_push_closure, ClassLoaderData::_claim_strong);
         ClassLoaderDataGraph::always_strong_cld_do(&cld_closure);
       }
@@ -2080,10 +2073,11 @@
 }
 
 class MarkFromRootsTask : public AbstractGangTask {
   typedef AbstractRefProcTaskExecutor::ProcessTask ProcessTask;
   StrongRootsScope _strong_roots_scope; // needed for Threads::possibly_parallel_threads_do
+  OopStorageSetStrongParState<false /* concurrent */, false /* is_const */> _oop_storage_set_par_state;
   SequentialSubTasksDone _subtasks;
   TaskTerminator _terminator;
   uint _active_workers;
 
 public:
@@ -2104,10 +2098,19 @@
     _subtasks.all_tasks_completed();
 
     PCAddThreadRootsMarkingTaskClosure closure(worker_id);
     Threads::possibly_parallel_threads_do(true /*parallel */, &closure);
 
+    // Mark from OopStorages
+    {
+      ParCompactionManager* cm = ParCompactionManager::gc_thread_compaction_manager(worker_id);
+      PCMarkAndPushClosure closure(cm);
+      _oop_storage_set_par_state.oops_do(&closure);
+      // Do the real work
+      cm->follow_marking_stacks();
+    }
+
     if (_active_workers > 1) {
       steal_marking_work(_terminator, worker_id);
     }
   }
 };
@@ -2234,16 +2237,15 @@
 
   PCAdjustPointerClosure oop_closure(cm);
 
   // General strong roots.
   Universe::oops_do(&oop_closure);
-  JNIHandles::oops_do(&oop_closure);   // Global (strong) JNI handles
   Threads::oops_do(&oop_closure, NULL);
   ObjectSynchronizer::oops_do(&oop_closure);
   Management::oops_do(&oop_closure);
   JvmtiExport::oops_do(&oop_closure);
-  OopStorageSet::vm_global()->oops_do(&oop_closure);
+  OopStorageSet::strong_oops_do(&oop_closure);
   CLDToOopClosure cld_closure(&oop_closure, ClassLoaderData::_claim_strong);
   ClassLoaderDataGraph::cld_do(&cld_closure);
 
   // Now adjust pointers in remaining weak roots.  (All of which should
   // have been cleared if they pointed to non-surviving objects.)
diff a/src/hotspot/share/gc/shared/collectedHeap.hpp b/src/hotspot/share/gc/shared/collectedHeap.hpp
--- a/src/hotspot/share/gc/shared/collectedHeap.hpp
+++ b/src/hotspot/share/gc/shared/collectedHeap.hpp
@@ -27,10 +27,11 @@
 
 #include "gc/shared/gcCause.hpp"
 #include "gc/shared/gcWhen.hpp"
 #include "gc/shared/verifyOption.hpp"
 #include "memory/allocation.hpp"
+#include "memory/universe.hpp"
 #include "runtime/handles.hpp"
 #include "runtime/perfData.hpp"
 #include "runtime/safepoint.hpp"
 #include "services/memoryUsage.hpp"
 #include "utilities/debug.hpp"
@@ -175,10 +176,24 @@
     Epsilon,
     Z,
     Shenandoah
   };
 
+ protected:
+  // Get a pointer to the derived heap object.  Used to implement
+  // derived class heap() functions rather than being called directly.
+  template<typename T>
+  static T* named_heap(Name kind) {
+    CollectedHeap* heap = Universe::heap();
+    assert(heap != NULL, "Uninitialized heap");
+    assert(kind == heap->kind(), "Heap kind %u should be %u",
+           static_cast<uint>(heap->kind()), static_cast<uint>(kind));
+    return static_cast<T*>(heap);
+  }
+
+ public:
+
   static inline size_t filler_array_max_size() {
     return _filler_array_max_size;
   }
 
   virtual Name kind() const = 0;
diff a/src/hotspot/share/interpreter/abstractInterpreter.hpp b/src/hotspot/share/interpreter/abstractInterpreter.hpp
--- a/src/hotspot/share/interpreter/abstractInterpreter.hpp
+++ b/src/hotspot/share/interpreter/abstractInterpreter.hpp
@@ -37,25 +37,23 @@
 
 // Organization of the interpreter(s). There exists two different interpreters in hotpot
 // an assembly language version (aka template interpreter) and a high level language version
 // (aka c++ interpreter). Th division of labor is as follows:
 
-// Template Interpreter          C++ Interpreter        Functionality
+// Template Interpreter          Zero Interpreter       Functionality
 //
 // templateTable*                bytecodeInterpreter*   actual interpretation of bytecodes
 //
-// templateInterpreter*          cppInterpreter*        generation of assembly code that creates
+// templateInterpreter*          zeroInterpreter*       generation of assembly code that creates
 //                                                      and manages interpreter runtime frames.
-//                                                      Also code for populating interpreter
-//                                                      frames created during deoptimization.
 //
 
 class InterpreterMacroAssembler;
 
 class AbstractInterpreter: AllStatic {
   friend class VMStructs;
-  friend class CppInterpreterGenerator;
+  friend class ZeroInterpreterGenerator;
   friend class TemplateInterpreterGenerator;
  public:
   enum MethodKind {
     zerolocals,                                                 // method needs locals initialization
     zerolocals_synchronized,                                    // method needs locals initialization & is synchronized
diff a/src/hotspot/share/interpreter/interpreterRuntime.cpp b/src/hotspot/share/interpreter/interpreterRuntime.cpp
--- a/src/hotspot/share/interpreter/interpreterRuntime.cpp
+++ b/src/hotspot/share/interpreter/interpreterRuntime.cpp
@@ -583,32 +583,10 @@
   methodHandle trap_method(thread, last_frame.method());
   int trap_bci = trap_method->bci_from(last_frame.bcp());
   note_trap_inner(thread, reason, trap_method, trap_bci, THREAD);
 }
 
-#ifdef CC_INTERP
-// As legacy note_trap, but we have more arguments.
-JRT_ENTRY(void, InterpreterRuntime::note_trap(JavaThread* thread, int reason, Method *method, int trap_bci))
-  methodHandle trap_method(thread, method);
-  note_trap_inner(thread, reason, trap_method, trap_bci, THREAD);
-JRT_END
-
-// Class Deoptimization is not visible in BytecodeInterpreter, so we need a wrapper
-// for each exception.
-void InterpreterRuntime::note_nullCheck_trap(JavaThread* thread, Method *method, int trap_bci)
-  { if (ProfileTraps) note_trap(thread, Deoptimization::Reason_null_check, method, trap_bci); }
-void InterpreterRuntime::note_div0Check_trap(JavaThread* thread, Method *method, int trap_bci)
-  { if (ProfileTraps) note_trap(thread, Deoptimization::Reason_div0_check, method, trap_bci); }
-void InterpreterRuntime::note_rangeCheck_trap(JavaThread* thread, Method *method, int trap_bci)
-  { if (ProfileTraps) note_trap(thread, Deoptimization::Reason_range_check, method, trap_bci); }
-void InterpreterRuntime::note_classCheck_trap(JavaThread* thread, Method *method, int trap_bci)
-  { if (ProfileTraps) note_trap(thread, Deoptimization::Reason_class_check, method, trap_bci); }
-void InterpreterRuntime::note_arrayCheck_trap(JavaThread* thread, Method *method, int trap_bci)
-  { if (ProfileTraps) note_trap(thread, Deoptimization::Reason_array_check, method, trap_bci); }
-#endif // CC_INTERP
-
-
 static Handle get_preinitialized_exception(Klass* k, TRAPS) {
   // get klass
   InstanceKlass* klass = InstanceKlass::cast(k);
   assert(klass->is_initialized(),
          "this klass should have been initialized during VM initialization");
@@ -731,14 +709,10 @@
     thread->set_vm_result(h_exception());
     // If the method is synchronized we already unlocked the monitor
     // during deoptimization so the interpreter needs to skip it when
     // the frame is popped.
     thread->set_do_not_unlock_if_synchronized(true);
-#ifdef CC_INTERP
-    return (address) -1;
-#else
-    return Interpreter::remove_activation_entry();
 #endif
   }
 
   // Need to do this check first since when _do_not_unlock_if_synchronized
   // is set, we don't want to trigger any classloading which may make calls
@@ -746,14 +720,10 @@
   // since at this moment the method hasn't been "officially" entered yet.
   if (thread->do_not_unlock_if_synchronized()) {
     ResourceMark rm;
     assert(current_bci == 0,  "bci isn't zero for do_not_unlock_if_synchronized");
     thread->set_vm_result(exception);
-#ifdef CC_INTERP
-    return (address) -1;
-#else
-    return Interpreter::remove_activation_entry();
 #endif
   }
 
   do {
     should_repeat = false;
@@ -820,35 +790,32 @@
   // time throw or a stack unwinding throw and accordingly notify the debugger
   if (JvmtiExport::can_post_on_exceptions()) {
     JvmtiExport::post_exception_throw(thread, h_method(), last_frame.bcp(), h_exception());
   }
 
-#ifdef CC_INTERP
-  address continuation = (address)(intptr_t) handler_bci;
-#else
-  address continuation = NULL;
 #endif
   address handler_pc = NULL;
   if (handler_bci < 0 || !thread->reguard_stack((address) &continuation)) {
     // Forward exception to callee (leaving bci/bcp untouched) because (a) no
     // handler in this method, or (b) after a stack overflow there is not yet
     // enough stack space available to reprotect the stack.
-#ifndef CC_INTERP
-    continuation = Interpreter::remove_activation_entry();
 #endif
 #if COMPILER2_OR_JVMCI
     // Count this for compilation purposes
     h_method->interpreter_throwout_increment(THREAD);
 #endif
   } else {
     // handler in this method => change bci/bcp to handler bci/bcp and continue there
     handler_pc = h_method->code_base() + handler_bci;
-#ifndef CC_INTERP
+#ifndef ZERO
     set_bcp_and_mdp(handler_pc, thread);
     continuation = Interpreter::dispatch_table(vtos)[*handler_pc];
+#else
+    continuation = (address)(intptr_t) handler_bci;
 #endif
   }
+
   // notify debugger of an exception catch
   // (this is good for exceptions caught in native methods as well)
   if (JvmtiExport::can_post_on_exceptions()) {
     JvmtiExport::notice_unwind_due_to_exception(thread, h_method(), handler_pc, h_exception(), (handler_pc != NULL));
   }
diff a/src/hotspot/share/interpreter/interpreterRuntime.hpp b/src/hotspot/share/interpreter/interpreterRuntime.hpp
--- a/src/hotspot/share/interpreter/interpreterRuntime.hpp
+++ b/src/hotspot/share/interpreter/interpreterRuntime.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -44,14 +44,10 @@
 
   static void      set_bcp_and_mdp(address bcp, JavaThread*thread);
   static void      note_trap_inner(JavaThread* thread, int reason,
                                    const methodHandle& trap_method, int trap_bci, TRAPS);
   static void      note_trap(JavaThread *thread, int reason, TRAPS);
-#ifdef CC_INTERP
-  // Profile traps in C++ interpreter.
-  static void      note_trap(JavaThread* thread, int reason, Method *method, int trap_bci);
-#endif // CC_INTERP
 
   // Inner work method for Interpreter's frequency counter overflow.
   static nmethod* frequency_counter_overflow_inner(JavaThread* thread, address branch_bcp);
 
  public:
@@ -101,21 +97,10 @@
 #if INCLUDE_JVMTI
   static void    member_name_arg_or_null(JavaThread* thread, address dmh, Method* m, address bcp);
 #endif
   static void    throw_pending_exception(JavaThread* thread);
 
-#ifdef CC_INTERP
-  // Profile traps in C++ interpreter.
-  static void    note_nullCheck_trap (JavaThread* thread, Method *method, int trap_bci);
-  static void    note_div0Check_trap (JavaThread* thread, Method *method, int trap_bci);
-  static void    note_rangeCheck_trap(JavaThread* thread, Method *method, int trap_bci);
-  static void    note_classCheck_trap(JavaThread* thread, Method *method, int trap_bci);
-  static void    note_arrayCheck_trap(JavaThread* thread, Method *method, int trap_bci);
-  // A dummy for macros that shall not profile traps.
-  static void    note_no_trap(JavaThread* thread, Method *method, int trap_bci) {}
-#endif // CC_INTERP
-
   static void resolve_from_cache(JavaThread* thread, Bytecodes::Code bytecode);
  private:
   // Statics & fields
   static void resolve_get_put(JavaThread* thread, Bytecodes::Code bytecode);
 
diff a/src/hotspot/share/interpreter/rewriter.cpp b/src/hotspot/share/interpreter/rewriter.cpp
--- a/src/hotspot/share/interpreter/rewriter.cpp
+++ b/src/hotspot/share/interpreter/rewriter.cpp
@@ -401,11 +401,11 @@
     // So guarantee here.
     guarantee(bc_length > 0, "Verifier should have caught this invalid bytecode");
 
     switch (c) {
       case Bytecodes::_lookupswitch   : {
-#ifndef CC_INTERP
+#ifndef ZERO
         Bytecode_lookupswitch bc(method, bcp);
         (*bcp) = (
           bc.number_of_pairs() < BinarySwitchThreshold
           ? Bytecodes::_fast_linearswitch
           : Bytecodes::_fast_binaryswitch
@@ -413,11 +413,11 @@
 #endif
         break;
       }
       case Bytecodes::_fast_linearswitch:
       case Bytecodes::_fast_binaryswitch: {
-#ifndef CC_INTERP
+#ifndef ZERO
         (*bcp) = Bytecodes::_lookupswitch;
 #endif
         break;
       }
 
diff a/src/hotspot/share/interpreter/templateInterpreter.cpp b/src/hotspot/share/interpreter/templateInterpreter.cpp
--- a/src/hotspot/share/interpreter/templateInterpreter.cpp
+++ b/src/hotspot/share/interpreter/templateInterpreter.cpp
@@ -33,12 +33,10 @@
 #include "memory/resourceArea.hpp"
 #include "runtime/safepoint.hpp"
 #include "runtime/timerTrace.hpp"
 #include "utilities/copy.hpp"
 
-#ifndef CC_INTERP
-
 # define __ _masm->
 
 void TemplateInterpreter::initialize_stub() {
   // assertions
   assert(_code == NULL, "must only initialize once");
@@ -367,7 +365,5 @@
 }
 
 InterpreterCodelet* TemplateInterpreter::codelet_containing(address pc) {
   return (InterpreterCodelet*)_code->stub_containing(pc);
 }
-
-#endif // !CC_INTERP
diff a/src/hotspot/share/interpreter/templateInterpreterGenerator.cpp b/src/hotspot/share/interpreter/templateInterpreterGenerator.cpp
--- a/src/hotspot/share/interpreter/templateInterpreterGenerator.cpp
+++ b/src/hotspot/share/interpreter/templateInterpreterGenerator.cpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -30,12 +30,10 @@
 #include "interpreter/templateInterpreter.hpp"
 #include "interpreter/templateInterpreterGenerator.hpp"
 #include "interpreter/templateTable.hpp"
 #include "oops/methodData.hpp"
 
-#ifndef CC_INTERP
-
 #define __ Disassembler::hook<InterpreterMacroAssembler>(__FILE__, __LINE__, _masm)->
 
 TemplateInterpreterGenerator::TemplateInterpreterGenerator(StubQueue* _code): AbstractInterpreterGenerator(_code) {
   _unimplemented_bytecode    = NULL;
   _illegal_bytecode_sequence = NULL;
@@ -484,6 +482,5 @@
     }
   }
 
   return entry_point;
 }
-#endif // !CC_INTERP
diff a/src/hotspot/share/interpreter/templateTable.cpp b/src/hotspot/share/interpreter/templateTable.cpp
--- a/src/hotspot/share/interpreter/templateTable.cpp
+++ b/src/hotspot/share/interpreter/templateTable.cpp
@@ -25,11 +25,11 @@
 #include "precompiled.hpp"
 #include "gc/shared/barrierSet.hpp"
 #include "interpreter/interp_masm.hpp"
 #include "interpreter/templateTable.hpp"
 
-#ifdef CC_INTERP
+#ifdef ZERO
 
 void templateTable_init() {
 }
 
 #else
@@ -511,6 +511,6 @@
 }
 
 void TemplateTable::unimplemented_bc() {
   _masm->unimplemented( Bytecodes::name(_desc->bytecode()));
 }
-#endif /* !CC_INTERP */
+#endif /* !ZERO */
diff a/src/hotspot/share/interpreter/templateTable.hpp b/src/hotspot/share/interpreter/templateTable.hpp
--- a/src/hotspot/share/interpreter/templateTable.hpp
+++ b/src/hotspot/share/interpreter/templateTable.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -28,11 +28,11 @@
 #include "interpreter/bytecodes.hpp"
 #include "memory/allocation.hpp"
 #include "runtime/frame.hpp"
 #include "utilities/macros.hpp"
 
-#ifndef CC_INTERP
+#ifndef ZERO
 // All the necessary definitions used for (bytecode) template generation. Instead of
 // spreading the implementation functionality for each bytecode in the interpreter
 // and the snippet generator, a template is assigned to each bytecode which can be
 // used to generate the bytecode's implementation if needed.
 
@@ -351,8 +351,8 @@
 
   // Platform specifics
 #include CPU_HEADER(templateTable)
 
 };
-#endif /* !CC_INTERP */
+#endif /* !ZERO */
 
 #endif // SHARE_INTERPRETER_TEMPLATETABLE_HPP
diff a/src/hotspot/share/memory/iterator.hpp b/src/hotspot/share/memory/iterator.hpp
--- a/src/hotspot/share/memory/iterator.hpp
+++ b/src/hotspot/share/memory/iterator.hpp
@@ -352,10 +352,16 @@
   static void store_symbol(Symbol** p, Symbol* sym) {
     *p = (Symbol*)(intptr_t(sym) | (intptr_t(*p) & 1));
   }
 };
 
+template <typename E>
+class CompareClosure : public Closure {
+public:
+    virtual int do_compare(const E&, const E&) = 0;
+};
+
 // Dispatches to the non-virtual functions if OopClosureType has
 // a concrete implementation, otherwise a virtual call is taken.
 class Devirtualizer {
  public:
   template <typename OopClosureType, typename T> static void do_oop_no_verify(OopClosureType* closure, T* p);
diff a/src/hotspot/share/memory/metaspaceShared.cpp b/src/hotspot/share/memory/metaspaceShared.cpp
--- a/src/hotspot/share/memory/metaspaceShared.cpp
+++ b/src/hotspot/share/memory/metaspaceShared.cpp
@@ -456,14 +456,14 @@
       }
     }
   }
 }
 
-static GrowableArray<Handle>* _extra_interned_strings = NULL;
+static GrowableArrayCHeap<Handle, mtClassShared>* _extra_interned_strings = NULL;
 
 void MetaspaceShared::read_extra_data(const char* filename, TRAPS) {
-  _extra_interned_strings = new (ResourceObj::C_HEAP, mtClassShared) GrowableArray<Handle>(10000, mtClassShared);
+  _extra_interned_strings = new GrowableArrayCHeap<Handle, mtClassShared>(10000);
 
   HashtableTextDump reader(filename);
   reader.check_version("VERSION: 1.0");
 
   while (reader.remain() > 0) {
diff a/src/hotspot/share/opto/library_call.cpp b/src/hotspot/share/opto/library_call.cpp
--- a/src/hotspot/share/opto/library_call.cpp
+++ b/src/hotspot/share/opto/library_call.cpp
@@ -6873,25 +6873,25 @@
   address     stub_addr = NULL;
   bool        long_state = false;
 
   switch (predicate) {
   case 0:
-    if (UseSHA1Intrinsics) {
+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_sha_implCompress)) {
       klass_SHA_name = "sun/security/provider/SHA";
       stub_name = "sha1_implCompressMB";
       stub_addr = StubRoutines::sha1_implCompressMB();
     }
     break;
   case 1:
-    if (UseSHA256Intrinsics) {
+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_sha2_implCompress)) {
       klass_SHA_name = "sun/security/provider/SHA2";
       stub_name = "sha256_implCompressMB";
       stub_addr = StubRoutines::sha256_implCompressMB();
     }
     break;
   case 2:
-    if (UseSHA512Intrinsics) {
+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_sha5_implCompress)) {
       klass_SHA_name = "sun/security/provider/SHA5";
       stub_name = "sha512_implCompressMB";
       stub_addr = StubRoutines::sha512_implCompressMB();
       long_state = true;
     }
diff a/src/hotspot/share/opto/loopopts.cpp b/src/hotspot/share/opto/loopopts.cpp
--- a/src/hotspot/share/opto/loopopts.cpp
+++ b/src/hotspot/share/opto/loopopts.cpp
@@ -914,10 +914,13 @@
             int count = phi->replace_edge(n, hook);
             assert(count > 0, "inconsistent phi");
 
             // Compute latest point this store can go
             Node* lca = get_late_ctrl(n, get_ctrl(n));
+            if (lca->is_OuterStripMinedLoop()) {
+              lca = lca->in(LoopNode::EntryControl);
+            }
             if (n_loop->is_member(get_loop(lca))) {
               // LCA is in the loop - bail out
               _igvn.replace_node(hook, n);
               return;
             }
diff a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -2843,19 +2843,15 @@
 
 
 // info_ptr - pre-checked for NULL
 jvmtiError
 JvmtiEnv::GetObjectMonitorUsage(jobject object, jvmtiMonitorUsage* info_ptr) {
-  JavaThread* calling_thread = JavaThread::current();
-  jvmtiError err = get_object_monitor_usage(calling_thread, object, info_ptr);
-  if (err == JVMTI_ERROR_THREAD_NOT_SUSPENDED) {
-    // Some of the critical threads were not suspended. go to a safepoint and try again
-    VM_GetObjectMonitorUsage op(this, calling_thread, object, info_ptr);
-    VMThread::execute(&op);
-    err = op.result();
-  }
-  return err;
+  // This needs to be performed at a safepoint to gather stable data
+  // because monitor owner / waiters might not be suspended.
+  VM_GetObjectMonitorUsage op(this, JavaThread::current(), object, info_ptr);
+  VMThread::execute(&op);
+  return op.result();
 } /* end GetObjectMonitorUsage */
 
 
   //
   // Field functions
diff a/src/hotspot/share/prims/jvmtiEnvBase.cpp b/src/hotspot/share/prims/jvmtiEnvBase.cpp
--- a/src/hotspot/share/prims/jvmtiEnvBase.cpp
+++ b/src/hotspot/share/prims/jvmtiEnvBase.cpp
@@ -935,25 +935,27 @@
 }
 
 
 jvmtiError
 JvmtiEnvBase::get_object_monitor_usage(JavaThread* calling_thread, jobject object, jvmtiMonitorUsage* info_ptr) {
-  HandleMark hm;
-  Handle hobj;
+  assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
+  Thread* current_thread = VMThread::vm_thread();
+  assert(current_thread == Thread::current(), "must be");
 
-  Thread* current_thread = Thread::current();
-  bool at_safepoint = SafepointSynchronize::is_at_safepoint();
+  HandleMark hm(current_thread);
+  Handle hobj;
 
   // Check arguments
   {
     oop mirror = JNIHandles::resolve_external_guard(object);
     NULL_CHECK(mirror, JVMTI_ERROR_INVALID_OBJECT);
     NULL_CHECK(info_ptr, JVMTI_ERROR_NULL_POINTER);
 
     hobj = Handle(current_thread, mirror);
   }
 
+  ThreadsListHandle tlh(current_thread);
   JavaThread *owning_thread = NULL;
   ObjectMonitor *mon = NULL;
   jvmtiMonitorUsage ret = {
       NULL, 0, 0, NULL, 0, NULL
   };
@@ -964,15 +966,11 @@
     // Inline types instances don't support synchronization operations
     // they are marked as always locked and no attempt to remove a
     // potential bias (which cannot exist) should be made
     if (!hobj()->mark().is_always_locked()) {
       // Revoke any biases before querying the mark word
-      if (at_safepoint) {
-        BiasedLocking::revoke_at_safepoint(hobj);
-      } else {
-        BiasedLocking::revoke(hobj, calling_thread);
-      }
+      BiasedLocking::revoke_at_safepoint(hobj);
     }
 
     address owner = NULL;
     {
       markWord mark = hobj()->mark();
@@ -997,43 +995,23 @@
         owner = (address)mon->owner();
       }
     }
 
     if (owner != NULL) {
-      // Use current thread since function can be called from a
-      // JavaThread or the VMThread.
-      ThreadsListHandle tlh;
       // This monitor is owned so we have to find the owning JavaThread.
       owning_thread = Threads::owning_thread_from_monitor_owner(tlh.list(), owner);
-      // Cannot assume (owning_thread != NULL) here because this function
-      // may not have been called at a safepoint and the owning_thread
-      // might not be suspended.
-      if (owning_thread != NULL) {
-        // The monitor's owner either has to be the current thread, at safepoint
-        // or it has to be suspended. Any of these conditions will prevent both
-        // contending and waiting threads from modifying the state of
-        // the monitor.
-        if (!at_safepoint && !owning_thread->is_thread_fully_suspended(true, &debug_bits)) {
-          // Don't worry! This return of JVMTI_ERROR_THREAD_NOT_SUSPENDED
-          // will not make it back to the JVM/TI agent. The error code will
-          // get intercepted in JvmtiEnv::GetObjectMonitorUsage() which
-          // will retry the call via a VM_GetObjectMonitorUsage VM op.
-          return JVMTI_ERROR_THREAD_NOT_SUSPENDED;
-        }
-        HandleMark hm;
-        Handle     th(current_thread, owning_thread->threadObj());
-        ret.owner = (jthread)jni_reference(calling_thread, th);
-      }
-      // implied else: no owner
-    } // ThreadsListHandle is destroyed here.
+      assert(owning_thread != NULL, "owning JavaThread must not be NULL");
+      Handle     th(current_thread, owning_thread->threadObj());
+      ret.owner = (jthread)jni_reference(calling_thread, th);
+    }
 
     if (owning_thread != NULL) {  // monitor is owned
       // The recursions field of a monitor does not reflect recursions
       // as lightweight locks before inflating the monitor are not included.
       // We have to count the number of recursive monitor entries the hard way.
       // We pass a handle to survive any GCs along the way.
-      ResourceMark rm;
+      ResourceMark rm(current_thread);
       ret.entry_count = count_locked_objects(owning_thread, hobj);
     }
     // implied else: entry_count == 0
   }
 
@@ -1072,41 +1050,22 @@
     memset(ret.waiters, 0, ret.waiter_count * sizeof(jthread *));
     memset(ret.notify_waiters, 0, ret.notify_waiter_count * sizeof(jthread *));
 
     if (ret.waiter_count > 0) {
       // we have contending and/or waiting threads
-      HandleMark hm;
-      // Use current thread since function can be called from a
-      // JavaThread or the VMThread.
-      ThreadsListHandle tlh;
       if (nWant > 0) {
         // we have contending threads
-        ResourceMark rm;
+        ResourceMark rm(current_thread);
         // get_pending_threads returns only java thread so we do not need to
         // check for non java threads.
         GrowableArray<JavaThread*>* wantList = Threads::get_pending_threads(tlh.list(), nWant, (address)mon);
         if (wantList->length() < nWant) {
           // robustness: the pending list has gotten smaller
           nWant = wantList->length();
         }
         for (int i = 0; i < nWant; i++) {
           JavaThread *pending_thread = wantList->at(i);
-          // If the monitor has no owner, then a non-suspended contending
-          // thread could potentially change the state of the monitor by
-          // entering it. The JVM/TI spec doesn't allow this.
-          if (owning_thread == NULL && !at_safepoint &&
-              !pending_thread->is_thread_fully_suspended(true, &debug_bits)) {
-            if (ret.owner != NULL) {
-              destroy_jni_reference(calling_thread, ret.owner);
-            }
-            for (int j = 0; j < i; j++) {
-              destroy_jni_reference(calling_thread, ret.waiters[j]);
-            }
-            deallocate((unsigned char*)ret.waiters);
-            deallocate((unsigned char*)ret.notify_waiters);
-            return JVMTI_ERROR_THREAD_NOT_SUSPENDED;
-          }
           Handle th(current_thread, pending_thread->threadObj());
           ret.waiters[i] = (jthread)jni_reference(calling_thread, th);
         }
       }
       if (nWait > 0) {
diff a/src/hotspot/share/prims/methodHandles.cpp b/src/hotspot/share/prims/methodHandles.cpp
--- a/src/hotspot/share/prims/methodHandles.cpp
+++ b/src/hotspot/share/prims/methodHandles.cpp
@@ -124,10 +124,11 @@
   IS_METHOD             = java_lang_invoke_MemberName::MN_IS_METHOD,
   IS_OBJECT_CONSTRUCTOR = java_lang_invoke_MemberName::MN_IS_OBJECT_CONSTRUCTOR,
   IS_FIELD              = java_lang_invoke_MemberName::MN_IS_FIELD,
   IS_TYPE               = java_lang_invoke_MemberName::MN_IS_TYPE,
   CALLER_SENSITIVE      = java_lang_invoke_MemberName::MN_CALLER_SENSITIVE,
+  TRUSTED_FINAL        = java_lang_invoke_MemberName::MN_TRUSTED_FINAL,
   REFERENCE_KIND_SHIFT  = java_lang_invoke_MemberName::MN_REFERENCE_KIND_SHIFT,
   REFERENCE_KIND_MASK   = java_lang_invoke_MemberName::MN_REFERENCE_KIND_MASK,
   SEARCH_SUPERCLASSES   = java_lang_invoke_MemberName::MN_SEARCH_SUPERCLASSES,
   SEARCH_INTERFACES     = java_lang_invoke_MemberName::MN_SEARCH_INTERFACES,
   ALL_KINDS      = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE
@@ -337,23 +338,25 @@
   // constructing any new objects.
   return mname();
 }
 
 oop MethodHandles::init_field_MemberName(Handle mname, fieldDescriptor& fd, bool is_setter) {
+  InstanceKlass* ik = fd.field_holder();
   int flags = (jushort)( fd.access_flags().as_short() & JVM_RECOGNIZED_FIELD_MODIFIERS );
   flags |= IS_FIELD | ((fd.is_static() ? JVM_REF_getStatic : JVM_REF_getField) << REFERENCE_KIND_SHIFT);
   if (fd.is_inlined()) {
     flags |= JVM_ACC_FIELD_INLINED;
   }
+  if (fd.is_trusted_final()) flags |= TRUSTED_FINAL;
   if (is_setter)  flags += ((JVM_REF_putField - JVM_REF_getField) << REFERENCE_KIND_SHIFT);
   int vmindex        = fd.offset();  // determines the field uniquely when combined with static bit
 
   oop mname_oop = mname();
   java_lang_invoke_MemberName::set_flags  (mname_oop, flags);
   java_lang_invoke_MemberName::set_method (mname_oop, NULL);
   java_lang_invoke_MemberName::set_vmindex(mname_oop, vmindex);
-  java_lang_invoke_MemberName::set_clazz  (mname_oop, fd.field_holder()->java_mirror());
+  java_lang_invoke_MemberName::set_clazz  (mname_oop, ik->java_mirror());
 
   oop type = field_signature_type_or_null(fd.signature());
   oop name = field_name_or_null(fd.name());
   if (name != NULL)
     java_lang_invoke_MemberName::set_name(mname_oop,   name);
@@ -1111,10 +1114,11 @@
     template(java_lang_invoke_MemberName,MN_IS_METHOD) \
     template(java_lang_invoke_MemberName,MN_IS_OBJECT_CONSTRUCTOR) \
     template(java_lang_invoke_MemberName,MN_IS_FIELD) \
     template(java_lang_invoke_MemberName,MN_IS_TYPE) \
     template(java_lang_invoke_MemberName,MN_CALLER_SENSITIVE) \
+    template(java_lang_invoke_MemberName,MN_TRUSTED_FINAL) \
     template(java_lang_invoke_MemberName,MN_SEARCH_SUPERCLASSES) \
     template(java_lang_invoke_MemberName,MN_SEARCH_INTERFACES) \
     template(java_lang_invoke_MemberName,MN_REFERENCE_KIND_SHIFT) \
     template(java_lang_invoke_MemberName,MN_REFERENCE_KIND_MASK) \
     template(java_lang_invoke_MemberName,MN_NESTMATE_CLASS) \
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -2653,11 +2653,11 @@
 
 JVM_ENTRY(void, JVM_RegisterWhiteBoxMethods(JNIEnv* env, jclass wbclass))
   {
     if (WhiteBoxAPI) {
       // Make sure that wbclass is loaded by the null classloader
-      InstanceKlass* ik = InstanceKlass::cast(JNIHandles::resolve(wbclass)->klass());
+      InstanceKlass* ik = InstanceKlass::cast(java_lang_Class::as_Klass(JNIHandles::resolve(wbclass)));
       Handle loader(THREAD, ik->class_loader());
       if (loader.is_null()) {
         WhiteBox::register_methods(env, wbclass, thread, methods, sizeof(methods) / sizeof(methods[0]));
         WhiteBox::set_used();
       }
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -520,11 +520,10 @@
   { "MinRAMFraction",               JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "InitialRAMFraction",           JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "UseMembar",                    JDK_Version::jdk(10), JDK_Version::jdk(12), JDK_Version::undefined() },
   { "AllowRedefinitionToAddDeleteMethods", JDK_Version::jdk(13), JDK_Version::undefined(), JDK_Version::undefined() },
   { "FlightRecorder",               JDK_Version::jdk(13), JDK_Version::undefined(), JDK_Version::undefined() },
-  { "PrintVMQWaitTime",             JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "UseNewFieldLayout",            JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "ForceNUMA",                    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "UseBiasedLocking",             JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "BiasedLockingStartupDelay",    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "PrintBiasedLockingStatistics", JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
@@ -550,10 +549,11 @@
   { "SharedMiscCodeSize",            JDK_Version::undefined(), JDK_Version::jdk(10), JDK_Version::undefined() },
 #ifdef BSD
   { "UseBsdPosixThreadCPUClocks",    JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "UseOprofile",                   JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },
 #endif
+  { "PrintVMQWaitTime",              JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
 
 #ifdef TEST_VERIFY_SPECIAL_JVM_FLAGS
   // These entries will generate build errors.  Their purpose is to test the macros.
   { "dep > obs",                    JDK_Version::jdk(9), JDK_Version::jdk(8), JDK_Version::undefined() },
   { "dep > exp ",                   JDK_Version::jdk(9), JDK_Version::undefined(), JDK_Version::jdk(8) },
@@ -3333,11 +3333,11 @@
   // Populates the JavaVMInitArgs object represented by this
   // ScopedVMInitArgs object with the arguments in options.  The
   // allocated memory is deleted by the destructor.  If this method
   // returns anything other than JNI_OK, then this object is in a
   // partially constructed state, and should be abandoned.
-  jint set_args(GrowableArray<JavaVMOption>* options) {
+  jint set_args(const GrowableArrayView<JavaVMOption>* options) {
     _is_set = true;
     JavaVMOption* options_arr = NEW_C_HEAP_ARRAY_RETURN_NULL(
         JavaVMOption, options->length(), mtArguments);
     if (options_arr == NULL) {
       return JNI_ENOMEM;
@@ -3391,27 +3391,25 @@
     assert(_args.options == NULL, "shouldn't be set yet");
     assert(args_to_insert->nOptions != 0, "there should be args to insert");
     assert(vm_options_file_pos != -1, "vm_options_file_pos should be set");
 
     int length = args->nOptions + args_to_insert->nOptions - 1;
-    GrowableArray<JavaVMOption> *options = new (ResourceObj::C_HEAP, mtArguments)
-              GrowableArray<JavaVMOption>(length, mtArguments);    // Construct new option array
+    // Construct new option array
+    GrowableArrayCHeap<JavaVMOption, mtArguments> options(length);
     for (int i = 0; i < args->nOptions; i++) {
       if (i == vm_options_file_pos) {
         // insert the new options starting at the same place as the
         // -XX:VMOptionsFile option
         for (int j = 0; j < args_to_insert->nOptions; j++) {
-          options->push(args_to_insert->options[j]);
+          options.push(args_to_insert->options[j]);
         }
       } else {
-        options->push(args->options[i]);
+        options.push(args->options[i]);
       }
     }
     // make into options array
-    jint result = set_args(options);
-    delete options;
-    return result;
+    return set_args(&options);
   }
 };
 
 jint Arguments::parse_java_options_environment_variable(ScopedVMInitArgs* args) {
   return parse_options_environment_variable("_JAVA_OPTIONS", args);
@@ -3504,11 +3502,12 @@
   FREE_C_HEAP_ARRAY(char, buf);
   return retcode;
 }
 
 jint Arguments::parse_options_buffer(const char* name, char* buffer, const size_t buf_len, ScopedVMInitArgs* vm_args) {
-  GrowableArray<JavaVMOption> *options = new (ResourceObj::C_HEAP, mtArguments) GrowableArray<JavaVMOption>(2, mtArguments);    // Construct option array
+  // Construct option array
+  GrowableArrayCHeap<JavaVMOption, mtArguments> options(2);
 
   // some pointers to help with parsing
   char *buffer_end = buffer + buf_len;
   char *opt_hd = buffer;
   char *wrt = buffer;
@@ -3544,11 +3543,10 @@
           rd++;                             // don't copy close quote
         } else {
                                             // did not see closing quote
           jio_fprintf(defaultStream::error_stream(),
                       "Unmatched quote in %s\n", name);
-          delete options;
           return JNI_ERR;
         }
       } else {
         *wrt++ = *rd++;                     // copy to option string
       }
@@ -3560,20 +3558,17 @@
 
     JavaVMOption option;
     option.optionString = opt_hd;
     option.extraInfo = NULL;
 
-    options->append(option);                // Fill in option
+    options.append(option);                // Fill in option
 
     rd++;  // Advance to next character
   }
 
   // Fill out JavaVMInitArgs structure.
-  jint status = vm_args->set_args(options);
-
-  delete options;
-  return status;
+  return vm_args->set_args(&options);
 }
 
 jint Arguments::set_shared_spaces_flags_and_archive_paths() {
   if (DumpSharedSpaces) {
     if (RequireSharedSpaces) {
@@ -4174,17 +4169,17 @@
               "; ignoring UseBiasedLocking flag." );
     }
     UseBiasedLocking = false;
   }
 
-#ifdef CC_INTERP
+#ifdef ZERO
   // Clear flags not supported on zero.
   FLAG_SET_DEFAULT(ProfileInterpreter, false);
   FLAG_SET_DEFAULT(UseBiasedLocking, false);
   LP64_ONLY(FLAG_SET_DEFAULT(UseCompressedOops, false));
   LP64_ONLY(FLAG_SET_DEFAULT(UseCompressedClassPointers, false));
-#endif // CC_INTERP
+#endif // ZERO
 
   if (PrintAssembly && FLAG_IS_DEFAULT(DebugNonSafepoints)) {
     warning("PrintAssembly is enabled; turning on DebugNonSafepoints to gain additional output");
     DebugNonSafepoints = true;
   }
diff a/src/hotspot/share/runtime/deoptimization.cpp b/src/hotspot/share/runtime/deoptimization.cpp
--- a/src/hotspot/share/runtime/deoptimization.cpp
+++ b/src/hotspot/share/runtime/deoptimization.cpp
@@ -617,20 +617,13 @@
   thread->set_deopt_mark(NULL);
   thread->set_deopt_compiled_method(NULL);
 
 
   if (JvmtiExport::can_pop_frame()) {
-#ifndef CC_INTERP
     // Regardless of whether we entered this routine with the pending
     // popframe condition bit set, we should always clear it now
     thread->clear_popframe_condition();
-#else
-    // C++ interpreter will clear has_pending_popframe when it enters
-    // with method_resume. For deopt_resume2 we clear it now.
-    if (thread->popframe_forcing_deopt_reexecution())
-        thread->clear_popframe_condition();
-#endif /* CC_INTERP */
   }
 
   // unpack_frames() is called at the end of the deoptimization handler
   // and (in C2) at the end of the uncommon trap handler. Note this fact
   // so that an asynchronous stack walker can work again. This counter is
diff a/src/hotspot/share/runtime/fieldDescriptor.cpp b/src/hotspot/share/runtime/fieldDescriptor.cpp
--- a/src/hotspot/share/runtime/fieldDescriptor.cpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.cpp
@@ -57,10 +57,15 @@
   }
   assert(false, "should never happen");
   return NULL;
 }
 
+bool fieldDescriptor::is_trusted_final() const {
+  InstanceKlass* ik = field_holder();
+  return is_final() && (is_static() || ik->is_hidden() || ik->is_record() || ik->is_inline_klass());
+}
+
 AnnotationArray* fieldDescriptor::annotations() const {
   InstanceKlass* ik = field_holder();
   Array<AnnotationArray*>* md = ik->fields_annotations();
   if (md == NULL)
     return NULL;
diff a/src/hotspot/share/runtime/fieldDescriptor.hpp b/src/hotspot/share/runtime/fieldDescriptor.hpp
--- a/src/hotspot/share/runtime/fieldDescriptor.hpp
+++ b/src/hotspot/share/runtime/fieldDescriptor.hpp
@@ -102,10 +102,12 @@
   bool is_field_modification_watched() const
                                            { return access_flags().is_field_modification_watched(); }
   bool has_initialized_final_update() const { return access_flags().has_field_initialized_final_update(); }
   bool has_generic_signature()    const    { return access_flags().field_has_generic_signature(); }
 
+  bool is_trusted_final()         const;
+
   inline void set_is_field_access_watched(const bool value);
   inline void set_is_field_modification_watched(const bool value);
   inline void set_has_initialized_final_update(const bool value);
 
   // Initialization
diff a/src/hotspot/share/runtime/frame.hpp b/src/hotspot/share/runtime/frame.hpp
--- a/src/hotspot/share/runtime/frame.hpp
+++ b/src/hotspot/share/runtime/frame.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -278,15 +278,13 @@
 
   jint  interpreter_frame_expression_stack_size() const;
 
   intptr_t* interpreter_frame_sender_sp() const;
 
-#ifndef CC_INTERP
   // template based interpreter deoptimization support
   void  set_interpreter_frame_sender_sp(intptr_t* sender_sp);
   void interpreter_frame_set_monitor_end(BasicObjectLock* value);
-#endif // CC_INTERP
 
   // Address of the temp oop in the frame. Needed as GC root.
   oop* interpreter_frame_temp_oop_addr() const;
 
   // BasicObjectLocks:
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -260,13 +260,10 @@
                                                                             \
   product_pd(bool, BackgroundCompilation,                                   \
           "A thread requesting compilation is not blocked during "          \
           "compilation")                                                    \
                                                                             \
-  product(bool, PrintVMQWaitTime, false,                                    \
-          "(Deprecated) Print out the waiting time in VM operation queue")  \
-                                                                            \
   product(bool, MethodFlushing, true,                                       \
           "Reclamation of zombie and not-entrant methods")                  \
                                                                             \
   develop(bool, VerifyStack, false,                                         \
           "Verify stack of each thread when it is entering a runtime call") \
@@ -351,10 +348,14 @@
           "Enables intrinsification of ArraysSupport.vectorizedMismatch()") \
                                                                             \
   diagnostic(ccstrlist, DisableIntrinsic, "",                               \
          "do not expand intrinsics whose (internal) names appear here")     \
                                                                             \
+  diagnostic(ccstrlist, ControlIntrinsic, "",                               \
+         "Control intrinsics using a list of +/- (internal) names, "        \
+         "separated by commas")                                             \
+                                                                            \
   develop(bool, TraceCallFixup, false,                                      \
           "Trace all call fixups")                                          \
                                                                             \
   develop(bool, DeoptimizeALot, false,                                      \
           "Deoptimize at every exit from the runtime system")               \
@@ -615,11 +616,11 @@
           "Collect backtrace in throwable when exception happens")          \
                                                                             \
   product(bool, OmitStackTraceInFastThrow, true,                            \
           "Omit backtraces for some 'hot' exceptions in optimized code")    \
                                                                             \
-  manageable(bool, ShowCodeDetailsInExceptionMessages, false,               \
+  manageable(bool, ShowCodeDetailsInExceptionMessages, true,                \
           "Show exception messages from RuntimeExceptions that contain "    \
           "snippets of the failing code. Disable this to improve privacy.") \
                                                                             \
   product(bool, PrintWarnings, true,                                        \
           "Print JVM warnings to output stream")                            \
diff a/src/hotspot/share/runtime/init.cpp b/src/hotspot/share/runtime/init.cpp
--- a/src/hotspot/share/runtime/init.cpp
+++ b/src/hotspot/share/runtime/init.cpp
@@ -66,11 +66,10 @@
 jint universe_init();          // depends on codeCache_init and stubRoutines_init
 // depends on universe_init, must be before interpreter_init (currently only on SPARC)
 void gc_barrier_stubs_init();
 void interpreter_init_stub();  // before any methods loaded
 void interpreter_init_code();  // after methods loaded, but before they are linked
-void invocationCounter_init(); // after methods loaded, but before they are linked
 void accessFlags_init();
 void InterfaceSupport_init();
 void universe2_init();  // dependent on codeCache_init and stubRoutines_init, loads primordial classes
 void referenceProcessor_init();
 void jni_handles_init();
@@ -126,11 +125,10 @@
   InterfaceSupport_init();
   SharedRuntime::generate_stubs();
   universe2_init();  // dependent on codeCache_init and stubRoutines_init1
   javaClasses_init();// must happen after vtable initialization, before referenceProcessor_init
   interpreter_init_code();  // after javaClasses_init and before any method gets linked
-  invocationCounter_init(); // after javaClasses_init and before any method gets linked
   referenceProcessor_init();
   jni_handles_init();
 #if INCLUDE_VM_STRUCTS
   vmStructs_init();
 #endif // INCLUDE_VM_STRUCTS
diff a/src/hotspot/share/runtime/reflection.cpp b/src/hotspot/share/runtime/reflection.cpp
--- a/src/hotspot/share/runtime/reflection.cpp
+++ b/src/hotspot/share/runtime/reflection.cpp
@@ -899,10 +899,13 @@
 
   java_lang_reflect_Field::set_clazz(rh(), fd->field_holder()->java_mirror());
   java_lang_reflect_Field::set_slot(rh(), fd->index());
   java_lang_reflect_Field::set_name(rh(), name());
   java_lang_reflect_Field::set_type(rh(), type());
+  if (fd->is_trusted_final()) {
+    java_lang_reflect_Field::set_trusted_final(rh());
+  }
   // Note the ACC_ANNOTATION bit, which is a per-class access flag, is never set here.
   int modifiers = fd->access_flags().as_int() & JVM_RECOGNIZED_FIELD_MODIFIERS;
   if (fd->is_inlined()) {
     modifiers |= JVM_ACC_FIELD_INLINED;
   }
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -501,29 +501,22 @@
   return false;
 }
 
 class ParallelSPCleanupThreadClosure : public ThreadClosure {
 private:
-  CodeBlobClosure* _nmethod_cl;
   DeflateMonitorCounters* _counters;
 
 public:
   ParallelSPCleanupThreadClosure(DeflateMonitorCounters* counters) :
-    _nmethod_cl(UseCodeAging ? NMethodSweeper::prepare_reset_hotness_counters() : NULL),
     _counters(counters) {}
 
   void do_thread(Thread* thread) {
     // deflate_thread_local_monitors() handles or requests deflation of
     // this thread's idle monitors. If !AsyncDeflateIdleMonitors or if
     // there is a special cleanup request, deflation is handled now.
     // Otherwise, async deflation is requested via a flag.
     ObjectSynchronizer::deflate_thread_local_monitors(thread, _counters);
-    if (_nmethod_cl != NULL && thread->is_Java_thread() &&
-        ! thread->is_Code_cache_sweeper_thread()) {
-      JavaThread* jt = (JavaThread*) thread;
-      jt->nmethods_do(_nmethod_cl);
-    }
   }
 };
 
 class ParallelSPCleanupTask : public AbstractGangTask {
 private:
@@ -1207,12 +1200,10 @@
       log_info(safepoint, stats)("%-28s" UINT64_FORMAT_W(10), VM_Operation::name(index),
                _op_count[index]);
     }
   }
 
-  log_info(safepoint, stats)("VM operations coalesced during safepoint " INT64_FORMAT,
-                              VMThread::get_coalesced_count());
   log_info(safepoint, stats)("Maximum sync time  " INT64_FORMAT" ns",
                               (int64_t)(_max_sync_time));
   log_info(safepoint, stats)("Maximum vm operation time (except for Exit VM operation)  "
                               INT64_FORMAT " ns",
                               (int64_t)(_max_vmop_time));
diff a/src/hotspot/share/runtime/sharedRuntime.cpp b/src/hotspot/share/runtime/sharedRuntime.cpp
--- a/src/hotspot/share/runtime/sharedRuntime.cpp
+++ b/src/hotspot/share/runtime/sharedRuntime.cpp
@@ -772,21 +772,16 @@
                                                            ImplicitExceptionKind exception_kind)
 {
   address target_pc = NULL;
 
   if (Interpreter::contains(pc)) {
-#ifdef CC_INTERP
-    // C++ interpreter doesn't throw implicit exceptions
-    ShouldNotReachHere();
-#else
     switch (exception_kind) {
       case IMPLICIT_NULL:           return Interpreter::throw_NullPointerException_entry();
       case IMPLICIT_DIVIDE_BY_ZERO: return Interpreter::throw_ArithmeticException_entry();
       case STACK_OVERFLOW:          return Interpreter::throw_StackOverflowError_entry();
       default:                      ShouldNotReachHere();
     }
-#endif // !CC_INTERP
   } else {
     switch (exception_kind) {
       case STACK_OVERFLOW: {
         // Stack overflow only occurs upon frame setup; the callee is
         // going to be unwound. Dispatch to a shared runtime stub
diff a/src/hotspot/share/runtime/thread.hpp b/src/hotspot/share/runtime/thread.hpp
--- a/src/hotspot/share/runtime/thread.hpp
+++ b/src/hotspot/share/runtime/thread.hpp
@@ -48,13 +48,10 @@
 #include "runtime/unhandledOops.hpp"
 #include "utilities/align.hpp"
 #include "utilities/exceptions.hpp"
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/macros.hpp"
-#ifdef ZERO
-# include "stack_zero.hpp"
-#endif
 #if INCLUDE_JFR
 #include "jfr/support/jfrThreadExtension.hpp"
 #endif
 
 
@@ -2006,17 +2003,14 @@
   void clear_popframe_condition()                     { _popframe_condition = popframe_inactive; }
   static ByteSize popframe_condition_offset()         { return byte_offset_of(JavaThread, _popframe_condition); }
   bool has_pending_popframe()                         { return (popframe_condition() & popframe_pending_bit) != 0; }
   bool popframe_forcing_deopt_reexecution()           { return (popframe_condition() & popframe_force_deopt_reexecution_bit) != 0; }
   void clear_popframe_forcing_deopt_reexecution()     { _popframe_condition &= ~popframe_force_deopt_reexecution_bit; }
-#ifdef CC_INTERP
-  bool pop_frame_pending(void)                        { return ((_popframe_condition & popframe_pending_bit) != 0); }
-  void clr_pop_frame_pending(void)                    { _popframe_condition = popframe_inactive; }
+
   bool pop_frame_in_process(void)                     { return ((_popframe_condition & popframe_processing_bit) != 0); }
   void set_pop_frame_in_process(void)                 { _popframe_condition |= popframe_processing_bit; }
   void clr_pop_frame_in_process(void)                 { _popframe_condition &= ~popframe_processing_bit; }
-#endif
 
   int frames_to_pop_failed_realloc() const            { return _frames_to_pop_failed_realloc; }
   void set_frames_to_pop_failed_realloc(int nb)       { _frames_to_pop_failed_realloc = nb; }
   void dec_frames_to_pop_failed_realloc()             { _frames_to_pop_failed_realloc--; }
 
diff a/src/hotspot/share/runtime/vmOperations.hpp b/src/hotspot/share/runtime/vmOperations.hpp
--- a/src/hotspot/share/runtime/vmOperations.hpp
+++ b/src/hotspot/share/runtime/vmOperations.hpp
@@ -108,11 +108,10 @@
   template(JVMCIResizeCounters)                   \
   template(ClassLoaderStatsOperation)             \
   template(ClassLoaderHierarchyOperation)         \
   template(DumpHashtable)                         \
   template(DumpTouchedMethods)                    \
-  template(MarkActiveNMethods)                    \
   template(PrintCompileQueue)                     \
   template(PrintClassHierarchy)                   \
   template(ThreadSuspend)                         \
   template(ThreadsSuspendJVMTI)                   \
   template(ICBufferFull)                          \
@@ -129,27 +128,23 @@
     VMOp_Terminating
   };
 
  private:
   Thread*         _calling_thread;
-  uint64_t        _timestamp;
   VM_Operation*   _next;
   VM_Operation*   _prev;
 
   // The VM operation name array
   static const char* _names[];
 
  public:
-  VM_Operation() : _calling_thread(NULL), _timestamp(0),  _next(NULL), _prev(NULL) {}
+  VM_Operation() : _calling_thread(NULL), _next(NULL), _prev(NULL) {}
 
   // VM operation support (used by VM thread)
   Thread* calling_thread() const                 { return _calling_thread; }
   void set_calling_thread(Thread* thread);
 
-  uint64_t timestamp() const              { return _timestamp; }
-  void set_timestamp(uint64_t timestamp)  { _timestamp = timestamp; }
-
   // Called by VM thread - does in turn invoke doit(). Do not override this
   void evaluate();
 
   // evaluate() is called by the VMThread and in turn calls doit().
   // If the thread invoking VMThread::execute((VM_Operation*) is a JavaThread,
diff a/src/hotspot/share/runtime/vmStructs.cpp b/src/hotspot/share/runtime/vmStructs.cpp
--- a/src/hotspot/share/runtime/vmStructs.cpp
+++ b/src/hotspot/share/runtime/vmStructs.cpp
@@ -44,11 +44,10 @@
 #include "code/stubs.hpp"
 #include "code/vmreg.hpp"
 #include "compiler/compileBroker.hpp"
 #include "compiler/oopMap.hpp"
 #include "gc/shared/vmStructs_gc.hpp"
-#include "interpreter/bytecodeInterpreter.hpp"
 #include "interpreter/bytecodes.hpp"
 #include "interpreter/interpreter.hpp"
 #include "memory/allocation.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/heap.hpp"
@@ -530,13 +529,12 @@
                                                                                                                                      \
   /*******************/                                                                                                              \
   /* GrowableArrays  */                                                                                                              \
   /*******************/                                                                                                              \
                                                                                                                                      \
-  nonstatic_field(GenericGrowableArray,        _len,                                          int)                                   \
-  nonstatic_field(GenericGrowableArray,        _max,                                          int)                                   \
-  nonstatic_field(GenericGrowableArray,        _arena,                                        Arena*)                                \
+  nonstatic_field(GrowableArrayBase,           _len,                                          int)                                   \
+  nonstatic_field(GrowableArrayBase,           _max,                                          int)                                   \
   nonstatic_field(GrowableArray<int>,          _data,                                         int*)                                  \
                                                                                                                                      \
   /********************************/                                                                                                 \
   /* CodeCache (NOTE: incomplete) */                                                                                                 \
   /********************************/                                                                                                 \
@@ -1337,11 +1335,11 @@
     declare_type(DictionaryEntry, KlassHashtableEntry)                    \
   declare_toplevel_type(HashtableBucket<mtInternal>)                      \
   declare_toplevel_type(SystemDictionary)                                 \
   declare_toplevel_type(vmSymbols)                                        \
                                                                           \
-  declare_toplevel_type(GenericGrowableArray)                             \
+  declare_toplevel_type(GrowableArrayBase)                                \
   declare_toplevel_type(GrowableArray<int>)                               \
   declare_toplevel_type(Arena)                                            \
     declare_type(ResourceArea, Arena)                                     \
                                                                           \
   /***********************************************************/           \
diff a/src/hotspot/share/utilities/globalDefinitions.hpp b/src/hotspot/share/utilities/globalDefinitions.hpp
--- a/src/hotspot/share/utilities/globalDefinitions.hpp
+++ b/src/hotspot/share/utilities/globalDefinitions.hpp
@@ -430,10 +430,15 @@
 // many C++ compilers.
 //
 #define CAST_TO_FN_PTR(func_type, value) (reinterpret_cast<func_type>(value))
 #define CAST_FROM_FN_PTR(new_type, func_ptr) ((new_type)((address_word)(func_ptr)))
 
+// Need the correct linkage to call qsort without warnings
+extern "C" {
+  typedef int (*_sort_Fn)(const void *, const void *);
+}
+
 // Unsigned byte types for os and stream.hpp
 
 // Unsigned one, two, four and eigth byte quantities used for describing
 // the .class file format. See JVM book chapter 4.
 
diff a/src/hotspot/share/utilities/growableArray.hpp b/src/hotspot/share/utilities/growableArray.hpp
--- a/src/hotspot/share/utilities/growableArray.hpp
+++ b/src/hotspot/share/utilities/growableArray.hpp
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 1997, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 1997, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.
@@ -26,10 +26,11 @@
 #define SHARE_UTILITIES_GROWABLEARRAY_HPP
 
 #include "memory/allocation.hpp"
 #include "oops/array.hpp"
 #include "oops/oop.hpp"
+#include "memory/iterator.hpp"
 #include "utilities/debug.hpp"
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/ostream.hpp"
 #include "utilities/powerOfTwo.hpp"
 
@@ -38,11 +39,11 @@
 /*************************************************************************/
 /*                                                                       */
 /*     WARNING WARNING WARNING WARNING WARNING WARNING WARNING WARNING   */
 /*                                                                       */
 /* Should you use GrowableArrays to contain handles you must be certain  */
-/* the the GrowableArray does not outlive the HandleMark that contains   */
+/* that the GrowableArray does not outlive the HandleMark that contains  */
 /* the handles. Since GrowableArrays are typically resource allocated    */
 /* the following is an example of INCORRECT CODE,                        */
 /*                                                                       */
 /* ResourceMark rm;                                                      */
 /* GrowableArray<Handle>* arr = new GrowableArray<Handle>(size);         */
@@ -58,178 +59,75 @@
 /*    oop bad_oop = arr->at(0)(); // Handle is BAD HERE.                 */
 /*    ...                                                                */
 /* }                                                                     */
 /*                                                                       */
 /* If the GrowableArrays you are creating is C_Heap allocated then it    */
-/* hould not old handles since the handles could trivially try and       */
+/* should not hold handles since the handles could trivially try and     */
 /* outlive their HandleMark. In some situations you might need to do     */
 /* this and it would be legal but be very careful and see if you can do  */
 /* the code in some other manner.                                        */
 /*                                                                       */
 /*************************************************************************/
 
-// To call default constructor the placement operator new() is used.
-// It should be empty (it only returns the passed void* pointer).
-// The definition of placement operator new(size_t, void*) in the <new>.
+// Non-template base class responsible for handling the length and max.
 
-#include <new>
-
-// Need the correct linkage to call qsort without warnings
-extern "C" {
-  typedef int (*_sort_Fn)(const void *, const void *);
-}
-
+
 class GenericGrowableArray : public ResourceObj {
   friend class VMStructs;
 
- protected:
-  int    _len;          // current length
-  int    _max;          // maximum length
-  Arena* _arena;        // Indicates where allocation occurs:
-                        //   0 means default ResourceArea
-                        //   1 means on C heap
-                        //   otherwise, allocate in _arena
-
-  MEMFLAGS   _memflags;   // memory type if allocation in C heap
-
-#ifdef ASSERT
-  int    _nesting;      // resource area nesting at creation
-  void   set_nesting();
-  void   check_nesting();
-#else
-#define  set_nesting();
-#define  check_nesting();
-#endif
-
-  // Where are we going to allocate memory?
-  bool on_C_heap() { return _arena == (Arena*)1; }
-  bool on_stack () { return _arena == NULL;      }
-  bool on_arena () { return _arena >  (Arena*)1;  }
-
-  // This GA will use the resource stack for storage if c_heap==false,
-  // Else it will use the C heap.  Use clear_and_deallocate to avoid leaks.
-  GenericGrowableArray(int initial_size, int initial_len, MEMFLAGS flags) {
-    _len = initial_len;
-    _max = initial_size;
-    _memflags = flags;
-
-    assert(_len >= 0 && _len <= _max, "initial_len too big");
-
-    const bool c_heap = flags != mtNone;
-    _arena = (c_heap ? (Arena*)1 : NULL);
-    set_nesting();
-    assert(!on_C_heap() || allocated_on_C_heap(), "growable array must be on C heap if elements are");
-    assert(!on_stack() ||
-           (allocated_on_res_area() || allocated_on_stack()),
-           "growable array must be on stack if elements are not on arena and not on C heap");
-  }
+protected:
+  // Current number of accessible elements
+  int _len;
+  // Current number of allocated elements
+  int _max;
 
-  // This GA will use the given arena for storage.
-  // Consider using new(arena) GrowableArray<T> to allocate the header.
-  GenericGrowableArray(Arena* arena, int initial_size, int initial_len) {
-    _len = initial_len;
-    _max = initial_size;
+  GrowableArrayBase(int initial_max, int initial_len) :
+      _len(initial_len),
+      _max(initial_max) {
     assert(_len >= 0 && _len <= _max, "initial_len too big");
-    _arena = arena;
-    _memflags = mtNone;
-
-    assert(on_arena(), "arena has taken on reserved value 0 or 1");
-    // Relax next assert to allow object allocation on resource area,
-    // on stack or embedded into an other object.
-    assert(allocated_on_arena() || allocated_on_stack(),
-           "growable array must be on arena or on stack if elements are on arena");
   }
 
-  void* raw_allocate(int elementSize);
+  ~GrowableArrayBase() {}
 
-  void free_C_heap(void* elements);
-};
-
-template<class E> class GrowableArrayIterator;
-template<class E, class UnaryPredicate> class GrowableArrayFilterIterator;
-
-template<class E>
-class CompareClosure : public Closure {
-public:
-    virtual int do_compare(const E&, const E&) = 0;
-};
-
-template<class E> class GrowableArray : public GenericGrowableArray {
-  friend class VMStructs;
-
- private:
-  E*     _data;         // data array
-
-  void grow(int j);
-  void raw_at_put_grow(int i, const E& p, const E& fill);
-  void  clear_and_deallocate();
-
-public:
-  GrowableArray(int initial_size, MEMFLAGS F = mtNone)
-    : GenericGrowableArray(initial_size, 0, F) {
-    _data = (E*)raw_allocate(sizeof(E));
-// Needed for Visual Studio 2012 and older
-#ifdef _MSC_VER
-#pragma warning(suppress: 4345)
-#endif
-    for (int i = 0; i < _max; i++) ::new ((void*)&_data[i]) E();
-  }
-
-  GrowableArray(int initial_size, int initial_len, const E& filler, MEMFLAGS memflags = mtNone)
-    : GenericGrowableArray(initial_size, initial_len, memflags) {
-    _data = (E*)raw_allocate(sizeof(E));
-    int i = 0;
-    for (; i < _len; i++) ::new ((void*)&_data[i]) E(filler);
-    for (; i < _max; i++) ::new ((void*)&_data[i]) E();
-  }
-
-  // Watch out, if filler was generated by a constructor, the destuctor might
-  // be called on the original object invalidating all the copies made here.
-  // Carefully design the copy constructor.
-  GrowableArray(Arena* arena, int initial_size, int initial_len, const E& filler) :
-      GenericGrowableArray(arena, initial_size, initial_len) {
-    _data = (E*)raw_allocate(sizeof(E));
-    int i = 0;
-    for (; i < _len; i++) ::new ((void*)&_data[i]) E(filler);
-    for (; i < _max; i++) ::new ((void*)&_data[i]) E();
-  }
-
-  GrowableArray() : GenericGrowableArray(2, 0, mtNone) {
-    _data = (E*)raw_allocate(sizeof(E));
-    ::new ((void*)&_data[0]) E();
-    ::new ((void*)&_data[1]) E();
-  }
-
-                                // Does nothing for resource and arena objects
-  ~GrowableArray()              { if (on_C_heap()) clear_and_deallocate(); }
-
   void  clear()                 { _len = 0; }
   int   length() const          { return _len; }
   int   max_length() const      { return _max; }
-  void  trunc_to(int l)         { assert(l <= _len,"cannot increase length"); _len = l; }
+
   bool  is_empty() const        { return _len == 0; }
   bool  is_nonempty() const     { return _len != 0; }
   bool  is_full() const         { return _len == _max; }
-  DEBUG_ONLY(E* data_addr() const      { return _data; })
-
-  void print();
-
-  int append(const E& elem) {
-    check_nesting();
-    if (_len == _max) grow(_len);
-    int idx = _len++;
-    _data[idx] = elem;
+
+  void  clear()                 { _len = 0; }
+  void  trunc_to(int length)    {
+    assert(length <= _len,"cannot increase length");
     return idx;
   }
+};
 
-  bool append_if_missing(const E& elem) {
-    // Returns TRUE if elem is added.
-    bool missed = !contains(elem);
-    if (missed) append(elem);
-    return missed;
-  }
+template <typename E> class GrowableArrayIterator;
+template <typename E, typename UnaryPredicate> class GrowableArrayFilterIterator;
+
+// Extends GrowableArrayBase with a typed data array.
+//
+// E: Element type
+//
+// The "view" adds function that don't grow or deallocate
+// the _data array, so there's no need for an allocator.
+//
+// The "view" can be used to type erase the allocator classes
+// of GrowableArrayWithAllocator.
+template <typename E>
+class GrowableArrayView : public GrowableArrayBase {
+protected:
+  E* _data; // data array
+
+  GrowableArrayView<E>(E* data, int initial_max, int initial_len) :
+      GrowableArrayBase(initial_max, initial_len), _data(data) {}
+
+  ~GrowableArrayView() {}
 
+public:
   E& at(int i) {
     assert(0 <= i && i < _len, "illegal index");
     return _data[i];
   }
 
@@ -263,40 +161,20 @@
 
   GrowableArrayIterator<E> end() const {
     return GrowableArrayIterator<E>(this, length());
   }
 
-  void push(const E& elem) { append(elem); }
-
   E pop() {
     assert(_len > 0, "empty list");
     return _data[--_len];
   }
 
   void at_put(int i, const E& elem) {
     assert(0 <= i && i < _len, "illegal index");
     _data[i] = elem;
   }
 
-  E at_grow(int i, const E& fill = E()) {
-    assert(0 <= i, "negative index");
-    check_nesting();
-    if (i >= _len) {
-      if (i >= _max) grow(i);
-      for (int j = _len; j <= i; j++)
-        _data[j] = fill;
-      _len = i+1;
-    }
-    return _data[i];
-  }
-
-  void at_put_grow(int i, const E& elem, const E& fill = E()) {
-    assert(0 <= i, "negative index");
-    check_nesting();
-    raw_at_put_grow(i, elem, fill);
-  }
-
   bool contains(const E& elem) const {
     for (int i = 0; i < _len; i++) {
       if (_data[i] == elem) return true;
     }
     return false;
@@ -356,73 +234,18 @@
       // Replace removed element with last one.
       _data[index] = _data[_len];
     }
   }
 
-  // inserts the given element before the element at index i
-  void insert_before(const int idx, const E& elem) {
-    assert(0 <= idx && idx <= _len, "illegal index");
-    check_nesting();
-    if (_len == _max) grow(_len);
-    for (int j = _len - 1; j >= idx; j--) {
-      _data[j + 1] = _data[j];
-    }
-    _len++;
-    _data[idx] = elem;
-  }
-
-  void insert_before(const int idx, const GrowableArray<E>* array) {
-    assert(0 <= idx && idx <= _len, "illegal index");
-    check_nesting();
-    int array_len = array->length();
-    int new_len = _len + array_len;
-    if (new_len >= _max) grow(new_len);
-
-    for (int j = _len - 1; j >= idx; j--) {
-      _data[j + array_len] = _data[j];
-    }
-
-    for (int j = 0; j < array_len; j++) {
-      _data[idx + j] = array->_data[j];
-    }
-
-    _len += array_len;
-  }
-
-  void appendAll(const GrowableArray<E>* l) {
-    for (int i = 0; i < l->_len; i++) {
-      raw_at_put_grow(_len, l->_data[i], E());
-    }
-  }
-
-  void appendAll(const Array<E>* l) {
-    for (int i = 0; i < l->length(); i++) {
-      raw_at_put_grow(_len, l->at(i), E());
-    }
-  }
-
-  void sort(int f(E*,E*)) {
+  void sort(int f(E*, E*)) {
     qsort(_data, length(), sizeof(E), (_sort_Fn)f);
   }
   // sort by fixed-stride sub arrays:
-  void sort(int f(E*,E*), int stride) {
+  void sort(int f(E*, E*), int stride) {
     qsort(_data, length() / stride, sizeof(E) * stride, (_sort_Fn)f);
   }
 
-  // Binary search and insertion utility.  Search array for element
-  // matching key according to the static compare function.  Insert
-  // that element is not already in the list.  Assumes the list is
-  // already sorted according to compare function.
-  template <int compare(const E&, const E&)> E insert_sorted(const E& key) {
-    bool found;
-    int location = find_sorted<E, compare>(key, found);
-    if (!found) {
-      insert_before(location, key);
-    }
-    return at(location);
-  }
-
   template <typename K, int compare(const K&, const E&)> int find_sorted(const K& key, bool& found) {
     found = false;
     int min = 0;
     int max = length() - 1;
 
@@ -440,20 +263,11 @@
       }
     }
     return min;
   }
 
-  E insert_sorted(CompareClosure<E>* cc, const E& key) {
-    bool found;
-    int location = find_sorted(cc, key, found);
-    if (!found) {
-      insert_before(location, key);
-    }
-    return at(location);
-  }
-
-  template<typename K>
+  template <typename K>
   int find_sorted(CompareClosure<E>* cc, const K& key, bool& found) {
     found = false;
     int min = 0;
     int max = length() - 1;
 
@@ -470,84 +284,452 @@
         return mid;
       }
     }
     return min;
   }
+
+  void print() {
+    tty->print("Growable Array " INTPTR_FORMAT, this);
+    tty->print(": length %ld (_max %ld) { ", _len, _max);
+    for (int i = 0; i < _len; i++) {
+      tty->print(INTPTR_FORMAT " ", *(intptr_t*)&(_data[i]));
+    }
+    tty->print("}\n");
+  }
 };
 
-// Global GrowableArray methods (one instance in the library per each 'E' type).
+// GrowableArrayWithAllocator extends the "view" with
+// the capability to grow and deallocate the data array.
+//
+// The allocator responsibility is delegated to the sub-class.
+//
+// Derived: The sub-class responsible for allocation / deallocation
+//  - E* Derived::allocate()       - member function responsible for allocation
+//  - void Derived::deallocate(E*) - member function responsible for deallocation
+template <typename E, typename Derived>
+class GrowableArrayWithAllocator : public GrowableArrayView<E> {
+  friend class VMStructs;
+
+  void grow(int j);
 
-template<class E> void GrowableArray<E>::grow(int j) {
-    int old_max = _max;
-    // grow the array by increasing _max to the first power of two larger than the size we need
-    _max = next_power_of_2((uint32_t)j);
-    // j < _max
-    E* newData = (E*)raw_allocate(sizeof(E));
+protected:
+  GrowableArrayWithAllocator(E* data, int initial_max) :
+      GrowableArrayView<E>(data, initial_max, 0) {
+    for (int i = 0; i < initial_max; i++) {
+      ::new ((void*)&data[i]) E();
+    }
+  }
+
+  GrowableArrayWithAllocator(E* data, int initial_max, int initial_len, const E& filler) :
+      GrowableArrayView<E>(data, initial_max, initial_len) {
     int i = 0;
-    for (     ; i < _len; i++) ::new ((void*)&newData[i]) E(_data[i]);
-// Needed for Visual Studio 2012 and older
-#ifdef _MSC_VER
-#pragma warning(suppress: 4345)
-#endif
-    for (     ; i < _max; i++) ::new ((void*)&newData[i]) E();
-    for (i = 0; i < old_max; i++) _data[i].~E();
-    if (on_C_heap() && _data != NULL) {
-      free_C_heap(_data);
+    for (; i < initial_len; i++) {
+      ::new ((void*)&data[i]) E(filler);
     }
-    _data = newData;
-}
+    for (; i < initial_max; i++) {
+      ::new ((void*)&data[i]) E();
+    }
+  }
+
+  ~GrowableArrayWithAllocator() {}
+
+public:
+  int append(const E& elem) {
+    if (this->_len == this->_max) grow(this->_len);
+    int idx = this->_len++;
+    this->_data[idx] = elem;
+    return idx;
+  }
+
+  bool append_if_missing(const E& elem) {
+    // Returns TRUE if elem is added.
+    bool missed = !this->contains(elem);
+    if (missed) append(elem);
+    return missed;
+  }
 
-template<class E> void GrowableArray<E>::raw_at_put_grow(int i, const E& p, const E& fill) {
-    if (i >= _len) {
-      if (i >= _max) grow(i);
-      for (int j = _len; j < i; j++)
-        _data[j] = fill;
-      _len = i+1;
+  void push(const E& elem) { append(elem); }
+
+  E at_grow(int i, const E& fill = E()) {
+    assert(0 <= i, "negative index");
+    if (i >= this->_len) {
+      if (i >= this->_max) grow(i);
+      for (int j = this->_len; j <= i; j++)
+        this->_data[j] = fill;
+      this->_len = i+1;
     }
-    _data[i] = p;
-}
+    return this->_data[i];
+  }
+
+  void at_put_grow(int i, const E& elem, const E& fill = E()) {
+    assert(0 <= i, "negative index");
+    if (i >= this->_len) {
+      if (i >= this->_max) grow(i);
+      for (int j = this->_len; j < i; j++)
+        this->_data[j] = fill;
+      this->_len = i+1;
+    }
+    this->_data[i] = elem;
+  }
+
+  // inserts the given element before the element at index i
+  void insert_before(const int idx, const E& elem) {
+    assert(0 <= idx && idx <= this->_len, "illegal index");
+    if (this->_len == this->_max) grow(this->_len);
+    for (int j = this->_len - 1; j >= idx; j--) {
+      this->_data[j + 1] = this->_data[j];
+    }
+    this->_len++;
+    this->_data[idx] = elem;
+  }
+
+  void insert_before(const int idx, const GrowableArrayView<E>* array) {
+    assert(0 <= idx && idx <= this->_len, "illegal index");
+    int array_len = array->length();
+    int new_len = this->_len + array_len;
+    if (new_len >= this->_max) grow(new_len);
+
+    for (int j = this->_len - 1; j >= idx; j--) {
+      this->_data[j + array_len] = this->_data[j];
+    }
+
+    for (int j = 0; j < array_len; j++) {
+      this->_data[idx + j] = array->at(j);
+    }
+
+    this->_len += array_len;
+  }
+
+  void appendAll(const GrowableArrayView<E>* l) {
+    for (int i = 0; i < l->length(); i++) {
+      this->at_put_grow(this->_len, l->at(i), E());
+    }
+  }
+
+  void appendAll(const Array<E>* l) {
+    for (int i = 0; i < l->length(); i++) {
+      this->at_put_grow(this->_len, l->at(i), E());
+    }
+  }
+
+  // Binary search and insertion utility.  Search array for element
+  // matching key according to the static compare function.  Insert
+  // that element is not already in the list.  Assumes the list is
+  // already sorted according to compare function.
+  template <int compare(const E&, const E&)> E insert_sorted(const E& key) {
+    bool found;
+    int location = GrowableArrayView<E>::template find_sorted<E, compare>(key, found);
+    if (!found) {
+      insert_before(location, key);
+    }
+    return this->at(location);
+  }
 
-// This function clears and deallocate the data in the growable array that
-// has been allocated on the C heap.  It's not public - called by the
-// destructor.
-template<class E> void GrowableArray<E>::clear_and_deallocate() {
-    assert(on_C_heap(),
-           "clear_and_deallocate should only be called when on C heap");
-    clear();
-    if (_data != NULL) {
-      for (int i = 0; i < _max; i++) _data[i].~E();
-      free_C_heap(_data);
-      _data = NULL;
+  E insert_sorted(CompareClosure<E>* cc, const E& key) {
+    bool found;
+    int location = find_sorted(cc, key, found);
+    if (!found) {
+      insert_before(location, key);
     }
+    return this->at(location);
+  }
+
+  void clear_and_deallocate();
+};
+
+template <typename E, typename Derived>
+void GrowableArrayWithAllocator<E, Derived>::grow(int j) {
+  int old_max = this->_max;
+  // grow the array by increasing _max to the first power of two larger than the size we need
+  this->_max = next_power_of_2((uint32_t)j);
+  // j < _max
+  E* newData = static_cast<Derived*>(this)->allocate();
+  int i = 0;
+  for (     ; i < this->_len; i++) ::new ((void*)&newData[i]) E(this->_data[i]);
+  for (     ; i < this->_max; i++) ::new ((void*)&newData[i]) E();
+  for (i = 0; i < old_max; i++) this->_data[i].~E();
+  if (this->_data != NULL) {
+    static_cast<Derived*>(this)->deallocate(this->_data);
+  }
+  this->_data = newData;
 }
 
-template<class E> void GrowableArray<E>::print() {
-    tty->print("Growable Array " INTPTR_FORMAT, this);
-    tty->print(": length %ld (_max %ld) { ", _len, _max);
-    for (int i = 0; i < _len; i++) tty->print(INTPTR_FORMAT " ", *(intptr_t*)&(_data[i]));
-    tty->print("}\n");
+template <typename E, typename Derived>
+void GrowableArrayWithAllocator<E, Derived>::clear_and_deallocate() {
+  if (this->_data != NULL) {
+    for (int i = 0; i < this->_max; i++) {
+      this->_data[i].~E();
+    }
+    static_cast<Derived*>(this)->deallocate(this->_data);
+    this->_data = NULL;
+  }
+  this->_len = 0;
+  this->_max = 0;
 }
 
+class GrowableArrayResourceAllocator {
+public:
+  static void* allocate(int max, int element_size);
+};
+
+// Arena allocator
+class GrowableArrayArenaAllocator {
+public:
+  static void* allocate(int max, int element_size, Arena* arena);
+};
+
+// CHeap allocator
+class GrowableArrayCHeapAllocator {
+public:
+  static void* allocate(int max, int element_size, MEMFLAGS memflags);
+  static void deallocate(void* mem);
+};
+
+#ifdef ASSERT
+
+// Checks resource allocation nesting
+class GrowableArrayNestingCheck {
+  // resource area nesting at creation
+  int _nesting;
+
+public:
+  GrowableArrayNestingCheck(bool on_stack);
+
+  void on_stack_alloc() const;
+};
+
+#endif // ASSERT
+
+// Encodes where the backing array is allocated
+// and performs necessary checks.
+class GrowableArrayMetadata {
+  uintptr_t _bits;
+
+  // resource area nesting at creation
+  debug_only(GrowableArrayNestingCheck _nesting_check;)
+
+  uintptr_t bits(MEMFLAGS memflags) const {
+    if (memflags == mtNone) {
+      // Stack allocation
+      return 0;
+    }
+
+    // CHeap allocation
+    return (uintptr_t(memflags) << 1) | 1;
+  }
+
+  uintptr_t bits(Arena* arena) const {
+    return uintptr_t(arena);
+  }
+
+public:
+  GrowableArrayMetadata(Arena* arena) :
+      _bits(bits(arena))
+      debug_only(COMMA _nesting_check(on_stack())) {
+  }
+
+  GrowableArrayMetadata(MEMFLAGS memflags) :
+      _bits(bits(memflags))
+      debug_only(COMMA _nesting_check(on_stack())) {
+  }
+
+#ifdef ASSERT
+  GrowableArrayMetadata(const GrowableArrayMetadata& other) :
+      _bits(other._bits),
+      _nesting_check(other._nesting_check) {
+    assert(!on_C_heap(), "Copying of CHeap arrays not supported");
+    assert(!other.on_C_heap(), "Copying of CHeap arrays not supported");
+  }
+
+  GrowableArrayMetadata& operator=(const GrowableArrayMetadata& other) {
+    _bits = other._bits;
+    _nesting_check = other._nesting_check;
+    assert(!on_C_heap(), "Assignment of CHeap arrays not supported");
+    assert(!other.on_C_heap(), "Assignment of CHeap arrays not supported");
+    return *this;
+  }
+
+  void init_checks(const GrowableArrayBase* array) const;
+  void on_stack_alloc_check() const;
+#endif // ASSERT
+
+  bool on_C_heap() const { return (_bits & 1) == 1; }
+  bool on_stack () const { return _bits == 0;      }
+  bool on_arena () const { return (_bits & 1) == 0 && _bits != 0; }
+
+  Arena* arena() const      { return (Arena*)_bits; }
+  MEMFLAGS memflags() const { return MEMFLAGS(_bits >> 1); }
+};
+
+// THE GrowableArray.
+//
+// Supports multiple allocation strategies:
+//  - Resource stack allocation: if memflags == mtNone
+//  - CHeap allocation: if memflags != mtNone
+//  - Arena allocation: if an arena is provided
+//
+// There are some drawbacks of using GrowableArray, that are removed in some
+// of the other implementations of GrowableArrayWithAllocator sub-classes:
+//
+// Memory overhead: The multiple allocation strategies uses extra metadata
+//  embedded in the instance.
+//
+// Strict allocation locations: There are rules about where the GrowableArray
+//  instance is allocated, that depends on where the data array is allocated.
+//  See: init_checks.
+
+template <typename E>
+class GrowableArray : public GrowableArrayWithAllocator<E, GrowableArray<E> > {
+  friend class GrowableArrayWithAllocator<E, GrowableArray<E> >;
+  friend class GrowableArrayTest;
+
+  static E* allocate(int max) {
+    return (E*)GrowableArrayResourceAllocator::allocate(max, sizeof(E));
+  }
+
+  static E* allocate(int max, MEMFLAGS memflags) {
+    if (memflags != mtNone) {
+      return (E*)GrowableArrayCHeapAllocator::allocate(max, sizeof(E), memflags);
+    }
+
+    return (E*)GrowableArrayResourceAllocator::allocate(max, sizeof(E));
+  }
+
+  static E* allocate(int max, Arena* arena) {
+    return (E*)GrowableArrayArenaAllocator::allocate(max, sizeof(E), arena);
+  }
+
+  GrowableArrayMetadata _metadata;
+
+  void init_checks() const { debug_only(_metadata.init_checks(this);) }
+
+  // Where are we going to allocate memory?
+  bool on_C_heap() const { return _metadata.on_C_heap(); }
+  bool on_stack () const { return _metadata.on_stack(); }
+  bool on_arena () const { return _metadata.on_arena(); }
+
+  E* allocate() {
+    if (on_stack()) {
+      debug_only(_metadata.on_stack_alloc_check());
+      return allocate(this->_max);
+    }
+
+    if (on_C_heap()) {
+      return allocate(this->_max, _metadata.memflags());
+    }
+
+    assert(on_arena(), "Sanity");
+    return allocate(this->_max, _metadata.arena());
+  }
+
+  void deallocate(E* mem) {
+    if (on_C_heap()) {
+      GrowableArrayCHeapAllocator::deallocate(mem);
+    }
+  }
+
+public:
+  GrowableArray(int initial_max = 2, MEMFLAGS memflags = mtNone) :
+      GrowableArrayWithAllocator<E, GrowableArray<E> >(
+          allocate(initial_max, memflags),
+          initial_max),
+      _metadata(memflags) {
+    init_checks();
+  }
+
+  GrowableArray(int initial_max, int initial_len, const E& filler, MEMFLAGS memflags = mtNone) :
+      GrowableArrayWithAllocator<E, GrowableArray<E> >(
+          allocate(initial_max, memflags),
+          initial_max, initial_len, filler),
+      _metadata(memflags) {
+    init_checks();
+  }
+
+  GrowableArray(Arena* arena, int initial_max, int initial_len, const E& filler) :
+      GrowableArrayWithAllocator<E, GrowableArray<E> >(
+          allocate(initial_max, arena),
+          initial_max, initial_len, filler),
+      _metadata(arena) {
+    init_checks();
+  }
+
+  ~GrowableArray() {
+    if (on_C_heap()) {
+      this->clear_and_deallocate();
+    }
+  }
+};
+
+// Leaner GrowableArray for CHeap backed data arrays, with compile-time decided MEMFLAGS.
+template <typename E, MEMFLAGS F>
+class GrowableArrayCHeap : public GrowableArrayWithAllocator<E, GrowableArrayCHeap<E, F> > {
+  friend class GrowableArrayWithAllocator<E, GrowableArrayCHeap<E, F> >;
+
+  STATIC_ASSERT(F != mtNone);
+
+  static E* allocate(int max, MEMFLAGS flags) {
+    if (max == 0) {
+      return NULL;
+    }
+
+    return (E*)GrowableArrayCHeapAllocator::allocate(max, sizeof(E), flags);
+  }
+
+  NONCOPYABLE(GrowableArrayCHeap);
+
+  E* allocate() {
+    return allocate(this->_max, F);
+  }
+
+  void deallocate(E* mem) {
+    GrowableArrayCHeapAllocator::deallocate(mem);
+  }
+
+public:
+  GrowableArrayCHeap(int initial_max) :
+      GrowableArrayWithAllocator<E, GrowableArrayCHeap<E, F> >(
+          allocate(initial_max, F),
+          initial_max) {}
+
+  GrowableArrayCHeap(int initial_max, int initial_len, const E& filler) :
+      GrowableArrayWithAllocator<E, GrowableArrayCHeap<E, F> >(
+          allocate(initial_max, F),
+          initial_max, initial_len, filler) {}
+
+  ~GrowableArrayCHeap() {
+    this->clear_and_deallocate();
+  }
+
+  void* operator new(size_t size) throw() {
+    return ResourceObj::operator new(size, ResourceObj::C_HEAP, F);
+  }
+
+  void* operator new(size_t size, const std::nothrow_t&  nothrow_constant) throw() {
+    return ResourceObj::operator new(size, nothrow_constant, ResourceObj::C_HEAP, F);
+  }
+};
+
 // Custom STL-style iterator to iterate over GrowableArrays
 // It is constructed by invoking GrowableArray::begin() and GrowableArray::end()
-template<class E> class GrowableArrayIterator : public StackObj {
-  friend class GrowableArray<E>;
-  template<class F, class UnaryPredicate> friend class GrowableArrayFilterIterator;
+template <typename E>
+class GrowableArrayIterator : public StackObj {
+  friend class GrowableArrayView<E>;
+  template <typename F, typename UnaryPredicate> friend class GrowableArrayFilterIterator;
 
  private:
-  const GrowableArray<E>* _array; // GrowableArray we iterate over
-  int _position;                  // The current position in the GrowableArray
+  const GrowableArrayView<E>* _array; // GrowableArray we iterate over
+  int _position;                      // The current position in the GrowableArray
 
   // Private constructor used in GrowableArray::begin() and GrowableArray::end()
-  GrowableArrayIterator(const GrowableArray<E>* array, int position) : _array(array), _position(position) {
+  GrowableArrayIterator(const GrowableArrayView<E>* array, int position) : _array(array), _position(position) {
     assert(0 <= position && position <= _array->length(), "illegal position");
   }
 
  public:
   GrowableArrayIterator() : _array(NULL), _position(0) { }
-  GrowableArrayIterator<E>& operator++()  { ++_position; return *this; }
-  E operator*()                           { return _array->at(_position); }
+  GrowableArrayIterator<E>& operator++() { ++_position; return *this; }
+  E operator*()                          { return _array->at(_position); }
 
   bool operator==(const GrowableArrayIterator<E>& rhs)  {
     assert(_array == rhs._array, "iterator belongs to different array");
     return _position == rhs._position;
   }
@@ -557,21 +739,22 @@
     return _position != rhs._position;
   }
 };
 
 // Custom STL-style iterator to iterate over elements of a GrowableArray that satisfy a given predicate
-template<class E, class UnaryPredicate> class GrowableArrayFilterIterator : public StackObj {
-  friend class GrowableArray<E>;
+template <typename E, class UnaryPredicate>
+class GrowableArrayFilterIterator : public StackObj {
+  friend class GrowableArrayView<E>;
 
  private:
-  const GrowableArray<E>* _array;   // GrowableArray we iterate over
-  int _position;                    // Current position in the GrowableArray
-  UnaryPredicate _predicate;        // Unary predicate the elements of the GrowableArray should satisfy
+  const GrowableArrayView<E>* _array; // GrowableArray we iterate over
+  int _position;                      // Current position in the GrowableArray
+  UnaryPredicate _predicate;          // Unary predicate the elements of the GrowableArray should satisfy
 
  public:
-  GrowableArrayFilterIterator(const GrowableArray<E>* array, UnaryPredicate filter_predicate)
-   : _array(array), _position(0), _predicate(filter_predicate) {
+  GrowableArrayFilterIterator(const GrowableArray<E>* array, UnaryPredicate filter_predicate) :
+      _array(array), _position(0), _predicate(filter_predicate) {
     // Advance to first element satisfying the predicate
     while(!at_end() && !_predicate(_array->at(_position))) {
       ++_position;
     }
   }
@@ -582,11 +765,11 @@
       ++_position;
     } while(!at_end() && !_predicate(_array->at(_position)));
     return *this;
   }
 
-  E operator*()   { return _array->at(_position); }
+  E operator*() { return _array->at(_position); }
 
   bool operator==(const GrowableArrayIterator<E>& rhs)  {
     assert(_array == rhs._array, "iterator belongs to different array");
     return _position == rhs._position;
   }
diff a/src/java.base/share/classes/java/lang/invoke/MemberName.java b/src/java.base/share/classes/java/lang/invoke/MemberName.java
--- a/src/java.base/share/classes/java/lang/invoke/MemberName.java
+++ b/src/java.base/share/classes/java/lang/invoke/MemberName.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2008, 2019, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2008, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -494,11 +494,12 @@
     static final int
             IS_METHOD             = MN_IS_METHOD,              // method (not object constructor)
             IS_OBJECT_CONSTRUCTOR = MN_IS_OBJECT_CONSTRUCTOR,  // object constructor
             IS_FIELD              = MN_IS_FIELD,               // field
             IS_TYPE               = MN_IS_TYPE,                // nested type
-            CALLER_SENSITIVE      = MN_CALLER_SENSITIVE;       // @CallerSensitive annotation detected
+            CALLER_SENSITIVE      = MN_CALLER_SENSITIVE,       // @CallerSensitive annotation detected
+            TRUSTED_FINAL         = MN_TRUSTED_FINAL;    // trusted final field
 
     static final int ALL_ACCESS = Modifier.PUBLIC | Modifier.PRIVATE | Modifier.PROTECTED;
     static final int ALL_KINDS = IS_METHOD | IS_OBJECT_CONSTRUCTOR | IS_FIELD | IS_TYPE;
     static final int IS_INVOCABLE = IS_METHOD | IS_OBJECT_CONSTRUCTOR;
     static final int IS_FIELD_OR_METHOD = IS_METHOD | IS_FIELD;
@@ -538,10 +539,12 @@
     }
     /** Query whether this member has a CallerSensitive annotation. */
     public boolean isCallerSensitive() {
         return testAllFlags(CALLER_SENSITIVE);
     }
+    /** Query whether this member is a trusted final field. */
+    public boolean isTrustedFinalField() { return testAllFlags(TRUSTED_FINAL|IS_FIELD); }
 
     /** Utility method to query whether this member is accessible from a given lookup class. */
     public boolean isAccessibleFrom(Class<?> lookupClass) {
         int mode = (ALL_ACCESS|MethodHandles.Lookup.PACKAGE|MethodHandles.Lookup.MODULE);
         return VerifyAccess.isMemberAccessible(this.getDeclaringClass(), this.getDeclaringClass(), flags,
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java b/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandleNatives.java
@@ -116,10 +116,11 @@
             MN_IS_METHOD             = 0x00010000, // method (not object constructor)
             MN_IS_OBJECT_CONSTRUCTOR = 0x00020000, // object constructor
             MN_IS_FIELD              = 0x00040000, // field
             MN_IS_TYPE               = 0x00080000, // nested type
             MN_CALLER_SENSITIVE      = 0x00100000, // @CallerSensitive annotation detected
+            MN_TRUSTED_FINAL         = 0x00200000, // trusted final field
             MN_REFERENCE_KIND_SHIFT  = 24, // refKind
             MN_REFERENCE_KIND_MASK   = 0x0F000000 >> MN_REFERENCE_KIND_SHIFT,
             // The SEARCH_* bits are not for MN.flags but for the matchFlags argument of MHN.getMembers:
             MN_SEARCH_SUPERCLASSES   = 0x00100000,
             MN_SEARCH_INTERFACES     = 0x00200000;
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -3295,14 +3295,14 @@
         }
 
         private MethodHandle unreflectField(Field f, boolean isSetter) throws IllegalAccessException {
             MemberName field = new MemberName(f, isSetter);
             if (isSetter && field.isFinal()) {
-                if (field.isStatic()) {
-                    throw field.makeAccessException("static final field has no write access", this);
-                } else if (field.getDeclaringClass().isHidden()){
-                    throw field.makeAccessException("final field in a hidden class has no write access", this);
+                if (field.isTrustedFinalField()) {
+                    String msg = field.isStatic() ? "static final field has no write access"
+                                                  : "final field has no write access";
+                    throw field.makeAccessException(msg, this);
                 }
             }
             assert(isSetter
                     ? MethodHandleNatives.refKindIsSetter(field.getReferenceKind())
                     : MethodHandleNatives.refKindIsGetter(field.getReferenceKind()));
@@ -3863,11 +3863,11 @@
                     throw getField.makeAccessException("caller class must be a subclass below the method", lookupClass());
                 }
                 refc = lookupClass();
             }
             return VarHandles.makeFieldHandle(getField, refc, getField.getFieldType(),
-                                             this.allowedModes == TRUSTED && !getField.getDeclaringClass().isHidden());
+                                              this.allowedModes == TRUSTED && !getField.isTrustedFinalField());
         }
         /** Check access and get the requested constructor. */
         private MethodHandle getDirectConstructor(Class<?> refc, MemberName ctor) throws IllegalAccessException {
             final boolean checkSecurity = true;
             return getDirectConstructorCommon(refc, ctor, checkSecurity);
diff a/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java b/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java
--- a/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java
+++ b/src/java.base/share/classes/java/lang/reflect/AccessibleObject.java
@@ -175,15 +175,21 @@
      * protected constructors when the declaring class is in a different module
      * to the caller and the package containing the declaring class is not open
      * to the caller's module. </p>
      *
      * <p> This method cannot be used to enable {@linkplain Field#set <em>write</em>}
-     * access to a final field declared in a {@linkplain Class#isHidden() hidden class}
-     * and an {@linkplain Class#isInlineClass() inline class},
-     * since such fields are not modifiable.  The {@code accessible} flag when
-     * {@code true} suppresses Java language access control checks to only
-     * enable {@linkplain Field#get <em>read</em>} access to such fields.
+     * access to a <em>non-modifiable</em> final field.  The following fields
+     * are non-modifiable:
+     * <ul>
+     * <li>static final fields declared in any class or interface</li>
+     * <li>final fields declared in a {@linkplain Class#isHidden() hidden class}</li>
+     * <li>fields declared in a {@linkplain Class#isInlineClass() inline class}</li>
+     * <li>final fields declared in a {@linkplain Class#isRecord() record}</li>
+     * </ul>
+     * <p> The {@code accessible} flag when {@code true} suppresses Java language access
+     * control checks to only enable {@linkplain Field#get <em>read</em>} access to
+     * these non-modifiable final fields.
      *
      * <p> If there is a security manager, its
      * {@code checkPermission} method is first called with a
      * {@code ReflectPermission("suppressAccessChecks")} permission.
      *
diff a/src/java.base/share/classes/java/lang/reflect/Field.java b/src/java.base/share/classes/java/lang/reflect/Field.java
--- a/src/java.base/share/classes/java/lang/reflect/Field.java
+++ b/src/java.base/share/classes/java/lang/reflect/Field.java
@@ -70,10 +70,11 @@
     // This is guaranteed to be interned by the VM in the 1.4
     // reflection implementation
     private String              name;
     private Class<?>            type;
     private int                 modifiers;
+    private boolean             trustedFinal;
     // Generics and annotations support
     private transient String    signature;
     // generic info repository; lazily initialized
     private transient FieldRepository genericInfo;
     private byte[]              annotations;
@@ -117,18 +118,20 @@
      */
     Field(Class<?> declaringClass,
           String name,
           Class<?> type,
           int modifiers,
+          boolean trustedFinal,
           int slot,
           String signature,
           byte[] annotations)
     {
         this.clazz = declaringClass;
         this.name = name;
         this.type = type;
         this.modifiers = modifiers;
+        this.trustedFinal = trustedFinal;
         this.slot = slot;
         this.signature = signature;
         this.annotations = annotations;
     }
 
@@ -146,11 +149,11 @@
         // objects be fabricated for each reflective call on Class
         // objects.)
         if (this.root != null)
             throw new IllegalArgumentException("Can not copy a non-root Field");
 
-        Field res = new Field(clazz, name, type, modifiers, slot, signature, annotations);
+        Field res = new Field(clazz, name, type, modifiers, trustedFinal, slot, signature, annotations);
         res.root = this;
         // Might as well eagerly propagate this if already present
         res.fieldAccessor = fieldAccessor;
         res.overrideFieldAccessor = overrideFieldAccessor;
 
@@ -729,11 +732,13 @@
      * <ul>
      * <li>{@link #setAccessible(boolean) setAccessible(true)} has succeeded for
      *     this {@code Field} object;</li>
      * <li>the field is non-static; and</li>
      * <li>the field's declaring class is not a {@linkplain Class#isHidden()
-     *     hidden class}.</li>
+     *     hidden class}; and</li>
+     * <li>the field's declaring class is not a {@linkplain Class#isRecord()
+     *     record class}.</li>
      * </ul>
      * If any of the above checks is not met, this method throws an
      * {@code IllegalAccessException}.
      *
      * <p> Setting a final field in this way
@@ -1146,14 +1151,18 @@
             root.setFieldAccessor(accessor, overrideFinalCheck);
         }
     }
 
     @Override
-    Field getRoot() {
+    /* package-private */ Field getRoot() {
         return root;
     }
 
+    /* package-private */ boolean isTrustedFinal() {
+        return trustedFinal;
+    }
+
     /**
      * {@inheritDoc}
      *
      * @throws NullPointerException {@inheritDoc}
      * @since 1.5
diff a/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java b/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/doclint/Checker.java
@@ -1168,11 +1168,11 @@
                     break;
                 default:
                     return;
             }
         }
-        env.messages.warning(SYNTAX, tree, "dc.empty", tree.getKind().tagName);
+        env.messages.warning(MISSING, tree, "dc.empty", tree.getKind().tagName);
     }
 
     boolean hasNonWhitespace(TextTree tree) {
         String s = tree.getBody();
         for (int i = 0; i < s.length(); i++) {
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/Attr.java
@@ -3170,11 +3170,18 @@
             needsRecovery = false;
             throw t;
         } finally {
             localEnv.info.scope.leave();
             if (needsRecovery) {
-                attribTree(that, env, recoveryInfo);
+                Type prevResult = result;
+                try {
+                    attribTree(that, env, recoveryInfo);
+                } finally {
+                    if (result == Type.recoveryType) {
+                        result = prevResult;
+                    }
+                }
             }
         }
     }
     //where
         class TargetInfo {
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/LambdaToMethod.java
@@ -997,11 +997,14 @@
         }
 
         private JCExpression makeReceiver(VarSymbol rcvr) {
             if (rcvr == null) return null;
             JCExpression rcvrExpr = make.Ident(rcvr);
-            Type rcvrType = tree.ownerAccessible ? tree.sym.enclClass().type : tree.expr.type;
+            boolean protAccess =
+                    isProtectedInSuperClassOfEnclosingClassInOtherPackage(tree.sym, owner);
+            Type rcvrType = tree.ownerAccessible && !protAccess ? tree.sym.enclClass().type
+                                                                : tree.expr.type;
             if (rcvrType == syms.arrayClass.type) {
                 // Map the receiver type to the actually type, not just "array"
                 rcvrType = tree.getQualifierExpression().type;
             }
             if (!rcvr.type.tsym.isSubClass(rcvrType.tsym, types)) {
@@ -2277,15 +2280,10 @@
                         !types.isSameType(
                               types.erasure(tree.sym.enclClass().asType()),
                               types.erasure(owner.enclClass().asType()));
             }
 
-            boolean isProtectedInSuperClassOfEnclosingClassInOtherPackage() {
-                return ((tree.sym.flags() & PROTECTED) != 0 &&
-                        tree.sym.packge() != owner.packge());
-            }
-
             /**
              * Erasure destroys the implementation parameter subtype
              * relationship for intersection types.
              * Have similar problems for union types too.
              */
@@ -2318,11 +2316,11 @@
                 return interfaceParameterIsIntersectionOrUnionType() ||
                         isSuper ||
                         needsVarArgsConversion() ||
                         isArrayOp() ||
                         (!nestmateLambdas && isPrivateInOtherClass()) ||
-                        isProtectedInSuperClassOfEnclosingClassInOtherPackage() ||
+                        isProtectedInSuperClassOfEnclosingClassInOtherPackage(tree.sym, owner) ||
                         !receiverAccessible() ||
                         receiverIsReferenceProjection() ||
                         (tree.getMode() == ReferenceMode.NEW &&
                           tree.kind != ReferenceKind.ARRAY_CTOR &&
                           (tree.sym.owner.isLocal() || tree.sym.owner.isInner() || tree.sym.owner.isValue()));
@@ -2394,10 +2392,16 @@
             log.error(Errors.CannotGenerateClass(c, Fragments.IllegalSignature(c, ex.type())));
             return "<ERRONEOUS>";
         }
     }
 
+    private boolean isProtectedInSuperClassOfEnclosingClassInOtherPackage(Symbol targetReference,
+                                                                          Symbol currentClass) {
+        return ((targetReference.flags() & PROTECTED) != 0 &&
+                targetReference.packge() != currentClass.packge());
+    }
+
     /**
      * Signature Generation
      */
     private class L2MSignatureGenerator extends Types.SignatureGenerator {
 
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties b/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/resources/compiler.properties
@@ -3148,13 +3148,10 @@
 # errors related to doc comments
 
 compiler.err.dc.bad.entity=\
     bad HTML entity
 
-compiler.err.dc.bad.gt=\
-    bad use of ''>''
-
 compiler.err.dc.bad.inline.tag=\
     incorrect use of inline tag
 
 compiler.err.dc.identifier.expected=\
     identifier expected
diff a/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java b/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
--- a/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
+++ b/src/jdk.unsupported/share/classes/sun/misc/Unsafe.java
@@ -1,7 +1,7 @@
 /*
- * Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.
  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  *
  * This code is free software; you can redistribute it and/or modify it
  * under the terms of the GNU General Public License version 2 only, as
  * published by the Free Software Foundation.  Oracle designates this
@@ -634,20 +634,25 @@
      * Therefore, code which will be ported to such JVMs on 64-bit platforms
      * must preserve all bits of static field offsets.
      * @see #getInt(Object, long)
      */
     @ForceInline
+    @SuppressWarnings("preview")
     public long objectFieldOffset(Field f) {
         if (f == null) {
             throw new NullPointerException();
         }
-        if (f.getDeclaringClass().isHidden()) {
+        Class<?> declaringClass = f.getDeclaringClass();
+        if (declaringClass.isHidden()) {
             throw new UnsupportedOperationException("can't get field offset on a hidden class: " + f);
         }
         if (f.getDeclaringClass().isInlineClass()) {
             throw new UnsupportedOperationException("can't get field offset on an inline class: " + f);
         }
+        if (declaringClass.isRecord()) {
+            throw new UnsupportedOperationException("can't get field offset on a record (preview): " + f);
+        }
         return theInternalUnsafe.objectFieldOffset(f);
     }
 
     /**
      * Reports the location of a given static field, in conjunction with {@link
@@ -665,20 +670,25 @@
      * However, for consistency with other methods in this class,
      * this method reports its result as a long value.
      * @see #getInt(Object, long)
      */
     @ForceInline
+    @SuppressWarnings("preview")
     public long staticFieldOffset(Field f) {
         if (f == null) {
             throw new NullPointerException();
         }
-        if (f.getDeclaringClass().isHidden()) {
+        Class<?> declaringClass = f.getDeclaringClass();
+        if (declaringClass.isHidden()) {
             throw new UnsupportedOperationException("can't get field offset on a hidden class: " + f);
         }
         if (f.getDeclaringClass().isInlineClass()) {
             throw new UnsupportedOperationException("can't get static field offset on an inline class: " + f);
         }
+        if (declaringClass.isRecord()) {
+            throw new UnsupportedOperationException("can't get field offset on a record (preview): " + f);
+        }
         return theInternalUnsafe.staticFieldOffset(f);
     }
 
     /**
      * Reports the location of a given static field, in conjunction with {@link
@@ -689,20 +699,25 @@
      * which is a "cookie", not guaranteed to be a real Object, and it should
      * not be used in any way except as argument to the get and put routines in
      * this class.
      */
     @ForceInline
+    @SuppressWarnings("preview")
     public Object staticFieldBase(Field f) {
         if (f == null) {
             throw new NullPointerException();
         }
-        if (f.getDeclaringClass().isHidden()) {
+        Class<?> declaringClass = f.getDeclaringClass();
+        if (declaringClass.isHidden()) {
             throw new UnsupportedOperationException("can't get base address on a hidden class: " + f);
         }
         if (f.getDeclaringClass().isInlineClass()) {
             throw new UnsupportedOperationException("can't get base address on an inline class: " + f);
         }
+        if (declaringClass.isRecord()) {
+            throw new UnsupportedOperationException("can't get base address on a record (preview): " + f);
+        }
         return theInternalUnsafe.staticFieldBase(f);
     }
 
     /**
      * Detects if the given class may need to be initialized. This is often

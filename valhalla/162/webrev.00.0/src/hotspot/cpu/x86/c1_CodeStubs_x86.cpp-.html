<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/c1_CodeStubs_x86.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 1999, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;c1/c1_CodeStubs.hpp&quot;
 27 #include &quot;c1/c1_FrameMap.hpp&quot;
 28 #include &quot;c1/c1_LIRAssembler.hpp&quot;
 29 #include &quot;c1/c1_MacroAssembler.hpp&quot;
 30 #include &quot;c1/c1_Runtime1.hpp&quot;
 31 #include &quot;nativeInst_x86.hpp&quot;
 32 #include &quot;oops/objArrayKlass.hpp&quot;
 33 #include &quot;runtime/sharedRuntime.hpp&quot;
 34 #include &quot;utilities/align.hpp&quot;
 35 #include &quot;utilities/macros.hpp&quot;
 36 #include &quot;vmreg_x86.inline.hpp&quot;
 37 
 38 
 39 #define __ ce-&gt;masm()-&gt;
 40 
 41 #ifndef _LP64
 42 float ConversionStub::float_zero = 0.0;
 43 double ConversionStub::double_zero = 0.0;
 44 
 45 void ConversionStub::emit_code(LIR_Assembler* ce) {
 46   __ bind(_entry);
 47   assert(bytecode() == Bytecodes::_f2i || bytecode() == Bytecodes::_d2i, &quot;other conversions do not require stub&quot;);
 48 
 49 
 50   if (input()-&gt;is_single_xmm()) {
 51     __ comiss(input()-&gt;as_xmm_float_reg(),
 52               ExternalAddress((address)&amp;float_zero));
 53   } else if (input()-&gt;is_double_xmm()) {
 54     __ comisd(input()-&gt;as_xmm_double_reg(),
 55               ExternalAddress((address)&amp;double_zero));
 56   } else {
 57     __ push(rax);
 58     __ ftst();
 59     __ fnstsw_ax();
 60     __ sahf();
 61     __ pop(rax);
 62   }
 63 
 64   Label NaN, do_return;
 65   __ jccb(Assembler::parity, NaN);
 66   __ jccb(Assembler::below, do_return);
 67 
 68   // input is &gt; 0 -&gt; return maxInt
 69   // result register already contains 0x80000000, so subtracting 1 gives 0x7fffffff
 70   __ decrement(result()-&gt;as_register());
 71   __ jmpb(do_return);
 72 
 73   // input is NaN -&gt; return 0
 74   __ bind(NaN);
 75   __ xorptr(result()-&gt;as_register(), result()-&gt;as_register());
 76 
 77   __ bind(do_return);
 78   __ jmp(_continuation);
 79 }
 80 #endif // !_LP64
 81 
 82 void CounterOverflowStub::emit_code(LIR_Assembler* ce) {
 83   __ bind(_entry);
 84   Metadata *m = _method-&gt;as_constant_ptr()-&gt;as_metadata();
 85   ce-&gt;store_parameter(m, 1);
 86   ce-&gt;store_parameter(_bci, 0);
 87   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::counter_overflow_id)));
 88   ce-&gt;add_call_info_here(_info);
 89   ce-&gt;verify_oop_map(_info);
 90   __ jmp(_continuation);
 91 }
 92 
 93 RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index, LIR_Opr array)
 94   : _index(index), _array(array), _throw_index_out_of_bounds_exception(false) {
 95   assert(info != NULL, &quot;must have info&quot;);
 96   _info = new CodeEmitInfo(info);
 97 }
 98 
 99 RangeCheckStub::RangeCheckStub(CodeEmitInfo* info, LIR_Opr index)
100   : _index(index), _array(NULL), _throw_index_out_of_bounds_exception(true) {
101   assert(info != NULL, &quot;must have info&quot;);
102   _info = new CodeEmitInfo(info);
103 }
104 
105 void RangeCheckStub::emit_code(LIR_Assembler* ce) {
106   __ bind(_entry);
107   if (_info-&gt;deoptimize_on_exception()) {
108     address a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);
109     __ call(RuntimeAddress(a));
110     ce-&gt;add_call_info_here(_info);
111     ce-&gt;verify_oop_map(_info);
112     debug_only(__ should_not_reach_here());
113     return;
114   }
115 
116   // pass the array index on stack because all registers must be preserved
117   if (_index-&gt;is_cpu_register()) {
118     ce-&gt;store_parameter(_index-&gt;as_register(), 0);
119   } else {
120     ce-&gt;store_parameter(_index-&gt;as_jint(), 0);
121   }
122   Runtime1::StubID stub_id;
123   if (_throw_index_out_of_bounds_exception) {
124     stub_id = Runtime1::throw_index_exception_id;
125   } else {
126     stub_id = Runtime1::throw_range_check_failed_id;
127     ce-&gt;store_parameter(_array-&gt;as_pointer_register(), 1);
128   }
129   __ call(RuntimeAddress(Runtime1::entry_for(stub_id)));
130   ce-&gt;add_call_info_here(_info);
131   ce-&gt;verify_oop_map(_info);
132   debug_only(__ should_not_reach_here());
133 }
134 
135 PredicateFailedStub::PredicateFailedStub(CodeEmitInfo* info) {
136   _info = new CodeEmitInfo(info);
137 }
138 
139 void PredicateFailedStub::emit_code(LIR_Assembler* ce) {
140   __ bind(_entry);
141   address a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);
142   __ call(RuntimeAddress(a));
143   ce-&gt;add_call_info_here(_info);
144   ce-&gt;verify_oop_map(_info);
145   debug_only(__ should_not_reach_here());
146 }
147 
148 void DivByZeroStub::emit_code(LIR_Assembler* ce) {
149   if (_offset != -1) {
150     ce-&gt;compilation()-&gt;implicit_exception_table()-&gt;append(_offset, __ offset());
151   }
152   __ bind(_entry);
153   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::throw_div0_exception_id)));
154   ce-&gt;add_call_info_here(_info);
155   debug_only(__ should_not_reach_here());
156 }
157 
158 
159 // Implementation of LoadFlattenedArrayStub
160 
161 LoadFlattenedArrayStub::LoadFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr result, CodeEmitInfo* info) {
162   _array = array;
163   _index = index;
164   _result = result;
165   // Tell the register allocator that the runtime call will scratch rax.
166   _scratch_reg = FrameMap::rax_oop_opr;
167   _info = new CodeEmitInfo(info);
168 }
169 
170 void LoadFlattenedArrayStub::emit_code(LIR_Assembler* ce) {
171   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
172   __ bind(_entry);
173   ce-&gt;store_parameter(_array-&gt;as_register(), 1);
174   ce-&gt;store_parameter(_index-&gt;as_register(), 0);
175   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::load_flattened_array_id)));
176   ce-&gt;add_call_info_here(_info);
177   ce-&gt;verify_oop_map(_info);
178   if (_result-&gt;as_register() != rax) {
179     __ movptr(_result-&gt;as_register(), rax);
180   }
181   __ jmp(_continuation);
182 }
183 
184 
185 // Implementation of StoreFlattenedArrayStub
186 
187 StoreFlattenedArrayStub::StoreFlattenedArrayStub(LIR_Opr array, LIR_Opr index, LIR_Opr value, CodeEmitInfo* info) {
188   _array = array;
189   _index = index;
190   _value = value;
191   // Tell the register allocator that the runtime call will scratch rax.
192   _scratch_reg = FrameMap::rax_oop_opr;
193   _info = new CodeEmitInfo(info);
194 }
195 
196 
197 void StoreFlattenedArrayStub::emit_code(LIR_Assembler* ce) {
198   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
199   __ bind(_entry);
200   ce-&gt;store_parameter(_array-&gt;as_register(), 2);
201   ce-&gt;store_parameter(_index-&gt;as_register(), 1);
202   ce-&gt;store_parameter(_value-&gt;as_register(), 0);
203   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::store_flattened_array_id)));
204   ce-&gt;add_call_info_here(_info);
205   ce-&gt;verify_oop_map(_info);
206   __ jmp(_continuation);
207 }
208 
209 
210 // Implementation of SubstitutabilityCheckStub
211 
212 SubstitutabilityCheckStub::SubstitutabilityCheckStub(LIR_Opr left, LIR_Opr right, CodeEmitInfo* info) {
213   _left = left;
214   _right = right;
215   // Tell the register allocator that the runtime call will scratch rax.
216   _scratch_reg = FrameMap::rax_oop_opr;
217   _info = new CodeEmitInfo(info);
218 }
219 
220 void SubstitutabilityCheckStub::emit_code(LIR_Assembler* ce) {
221   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
222   __ bind(_entry);
223   ce-&gt;store_parameter(_left-&gt;as_register(), 1);
224   ce-&gt;store_parameter(_right-&gt;as_register(), 0);
225   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::substitutability_check_id)));
226   ce-&gt;add_call_info_here(_info);
227   ce-&gt;verify_oop_map(_info);
228   __ jmp(_continuation);
229 }
230 
231 
232 // Implementation of NewInstanceStub
233 
234 NewInstanceStub::NewInstanceStub(LIR_Opr klass_reg, LIR_Opr result, ciInstanceKlass* klass, CodeEmitInfo* info, Runtime1::StubID stub_id) {
235   _result = result;
236   _klass = klass;
237   _klass_reg = klass_reg;
238   _info = new CodeEmitInfo(info);
239   assert(stub_id == Runtime1::new_instance_id                 ||
240          stub_id == Runtime1::fast_new_instance_id            ||
241          stub_id == Runtime1::fast_new_instance_init_check_id,
242          &quot;need new_instance id&quot;);
243   _stub_id   = stub_id;
244 }
245 
246 
247 void NewInstanceStub::emit_code(LIR_Assembler* ce) {
248   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
249   __ bind(_entry);
250   __ movptr(rdx, _klass_reg-&gt;as_register());
251   __ call(RuntimeAddress(Runtime1::entry_for(_stub_id)));
252   ce-&gt;add_call_info_here(_info);
253   ce-&gt;verify_oop_map(_info);
254   assert(_result-&gt;as_register() == rax, &quot;result must in rax,&quot;);
255   __ jmp(_continuation);
256 }
257 
258 
259 // Implementation of NewTypeArrayStub
260 
261 NewTypeArrayStub::NewTypeArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info) {
262   _klass_reg = klass_reg;
263   _length = length;
264   _result = result;
265   _info = new CodeEmitInfo(info);
266 }
267 
268 
269 void NewTypeArrayStub::emit_code(LIR_Assembler* ce) {
270   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
271   __ bind(_entry);
272   assert(_length-&gt;as_register() == rbx, &quot;length must in rbx,&quot;);
273   assert(_klass_reg-&gt;as_register() == rdx, &quot;klass_reg must in rdx&quot;);
274   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_type_array_id)));
275   ce-&gt;add_call_info_here(_info);
276   ce-&gt;verify_oop_map(_info);
277   assert(_result-&gt;as_register() == rax, &quot;result must in rax,&quot;);
278   __ jmp(_continuation);
279 }
280 
281 
282 // Implementation of NewObjectArrayStub
283 
284 NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,
285                                        CodeEmitInfo* info, bool is_inline_type) {
286   _klass_reg = klass_reg;
287   _result = result;
288   _length = length;
289   _info = new CodeEmitInfo(info);
290   _is_inline_type = is_inline_type;
291 }
292 
293 
294 void NewObjectArrayStub::emit_code(LIR_Assembler* ce) {
295   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
296   __ bind(_entry);
297   assert(_length-&gt;as_register() == rbx, &quot;length must in rbx,&quot;);
298   assert(_klass_reg-&gt;as_register() == rdx, &quot;klass_reg must in rdx&quot;);
299   if (_is_inline_type) {
300     __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_flat_array_id)));
301   } else {
302     __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::new_object_array_id)));
303   }
304   ce-&gt;add_call_info_here(_info);
305   ce-&gt;verify_oop_map(_info);
306   assert(_result-&gt;as_register() == rax, &quot;result must in rax,&quot;);
307   __ jmp(_continuation);
308 }
309 
310 
311 // Implementation of MonitorAccessStubs
312 
313 MonitorEnterStub::MonitorEnterStub(LIR_Opr obj_reg, LIR_Opr lock_reg, CodeEmitInfo* info, CodeStub* throw_imse_stub, LIR_Opr scratch_reg)
314 : MonitorAccessStub(obj_reg, lock_reg)
315 {
316   _info = new CodeEmitInfo(info);
317   _throw_imse_stub = throw_imse_stub;
318   _scratch_reg = scratch_reg;
319   if (_throw_imse_stub != NULL) {
320     assert(_scratch_reg != LIR_OprFact::illegalOpr, &quot;must be&quot;);
321   }
322 }
323 
324 
325 void MonitorEnterStub::emit_code(LIR_Assembler* ce) {
326   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
327   __ bind(_entry);
328   if (_throw_imse_stub != NULL) {
329     // When we come here, _obj_reg has already been checked to be non-null.
330     const int is_value_mask = markWord::always_locked_pattern;
331     Register mark = _scratch_reg-&gt;as_register();
332     __ movptr(mark, Address(_obj_reg-&gt;as_register(), oopDesc::mark_offset_in_bytes()));
333     __ andptr(mark, is_value_mask);
334     __ cmpl(mark, is_value_mask);
335     __ jcc(Assembler::equal, *_throw_imse_stub-&gt;entry());
336   }
337   ce-&gt;store_parameter(_obj_reg-&gt;as_register(),  1);
338   ce-&gt;store_parameter(_lock_reg-&gt;as_register(), 0);
339   Runtime1::StubID enter_id;
340   if (ce-&gt;compilation()-&gt;has_fpu_code()) {
341     enter_id = Runtime1::monitorenter_id;
342   } else {
343     enter_id = Runtime1::monitorenter_nofpu_id;
344   }
345   __ call(RuntimeAddress(Runtime1::entry_for(enter_id)));
346   ce-&gt;add_call_info_here(_info);
347   ce-&gt;verify_oop_map(_info);
348   __ jmp(_continuation);
349 }
350 
351 
352 void MonitorExitStub::emit_code(LIR_Assembler* ce) {
353   __ bind(_entry);
354   if (_compute_lock) {
355     // lock_reg was destroyed by fast unlocking attempt =&gt; recompute it
356     ce-&gt;monitor_address(_monitor_ix, _lock_reg);
357   }
358   ce-&gt;store_parameter(_lock_reg-&gt;as_register(), 0);
359   // note: non-blocking leaf routine =&gt; no call info needed
360   Runtime1::StubID exit_id;
361   if (ce-&gt;compilation()-&gt;has_fpu_code()) {
362     exit_id = Runtime1::monitorexit_id;
363   } else {
364     exit_id = Runtime1::monitorexit_nofpu_id;
365   }
366   __ call(RuntimeAddress(Runtime1::entry_for(exit_id)));
367   __ jmp(_continuation);
368 }
369 
370 
371 // Implementation of patching:
372 // - Copy the code at given offset to an inlined buffer (first the bytes, then the number of bytes)
373 // - Replace original code with a call to the stub
374 // At Runtime:
375 // - call to stub, jump to runtime
376 // - in runtime: preserve all registers (rspecially objects, i.e., source and destination object)
377 // - in runtime: after initializing class, restore original code, reexecute instruction
378 
379 int PatchingStub::_patch_info_offset = -NativeGeneralJump::instruction_size;
380 
381 void PatchingStub::align_patch_site(MacroAssembler* masm) {
382   // We&#39;re patching a 5-7 byte instruction on intel and we need to
383   // make sure that we don&#39;t see a piece of the instruction.  It
384   // appears mostly impossible on Intel to simply invalidate other
385   // processors caches and since they may do aggressive prefetch it&#39;s
386   // very hard to make a guess about what code might be in the icache.
387   // Force the instruction to be double word aligned so that it
388   // doesn&#39;t span a cache line.
389   masm-&gt;align(align_up((int)NativeGeneralJump::instruction_size, wordSize));
390 }
391 
392 void PatchingStub::emit_code(LIR_Assembler* ce) {
393   assert(NativeCall::instruction_size &lt;= _bytes_to_copy &amp;&amp; _bytes_to_copy &lt;= 0xFF, &quot;not enough room for call&quot;);
394 
395   Label call_patch;
396 
397   // static field accesses have special semantics while the class
398   // initializer is being run so we emit a test which can be used to
399   // check that this code is being executed by the initializing
400   // thread.
401   address being_initialized_entry = __ pc();
402   if (CommentedAssembly) {
403     __ block_comment(&quot; patch template&quot;);
404   }
405   if (_id == load_klass_id) {
406     // produce a copy of the load klass instruction for use by the being initialized case
407 #ifdef ASSERT
408     address start = __ pc();
409 #endif
410     Metadata* o = NULL;
411     __ mov_metadata(_obj, o);
412 #ifdef ASSERT
413     for (int i = 0; i &lt; _bytes_to_copy; i++) {
414       address ptr = (address)(_pc_start + i);
415       int a_byte = (*ptr) &amp; 0xFF;
416       assert(a_byte == *start++, &quot;should be the same code&quot;);
417     }
418 #endif
419   } else if (_id == load_mirror_id) {
420     // produce a copy of the load mirror instruction for use by the being
421     // initialized case
422 #ifdef ASSERT
423     address start = __ pc();
424 #endif
425     jobject o = NULL;
426     __ movoop(_obj, o);
427 #ifdef ASSERT
428     for (int i = 0; i &lt; _bytes_to_copy; i++) {
429       address ptr = (address)(_pc_start + i);
430       int a_byte = (*ptr) &amp; 0xFF;
431       assert(a_byte == *start++, &quot;should be the same code&quot;);
432     }
433 #endif
434   } else {
435     // make a copy the code which is going to be patched.
436     for (int i = 0; i &lt; _bytes_to_copy; i++) {
437       address ptr = (address)(_pc_start + i);
438       int a_byte = (*ptr) &amp; 0xFF;
439       __ emit_int8(a_byte);
440       *ptr = 0x90; // make the site look like a nop
441     }
442   }
443 
444   address end_of_patch = __ pc();
445   int bytes_to_skip = 0;
446   if (_id == load_mirror_id) {
447     int offset = __ offset();
448     if (CommentedAssembly) {
449       __ block_comment(&quot; being_initialized check&quot;);
450     }
451     assert(_obj != noreg, &quot;must be a valid register&quot;);
452     Register tmp = rax;
453     Register tmp2 = rbx;
454     __ push(tmp);
455     __ push(tmp2);
456     // Load without verification to keep code size small. We need it because
457     // begin_initialized_entry_offset has to fit in a byte. Also, we know it&#39;s not null.
458     __ movptr(tmp2, Address(_obj, java_lang_Class::klass_offset()));
459     __ get_thread(tmp);
460     __ cmpptr(tmp, Address(tmp2, InstanceKlass::init_thread_offset()));
461     __ pop(tmp2);
462     __ pop(tmp);
463     __ jcc(Assembler::notEqual, call_patch);
464 
465     // access_field patches may execute the patched code before it&#39;s
466     // copied back into place so we need to jump back into the main
467     // code of the nmethod to continue execution.
468     __ jmp(_patch_site_continuation);
469 
470     // make sure this extra code gets skipped
471     bytes_to_skip += __ offset() - offset;
472   }
473   if (CommentedAssembly) {
474     __ block_comment(&quot;patch data encoded as movl&quot;);
475   }
476   // Now emit the patch record telling the runtime how to find the
477   // pieces of the patch.  We only need 3 bytes but for readability of
478   // the disassembly we make the data look like a movl reg, imm32,
479   // which requires 5 bytes
480   int sizeof_patch_record = 5;
481   bytes_to_skip += sizeof_patch_record;
482 
483   // emit the offsets needed to find the code to patch
484   int being_initialized_entry_offset = __ pc() - being_initialized_entry + sizeof_patch_record;
485 
486   __ emit_int8((unsigned char)0xB8);
487   __ emit_int8(0);
488   __ emit_int8(being_initialized_entry_offset);
489   __ emit_int8(bytes_to_skip);
490   __ emit_int8(_bytes_to_copy);
491   address patch_info_pc = __ pc();
492   assert(patch_info_pc - end_of_patch == bytes_to_skip, &quot;incorrect patch info&quot;);
493 
494   address entry = __ pc();
495   NativeGeneralJump::insert_unconditional((address)_pc_start, entry);
496   address target = NULL;
497   relocInfo::relocType reloc_type = relocInfo::none;
498   switch (_id) {
499     case access_field_id:  target = Runtime1::entry_for(Runtime1::access_field_patching_id); break;
500     case load_klass_id:    target = Runtime1::entry_for(Runtime1::load_klass_patching_id); reloc_type = relocInfo::metadata_type; break;
501     case load_mirror_id:   target = Runtime1::entry_for(Runtime1::load_mirror_patching_id); reloc_type = relocInfo::oop_type; break;
502     case load_appendix_id:      target = Runtime1::entry_for(Runtime1::load_appendix_patching_id); reloc_type = relocInfo::oop_type; break;
503     default: ShouldNotReachHere();
504   }
505   __ bind(call_patch);
506 
507   if (CommentedAssembly) {
508     __ block_comment(&quot;patch entry point&quot;);
509   }
510   __ call(RuntimeAddress(target));
511   assert(_patch_info_offset == (patch_info_pc - __ pc()), &quot;must not change&quot;);
512   ce-&gt;add_call_info_here(_info);
513   int jmp_off = __ offset();
514   __ jmp(_patch_site_entry);
515   // Add enough nops so deoptimization can overwrite the jmp above with a call
516   // and not destroy the world. We cannot use fat nops here, since the concurrent
517   // code rewrite may transiently create the illegal instruction sequence.
518   for (int j = __ offset() ; j &lt; jmp_off + 5 ; j++ ) {
519     __ nop();
520   }
521   if (_id == load_klass_id || _id == load_mirror_id || _id == load_appendix_id) {
522     CodeSection* cs = __ code_section();
523     RelocIterator iter(cs, (address)_pc_start, (address)(_pc_start + 1));
524     relocInfo::change_reloc_info_for_address(&amp;iter, (address) _pc_start, reloc_type, relocInfo::none);
525   }
526 }
527 
528 
529 void DeoptimizeStub::emit_code(LIR_Assembler* ce) {
530   __ bind(_entry);
531   ce-&gt;store_parameter(_trap_request, 0);
532   __ call(RuntimeAddress(Runtime1::entry_for(Runtime1::deoptimize_id)));
533   ce-&gt;add_call_info_here(_info);
534   DEBUG_ONLY(__ should_not_reach_here());
535 }
536 
537 
538 void ImplicitNullCheckStub::emit_code(LIR_Assembler* ce) {
539   address a;
540   if (_info-&gt;deoptimize_on_exception()) {
541     // Deoptimize, do not throw the exception, because it is probably wrong to do it here.
542     a = Runtime1::entry_for(Runtime1::predicate_failed_trap_id);
543   } else {
544     a = Runtime1::entry_for(Runtime1::throw_null_pointer_exception_id);
545   }
546 
547   ce-&gt;compilation()-&gt;implicit_exception_table()-&gt;append(_offset, __ offset());
548   __ bind(_entry);
549   __ call(RuntimeAddress(a));
550   ce-&gt;add_call_info_here(_info);
551   ce-&gt;verify_oop_map(_info);
552   debug_only(__ should_not_reach_here());
553 }
554 
555 
556 void SimpleExceptionStub::emit_code(LIR_Assembler* ce) {
557   assert(__ rsp_offset() == 0, &quot;frame size should be fixed&quot;);
558 
559   __ bind(_entry);
560   // pass the object on stack because all registers must be preserved
561   if (_obj-&gt;is_cpu_register()) {
562     ce-&gt;store_parameter(_obj-&gt;as_register(), 0);
563   }
564   __ call(RuntimeAddress(Runtime1::entry_for(_stub)));
565   ce-&gt;add_call_info_here(_info);
566   debug_only(__ should_not_reach_here());
567 }
568 
569 
570 void ArrayCopyStub::emit_code(LIR_Assembler* ce) {
571   //---------------slow case: call to native-----------------
572   __ bind(_entry);
573   // Figure out where the args should go
574   // This should really convert the IntrinsicID to the Method* and signature
575   // but I don&#39;t know how to do that.
576   //
577   VMRegPair args[5];
578   BasicType signature[5] = { T_OBJECT, T_INT, T_OBJECT, T_INT, T_INT};
579   SharedRuntime::java_calling_convention(signature, args, 5, true);
580 
581   // push parameters
582   // (src, src_pos, dest, destPos, length)
583   Register r[5];
584   r[0] = src()-&gt;as_register();
585   r[1] = src_pos()-&gt;as_register();
586   r[2] = dst()-&gt;as_register();
587   r[3] = dst_pos()-&gt;as_register();
588   r[4] = length()-&gt;as_register();
589 
590   // next registers will get stored on the stack
591   for (int i = 0; i &lt; 5 ; i++ ) {
592     VMReg r_1 = args[i].first();
593     if (r_1-&gt;is_stack()) {
594       int st_off = r_1-&gt;reg2stack() * wordSize;
595       __ movptr (Address(rsp, st_off), r[i]);
596     } else {
597       assert(r[i] == args[i].first()-&gt;as_Register(), &quot;Wrong register for arg &quot;);
598     }
599   }
600 
601   ce-&gt;align_call(lir_static_call);
602 
603   ce-&gt;emit_static_call_stub();
604   if (ce-&gt;compilation()-&gt;bailed_out()) {
605     return; // CodeCache is full
606   }
607   AddressLiteral resolve(SharedRuntime::get_resolve_static_call_stub(),
608                          relocInfo::static_call_type);
609   __ call(resolve);
610   ce-&gt;add_call_info_here(info());
611 
612 #ifndef PRODUCT
613   __ incrementl(ExternalAddress((address)&amp;Runtime1::_arraycopy_slowcase_cnt));
614 #endif
615 
616   __ jmp(_continuation);
617 }
618 
619 #undef __
    </pre>
  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="sharedRuntime_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../x86/c1_CodeStubs_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/templateTable_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
3180     case Bytecodes::_fast_dputfield: __ pop_d(); break;
3181     case Bytecodes::_fast_fputfield: __ pop_f(); break;
3182     case Bytecodes::_fast_lputfield: __ pop_l(r0); break;
3183     default: break;
3184     }
3185     __ bind(L2);
3186   }
3187 }
3188 
3189 void TemplateTable::fast_storefield(TosState state)
3190 {
3191   transition(state, vtos);
3192 
3193   ByteSize base = ConstantPoolCache::base_offset();
3194 
3195   jvmti_post_fast_field_mod();
3196 
3197   // access constant pool cache
3198   __ get_cache_and_index_at_bcp(r2, r1, 1);
3199 



3200   // test for volatile with r3
3201   __ ldrw(r3, Address(r2, in_bytes(base +
3202                                    ConstantPoolCacheEntry::flags_offset())));
3203 
3204   // replace index with field offset from cache entry
3205   __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));
3206 
3207   {
3208     Label notVolatile;
3209     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3210     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
3211     __ bind(notVolatile);
3212   }
3213 
3214   Label notVolatile;
3215 
3216   // Get object from stack
3217   pop_and_check_object(r2);
3218 
3219   // field address
</pre>
<hr />
<pre>
3285     __ lea(rscratch1, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
3286     __ ldrw(r2, Address(rscratch1));
3287     __ cbzw(r2, L1);
3288     // access constant pool cache entry
3289     __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);
3290     __ verify_oop(r0);
3291     __ push_ptr(r0);  // save object pointer before call_VM() clobbers it
3292     __ mov(c_rarg1, r0);
3293     // c_rarg1: object pointer copied above
3294     // c_rarg2: cache entry pointer
3295     __ call_VM(noreg,
3296                CAST_FROM_FN_PTR(address,
3297                                 InterpreterRuntime::post_field_access),
3298                c_rarg1, c_rarg2);
3299     __ pop_ptr(r0); // restore object pointer
3300     __ bind(L1);
3301   }
3302 
3303   // access constant pool cache
3304   __ get_cache_and_index_at_bcp(r2, r1, 1);




3305   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3306                                   ConstantPoolCacheEntry::f2_offset())));
3307   __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3308                                    ConstantPoolCacheEntry::flags_offset())));
3309 
3310   // r0: object
3311   __ verify_oop(r0);
3312   __ null_check(r0);
3313   const Address field(r0, r1);
3314 
3315   // 8179954: We need to make sure that the code generated for
3316   // volatile accesses forms a sequentially-consistent set of
3317   // operations when combined with STLR and LDAR.  Without a leading
3318   // membar it&#39;s possible for a simple Dekker test to fail if loads
3319   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3320   // the stores in one method and we interpret the loads in another.
3321   if (!is_c1_or_interpreter_only()) {
3322     Label notVolatile;
3323     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3324     __ membar(MacroAssembler::AnyAny);
</pre>
</td>
<td>
<hr />
<pre>
3180     case Bytecodes::_fast_dputfield: __ pop_d(); break;
3181     case Bytecodes::_fast_fputfield: __ pop_f(); break;
3182     case Bytecodes::_fast_lputfield: __ pop_l(r0); break;
3183     default: break;
3184     }
3185     __ bind(L2);
3186   }
3187 }
3188 
3189 void TemplateTable::fast_storefield(TosState state)
3190 {
3191   transition(state, vtos);
3192 
3193   ByteSize base = ConstantPoolCache::base_offset();
3194 
3195   jvmti_post_fast_field_mod();
3196 
3197   // access constant pool cache
3198   __ get_cache_and_index_at_bcp(r2, r1, 1);
3199 
<span class="line-added">3200   // Must prevent reordering of the following cp cache loads with bytecode load</span>
<span class="line-added">3201   __ membar(MacroAssembler::LoadLoad);</span>
<span class="line-added">3202 </span>
3203   // test for volatile with r3
3204   __ ldrw(r3, Address(r2, in_bytes(base +
3205                                    ConstantPoolCacheEntry::flags_offset())));
3206 
3207   // replace index with field offset from cache entry
3208   __ ldr(r1, Address(r2, in_bytes(base + ConstantPoolCacheEntry::f2_offset())));
3209 
3210   {
3211     Label notVolatile;
3212     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3213     __ membar(MacroAssembler::StoreStore | MacroAssembler::LoadStore);
3214     __ bind(notVolatile);
3215   }
3216 
3217   Label notVolatile;
3218 
3219   // Get object from stack
3220   pop_and_check_object(r2);
3221 
3222   // field address
</pre>
<hr />
<pre>
3288     __ lea(rscratch1, ExternalAddress((address) JvmtiExport::get_field_access_count_addr()));
3289     __ ldrw(r2, Address(rscratch1));
3290     __ cbzw(r2, L1);
3291     // access constant pool cache entry
3292     __ get_cache_entry_pointer_at_bcp(c_rarg2, rscratch2, 1);
3293     __ verify_oop(r0);
3294     __ push_ptr(r0);  // save object pointer before call_VM() clobbers it
3295     __ mov(c_rarg1, r0);
3296     // c_rarg1: object pointer copied above
3297     // c_rarg2: cache entry pointer
3298     __ call_VM(noreg,
3299                CAST_FROM_FN_PTR(address,
3300                                 InterpreterRuntime::post_field_access),
3301                c_rarg1, c_rarg2);
3302     __ pop_ptr(r0); // restore object pointer
3303     __ bind(L1);
3304   }
3305 
3306   // access constant pool cache
3307   __ get_cache_and_index_at_bcp(r2, r1, 1);
<span class="line-added">3308 </span>
<span class="line-added">3309   // Must prevent reordering of the following cp cache loads with bytecode load</span>
<span class="line-added">3310   __ membar(MacroAssembler::LoadLoad);</span>
<span class="line-added">3311 </span>
3312   __ ldr(r1, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3313                                   ConstantPoolCacheEntry::f2_offset())));
3314   __ ldrw(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() +
3315                                    ConstantPoolCacheEntry::flags_offset())));
3316 
3317   // r0: object
3318   __ verify_oop(r0);
3319   __ null_check(r0);
3320   const Address field(r0, r1);
3321 
3322   // 8179954: We need to make sure that the code generated for
3323   // volatile accesses forms a sequentially-consistent set of
3324   // operations when combined with STLR and LDAR.  Without a leading
3325   // membar it&#39;s possible for a simple Dekker test to fail if loads
3326   // use LDR;DMB but stores use STLR.  This can happen if C2 compiles
3327   // the stores in one method and we interpret the loads in another.
3328   if (!is_c1_or_interpreter_only()) {
3329     Label notVolatile;
3330     __ tbz(r3, ConstantPoolCacheEntry::is_volatile_shift, notVolatile);
3331     __ membar(MacroAssembler::AnyAny);
</pre>
</td>
</tr>
</table>
<center><a href="sharedRuntime_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="../x86/c1_CodeStubs_x86.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
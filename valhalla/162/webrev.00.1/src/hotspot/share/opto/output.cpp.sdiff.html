<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/output.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="memnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stringopts.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/output.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 225 };
 226 
 227 
 228 PhaseOutput::PhaseOutput()
 229   : Phase(Phase::Output),
 230     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 231     _first_block_size(0),
 232     _handler_table(),
 233     _inc_table(),
 234     _oop_map_set(NULL),
 235     _scratch_buffer_blob(NULL),
 236     _scratch_locs_memory(NULL),
 237     _scratch_const_size(-1),
 238     _in_scratch_emit_size(false),
 239     _frame_slots(0),
 240     _code_offsets(),
 241     _node_bundling_limit(0),
 242     _node_bundling_base(NULL),
 243     _orig_pc_slot(0),
 244     _orig_pc_slot_offset_in_bytes(0),


 245     _buf_sizes(),
 246     _block(NULL),
 247     _index(0) {
 248   C-&gt;set_output(this);
 249   if (C-&gt;stub_name() == NULL) {
<span class="line-modified"> 250     _orig_pc_slot = C-&gt;fixed_slots() - (sizeof(address) / VMRegImpl::stack_slot_size);</span>





 251   }
 252 }
 253 
 254 PhaseOutput::~PhaseOutput() {
 255   C-&gt;set_output(NULL);
 256   if (_scratch_buffer_blob != NULL) {
 257     BufferBlob::free(_scratch_buffer_blob);
 258   }
 259 }
 260 
 261 void PhaseOutput::perform_mach_node_analysis() {
 262   // Late barrier analysis must be done after schedule and bundle
 263   // Otherwise liveness based spilling will fail
 264   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 265   bs-&gt;late_barrier_analysis();
 266 
 267   pd_perform_mach_node_analysis();
 268 }
 269 
 270 // Convert Nodes to instruction bits and pass off to the VM
 271 void PhaseOutput::Output() {
 272   // RootNode goes
 273   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 274 
 275   // The number of new nodes (mostly MachNop) is proportional to
 276   // the number of java calls and inner loops which are aligned.
 277   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 278                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 279                            &quot;out of nodes before code generation&quot; ) ) {
 280     return;
 281   }
 282   // Make sure I can find the Start Node
 283   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 284   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 285 
 286   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 287 
 288   // Replace StartNode with prolog
<span class="line-modified"> 289   MachPrologNode *prolog = new MachPrologNode();</span>

 290   entry-&gt;map_node(prolog, 0);
 291   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);
 292   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block
 293 
 294   // Virtual methods need an unverified entry point
<span class="line-modified"> 295 </span>
<span class="line-modified"> 296   if( C-&gt;is_osr_compilation() ) {</span>
<span class="line-removed"> 297     if( PoisonOSREntry ) {</span>
 298       // TODO: Should use a ShouldNotReachHereNode...
 299       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );
 300     }
 301   } else {
<span class="line-modified"> 302     if( C-&gt;method() &amp;&amp; !C-&gt;method()-&gt;flags().is_static() ) {</span>
<span class="line-modified"> 303       // Insert unvalidated entry point</span>
<span class="line-modified"> 304       C-&gt;cfg()-&gt;insert( broot, 0, new MachUEPNode() );</span>











 305     }
<span class="line-removed"> 306 </span>
 307   }
 308 
 309   // Break before main entry point
 310   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
 311       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||
 312       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||
 313       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 314     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 315     // runtime stubs or frame converters
 316     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 317   }
 318 
 319   // Insert epilogs before every return
 320   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 321     Block* block = C-&gt;cfg()-&gt;get_block(i);
 322     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 323       Node* m = block-&gt;end();
 324       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 325         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 326         block-&gt;add_inst(epilog);
 327         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 328       }
 329     }
 330   }
 331 
 332   // Keeper of sizing aspects
 333   _buf_sizes = BufferSizingData();
 334 
 335   // Initialize code buffer
 336   estimate_buffer_size(_buf_sizes._const);
 337   if (C-&gt;failing()) return;
 338 
 339   // Pre-compute the length of blocks and replace
 340   // long branches with short if machine supports it.
 341   // Must be done before ScheduleAndBundle due to SPARC delay slots
 342   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 343   blk_starts[0] = 0;
 344   shorten_branches(blk_starts);
 345 

























 346   ScheduleAndBundle();
 347   if (C-&gt;failing()) {
 348     return;
 349   }
 350 
 351   perform_mach_node_analysis();
 352 
 353   // Complete sizing of codebuffer
 354   CodeBuffer* cb = init_buffer();
 355   if (cb == NULL || C-&gt;failing()) {
 356     return;
 357   }
 358 
 359   BuildOopMaps();
 360 
 361   if (C-&gt;failing())  {
 362     return;
 363   }
 364 
 365   fill_buffer(cb, blk_starts);
</pre>
<hr />
<pre>
 483     // Sum all instruction sizes to compute block size
 484     uint last_inst = block-&gt;number_of_nodes();
 485     uint blk_size = 0;
 486     for (uint j = 0; j &lt; last_inst; j++) {
 487       _index = j;
 488       Node* nj = block-&gt;get_node(_index);
 489       // Handle machine instruction nodes
 490       if (nj-&gt;is_Mach()) {
 491         MachNode* mach = nj-&gt;as_Mach();
 492         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 493         reloc_size += mach-&gt;reloc();
 494         if (mach-&gt;is_MachCall()) {
 495           // add size information for trampoline stub
 496           // class CallStubImpl is platform-specific and defined in the *.ad files.
 497           stub_size  += CallStubImpl::size_call_trampoline();
 498           reloc_size += CallStubImpl::reloc_call_trampoline();
 499 
 500           MachCallNode *mcall = mach-&gt;as_MachCall();
 501           // This destination address is NOT PC-relative
 502 
<span class="line-modified"> 503           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>


 504 
 505           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 506             stub_size  += CompiledStaticCall::to_interp_stub_size();
 507             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 508 #if INCLUDE_AOT
 509             stub_size  += CompiledStaticCall::to_aot_stub_size();
 510             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 511 #endif
 512           }
 513         } else if (mach-&gt;is_MachSafePoint()) {
 514           // If call/safepoint are adjacent, account for possible
 515           // nop to disambiguate the two safepoints.
 516           // ScheduleAndBundle() can rearrange nodes in a block,
 517           // check for all offsets inside this block.
 518           if (last_call_adr &gt;= blk_starts[i]) {
 519             blk_size += nop_size;
 520           }
 521         }
 522         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 523           // Nop is inserted between &quot;avoid back to back&quot; instructions.
</pre>
<hr />
<pre>
 916       ShouldNotReachHere();
 917       break;
 918   }
 919 }
 920 
 921 // Determine if this node starts a bundle
 922 bool PhaseOutput::starts_bundle(const Node *n) const {
 923   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 924           _node_bundling_base[n-&gt;_idx].starts_bundle());
 925 }
 926 
 927 //--------------------------Process_OopMap_Node--------------------------------
 928 void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {
 929   // Handle special safepoint nodes for synchronization
 930   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 931   MachCallNode      *mcall;
 932 
 933   int safepoint_pc_offset = current_offset;
 934   bool is_method_handle_invoke = false;
 935   bool return_oop = false;

 936 
 937   // Add the safepoint in the DebugInfoRecorder
 938   if( !mach-&gt;is_MachCall() ) {
 939     mcall = NULL;
 940     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 941   } else {
 942     mcall = mach-&gt;as_MachCall();
 943 
 944     // Is the call a MethodHandle call?
 945     if (mcall-&gt;is_MachCallJava()) {
 946       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 947         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);
 948         is_method_handle_invoke = true;
 949       }
 950     }
 951 
 952     // Check if a call returns an object.
<span class="line-modified"> 953     if (mcall-&gt;returns_pointer()) {</span>
 954       return_oop = true;
 955     }



 956     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
 957     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
 958   }
 959 
 960   // Loop over the JVMState list to add scope information
 961   // Do not skip safepoints with a NULL method, they need monitor info
 962   JVMState* youngest_jvms = sfn-&gt;jvms();
 963   int max_depth = youngest_jvms-&gt;depth();
 964 
 965   // Allocate the object pool for scalar-replaced objects -- the map from
 966   // small-integer keys (which can be recorded in the local and ostack
 967   // arrays) to descriptions of the object state.
 968   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
 969 
 970   // Visit scopes from oldest to youngest.
 971   for (int depth = 1; depth &lt;= max_depth; depth++) {
 972     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
 973     int idx;
 974     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
 975     // Safepoints that do not have method() set only provide oop-map and monitor info
</pre>
<hr />
<pre>
1050       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
1051       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
1052     }
1053 
1054     // We dump the object pool first, since deoptimization reads it in first.
1055     C-&gt;debug_info()-&gt;dump_object_pool(objs);
1056 
1057     // Build first class objects to pass to scope
1058     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);
1059     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);
1060     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);
1061 
1062     // Make method available for all Safepoints
1063     ciMethod* scope_method = method ? method : C-&gt;method();
1064     // Describe the scope here
1065     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
1066     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
1067     // Now we can describe the scope.
1068     methodHandle null_mh;
1069     bool rethrow_exception = false;
<span class="line-modified">1070     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);</span>
1071   } // End jvms loop
1072 
1073   // Mark the end of the scope set.
1074   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);
1075 }
1076 
1077 
1078 
1079 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
1080 class NonSafepointEmitter {
1081     Compile*  C;
1082     JVMState* _pending_jvms;
1083     int       _pending_offset;
1084 
1085     void emit_non_safepoint();
1086 
1087  public:
1088     NonSafepointEmitter(Compile* compile) {
1089       this-&gt;C = compile;
1090       _pending_jvms = NULL;
</pre>
<hr />
<pre>
1155   }
1156 
1157   // Mark the end of the scope set.
1158   debug_info-&gt;end_non_safepoint(pc_offset);
1159 }
1160 
1161 //------------------------------init_buffer------------------------------------
1162 void PhaseOutput::estimate_buffer_size(int&amp; const_req) {
1163 
1164   // Set the initially allocated size
1165   const_req = initial_const_capacity;
1166 
1167   // The extra spacing after the code is necessary on some platforms.
1168   // Sometimes we need to patch in a jump after the last instruction,
1169   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
1170 
1171   // Compute the byte offset where we can store the deopt pc.
1172   if (C-&gt;fixed_slots() != 0) {
1173     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
1174   }




1175 
1176   // Compute prolog code size
1177   _method_size = 0;
1178   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;
1179 #if defined(IA64) &amp;&amp; !defined(AIX)
1180   if (save_argument_registers()) {
1181     // 4815101: this is a stub with implicit and unknown precision fp args.
1182     // The usual spill mechanism can only generate stfd&#39;s in this case, which
1183     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
1184     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
1185     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
1186     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
1187     //
1188     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
1189     // get rid of this hack and have SpillCopy generate stfspill/ldffill
1190     // instead of stfd/stfs/ldfd/ldfs.
1191     _frame_slots += 8*(16/BytesPerInt);
1192   }
1193 #endif
1194   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
</pre>
<hr />
<pre>
1431           int nops_cnt = padding / nop_size;
1432           MachNode *nop = new MachNopNode(nops_cnt);
1433           block-&gt;insert_node(nop, j++);
1434           last_inst++;
1435           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1436           // Ensure enough space.
1437           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1438           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1439             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1440             return;
1441           }
1442           nop-&gt;emit(*cb, C-&gt;regalloc());
1443           cb-&gt;flush_bundle(true);
1444           current_offset = cb-&gt;insts_size();
1445         }
1446 
1447         // Remember the start of the last call in a basic block
1448         if (is_mcall) {
1449           MachCallNode *mcall = mach-&gt;as_MachCall();
1450 
<span class="line-modified">1451           // This destination address is NOT PC-relative</span>
<span class="line-modified">1452           mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>


1453 
1454           // Save the return address
1455           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1456 
1457           if (mcall-&gt;is_MachCallLeaf()) {
1458             is_mcall = false;
1459             is_sfn = false;
1460           }
1461         }
1462 
1463         // sfn will be valid whenever mcall is valid now because of inheritance
1464         if (is_sfn || is_mcall) {
1465 
1466           // Handle special safepoint nodes for synchronization
1467           if (!is_mcall) {
1468             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1469             // !!!!! Stubs only need an oopmap right now, so bail out
1470             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1471               // Write the oopmap directly to the code blob??!!
1472               continue;
</pre>
<hr />
<pre>
3163 }
3164 #endif
3165 
3166 //-----------------------init_scratch_buffer_blob------------------------------
3167 // Construct a temporary BufferBlob and cache it for this compile.
3168 void PhaseOutput::init_scratch_buffer_blob(int const_size) {
3169   // If there is already a scratch buffer blob allocated and the
3170   // constant section is big enough, use it.  Otherwise free the
3171   // current and allocate a new one.
3172   BufferBlob* blob = scratch_buffer_blob();
3173   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
3174     // Use the current blob.
3175   } else {
3176     if (blob != NULL) {
3177       BufferBlob::free(blob);
3178     }
3179 
3180     ResourceMark rm;
3181     _scratch_const_size = const_size;
3182     int size = C2Compiler::initial_code_buffer_size(const_size);






3183     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);
3184     // Record the buffer blob for next time.
3185     set_scratch_buffer_blob(blob);
3186     // Have we run out of code space?
3187     if (scratch_buffer_blob() == NULL) {
3188       // Let CompilerBroker disable further compilations.
3189       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);
3190       return;
3191     }
3192   }
3193 
3194   // Initialize the relocation buffers
3195   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
3196   set_scratch_locs_memory(locs_buf);
3197 }
3198 
3199 
3200 //-----------------------scratch_emit_size-------------------------------------
3201 // Helper function that computes size by emitting code
3202 uint PhaseOutput::scratch_emit_size(const Node* n) {
</pre>
<hr />
<pre>
3227   int lsize = MAX_locs_size / 3;
3228   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
3229   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
3230   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
3231   // Mark as scratch buffer.
3232   buf.consts()-&gt;set_scratch_emit();
3233   buf.insts()-&gt;set_scratch_emit();
3234   buf.stubs()-&gt;set_scratch_emit();
3235 
3236   // Do the emission.
3237 
3238   Label fakeL; // Fake label for branch instructions.
3239   Label*   saveL = NULL;
3240   uint save_bnum = 0;
3241   bool is_branch = n-&gt;is_MachBranch();
3242   if (is_branch) {
3243     MacroAssembler masm(&amp;buf);
3244     masm.bind(fakeL);
3245     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
3246     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);






3247   }
3248   n-&gt;emit(buf, C-&gt;regalloc());
3249 
3250   // Emitting into the scratch buffer should not fail
3251   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());
3252 
<span class="line-modified">3253   if (is_branch) // Restore label.</span>

3254     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);





3255 
3256   // End scratch_emit_size section.
3257   set_in_scratch_emit_size(false);
3258 
3259   return buf.insts_size();
3260 }
3261 
3262 void PhaseOutput::install() {
3263   if (!C-&gt;should_install_code()) {
3264     return;
3265   } else if (C-&gt;stub_function() != NULL) {
3266     install_stub(C-&gt;stub_name(),
3267                  C-&gt;save_argument_registers());
3268   } else {
3269     install_code(C-&gt;method(),
3270                  C-&gt;entry_bci(),
3271                  CompileBroker::compiler2(),
3272                  C-&gt;has_unsafe_access(),
3273                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),
3274                  C-&gt;rtm_state());
</pre>
<hr />
<pre>
3279                                int               entry_bci,
3280                                AbstractCompiler* compiler,
3281                                bool              has_unsafe_access,
3282                                bool              has_wide_vectors,
3283                                RTMState          rtm_state) {
3284   // Check if we want to skip execution of all compiled code.
3285   {
3286 #ifndef PRODUCT
3287     if (OptoNoExecute) {
3288       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed
3289       return;
3290     }
3291 #endif
3292     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);
3293 
3294     if (C-&gt;is_osr_compilation()) {
3295       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
3296       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
3297     } else {
3298       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);









3299       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
3300     }
3301 
3302     C-&gt;env()-&gt;register_method(target,
<span class="line-modified">3303                                      entry_bci,</span>
<span class="line-modified">3304                                      &amp;_code_offsets,</span>
<span class="line-modified">3305                                      _orig_pc_slot_offset_in_bytes,</span>
<span class="line-modified">3306                                      code_buffer(),</span>
<span class="line-modified">3307                                      frame_size_in_words(),</span>
<span class="line-modified">3308                                      oop_map_set(),</span>
<span class="line-modified">3309                                      &amp;_handler_table,</span>
<span class="line-modified">3310                                      inc_table(),</span>
<span class="line-modified">3311                                      compiler,</span>
<span class="line-modified">3312                                      has_unsafe_access,</span>
<span class="line-modified">3313                                      SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-modified">3314                                      C-&gt;rtm_state());</span>
3315 
3316     if (C-&gt;log() != NULL) { // Print code cache state into compiler log
3317       C-&gt;log()-&gt;code_cache_state();
3318     }
3319   }
3320 }
3321 void PhaseOutput::install_stub(const char* stub_name,
3322                                bool        caller_must_gc_arguments) {
3323   // Entry point will be accessed using stub_entry_point();
3324   if (code_buffer() == NULL) {
3325     Matcher::soft_match_failure();
3326   } else {
3327     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
3328       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);
3329 
3330     if (!C-&gt;failing()) {
3331       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);
3332 
3333       // Make the NMethod
3334       // For now we mark the frame as never safe for profile stackwalking
</pre>
</td>
<td>
<hr />
<pre>
 225 };
 226 
 227 
 228 PhaseOutput::PhaseOutput()
 229   : Phase(Phase::Output),
 230     _code_buffer(&quot;Compile::Fill_buffer&quot;),
 231     _first_block_size(0),
 232     _handler_table(),
 233     _inc_table(),
 234     _oop_map_set(NULL),
 235     _scratch_buffer_blob(NULL),
 236     _scratch_locs_memory(NULL),
 237     _scratch_const_size(-1),
 238     _in_scratch_emit_size(false),
 239     _frame_slots(0),
 240     _code_offsets(),
 241     _node_bundling_limit(0),
 242     _node_bundling_base(NULL),
 243     _orig_pc_slot(0),
 244     _orig_pc_slot_offset_in_bytes(0),
<span class="line-added"> 245     _sp_inc_slot(0),</span>
<span class="line-added"> 246     _sp_inc_slot_offset_in_bytes(0),</span>
 247     _buf_sizes(),
 248     _block(NULL),
 249     _index(0) {
 250   C-&gt;set_output(this);
 251   if (C-&gt;stub_name() == NULL) {
<span class="line-modified"> 252     int fixed_slots = C-&gt;fixed_slots();</span>
<span class="line-added"> 253     if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added"> 254       fixed_slots -= 2;</span>
<span class="line-added"> 255       _sp_inc_slot = fixed_slots;</span>
<span class="line-added"> 256     }</span>
<span class="line-added"> 257     _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);</span>
 258   }
 259 }
 260 
 261 PhaseOutput::~PhaseOutput() {
 262   C-&gt;set_output(NULL);
 263   if (_scratch_buffer_blob != NULL) {
 264     BufferBlob::free(_scratch_buffer_blob);
 265   }
 266 }
 267 
 268 void PhaseOutput::perform_mach_node_analysis() {
 269   // Late barrier analysis must be done after schedule and bundle
 270   // Otherwise liveness based spilling will fail
 271   BarrierSetC2* bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();
 272   bs-&gt;late_barrier_analysis();
 273 
 274   pd_perform_mach_node_analysis();
 275 }
 276 
 277 // Convert Nodes to instruction bits and pass off to the VM
 278 void PhaseOutput::Output() {
 279   // RootNode goes
 280   assert( C-&gt;cfg()-&gt;get_root_block()-&gt;number_of_nodes() == 0, &quot;&quot; );
 281 
 282   // The number of new nodes (mostly MachNop) is proportional to
 283   // the number of java calls and inner loops which are aligned.
 284   if ( C-&gt;check_node_count((NodeLimitFudgeFactor + C-&gt;java_calls()*3 +
 285                             C-&gt;inner_loops()*(OptoLoopAlignment-1)),
 286                            &quot;out of nodes before code generation&quot; ) ) {
 287     return;
 288   }
 289   // Make sure I can find the Start Node
 290   Block *entry = C-&gt;cfg()-&gt;get_block(1);
 291   Block *broot = C-&gt;cfg()-&gt;get_root_block();
 292 
 293   const StartNode *start = entry-&gt;head()-&gt;as_Start();
 294 
 295   // Replace StartNode with prolog
<span class="line-modified"> 296   Label verified_entry;</span>
<span class="line-added"> 297   MachPrologNode* prolog = new MachPrologNode(&amp;verified_entry);</span>
 298   entry-&gt;map_node(prolog, 0);
 299   C-&gt;cfg()-&gt;map_node_to_block(prolog, entry);
 300   C-&gt;cfg()-&gt;unmap_node_from_block(start); // start is no longer in any block
 301 
 302   // Virtual methods need an unverified entry point
<span class="line-modified"> 303   if (C-&gt;is_osr_compilation()) {</span>
<span class="line-modified"> 304     if (PoisonOSREntry) {</span>

 305       // TODO: Should use a ShouldNotReachHereNode...
 306       C-&gt;cfg()-&gt;insert( broot, 0, new MachBreakpointNode() );
 307     }
 308   } else {
<span class="line-modified"> 309     if (C-&gt;method()) {</span>
<span class="line-modified"> 310       if (C-&gt;method()-&gt;has_scalarized_args()) {</span>
<span class="line-modified"> 311         // Add entry point to unpack all inline type arguments</span>
<span class="line-added"> 312         C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true, /* receiver_only */ false));</span>
<span class="line-added"> 313         if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 314           // Add verified/unverified entry points to only unpack inline type receiver at interface calls</span>
<span class="line-added"> 315           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ false));</span>
<span class="line-added"> 316           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ true,  /* receiver_only */ true));</span>
<span class="line-added"> 317           C-&gt;cfg()-&gt;insert(broot, 0, new MachVEPNode(&amp;verified_entry, /* verified */ false, /* receiver_only */ true));</span>
<span class="line-added"> 318         }</span>
<span class="line-added"> 319       } else if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 320         // Insert unvalidated entry point</span>
<span class="line-added"> 321         C-&gt;cfg()-&gt;insert(broot, 0, new MachUEPNode());</span>
<span class="line-added"> 322       }</span>
 323     }

 324   }
 325 
 326   // Break before main entry point
 327   if ((C-&gt;method() &amp;&amp; C-&gt;directive()-&gt;BreakAtExecuteOption) ||
 328       (OptoBreakpoint &amp;&amp; C-&gt;is_method_compilation())       ||
 329       (OptoBreakpointOSR &amp;&amp; C-&gt;is_osr_compilation())       ||
 330       (OptoBreakpointC2R &amp;&amp; !C-&gt;method())                   ) {
 331     // checking for C-&gt;method() means that OptoBreakpoint does not apply to
 332     // runtime stubs or frame converters
 333     C-&gt;cfg()-&gt;insert( entry, 1, new MachBreakpointNode() );
 334   }
 335 
 336   // Insert epilogs before every return
 337   for (uint i = 0; i &lt; C-&gt;cfg()-&gt;number_of_blocks(); i++) {
 338     Block* block = C-&gt;cfg()-&gt;get_block(i);
 339     if (!block-&gt;is_connector() &amp;&amp; block-&gt;non_connector_successor(0) == C-&gt;cfg()-&gt;get_root_block()) { // Found a program exit point?
 340       Node* m = block-&gt;end();
 341       if (m-&gt;is_Mach() &amp;&amp; m-&gt;as_Mach()-&gt;ideal_Opcode() != Op_Halt) {
 342         MachEpilogNode* epilog = new MachEpilogNode(m-&gt;as_Mach()-&gt;ideal_Opcode() == Op_Return);
 343         block-&gt;add_inst(epilog);
 344         C-&gt;cfg()-&gt;map_node_to_block(epilog, block);
 345       }
 346     }
 347   }
 348 
 349   // Keeper of sizing aspects
 350   _buf_sizes = BufferSizingData();
 351 
 352   // Initialize code buffer
 353   estimate_buffer_size(_buf_sizes._const);
 354   if (C-&gt;failing()) return;
 355 
 356   // Pre-compute the length of blocks and replace
 357   // long branches with short if machine supports it.
 358   // Must be done before ScheduleAndBundle due to SPARC delay slots
 359   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C-&gt;cfg()-&gt;number_of_blocks() + 1);
 360   blk_starts[0] = 0;
 361   shorten_branches(blk_starts);
 362 
<span class="line-added"> 363   if (!C-&gt;is_osr_compilation() &amp;&amp; C-&gt;has_scalarized_args()) {</span>
<span class="line-added"> 364     // Compute the offsets of the entry points required by the inline type calling convention</span>
<span class="line-added"> 365     if (!C-&gt;method()-&gt;is_static()) {</span>
<span class="line-added"> 366       // We have entries at the beginning of the method, implemented by the first 4 nodes.</span>
<span class="line-added"> 367       // Entry                     (unverified) @ offset 0</span>
<span class="line-added"> 368       // Verified_Inline_Entry_RO</span>
<span class="line-added"> 369       // Inline_Entry              (unverified)</span>
<span class="line-added"> 370       // Verified_Inline_Entry</span>
<span class="line-added"> 371       uint offset = 0;</span>
<span class="line-added"> 372       _code_offsets.set_value(CodeOffsets::Entry, offset);</span>
<span class="line-added"> 373 </span>
<span class="line-added"> 374       offset += ((MachVEPNode*)broot-&gt;get_node(0))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 375       _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);</span>
<span class="line-added"> 376 </span>
<span class="line-added"> 377       offset += ((MachVEPNode*)broot-&gt;get_node(1))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 378       _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);</span>
<span class="line-added"> 379 </span>
<span class="line-added"> 380       offset += ((MachVEPNode*)broot-&gt;get_node(2))-&gt;size(C-&gt;regalloc());</span>
<span class="line-added"> 381       _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);</span>
<span class="line-added"> 382     } else {</span>
<span class="line-added"> 383       _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later</span>
<span class="line-added"> 384       _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);</span>
<span class="line-added"> 385     }</span>
<span class="line-added"> 386   }</span>
<span class="line-added"> 387 </span>
 388   ScheduleAndBundle();
 389   if (C-&gt;failing()) {
 390     return;
 391   }
 392 
 393   perform_mach_node_analysis();
 394 
 395   // Complete sizing of codebuffer
 396   CodeBuffer* cb = init_buffer();
 397   if (cb == NULL || C-&gt;failing()) {
 398     return;
 399   }
 400 
 401   BuildOopMaps();
 402 
 403   if (C-&gt;failing())  {
 404     return;
 405   }
 406 
 407   fill_buffer(cb, blk_starts);
</pre>
<hr />
<pre>
 525     // Sum all instruction sizes to compute block size
 526     uint last_inst = block-&gt;number_of_nodes();
 527     uint blk_size = 0;
 528     for (uint j = 0; j &lt; last_inst; j++) {
 529       _index = j;
 530       Node* nj = block-&gt;get_node(_index);
 531       // Handle machine instruction nodes
 532       if (nj-&gt;is_Mach()) {
 533         MachNode* mach = nj-&gt;as_Mach();
 534         blk_size += (mach-&gt;alignment_required() - 1) * relocInfo::addr_unit(); // assume worst case padding
 535         reloc_size += mach-&gt;reloc();
 536         if (mach-&gt;is_MachCall()) {
 537           // add size information for trampoline stub
 538           // class CallStubImpl is platform-specific and defined in the *.ad files.
 539           stub_size  += CallStubImpl::size_call_trampoline();
 540           reloc_size += CallStubImpl::reloc_call_trampoline();
 541 
 542           MachCallNode *mcall = mach-&gt;as_MachCall();
 543           // This destination address is NOT PC-relative
 544 
<span class="line-modified"> 545           if (mcall-&gt;entry_point() != NULL) {</span>
<span class="line-added"> 546             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>
<span class="line-added"> 547           }</span>
 548 
 549           if (mcall-&gt;is_MachCallJava() &amp;&amp; mcall-&gt;as_MachCallJava()-&gt;_method) {
 550             stub_size  += CompiledStaticCall::to_interp_stub_size();
 551             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 552 #if INCLUDE_AOT
 553             stub_size  += CompiledStaticCall::to_aot_stub_size();
 554             reloc_size += CompiledStaticCall::reloc_to_aot_stub();
 555 #endif
 556           }
 557         } else if (mach-&gt;is_MachSafePoint()) {
 558           // If call/safepoint are adjacent, account for possible
 559           // nop to disambiguate the two safepoints.
 560           // ScheduleAndBundle() can rearrange nodes in a block,
 561           // check for all offsets inside this block.
 562           if (last_call_adr &gt;= blk_starts[i]) {
 563             blk_size += nop_size;
 564           }
 565         }
 566         if (mach-&gt;avoid_back_to_back(MachNode::AVOID_BEFORE)) {
 567           // Nop is inserted between &quot;avoid back to back&quot; instructions.
</pre>
<hr />
<pre>
 960       ShouldNotReachHere();
 961       break;
 962   }
 963 }
 964 
 965 // Determine if this node starts a bundle
 966 bool PhaseOutput::starts_bundle(const Node *n) const {
 967   return (_node_bundling_limit &gt; n-&gt;_idx &amp;&amp;
 968           _node_bundling_base[n-&gt;_idx].starts_bundle());
 969 }
 970 
 971 //--------------------------Process_OopMap_Node--------------------------------
 972 void PhaseOutput::Process_OopMap_Node(MachNode *mach, int current_offset) {
 973   // Handle special safepoint nodes for synchronization
 974   MachSafePointNode *sfn   = mach-&gt;as_MachSafePoint();
 975   MachCallNode      *mcall;
 976 
 977   int safepoint_pc_offset = current_offset;
 978   bool is_method_handle_invoke = false;
 979   bool return_oop = false;
<span class="line-added"> 980   bool return_vt = false;</span>
 981 
 982   // Add the safepoint in the DebugInfoRecorder
 983   if( !mach-&gt;is_MachCall() ) {
 984     mcall = NULL;
 985     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, sfn-&gt;_oop_map);
 986   } else {
 987     mcall = mach-&gt;as_MachCall();
 988 
 989     // Is the call a MethodHandle call?
 990     if (mcall-&gt;is_MachCallJava()) {
 991       if (mcall-&gt;as_MachCallJava()-&gt;_method_handle_invoke) {
 992         assert(C-&gt;has_method_handle_invokes(), &quot;must have been set during call generation&quot;);
 993         is_method_handle_invoke = true;
 994       }
 995     }
 996 
 997     // Check if a call returns an object.
<span class="line-modified"> 998     if (mcall-&gt;returns_pointer() || mcall-&gt;returns_vt()) {</span>
 999       return_oop = true;
1000     }
<span class="line-added">1001     if (mcall-&gt;returns_vt()) {</span>
<span class="line-added">1002       return_vt = true;</span>
<span class="line-added">1003     }</span>
1004     safepoint_pc_offset += mcall-&gt;ret_addr_offset();
1005     C-&gt;debug_info()-&gt;add_safepoint(safepoint_pc_offset, mcall-&gt;_oop_map);
1006   }
1007 
1008   // Loop over the JVMState list to add scope information
1009   // Do not skip safepoints with a NULL method, they need monitor info
1010   JVMState* youngest_jvms = sfn-&gt;jvms();
1011   int max_depth = youngest_jvms-&gt;depth();
1012 
1013   // Allocate the object pool for scalar-replaced objects -- the map from
1014   // small-integer keys (which can be recorded in the local and ostack
1015   // arrays) to descriptions of the object state.
1016   GrowableArray&lt;ScopeValue*&gt; *objs = new GrowableArray&lt;ScopeValue*&gt;();
1017 
1018   // Visit scopes from oldest to youngest.
1019   for (int depth = 1; depth &lt;= max_depth; depth++) {
1020     JVMState* jvms = youngest_jvms-&gt;of_depth(depth);
1021     int idx;
1022     ciMethod* method = jvms-&gt;has_method() ? jvms-&gt;method() : NULL;
1023     // Safepoints that do not have method() set only provide oop-map and monitor info
</pre>
<hr />
<pre>
1098       bool eliminated = (box_node-&gt;is_BoxLock() &amp;&amp; box_node-&gt;as_BoxLock()-&gt;is_eliminated());
1099       monarray-&gt;append(new MonitorValue(scval, basic_lock, eliminated));
1100     }
1101 
1102     // We dump the object pool first, since deoptimization reads it in first.
1103     C-&gt;debug_info()-&gt;dump_object_pool(objs);
1104 
1105     // Build first class objects to pass to scope
1106     DebugToken *locvals = C-&gt;debug_info()-&gt;create_scope_values(locarray);
1107     DebugToken *expvals = C-&gt;debug_info()-&gt;create_scope_values(exparray);
1108     DebugToken *monvals = C-&gt;debug_info()-&gt;create_monitor_values(monarray);
1109 
1110     // Make method available for all Safepoints
1111     ciMethod* scope_method = method ? method : C-&gt;method();
1112     // Describe the scope here
1113     assert(jvms-&gt;bci() &gt;= InvocationEntryBci &amp;&amp; jvms-&gt;bci() &lt;= 0x10000, &quot;must be a valid or entry BCI&quot;);
1114     assert(!jvms-&gt;should_reexecute() || depth == max_depth, &quot;reexecute allowed only for the youngest&quot;);
1115     // Now we can describe the scope.
1116     methodHandle null_mh;
1117     bool rethrow_exception = false;
<span class="line-modified">1118     C-&gt;debug_info()-&gt;describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms-&gt;bci(), jvms-&gt;should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);</span>
1119   } // End jvms loop
1120 
1121   // Mark the end of the scope set.
1122   C-&gt;debug_info()-&gt;end_safepoint(safepoint_pc_offset);
1123 }
1124 
1125 
1126 
1127 // A simplified version of Process_OopMap_Node, to handle non-safepoints.
1128 class NonSafepointEmitter {
1129     Compile*  C;
1130     JVMState* _pending_jvms;
1131     int       _pending_offset;
1132 
1133     void emit_non_safepoint();
1134 
1135  public:
1136     NonSafepointEmitter(Compile* compile) {
1137       this-&gt;C = compile;
1138       _pending_jvms = NULL;
</pre>
<hr />
<pre>
1203   }
1204 
1205   // Mark the end of the scope set.
1206   debug_info-&gt;end_non_safepoint(pc_offset);
1207 }
1208 
1209 //------------------------------init_buffer------------------------------------
1210 void PhaseOutput::estimate_buffer_size(int&amp; const_req) {
1211 
1212   // Set the initially allocated size
1213   const_req = initial_const_capacity;
1214 
1215   // The extra spacing after the code is necessary on some platforms.
1216   // Sometimes we need to patch in a jump after the last instruction,
1217   // if the nmethod has been deoptimized.  (See 4932387, 4894843.)
1218 
1219   // Compute the byte offset where we can store the deopt pc.
1220   if (C-&gt;fixed_slots() != 0) {
1221     _orig_pc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_orig_pc_slot));
1222   }
<span class="line-added">1223   if (C-&gt;needs_stack_repair()) {</span>
<span class="line-added">1224     // Compute the byte offset of the stack increment value</span>
<span class="line-added">1225     _sp_inc_slot_offset_in_bytes = C-&gt;regalloc()-&gt;reg2offset(OptoReg::stack2reg(_sp_inc_slot));</span>
<span class="line-added">1226   }</span>
1227 
1228   // Compute prolog code size
1229   _method_size = 0;
1230   _frame_slots = OptoReg::reg2stack(C-&gt;matcher()-&gt;_old_SP) + C-&gt;regalloc()-&gt;_framesize;
1231 #if defined(IA64) &amp;&amp; !defined(AIX)
1232   if (save_argument_registers()) {
1233     // 4815101: this is a stub with implicit and unknown precision fp args.
1234     // The usual spill mechanism can only generate stfd&#39;s in this case, which
1235     // doesn&#39;t work if the fp reg to spill contains a single-precision denorm.
1236     // Instead, we hack around the normal spill mechanism using stfspill&#39;s and
1237     // ldffill&#39;s in the MachProlog and MachEpilog emit methods.  We allocate
1238     // space here for the fp arg regs (f8-f15) we&#39;re going to thusly spill.
1239     //
1240     // If we ever implement 16-byte &#39;registers&#39; == stack slots, we can
1241     // get rid of this hack and have SpillCopy generate stfspill/ldffill
1242     // instead of stfd/stfs/ldfd/ldfs.
1243     _frame_slots += 8*(16/BytesPerInt);
1244   }
1245 #endif
1246   assert(_frame_slots &gt;= 0 &amp;&amp; _frame_slots &lt; 1000000, &quot;sanity check&quot;);
</pre>
<hr />
<pre>
1483           int nops_cnt = padding / nop_size;
1484           MachNode *nop = new MachNopNode(nops_cnt);
1485           block-&gt;insert_node(nop, j++);
1486           last_inst++;
1487           C-&gt;cfg()-&gt;map_node_to_block(nop, block);
1488           // Ensure enough space.
1489           cb-&gt;insts()-&gt;maybe_expand_to_ensure_remaining(MAX_inst_size);
1490           if ((cb-&gt;blob() == NULL) || (!CompileBroker::should_compile_new_jobs())) {
1491             C-&gt;record_failure(&quot;CodeCache is full&quot;);
1492             return;
1493           }
1494           nop-&gt;emit(*cb, C-&gt;regalloc());
1495           cb-&gt;flush_bundle(true);
1496           current_offset = cb-&gt;insts_size();
1497         }
1498 
1499         // Remember the start of the last call in a basic block
1500         if (is_mcall) {
1501           MachCallNode *mcall = mach-&gt;as_MachCall();
1502 
<span class="line-modified">1503           if (mcall-&gt;entry_point() != NULL) {</span>
<span class="line-modified">1504             // This destination address is NOT PC-relative</span>
<span class="line-added">1505             mcall-&gt;method_set((intptr_t)mcall-&gt;entry_point());</span>
<span class="line-added">1506           }</span>
1507 
1508           // Save the return address
1509           call_returns[block-&gt;_pre_order] = current_offset + mcall-&gt;ret_addr_offset();
1510 
1511           if (mcall-&gt;is_MachCallLeaf()) {
1512             is_mcall = false;
1513             is_sfn = false;
1514           }
1515         }
1516 
1517         // sfn will be valid whenever mcall is valid now because of inheritance
1518         if (is_sfn || is_mcall) {
1519 
1520           // Handle special safepoint nodes for synchronization
1521           if (!is_mcall) {
1522             MachSafePointNode *sfn = mach-&gt;as_MachSafePoint();
1523             // !!!!! Stubs only need an oopmap right now, so bail out
1524             if (sfn-&gt;jvms()-&gt;method() == NULL) {
1525               // Write the oopmap directly to the code blob??!!
1526               continue;
</pre>
<hr />
<pre>
3217 }
3218 #endif
3219 
3220 //-----------------------init_scratch_buffer_blob------------------------------
3221 // Construct a temporary BufferBlob and cache it for this compile.
3222 void PhaseOutput::init_scratch_buffer_blob(int const_size) {
3223   // If there is already a scratch buffer blob allocated and the
3224   // constant section is big enough, use it.  Otherwise free the
3225   // current and allocate a new one.
3226   BufferBlob* blob = scratch_buffer_blob();
3227   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
3228     // Use the current blob.
3229   } else {
3230     if (blob != NULL) {
3231       BufferBlob::free(blob);
3232     }
3233 
3234     ResourceMark rm;
3235     _scratch_const_size = const_size;
3236     int size = C2Compiler::initial_code_buffer_size(const_size);
<span class="line-added">3237 #ifdef ASSERT</span>
<span class="line-added">3238     if (C-&gt;has_scalarized_args()) {</span>
<span class="line-added">3239       // Oop verification for loading object fields from scalarized inline types in the new entry point requires lots of space</span>
<span class="line-added">3240       size += 5120;</span>
<span class="line-added">3241     }</span>
<span class="line-added">3242 #endif</span>
3243     blob = BufferBlob::create(&quot;Compile::scratch_buffer&quot;, size);
3244     // Record the buffer blob for next time.
3245     set_scratch_buffer_blob(blob);
3246     // Have we run out of code space?
3247     if (scratch_buffer_blob() == NULL) {
3248       // Let CompilerBroker disable further compilations.
3249       C-&gt;record_failure(&quot;Not enough space for scratch buffer in CodeCache&quot;);
3250       return;
3251     }
3252   }
3253 
3254   // Initialize the relocation buffers
3255   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
3256   set_scratch_locs_memory(locs_buf);
3257 }
3258 
3259 
3260 //-----------------------scratch_emit_size-------------------------------------
3261 // Helper function that computes size by emitting code
3262 uint PhaseOutput::scratch_emit_size(const Node* n) {
</pre>
<hr />
<pre>
3287   int lsize = MAX_locs_size / 3;
3288   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
3289   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
3290   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
3291   // Mark as scratch buffer.
3292   buf.consts()-&gt;set_scratch_emit();
3293   buf.insts()-&gt;set_scratch_emit();
3294   buf.stubs()-&gt;set_scratch_emit();
3295 
3296   // Do the emission.
3297 
3298   Label fakeL; // Fake label for branch instructions.
3299   Label*   saveL = NULL;
3300   uint save_bnum = 0;
3301   bool is_branch = n-&gt;is_MachBranch();
3302   if (is_branch) {
3303     MacroAssembler masm(&amp;buf);
3304     masm.bind(fakeL);
3305     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
3306     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);
<span class="line-added">3307   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">3308     saveL = ((MachPrologNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">3309     ((MachPrologNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
<span class="line-added">3310   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">3311     saveL = ((MachVEPNode*)n)-&gt;_verified_entry;</span>
<span class="line-added">3312     ((MachVEPNode*)n)-&gt;_verified_entry = &amp;fakeL;</span>
3313   }
3314   n-&gt;emit(buf, C-&gt;regalloc());
3315 
3316   // Emitting into the scratch buffer should not fail
3317   assert (!C-&gt;failing(), &quot;Must not have pending failure. Reason is: %s&quot;, C-&gt;failure_reason());
3318 
<span class="line-modified">3319   // Restore label.</span>
<span class="line-added">3320   if (is_branch) {</span>
3321     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);
<span class="line-added">3322   } else if (n-&gt;is_MachProlog()) {</span>
<span class="line-added">3323     ((MachPrologNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">3324   } else if (n-&gt;is_MachVEP()) {</span>
<span class="line-added">3325     ((MachVEPNode*)n)-&gt;_verified_entry = saveL;</span>
<span class="line-added">3326   }</span>
3327 
3328   // End scratch_emit_size section.
3329   set_in_scratch_emit_size(false);
3330 
3331   return buf.insts_size();
3332 }
3333 
3334 void PhaseOutput::install() {
3335   if (!C-&gt;should_install_code()) {
3336     return;
3337   } else if (C-&gt;stub_function() != NULL) {
3338     install_stub(C-&gt;stub_name(),
3339                  C-&gt;save_argument_registers());
3340   } else {
3341     install_code(C-&gt;method(),
3342                  C-&gt;entry_bci(),
3343                  CompileBroker::compiler2(),
3344                  C-&gt;has_unsafe_access(),
3345                  SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),
3346                  C-&gt;rtm_state());
</pre>
<hr />
<pre>
3351                                int               entry_bci,
3352                                AbstractCompiler* compiler,
3353                                bool              has_unsafe_access,
3354                                bool              has_wide_vectors,
3355                                RTMState          rtm_state) {
3356   // Check if we want to skip execution of all compiled code.
3357   {
3358 #ifndef PRODUCT
3359     if (OptoNoExecute) {
3360       C-&gt;record_method_not_compilable(&quot;+OptoNoExecute&quot;);  // Flag as failed
3361       return;
3362     }
3363 #endif
3364     Compile::TracePhase tp(&quot;install_code&quot;, &amp;timers[_t_registerMethod]);
3365 
3366     if (C-&gt;is_osr_compilation()) {
3367       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
3368       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
3369     } else {
3370       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
<span class="line-added">3371       if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {</span>
<span class="line-added">3372         _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);</span>
<span class="line-added">3373       }</span>
<span class="line-added">3374       if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {</span>
<span class="line-added">3375         _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);</span>
<span class="line-added">3376       }</span>
<span class="line-added">3377       if (_code_offsets.value(CodeOffsets::Entry) == -1) {</span>
<span class="line-added">3378         _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);</span>
<span class="line-added">3379       }</span>
3380       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
3381     }
3382 
3383     C-&gt;env()-&gt;register_method(target,
<span class="line-modified">3384                               entry_bci,</span>
<span class="line-modified">3385                               &amp;_code_offsets,</span>
<span class="line-modified">3386                               _orig_pc_slot_offset_in_bytes,</span>
<span class="line-modified">3387                               code_buffer(),</span>
<span class="line-modified">3388                               frame_size_in_words(),</span>
<span class="line-modified">3389                               _oop_map_set,</span>
<span class="line-modified">3390                               &amp;_handler_table,</span>
<span class="line-modified">3391                               &amp;_inc_table,</span>
<span class="line-modified">3392                               compiler,</span>
<span class="line-modified">3393                               has_unsafe_access,</span>
<span class="line-modified">3394                               SharedRuntime::is_wide_vector(C-&gt;max_vector_size()),</span>
<span class="line-modified">3395                               C-&gt;rtm_state());</span>
3396 
3397     if (C-&gt;log() != NULL) { // Print code cache state into compiler log
3398       C-&gt;log()-&gt;code_cache_state();
3399     }
3400   }
3401 }
3402 void PhaseOutput::install_stub(const char* stub_name,
3403                                bool        caller_must_gc_arguments) {
3404   // Entry point will be accessed using stub_entry_point();
3405   if (code_buffer() == NULL) {
3406     Matcher::soft_match_failure();
3407   } else {
3408     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
3409       tty-&gt;print_cr(&quot;### Stub::%s&quot;, stub_name);
3410 
3411     if (!C-&gt;failing()) {
3412       assert(C-&gt;fixed_slots() == 0, &quot;no fixed slots used for runtime stubs&quot;);
3413 
3414       // Make the NMethod
3415       // For now we mark the frame as never safe for profile stackwalking
</pre>
</td>
</tr>
</table>
<center><a href="memnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="stringopts.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
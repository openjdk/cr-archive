diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -487,10 +487,11 @@
 758deedaae8406ae60147486107a54e9864aa7b0 jdk-11+13
 3595bd343b65f8c37818ebe6a4c343ddeb1a5f88 jdk-11+14
 a11c1cb542bbd1671d25b85efe7d09b983c48525 jdk-11+15
 02934b0d661b82b7fe1052a04998d2091352e08d jdk-11+16
 64e4b1686141e57a681936a8283983341484676e jdk-11+17
+d2aa5d494481a1039a092d70efa1f5c9826c5b77 lw1_0
 e1b3def126240d5433902f3cb0e91a4c27f6db50 jdk-11+18
 36ca515343e00b021dcfc902e986d26ec994a2e5 jdk-11+19
 95aad0c785e497f1bade3955c4e4a677b629fa9d jdk-12+0
 9816d7cc655e53ba081f938b656e31971b8f097a jdk-11+20
 14708e1acdc3974f4539027cbbcfa6d69f83cf51 jdk-11+21
@@ -509,10 +510,11 @@
 ef57958c7c511162da8d9a75f0b977f0f7ac464e jdk-12+7
 76072a077ee1d815152d45d1692c4b36c53c5c49 jdk-11+28
 492b366f8e5784cc4927c2c98f9b8a3f16c067eb jdk-12+8
 31b159f30fb281016c5f0c103552809aeda84063 jdk-12+9
 8f594f75e0547d4ca16649cb3501659e3155e81b jdk-12+10
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
 f0f5d23449d31f1b3580c8a73313918cafeaefd7 jdk-12+11
 15094d12a632f452a2064318a4e416d0c7a9ce0c jdk-12+12
 511a9946f83e3e3c7b9dbe1840367063fb39b4e1 jdk-12+13
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
 8897e41b327c0a5601c6ba2bba5d07f15a3ffc91 jdk-12+14
@@ -565,10 +567,14 @@
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-14+0
 22b3b7983adab54e318f75aeb94471f7a4429c1e jdk-13+25
 2f4e214781a1d597ed36bf5a36f20928c6c82996 jdk-14+1
 0692b67f54621991ba7afbf23e55b788f3555e69 jdk-13+26
 43627549a488b7d0b4df8fad436e36233df89877 jdk-14+2
+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+2b098533f1e52d7d541121409b745d9420886945 lworld_stable
+7c637fd25e7d6fccdab1098bedd48ed195a86cc7 lworld_stable
 b7f68ddec66f996ae3aad03291d129ca9f02482d jdk-13+27
 e64383344f144217c36196c3c8a2df8f588a2af3 jdk-14+3
 1e95931e7d8fa7e3899340a9c7cb28dbea50c10c jdk-13+28
 19d0b382f0869f72d4381b54fa129f1c74b6e766 jdk-14+4
 3081f39a3d30d63b112098386ac2bb027c2b7223 jdk-13+29
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -1345,10 +1345,11 @@
             preString = version_numbers.get("DEFAULT_PROMOTED_VERSION_PRE");
         }
         args = concat(args, "--with-version-pre=" + preString,
                      "--with-version-opt=" + optString);
     } else {
+        args = concat(args, "--with-version-pre=" + version_numbers.get("DEFAULT_PROMOTED_VERSION_PRE"));
         args = concat(args, "--with-version-opt=" + common.build_id);
     }
     return args;
 }
 
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -1623,10 +1623,12 @@
 
 void MachPrologNode::emit(CodeBuffer &cbuf, PhaseRegAlloc *ra_) const {
   Compile* C = ra_->C;
   C2_MacroAssembler _masm(&cbuf);
 
+  __ verified_entry(C, 0);
+  __ bind(*_verified_entry);
   // n.b. frame size includes space for return pc and rfp
   const long framesize = C->output()->frame_size_in_bytes();
   assert(framesize%(2*wordSize) == 0, "must preserve 2*wordSize alignment");
 
   // insert a nop at the start of the prolog so we can patch in a
@@ -1969,12 +1971,50 @@
   } else {
     return 2 * NativeInstruction::instruction_size;
   }
 }
 
-//=============================================================================
+///=============================================================================
+#ifndef PRODUCT
+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
+{
+  st->print_cr("# MachVEPNode");
+  if (!_verified) {
+    st->print_cr("\t load_class");
+  } else {
+    st->print_cr("\t unpack_inline_arg");
+  }
+}
+#endif
+
+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
+{
+  MacroAssembler _masm(&cbuf);
+
+  if (!_verified) {
+    Label skip;
+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);
+    __ br(Assembler::EQ, skip);
+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
+    __ bind(skip);
+
+  } else {
+    // Unpack inline type args passed as oop and then jump to
+    // the verified entry point (skipping the unverified entry).
+    __ unpack_inline_args(ra_->C, _receiver_only);
+    __ b(*_verified_entry);
+  }
+}
 
+
+uint MachVEPNode::size(PhaseRegAlloc* ra_) const
+{
+  return MachNode::size(ra_); // too many variables; just compute it the hard way
+}
+
+
+//=============================================================================
 #ifndef PRODUCT
 void MachUEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
 {
   st->print_cr("# MachUEPNode");
   if (UseCompressedClassPointers) {
@@ -1992,13 +2032,15 @@
 
 void MachUEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const
 {
   // This is the unverified entry point.
   C2_MacroAssembler _masm(&cbuf);
+  Label skip;
 
+  // UseCompressedClassPointers logic are inside cmp_klass
   __ cmp_klass(j_rarg0, rscratch2, rscratch1);
-  Label skip;
+
   // TODO
   // can we avoid this skip and still use a reloc?
   __ br(Assembler::EQ, skip);
   __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
   __ bind(skip);
@@ -2401,11 +2443,10 @@
 }
 
 void Compile::reshape_address(AddPNode* addp) {
 }
 
-
 #define MOV_VOLATILE(REG, BASE, INDEX, SCALE, DISP, SCRATCH, INSN)      \
   C2_MacroAssembler _masm(&cbuf);                                       \
   {                                                                     \
     guarantee(INDEX == -1, "mode not permitted for volatile");          \
     guarantee(DISP == 0, "mode not permitted for volatile");            \
@@ -8261,10 +8302,25 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2X(iRegLNoSp dst, iRegN src) %{
+  match(Set dst (CastP2X src));
+
+  ins_cost(INSN_COST);
+  format %{ "mov $dst, $src\t# ptr -> long" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
 instruct castP2X(iRegLNoSp dst, iRegP src) %{
   match(Set dst (CastP2X src));
 
   ins_cost(INSN_COST);
   format %{ "mov $dst, $src\t# ptr -> long" %}
@@ -8276,10 +8332,41 @@
   %}
 
   ins_pipe(ialu_reg);
 %}
 
+instruct castN2I(iRegINoSp dst, iRegN src) %{
+  match(Set dst (CastN2I src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# compressed ptr -> int" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+instruct castI2N(iRegNNoSp dst, iRegI src) %{
+  match(Set dst (CastI2N src));
+
+  ins_cost(INSN_COST);
+  format %{ "movw $dst, $src\t# int -> compressed ptr" %}
+
+  ins_encode %{
+    if ($dst$$reg != $src$$reg) {
+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));
+    }
+  %}
+
+  ins_pipe(ialu_reg);
+%}
+
+
 // Convert oop into int for vectors alignment masking
 instruct convP2I(iRegINoSp dst, iRegP src) %{
   match(Set dst (ConvL2I (CastP2X src)));
 
   ins_cost(INSN_COST);
@@ -13844,13 +13931,13 @@
 %}
 
 // ============================================================================
 // clearing of an array
 
-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)
 %{
-  match(Set dummy (ClearArray cnt base));
+  match(Set dummy (ClearArray (Binary cnt base) val));
   effect(USE_KILL cnt, USE_KILL base, KILL cr);
 
   ins_cost(4 * INSN_COST);
   format %{ "ClearArray $cnt, $base" %}
 
diff a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
@@ -31,15 +31,17 @@
 #include "c1/c1_LIRAssembler.hpp"
 #include "c1/c1_MacroAssembler.hpp"
 #include "c1/c1_Runtime1.hpp"
 #include "c1/c1_ValueStack.hpp"
 #include "ci/ciArrayKlass.hpp"
+#include "ci/ciInlineKlass.hpp"
 #include "ci/ciInstance.hpp"
 #include "code/compiledIC.hpp"
 #include "gc/shared/collectedHeap.hpp"
 #include "nativeInst_aarch64.hpp"
 #include "oops/objArrayKlass.hpp"
+#include "oops/oop.inline.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/sharedRuntime.hpp"
 #include "utilities/powerOfTwo.hpp"
 #include "vmreg_aarch64.inline.hpp"
 
@@ -226,11 +228,11 @@
   // r2: osr buffer
   //
 
   // build frame
   ciMethod* m = compilation()->method();
-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());
+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);
 
   // OSR buffer is
   //
   // locals[nlocals-1..0]
   // monitors[0..number_of_locks]
@@ -441,11 +443,11 @@
     __ mov(r0, r19);  // Restore the exception
   }
 
   // remove the activation and dispatch to the unwind handler
   __ block_comment("remove_frame and dispatch to the unwind handler");
-  __ remove_frame(initial_frame_size_in_bytes());
+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
   __ far_jump(RuntimeAddress(Runtime1::entry_for(Runtime1::unwind_exception_id)));
 
   // Emit the slow path assembly
   if (stub != NULL) {
     stub->emit_code(this);
@@ -492,21 +494,40 @@
 }
 
 void LIR_Assembler::return_op(LIR_Opr result) {
   assert(result->is_illegal() || !result->is_single_cpu() || result->as_register() == r0, "word returns are in r0,");
 
+  ciMethod* method = compilation()->method();
+
+  ciType* return_type = method->return_type();
+  if (InlineTypeReturnedAsFields && return_type->is_inlinetype()) {
+    ciInlineKlass* vk = return_type->as_inline_klass();
+    if (vk->can_be_returned_as_fields()) {
+      address unpack_handler = vk->unpack_handler();
+      assert(unpack_handler != NULL, "must be");
+      __ far_call(RuntimeAddress(unpack_handler));
+      // At this point, rax points to the value object (for interpreter or C1 caller).
+      // The fields of the object are copied into registers (for C2 caller).
+    }
+  }
+
+
   // Pop the stack before the safepoint code
-  __ remove_frame(initial_frame_size_in_bytes());
+  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());
 
   if (StackReservedPages > 0 && compilation()->has_reserved_stack_access()) {
     __ reserved_stack_check();
   }
 
   __ fetch_and_read_polling_page(rscratch1, relocInfo::poll_return_type);
   __ ret(lr);
 }
 
+int LIR_Assembler::store_inline_type_fields_to_buf(ciInlineKlass* vk) {
+  return (__ store_inline_type_fields_to_buf(vk, false));
+}
+
 int LIR_Assembler::safepoint_poll(LIR_Opr tmp, CodeEmitInfo* info) {
   guarantee(info != NULL, "Shouldn't be NULL");
   __ get_polling_page(rscratch1, relocInfo::poll_type);
   add_debug_info_for_branch(info);  // This isn't just debug info:
                                     // it's the oop map
@@ -548,15 +569,16 @@
       assert(patch_code == lir_patch_none, "no patching handled here");
       __ mov(dest->as_register_lo(), (intptr_t)c->as_jlong());
       break;
     }
 
+    case T_INLINE_TYPE:
     case T_OBJECT: {
-        if (patch_code == lir_patch_none) {
-          jobject2reg(c->as_jobject(), dest->as_register());
-        } else {
+        if (patch_code != lir_patch_none) {
           jobject2reg_with_patching(dest->as_register(), info);
+        } else {
+          jobject2reg(c->as_jobject(), dest->as_register());
         }
       break;
     }
 
     case T_METADATA: {
@@ -594,10 +616,11 @@
 }
 
 void LIR_Assembler::const2stack(LIR_Opr src, LIR_Opr dest) {
   LIR_Const* c = src->as_constant_ptr();
   switch (c->type()) {
+  case T_INLINE_TYPE:
   case T_OBJECT:
     {
       if (! c->as_jobject())
         __ str(zr, frame_map()->address_for_slot(dest->single_stack_ix()));
       else {
@@ -660,12 +683,15 @@
     break;
   case T_INT:
     assert(c->as_jint() == 0, "should be");
     insn = &Assembler::strw;
     break;
+  case T_INLINE_TYPE:
   case T_OBJECT:
   case T_ARRAY:
+    // Non-null case is not handled on aarch64 but handled on x86
+    // FIXME: do we need to add it here?
     assert(c->as_jobject() == 0, "should be");
     if (UseCompressedOops && !wide) {
       insn = &Assembler::strw;
     } else {
       insn = &Assembler::str;
@@ -700,11 +726,11 @@
       // Can do LONG -> OBJECT
       move_regs(src->as_register_lo(), dest->as_register());
       return;
     }
     assert(src->is_single_cpu(), "must match");
-    if (src->type() == T_OBJECT) {
+    if (src->type() == T_OBJECT || src->type() == T_INLINE_TYPE) {
       __ verify_oop(src->as_register());
     }
     move_regs(src->as_register(), dest->as_register());
 
   } else if (dest->is_double_cpu()) {
@@ -794,10 +820,11 @@
     case T_DOUBLE: {
       __ strd(src->as_double_reg(), as_Address(to_addr));
       break;
     }
 
+    case T_INLINE_TYPE: // fall through
     case T_ARRAY:   // fall through
     case T_OBJECT:  // fall through
       if (UseCompressedOops && !wide) {
         __ strw(compressed_src, as_Address(to_addr, rscratch2));
       } else {
@@ -919,11 +946,11 @@
 
 void LIR_Assembler::mem2reg(LIR_Opr src, LIR_Opr dest, BasicType type, LIR_PatchCode patch_code, CodeEmitInfo* info, bool wide, bool /* unaligned */) {
   LIR_Address* addr = src->as_address_ptr();
   LIR_Address* from_addr = src->as_address_ptr();
 
-  if (addr->base()->type() == T_OBJECT) {
+  if (addr->base()->type() == T_OBJECT || addr->base()->type() == T_INLINE_TYPE) {
     __ verify_oop(addr->base()->as_pointer_register());
   }
 
   if (patch_code != lir_patch_none) {
     deoptimize_trap(info);
@@ -943,10 +970,11 @@
     case T_DOUBLE: {
       __ ldrd(dest->as_double_reg(), as_Address(from_addr));
       break;
     }
 
+    case T_INLINE_TYPE: // fall through
     case T_ARRAY:   // fall through
     case T_OBJECT:  // fall through
       if (UseCompressedOops && !wide) {
         __ ldrw(dest->as_register(), as_Address(from_addr));
       } else {
@@ -1008,15 +1036,32 @@
       // Load barrier has not yet been applied, so ZGC can't verify the oop here
       __ verify_oop(dest->as_register());
     }
   } else if (type == T_ADDRESS && addr->disp() == oopDesc::klass_offset_in_bytes()) {
     if (UseCompressedClassPointers) {
+      __ andr(dest->as_register(), dest->as_register(), oopDesc::compressed_klass_mask());
       __ decode_klass_not_null(dest->as_register());
+    } else {
+      __   ubfm(dest->as_register(), dest->as_register(), 0, 63 - oopDesc::storage_props_nof_bits);
     }
   }
 }
 
+void LIR_Assembler::move(LIR_Opr src, LIR_Opr dst) {
+  assert(dst->is_cpu_register(), "must be");
+  assert(dst->type() == src->type(), "must be");
+
+  if (src->is_cpu_register()) {
+    reg2reg(src, dst);
+  } else if (src->is_stack()) {
+    stack2reg(src, dst, dst->type());
+  } else if (src->is_constant()) {
+    const2reg(src, dst, lir_patch_none, NULL);
+  } else {
+    ShouldNotReachHere();
+  }
+}
 
 int LIR_Assembler::array_element_size(BasicType type) const {
   int elem_size = type2aelembytes(type);
   return exact_log2(elem_size);
 }
@@ -1204,11 +1249,11 @@
 
 void LIR_Assembler::emit_alloc_array(LIR_OpAllocArray* op) {
   Register len =  op->len()->as_register();
   __ uxtw(len, len);
 
-  if (UseSlowPath ||
+  if (UseSlowPath || op->type() == T_INLINE_TYPE ||
       (!UseFastNewObjectArray && is_reference_type(op->type())) ||
       (!UseFastNewTypeArray   && !is_reference_type(op->type()))) {
     __ b(*op->stub()->entry());
   } else {
     Register tmp1 = op->tmp1()->as_register();
@@ -1516,10 +1561,132 @@
   } else {
     ShouldNotReachHere();
   }
 }
 
+void LIR_Assembler::emit_opFlattenedArrayCheck(LIR_OpFlattenedArrayCheck* op) {
+  // We are loading/storing an array that *may* be a flattened array (the declared type
+  // Object[], interface[], or VT?[]). If this array is flattened, take slow path.
+
+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());
+  __ tst(op->tmp()->as_register(), ArrayStorageProperties::flattened_value);
+  __ br(Assembler::NE, *op->stub()->entry());
+  if (!op->value()->is_illegal()) {
+    // We are storing into the array.
+    Label skip;
+    __ tst(op->tmp()->as_register(), ArrayStorageProperties::null_free_value);
+    __ br(Assembler::EQ, skip);
+    // The array is not flattened, but it is null_free. If we are storing
+    // a null, take the slow path (which will throw NPE).
+    __ cbz(op->value()->as_register(), *op->stub()->entry());
+    __ bind(skip);
+  }
+
+}
+
+void LIR_Assembler::emit_opNullFreeArrayCheck(LIR_OpNullFreeArrayCheck* op) {
+  // This is called when we use aastore into a an array declared as "[LVT;",
+  // where we know VT is not flattened (due to FlatArrayElementMaxSize, etc).
+  // However, we need to do a NULL check if the actual array is a "[QVT;".
+
+  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());
+  __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);
+  __ cmp(op->tmp()->as_register(), rscratch1);
+}
+
+void LIR_Assembler::emit_opSubstitutabilityCheck(LIR_OpSubstitutabilityCheck* op) {
+  Label L_oops_equal;
+  Label L_oops_not_equal;
+  Label L_end;
+
+  Register left  = op->left()->as_register();
+  Register right = op->right()->as_register();
+
+  __ cmp(left, right);
+  __ br(Assembler::EQ, L_oops_equal);
+
+  // (1) Null check -- if one of the operands is null, the other must not be null (because
+  //     the two references are not equal), so they are not substitutable,
+  //     FIXME: do null check only if the operand is nullable
+  {
+    __ cbz(left, L_oops_not_equal);
+    __ cbz(right, L_oops_not_equal);
+  }
+
+
+  ciKlass* left_klass = op->left_klass();
+  ciKlass* right_klass = op->right_klass();
+
+  // (2) Value object check -- if either of the operands is not a value object,
+  //     they are not substitutable. We do this only if we are not sure that the
+  //     operands are value objects
+  if ((left_klass == NULL || right_klass == NULL) ||// The klass is still unloaded, or came from a Phi node.
+      !left_klass->is_inlinetype() || !right_klass->is_inlinetype()) {
+    Register tmp1  = rscratch1; /* op->tmp1()->as_register(); */
+    Register tmp2  = rscratch2; /* op->tmp2()->as_register(); */
+
+    __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);
+
+    __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));
+    __ andr(tmp1, tmp1, tmp2);
+
+    __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));
+    __ andr(tmp1, tmp1, tmp2);
+
+    __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);
+    __ cmp(tmp1, tmp2);
+    __ br(Assembler::NE, L_oops_not_equal);
+  }
+
+  // (3) Same klass check: if the operands are of different klasses, they are not substitutable.
+  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {
+    // No need to load klass -- the operands are statically known to be the same inline klass.
+    __ b(*op->stub()->entry());
+  } else {
+    Register left_klass_op = op->left_klass_op()->as_register();
+    Register right_klass_op = op->right_klass_op()->as_register();
+
+    if (UseCompressedOops) {
+      __ ldrw(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
+      __ ldrw(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
+      __ cmpw(left_klass_op, right_klass_op);
+    } else {
+      __ ldr(left_klass_op,  Address(left,  oopDesc::klass_offset_in_bytes()));
+      __ ldr(right_klass_op, Address(right, oopDesc::klass_offset_in_bytes()));
+      __ cmp(left_klass_op, right_klass_op);
+    }
+
+    __ br(Assembler::EQ, *op->stub()->entry()); // same klass -> do slow check
+    // fall through to L_oops_not_equal
+  }
+
+  __ bind(L_oops_not_equal);
+  move(op->not_equal_result(), op->result_opr());
+  __ b(L_end);
+
+  __ bind(L_oops_equal);
+  move(op->equal_result(), op->result_opr());
+  __ b(L_end);
+
+  // We've returned from the stub. op->result_opr() contains 0x0 IFF the two
+  // operands are not substitutable. (Don't compare against 0x1 in case the
+  // C compiler is naughty)
+  __ bind(*op->stub()->continuation());
+
+  if (op->result_opr()->type() == T_LONG) {
+    __ cbzw(op->result_opr()->as_register(), L_oops_not_equal); // (call_stub() == 0x0) -> not_equal
+  } else {
+    __ cbz(op->result_opr()->as_register(), L_oops_not_equal); // (call_stub() == 0x0) -> not_equal
+  }
+
+  move(op->equal_result(), op->result_opr()); // (call_stub() != 0x0) -> equal
+  // fall-through
+  __ bind(L_end);
+
+}
+
+
 void LIR_Assembler::casw(Register addr, Register newval, Register cmpval) {
   __ cmpxchg(addr, cmpval, newval, Assembler::word, /* acquire*/ true, /* release*/ true, /* weak*/ false, rscratch1);
   __ cset(rscratch1, Assembler::NE);
   __ membar(__ AnyAny);
 }
@@ -1959,10 +2126,11 @@
         imm = opr2->as_constant_ptr()->as_jint();
         break;
       case T_METADATA:
         imm = (intptr_t)(opr2->as_constant_ptr()->as_metadata());
         break;
+      case T_INLINE_TYPE:
       case T_OBJECT:
       case T_ARRAY:
         jobject2reg(opr2->as_constant_ptr()->as_jobject(), rscratch1);
         __ cmpoop(reg1, rscratch1);
         return;
@@ -2125,10 +2293,11 @@
         ShouldNotReachHere();
         break;
       }
       break;
     case T_LONG:
+    case T_INLINE_TYPE:
     case T_ADDRESS:
     case T_OBJECT:
       switch (code) {
       case lir_shl:  __ lslv (dreg, lreg, count->as_register()); break;
       case lir_shr:  __ asrv (dreg, lreg, count->as_register()); break;
@@ -2161,10 +2330,11 @@
         break;
       }
       break;
     case T_LONG:
     case T_ADDRESS:
+    case T_INLINE_TYPE:
     case T_OBJECT:
       switch (code) {
       case lir_shl:  __ lsl (dreg, lreg, count); break;
       case lir_shr:  __ asr (dreg, lreg, count); break;
       case lir_ushr: __ lsr (dreg, lreg, count); break;
@@ -2205,10 +2375,23 @@
   assert(offset_from_rsp_in_bytes < frame_map()->reserved_argument_area_size(), "invalid offset");
   __ lea(rscratch1, __ constant_oop_address(o));
   __ str(rscratch1, Address(sp, offset_from_rsp_in_bytes));
 }
 
+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {
+  __ load_storage_props(tmp, obj);
+  if (is_dest) {
+    // We also take slow path if it's a null_free destination array, just in case the source array
+    // contains NULLs.
+    __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);
+  } else {
+    __ tst(tmp, ArrayStorageProperties::flattened_value);
+  }
+  __ br(Assembler::NE, *slow_path->entry());
+}
+
+
 
 // This code replaces a call to arraycopy; no exception may
 // be thrown in this code, they must be thrown in the System.arraycopy
 // activation frame; we could save some checks if this would not be the case
 void LIR_Assembler::emit_arraycopy(LIR_OpArrayCopy* op) {
@@ -2226,10 +2409,26 @@
   CodeStub* stub = op->stub();
   int flags = op->flags();
   BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;
   if (is_reference_type(basic_type)) basic_type = T_OBJECT;
 
+  if (flags & LIR_OpArrayCopy::always_slow_path) {
+    __ b(*stub->entry());
+    __ bind(*stub->continuation());
+    return;
+  }
+
+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {
+    arraycopy_inlinetype_check(src, tmp, stub, false);
+  }
+
+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {
+    arraycopy_inlinetype_check(dst, tmp, stub, true);
+  }
+
+
+
   // if we don't know anything, just go through the generic arraycopy
   if (default_type == NULL // || basic_type == T_OBJECT
       ) {
     Label done;
     assert(src == r1 && src_pos == r2, "mismatch in calling convention");
@@ -3126,10 +3325,11 @@
     break;
   case T_LONG:
     xchg = &MacroAssembler::atomic_xchgal;
     add = &MacroAssembler::atomic_addal;
     break;
+  case T_INLINE_TYPE:
   case T_OBJECT:
   case T_ARRAY:
     if (UseCompressedOops) {
       xchg = &MacroAssembler::atomic_xchgalw;
       add = &MacroAssembler::atomic_addalw;
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -44,10 +44,11 @@
 #include "runtime/biasedLocking.hpp"
 #include "runtime/icache.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 #include "runtime/sharedRuntime.hpp"
+#include "runtime/signature_cc.hpp"
 #include "runtime/thread.hpp"
 #include "utilities/powerOfTwo.hpp"
 #ifdef COMPILER1
 #include "c1/c1_LIRAssembler.hpp"
 #endif
@@ -1312,11 +1313,15 @@
     Unimplemented();
   }
 }
 
 void MacroAssembler::verify_oop(Register reg, const char* s) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   // Pass register number to verify_oop_subroutine
   const char* b = NULL;
   {
     ResourceMark rm;
@@ -1342,11 +1347,15 @@
 
   BLOCK_COMMENT("} verify_oop");
 }
 
 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
-  if (!VerifyOops) return;
+  if (!VerifyOops || VerifyAdapterSharing) {
+    // Below address of the code string confuses VerifyAdapterSharing
+    // because it may differ between otherwise equivalent adapters.
+    return;
+  }
 
   const char* b = NULL;
   {
     ResourceMark rm;
     stringStream ss;
@@ -1435,10 +1444,14 @@
   pass_arg1(this, arg_1);
   pass_arg2(this, arg_2);
   call_VM_leaf_base(entry_point, 3);
 }
 
+void MacroAssembler::super_call_VM_leaf(address entry_point) {
+  MacroAssembler::call_VM_leaf_base(entry_point, 1);
+}
+
 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
   pass_arg0(this, arg_0);
   MacroAssembler::call_VM_leaf_base(entry_point, 1);
 }
 
@@ -1484,10 +1497,43 @@
     // nothing to do, (later) access of M[reg + offset]
     // will provoke OS NULL exception if reg = NULL
   }
 }
 
+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {
+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));
+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);
+  cbnz(temp_reg, is_value);
+}
+
+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);
+}
+
+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);
+}
+
+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {
+  (void) temp_reg; // keep signature uniform with x86
+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
+}
+
+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {
+  load_storage_props(temp_reg, oop);
+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);
+  cbnz(temp_reg, is_flattened_array);
+}
+
+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {
+  load_storage_props(temp_reg, oop);
+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);
+  cbnz(temp_reg, is_null_free_array);
+}
+
 // MacroAssembler protected routines needed to implement
 // public methods
 
 void MacroAssembler::mov(Register r, Address dest) {
   code_section()->relocate(pc(), dest.rspec());
@@ -3715,19 +3761,28 @@
   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
 }
 
-void MacroAssembler::load_klass(Register dst, Register src) {
+void MacroAssembler::load_metadata(Register dst, Register src) {
   if (UseCompressedClassPointers) {
     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
-    decode_klass_not_null(dst);
   } else {
     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
   }
 }
 
+void MacroAssembler::load_klass(Register dst, Register src) {
+  load_metadata(dst, src);
+  if (UseCompressedClassPointers) {
+    andr(dst, dst, oopDesc::compressed_klass_mask());
+    decode_klass_not_null(dst);
+  } else {
+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);
+  }
+}
+
 // ((OopHandle)result).resolve();
 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
   // OopHandle::resolve is an indirection.
   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
 }
@@ -3755,10 +3810,19 @@
   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
   ldr(dst, Address(dst, mirror_offset));
   resolve_oop_handle(dst, tmp);
 }
 
+void MacroAssembler::load_storage_props(Register dst, Register src) {
+  load_metadata(dst, src);
+  if (UseCompressedClassPointers) {
+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);
+  } else {
+    asr(dst, dst, oopDesc::wide_storage_props_shift);
+  }
+}
+
 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
   if (UseCompressedClassPointers) {
     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
     if (CompressedKlassPointers::base() == NULL) {
       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
@@ -4092,18 +4156,19 @@
   }
 }
 
 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
                                      Address dst, Register src,
-                                     Register tmp1, Register thread_tmp) {
+                                     Register tmp1, Register thread_tmp, Register tmp3) {
+
   BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();
   decorators = AccessInternal::decorator_fixup(decorators);
   bool as_raw = (decorators & AS_RAW) != 0;
   if (as_raw) {
-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
   } else {
-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);
+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);
   }
 }
 
 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
@@ -4123,17 +4188,17 @@
                                             Register thread_tmp, DecoratorSet decorators) {
   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
 }
 
 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
-                                    Register thread_tmp, DecoratorSet decorators) {
-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {
+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);
 }
 
 // Used for storing NULLs.
 void MacroAssembler::store_heap_oop_null(Address dst) {
-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);
+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);
 }
 
 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
   assert(oop_recorder() != NULL, "this assembler needs a Recorder");
   int index = oop_recorder()->allocate_metadata_index(obj);
@@ -5202,10 +5267,399 @@
   }
 
   pop(saved_regs, sp);
 }
 
+// C2 compiled method's prolog code
+// Moved here from aarch64.ad to support Valhalla code belows
+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {
+
+// n.b. frame size includes space for return pc and rfp
+  const long framesize = C->frame_size_in_bytes();
+  assert(framesize % (2 * wordSize) == 0, "must preserve 2 * wordSize alignment");
+
+  // insert a nop at the start of the prolog so we can patch in a
+  // branch if we need to invalidate the method later
+  nop();
+
+  int bangsize = C->bang_size_in_bytes();
+  if (C->need_stack_bang(bangsize) && UseStackBanging)
+     generate_stack_overflow_check(bangsize);
+
+  build_frame(framesize);
+
+  if (VerifyStackAtCalls) {
+    Unimplemented();
+  }
+}
+
+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {
+  // An inline type might be returned. If fields are in registers we
+  // need to allocate an inline type instance and initialize it with
+  // the value of the fields.
+  Label skip;
+  // We only need a new buffered inline type if a new one is not returned
+  cmp(r0, (u1) 1);
+  br(Assembler::EQ, skip);
+  int call_offset = -1;
+
+  Label slow_case;
+
+  // Try to allocate a new buffered inline type (from the heap)
+  if (UseTLAB) {
+
+    if (vk != NULL) {
+      // Called from C1, where the return type is statically known.
+      mov(r1, (intptr_t)vk->get_InlineKlass());
+      jint lh = vk->layout_helper();
+      assert(lh != Klass::_lh_neutral_value, "inline class in return type must have been resolved");
+      mov(r14, lh);
+    } else {
+       // Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)
+       andr(r1, r0, -2);
+       // get obj size
+       ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));
+    }
+
+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
+
+     // check whether we have space in TLAB,
+     // rscratch1 contains pointer to just allocated obj
+      lea(r14, Address(r13, r14));
+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));
+
+      cmp(r14, rscratch1);
+      br(Assembler::GT, slow_case);
+
+      // OK we have room in TLAB,
+      // Set new TLAB top
+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));
+
+      // Set new class always locked
+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());
+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));
+
+      store_klass_gap(r13, zr);  // zero klass gap for compressed oops
+      if (vk == NULL) {
+        // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).
+         mov(r0, r1);
+      }
+
+      store_klass(r13, r1);  // klass
+
+      if (vk != NULL) {
+        // FIXME -- do the packing in-line to avoid the runtime call
+        mov(r0, r13);
+        far_call(RuntimeAddress(vk->pack_handler())); // no need for call info as this will not safepoint.
+      } else {
+
+        // We have our new buffered inline type, initialize its fields with an inline class specific handler
+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));
+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));
+
+        // Mov new class to r0 and call pack_handler
+        mov(r0, r13);
+        blr(r1);
+      }
+      b(skip);
+  }
+
+  bind(slow_case);
+  // We failed to allocate a new inline type, fall back to a runtime
+  // call. Some oop field may be live in some registers but we can't
+  // tell. That runtime call will take care of preserving them
+  // across a GC if there's one.
+
+
+  if (from_interpreter) {
+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());
+  } else {
+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));
+    blr(rscratch1);
+    call_offset = offset();
+  }
+
+  bind(skip);
+  return call_offset;
+}
+
+// Move a value between registers/stack slots and update the reg_state
+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  if (reg_state[to->value()] == reg_written) {
+    return true; // Already written
+  }
+
+  if (from != to && bt != T_VOID) {
+    if (reg_state[to->value()] == reg_readonly) {
+      return false; // Not yet writable
+    }
+    if (from->is_reg()) {
+      if (to->is_reg()) {
+        mov(to->as_Register(), from->as_Register());
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        Address to_addr = Address(sp, st_off);
+        if (from->is_FloatRegister()) {
+          if (bt == T_DOUBLE) {
+             strd(from->as_FloatRegister(), to_addr);
+          } else {
+             assert(bt == T_FLOAT, "must be float");
+             strs(from->as_FloatRegister(), to_addr);
+          }
+        } else {
+          str(from->as_Register(), to_addr);
+        }
+      }
+    } else {
+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);
+      if (to->is_reg()) {
+        if (to->is_FloatRegister()) {
+          if (bt == T_DOUBLE) {
+             ldrd(to->as_FloatRegister(), from_addr);
+          } else {
+            assert(bt == T_FLOAT, "must be float");
+            ldrs(to->as_FloatRegister(), from_addr);
+          }
+        } else {
+          ldr(to->as_Register(), from_addr);
+        }
+      } else {
+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        ldr(rscratch1, from_addr);
+        str(rscratch1, Address(sp, st_off));
+      }
+    }
+  }
+
+  // Update register states
+  reg_state[from->value()] = reg_writable;
+  reg_state[to->value()] = reg_written;
+  return true;
+}
+
+// Read all fields from an inline type oop and store the values in registers/stack slots
+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,
+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {
+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;
+  assert(sig->at(sig_index)._bt == T_VOID, "should be at end delimiter");
+
+
+  int vt = 1;
+  bool done = true;
+  bool mark_done = true;
+  do {
+    sig_index--;
+    BasicType bt = sig->at(sig_index)._bt;
+    if (bt == T_INLINE_TYPE) {
+      vt--;
+    } else if (bt == T_VOID &&
+               sig->at(sig_index-1)._bt != T_LONG &&
+               sig->at(sig_index-1)._bt != T_DOUBLE) {
+      vt++;
+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {
+      to_index--; // Ignore this
+    } else {
+      assert(to_index >= 0, "invalid to_index");
+      VMRegPair pair_to = regs_to[to_index--];
+      VMReg to = pair_to.first();
+
+      if (bt == T_VOID) continue;
+
+      int idx = (int) to->value();
+      if (reg_state[idx] == reg_readonly) {
+         if (idx != from->value()) {
+           mark_done = false;
+         }
+         done = false;
+         continue;
+      } else if (reg_state[idx] == reg_written) {
+        continue;
+      } else {
+        assert(reg_state[idx] == reg_writable, "must be writable");
+        reg_state[idx] = reg_written;
+      }
+
+      if (fromReg == noreg) {
+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        ldr(rscratch2, Address(sp, st_off));
+        fromReg = rscratch2;
+      }
+
+      int off = sig->at(sig_index)._offset;
+      assert(off > 0, "offset in object should be positive");
+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+
+      Address fromAddr = Address(fromReg, off);
+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);
+
+      if (!to->is_FloatRegister()) {
+
+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();
+
+        if (is_oop) {
+          load_heap_oop(dst, fromAddr);
+        } else {
+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);
+        }
+        if (to->is_stack()) {
+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+          str(dst, Address(sp, st_off));
+        }
+      } else {
+        if (bt == T_DOUBLE) {
+          ldrd(to->as_FloatRegister(), fromAddr);
+        } else {
+          assert(bt == T_FLOAT, "must be float");
+          ldrs(to->as_FloatRegister(), fromAddr);
+        }
+     }
+
+    }
+
+  } while (vt != 0);
+
+  if (mark_done && reg_state[from->value()] != reg_written) {
+    // This is okay because no one else will write to that slot
+    reg_state[from->value()] = reg_writable;
+  }
+  return done;
+}
+
+// Pack fields back into an inline type oop
+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                                        int ret_off, int extra_stack_offset) {
+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, "should be at end delimiter");
+  assert(to->is_valid(), "must be");
+
+  if (reg_state[to->value()] == reg_written) {
+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+    return true; // Already written
+  }
+
+  Register val_array = r0;
+  Register val_obj_tmp = r11;
+  Register from_reg_tmp = r10;
+  Register tmp1 = r14;
+  Register tmp2 = r13;
+  Register tmp3 = r1;
+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();
+
+  if (reg_state[to->value()] == reg_readonly) {
+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {
+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);
+      return false; // Not yet writable
+    }
+    val_obj = val_obj_tmp;
+  }
+
+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);
+  load_heap_oop(val_obj, Address(val_array, index));
+
+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);
+  VMRegPair from_pair;
+  BasicType bt;
+
+  while (stream.next(from_pair, bt)) {
+    int off = sig->at(stream.sig_cc_index())._offset;
+    assert(off > 0, "offset in object should be positive");
+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);
+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
+
+    VMReg from_r1 = from_pair.first();
+    VMReg from_r2 = from_pair.second();
+
+    // Pack the scalarized field into the value object.
+    Address dst(val_obj, off);
+
+    if (!from_r1->is_FloatRegister()) {
+      Register from_reg;
+      if (from_r1->is_stack()) {
+        from_reg = from_reg_tmp;
+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;
+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);
+      } else {
+        from_reg = from_r1->as_Register();
+      }
+
+      if (is_oop) {
+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;
+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);
+      } else {
+        store_sized_value(dst, from_reg, size_in_bytes);
+      }
+    } else {
+      if (from_r2->is_valid()) {
+        strd(from_r1->as_FloatRegister(), dst);
+      } else {
+        strs(from_r1->as_FloatRegister(), dst);
+      }
+    }
+
+    reg_state[from_r1->value()] = reg_writable;
+  }
+  sig_index = stream.sig_cc_index();
+  from_index = stream.regs_cc_index();
+
+  assert(reg_state[to->value()] == reg_writable, "must have already been read");
+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);
+  assert(success, "to register must be writeable");
+
+  return true;
+}
+
+// Unpack all inline type arguments passed as oops
+void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {
+  int sp_inc = unpack_inline_args_common(C, receiver_only);
+  // Emit code for verified entry and save increment for stack repair on return
+  verified_entry(C, sp_inc);
+}
+
+int MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                                        int args_passed, int args_on_stack, VMRegPair* regs,            // from
+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to
+  // Check if we need to extend the stack for packing/unpacking
+  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;
+  if (sp_inc > 0) {
+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);
+    if (!is_packing) {
+      // Save the return address, adjust the stack (make sure it is properly
+      // 16-byte aligned) and copy the return address to the new top of the stack.
+      // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).
+      // FIXME: We need not to preserve return address on aarch64
+      pop(rscratch1);
+      sub(sp, sp, sp_inc);
+      push(rscratch1);
+    }
+  } else {
+    // The scalarized calling convention needs less stack space than the unscalarized one.
+    // No need to extend the stack, the caller will take care of these adjustments.
+    sp_inc = 0;
+  }
+
+  int ret_off; // make sure we don't overwrite the return address
+  if (is_packing) {
+    // For C1 code, the VIEP doesn't have reserved slots, so we store the returned address at
+    // rsp[0] during shuffling.
+    ret_off = 0;
+  } else {
+    // C2 code ensures that sp_inc is a reserved slot.
+    ret_off = sp_inc;
+  }
+
+  return shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,
+                                    sig_bt, sig_cc,
+                                    args_passed, args_on_stack, regs,
+                                    args_passed_to, args_on_stack_to, regs_to,
+                                    sp_inc, ret_off);
+}
+
+VMReg MacroAssembler::spill_reg_for(VMReg reg) {
+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();
+}
+
 void MacroAssembler::cache_wb(Address line) {
   assert(line.getMode() == Address::base_plus_offset, "mode should be base_plus_offset");
   assert(line.index() == noreg, "index should be noreg");
   assert(line.offset() == 0, "offset should be 0");
   // would like to assert this
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
@@ -26,11 +26,16 @@
 #ifndef CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
 #define CPU_AARCH64_MACROASSEMBLER_AARCH64_HPP
 
 #include "asm/assembler.hpp"
 #include "oops/compressedOops.hpp"
+#include "utilities/macros.hpp"
 #include "utilities/powerOfTwo.hpp"
+#include "runtime/signature.hpp"
+
+
+class ciInlineKlass;
 
 // MacroAssembler extends Assembler by frequently used macros.
 //
 // Instructions for which a 'better' code sequence exists depending
 // on arguments should also go in here.
@@ -606,10 +611,20 @@
 
   virtual void null_check(Register reg, int offset = -1);
   static bool needs_explicit_null_check(intptr_t offset);
   static bool uses_implicit_null_check(void* address);
 
+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);
+
+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);
+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);
+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);
+
+  // Check klass/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)
+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);
+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);
+
   static address target_addr_for_insn(address insn_addr, unsigned insn);
   static address target_addr_for_insn(address insn_addr) {
     unsigned insn = *(unsigned*)insn_addr;
     return target_addr_for_insn(insn_addr, insn);
   }
@@ -813,10 +828,13 @@
 
   void load_method_holder_cld(Register rresult, Register rmethod);
   void load_method_holder(Register holder, Register method);
 
   // oop manipulations
+  void load_metadata(Register dst, Register src);
+  void load_storage_props(Register dst, Register src);
+
   void load_klass(Register dst, Register src);
   void store_klass(Register dst, Register src);
   void cmp_klass(Register oop, Register trial_klass, Register tmp);
 
   void resolve_weak_handle(Register result, Register tmp);
@@ -825,11 +843,11 @@
 
   void access_load_at(BasicType type, DecoratorSet decorators, Register dst, Address src,
                       Register tmp1, Register tmp_thread);
 
   void access_store_at(BasicType type, DecoratorSet decorators, Address dst, Register src,
-                       Register tmp1, Register tmp_thread);
+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);
 
   // Resolves obj for access. Result is placed in the same register.
   // All other registers are preserved.
   void resolve(DecoratorSet decorators, Register obj);
 
@@ -837,11 +855,11 @@
                      Register thread_tmp = noreg, DecoratorSet decorators = 0);
 
   void load_heap_oop_not_null(Register dst, Address src, Register tmp1 = noreg,
                               Register thread_tmp = noreg, DecoratorSet decorators = 0);
   void store_heap_oop(Address dst, Register src, Register tmp1 = noreg,
-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);
+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);
 
   // currently unimplemented
   // Used for storing NULL. All other oop constants should be
   // stored using routines that take a jobject.
   void store_heap_oop_null(Address dst);
@@ -1170,10 +1188,41 @@
   void sub(Register Rd, Register Rn, RegisterOrConstant decrement);
   void subw(Register Rd, Register Rn, RegisterOrConstant decrement);
 
   void adrp(Register reg1, const Address &dest, unsigned long &byte_offset);
 
+
+  enum RegState {
+     reg_readonly,
+     reg_writable,
+     reg_written
+  };
+
+  void verified_entry(Compile* C, int sp_inc);
+
+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);
+
+// Unpack all inline type arguments passed as oops
+  void unpack_inline_args(Compile* C, bool receiver_only);
+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);
+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,
+                            RegState reg_state[], int ret_off, int extra_stack_offset);
+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,
+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],
+                          int ret_off, int extra_stack_offset);
+  void restore_stack(Compile* C);
+
+  int shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,
+                          BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,
+                          int args_passed, int args_on_stack, VMRegPair* regs,
+                          int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);
+  bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,
+                                 VMRegPair* regs_from, int from_index, int regs_from_count,
+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);
+  VMReg spill_reg_for(VMReg reg);
+
+
   void tableswitch(Register index, jint lowbound, jint highbound,
                    Label &jumptable, Label &jumptable_end, int stride = 1) {
     adr(rscratch1, jumptable);
     subsw(rscratch2, index, lowbound);
     subsw(zr, rscratch2, highbound - lowbound);
@@ -1235,10 +1284,12 @@
 
   void string_equals(Register a1, Register a2, Register result, Register cnt1,
                      int elem_size);
 
   void fill_words(Register base, Register cnt, Register value);
+  void fill_words(Register base, uint64_t cnt, Register value);
+
   void zero_words(Register base, uint64_t cnt);
   void zero_words(Register ptr, Register cnt);
   void zero_dcache_blocks(Register base, Register cnt);
 
   static const int zero_words_block_size;
@@ -1356,10 +1407,13 @@
     }
   }
 
   void cache_wb(Address line);
   void cache_wbsync(bool is_pre);
+
+  #include "asm/macroAssembler_common.hpp"
+
 };
 
 #ifdef ASSERT
 inline bool AbstractAssembler::pd_check_instruction_mark() { return false; }
 #endif
diff a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
@@ -146,11 +146,11 @@
 static void do_oop_store(InterpreterMacroAssembler* _masm,
                          Address dst,
                          Register val,
                          DecoratorSet decorators) {
   assert(val == noreg || val == r0, "parameter is just for looks");
-  __ store_heap_oop(dst, val, r10, r1, decorators);
+  __ store_heap_oop(dst, val, r10, r1, noreg, decorators);
 }
 
 static void do_oop_load(InterpreterMacroAssembler* _masm,
                         Address src,
                         Register dst,
@@ -169,10 +169,11 @@
 {
   if (!RewriteBytecodes)  return;
   Label L_patch_done;
 
   switch (bc) {
+  case Bytecodes::_fast_qputfield:
   case Bytecodes::_fast_aputfield:
   case Bytecodes::_fast_bputfield:
   case Bytecodes::_fast_zputfield:
   case Bytecodes::_fast_cputfield:
   case Bytecodes::_fast_dputfield:
@@ -744,14 +745,14 @@
     assert(r1 != array, "different registers");
     __ mov(r1, index);
   }
   Label ok;
   __ br(Assembler::LO, ok);
-    // ??? convention: move array into r3 for exception message
-  __ mov(r3, array);
-  __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);
-  __ br(rscratch1);
+  // ??? convention: move array into r3 for exception message
+   __ mov(r3, array);
+   __ mov(rscratch1, Interpreter::_throw_ArrayIndexOutOfBoundsException_entry);
+   __ br(rscratch1);
   __ bind(ok);
 }
 
 void TemplateTable::iaload()
 {
@@ -807,15 +808,25 @@
   __ mov(r1, r0);
   __ pop_ptr(r0);
   // r0: array
   // r1: index
   index_check(r0, r1); // leaves index in r1, kills rscratch1
-  __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);
-  do_oop_load(_masm,
-              Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)),
-              r0,
-              IS_ARRAY);
+  if (UseFlatArray) {
+    Label is_flat_array, done;
+
+    __ test_flattened_array_oop(r0, r8 /*temp*/, is_flat_array);
+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);
+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);
+
+    __ b(done);
+    __ bind(is_flat_array);
+    __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), r0, r1);
+    __ bind(done);
+  } else {
+    __ add(r1, r1, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);
+    do_oop_load(_masm, Address(r0, r1, Address::uxtw(LogBytesPerHeapOop)), r0, IS_ARRAY);
+  }
 }
 
 void TemplateTable::baload()
 {
   transition(itos, itos);
@@ -1108,46 +1119,109 @@
   __ ldr(r3, at_tos_p2()); // array
 
   Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));
 
   index_check(r3, r2);     // kills r1
+
+  // FIXME: Could we remove the line below?
   __ add(r4, r2, arrayOopDesc::base_offset_in_bytes(T_OBJECT) >> LogBytesPerHeapOop);
 
   // do array store check - check for NULL value first
   __ cbz(r0, is_null);
 
+  Label  is_flat_array;
+  if (UseFlatArray) {
+    __ test_flattened_array_oop(r3, r8 /*temp*/, is_flat_array);
+  }
+
   // Move subklass into r1
   __ load_klass(r1, r0);
+
   // Move superklass into r0
   __ load_klass(r0, r3);
-  __ ldr(r0, Address(r0,
-                     ObjArrayKlass::element_klass_offset()));
+  __ ldr(r0, Address(r0, ObjArrayKlass::element_klass_offset()));
   // Compress array + index*oopSize + 12 into a single register.  Frees r2.
 
   // Generate subtype check.  Blows r2, r5
   // Superklass in r0.  Subklass in r1.
+
   __ gen_subtype_check(r1, ok_is_subtype);
 
   // Come here on failure
   // object is at TOS
   __ b(Interpreter::_throw_ArrayStoreException_entry);
 
+
   // Come here on success
   __ bind(ok_is_subtype);
 
+
   // Get the value we will store
   __ ldr(r0, at_tos());
   // Now store using the appropriate barrier
   do_oop_store(_masm, element_address, r0, IS_ARRAY);
   __ b(done);
 
   // Have a NULL in r0, r3=array, r2=index.  Store NULL at ary[idx]
   __ bind(is_null);
   __ profile_null_seen(r2);
 
+  if (EnableValhalla) {
+    Label is_null_into_value_array_npe, store_null;
+
+    // No way to store null in flat array
+    __ test_null_free_array_oop(r3, r8, is_null_into_value_array_npe);
+    __ b(store_null);
+
+    __ bind(is_null_into_value_array_npe);
+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
+
+    __ bind(store_null);
+  }
+
   // Store a NULL
   do_oop_store(_masm, element_address, noreg, IS_ARRAY);
+  __ b(done);
+
+  if (EnableValhalla) {
+     Label is_type_ok;
+
+    // store non-null value
+    __ bind(is_flat_array);
+
+    // Simplistic type check...
+    // r0 - value, r2 - index, r3 - array.
+
+    // Profile the not-null value's klass.
+    // Load value class
+     __ load_klass(r1, r0);
+     __ profile_typecheck(r2, r1, r0); // blows r2, and r0
+
+    // flat value array needs exact type match
+    // is "r8 == r0" (value subclass == array element superclass)
+
+    // Move element klass into r0
+
+     __ load_klass(r0, r3);
+
+     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));
+     __ cmp(r0, r1);
+     __ br(Assembler::EQ, is_type_ok);
+
+     __ profile_typecheck_failed(r2);
+     __ b(ExternalAddress(Interpreter::_throw_ArrayStoreException_entry));
+
+     __ bind(is_type_ok);
+
+    // Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.
+    // FIXME: Should we really do it?
+     __ ldr(r1, at_tos());  // value
+     __ mov(r2, r3); // array, ldr(r2, at_tos_p2());
+     __ ldr(r3, at_tos_p1()); // index
+     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);
+  }
+
 
   // Pop stack arguments
   __ bind(done);
   __ add(esp, esp, 3 * Interpreter::stackElementSize);
 }
@@ -2014,23 +2088,69 @@
   branch(false, false);
   __ bind(not_taken);
   __ profile_not_taken_branch(r0);
 }
 
-void TemplateTable::if_acmp(Condition cc)
-{
+void TemplateTable::if_acmp(Condition cc) {
   transition(atos, vtos);
   // assume branch is more often taken than not (loops use backward branches)
-  Label not_taken;
+  Label taken, not_taken;
   __ pop_ptr(r1);
+
+  Register is_value_mask = rscratch1;
+  __ mov(is_value_mask, markWord::always_locked_pattern);
+
+  if (EnableValhalla) {
+    __ cmp(r1, r0);
+    __ br(Assembler::EQ, (cc == equal) ? taken : not_taken);
+
+    // might be substitutable, test if either r0 or r1 is null
+    __ andr(r2, r0, r1);
+    __ cbz(r2, (cc == equal) ? not_taken : taken);
+
+    // and both are values ?
+    __ ldr(r2, Address(r1, oopDesc::mark_offset_in_bytes()));
+    __ andr(r2, r2, is_value_mask);
+    __ ldr(r4, Address(r0, oopDesc::mark_offset_in_bytes()));
+    __ andr(r4, r4, is_value_mask);
+    __ andr(r2, r2, r4);
+    __ cmp(r2,  is_value_mask);
+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);
+
+    // same value klass ?
+    __ load_metadata(r2, r1);
+    __ load_metadata(r4, r0);
+    __ cmp(r2, r4);
+    __ br(Assembler::NE, (cc == equal) ? not_taken : taken);
+
+    // Know both are the same type, let's test for substitutability...
+    if (cc == equal) {
+      invoke_is_substitutable(r0, r1, taken, not_taken);
+    } else {
+      invoke_is_substitutable(r0, r1, not_taken, taken);
+    }
+    __ stop("Not reachable");
+  }
+
   __ cmpoop(r1, r0);
   __ br(j_not(cc), not_taken);
+  __ bind(taken);
   branch(false, false);
   __ bind(not_taken);
   __ profile_not_taken_branch(r0);
 }
 
+void TemplateTable::invoke_is_substitutable(Register aobj, Register bobj,
+                                            Label& is_subst, Label& not_subst) {
+
+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::is_substitutable), aobj, bobj);
+  // Restored... r0 answer, jmp to outcome...
+  __ cbz(r0, not_subst);
+  __ b(is_subst);
+}
+
+
 void TemplateTable::ret() {
   transition(vtos, vtos);
   // We might be moving to a safepoint.  The thread which calls
   // Interpreter::notice_safepoints() will effectively flush its cache
   // when it makes a system call, but we need to do something to
@@ -2499,12 +2619,11 @@
   Label Done, notByte, notBool, notInt, notShort, notChar,
               notLong, notFloat, notObj, notDouble;
 
   // x86 uses a shift and mask or wings it with a shift plus assert
   // the mask is not needed. aarch64 just uses bitfield extract
-  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift,
-           ConstantPoolCacheEntry::tos_state_bits);
+  __ ubfxw(flags, raw_flags, ConstantPoolCacheEntry::tos_state_shift, ConstantPoolCacheEntry::tos_state_bits);
 
   assert(btos == 0, "change code, btos != 0");
   __ cbnz(flags, notByte);
 
   // Don't rewrite getstatic, only getfield
@@ -2535,16 +2654,72 @@
 
   __ bind(notBool);
   __ cmp(flags, (u1)atos);
   __ br(Assembler::NE, notObj);
   // atos
-  do_oop_load(_masm, field, r0, IN_HEAP);
-  __ push(atos);
-  if (rc == may_rewrite) {
-    patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
+  if (!EnableValhalla) {
+    do_oop_load(_masm, field, r0, IN_HEAP);
+    __ push(atos);
+    if (rc == may_rewrite) {
+      patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
+    }
+    __ b(Done);
+  } else { // Valhalla
+
+    if (is_static) {
+      __ load_heap_oop(r0, field);
+      Label is_inline, isUninitialized;
+      // Issue below if the static field has not been initialized yet
+      __ test_field_is_inline_type(raw_flags, r8 /*temp*/, is_inline);
+        // Not inline case
+        __ push(atos);
+        __ b(Done);
+      // Inline case, must not return null even if uninitialized
+      __ bind(is_inline);
+        __ cbz(r0, isUninitialized);
+          __ push(atos);
+          __ b(Done);
+        __ bind(isUninitialized);
+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_static_inline_type_field), obj, raw_flags);
+          __ verify_oop(r0);
+          __ push(atos);
+          __ b(Done);
+    } else {
+      Label isFlattened, isInitialized, is_inline, rewrite_inline;
+        __ test_field_is_inline_type(raw_flags, r8 /*temp*/, is_inline);
+        // Non-inline field case
+        __ load_heap_oop(r0, field);
+        __ push(atos);
+        if (rc == may_rewrite) {
+          patch_bytecode(Bytecodes::_fast_agetfield, bc, r1);
+        }
+        __ b(Done);
+      __ bind(is_inline);
+        __ test_field_is_inlined(raw_flags, r8 /* temp */, isFlattened);
+         // Non-inline field case
+          __ load_heap_oop(r0, field);
+          __ cbnz(r0, isInitialized);
+            __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
+            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), obj, raw_flags);
+          __ bind(isInitialized);
+          __ verify_oop(r0);
+          __ push(atos);
+          __ b(rewrite_inline);
+        __ bind(isFlattened);
+          __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
+          __ andw(raw_flags, raw_flags, ConstantPoolCacheEntry::field_index_mask);
+          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);
+          __ verify_oop(r0);
+          __ push(atos);
+      __ bind(rewrite_inline);
+      if (rc == may_rewrite) {
+         patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);
+      }
+      __ b(Done);
+    }
   }
-  __ b(Done);
 
   __ bind(notObj);
   __ cmp(flags, (u1)itos);
   __ br(Assembler::NE, notInt);
   // itos
@@ -2710,10 +2885,11 @@
   const Register cache = r2;
   const Register index = r3;
   const Register obj   = r2;
   const Register off   = r19;
   const Register flags = r0;
+  const Register flags2 = r6;
   const Register bc    = r4;
 
   resolve_cache_and_index(byte_no, cache, index, sizeof(u2));
   jvmti_post_field_mod(cache, index, is_static);
   load_field_cp_cache_entry(obj, cache, index, off, flags, is_static);
@@ -2732,10 +2908,12 @@
   const Address field(obj, off);
 
   Label notByte, notBool, notInt, notShort, notChar,
         notLong, notFloat, notObj, notDouble;
 
+  __ mov(flags2, flags);
+
   // x86 uses a shift and mask or wings it with a shift plus assert
   // the mask is not needed. aarch64 just uses bitfield extract
   __ ubfxw(flags, flags, ConstantPoolCacheEntry::tos_state_shift,  ConstantPoolCacheEntry::tos_state_bits);
 
   assert(btos == 0, "change code, btos != 0");
@@ -2774,18 +2952,60 @@
   __ cmp(flags, (u1)atos);
   __ br(Assembler::NE, notObj);
 
   // atos
   {
-    __ pop(atos);
-    if (!is_static) pop_and_check_object(obj);
-    // Store into the field
-    do_oop_store(_masm, field, r0, IN_HEAP);
-    if (rc == may_rewrite) {
-      patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);
-    }
-    __ b(Done);
+     if (!EnableValhalla) {
+      __ pop(atos);
+      if (!is_static) pop_and_check_object(obj);
+      // Store into the field
+      do_oop_store(_masm, field, r0, IN_HEAP);
+      if (rc == may_rewrite) {
+        patch_bytecode(Bytecodes::_fast_aputfield, bc, r1, true, byte_no);
+      }
+      __ b(Done);
+     } else { // Valhalla
+
+      __ pop(atos);
+      if (is_static) {
+        Label not_inline;
+         __ test_field_is_not_inline_type(flags2, r8 /* temp */, not_inline);
+         __ null_check(r0);
+         __ bind(not_inline);
+         do_oop_store(_masm, field, r0, IN_HEAP);
+         __ b(Done);
+      } else {
+        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;
+        __ test_field_is_inline_type(flags2, r8 /*temp*/, is_inline);
+        // Not inline case
+        pop_and_check_object(obj);
+        // Store into the field
+        do_oop_store(_masm, field, r0, IN_HEAP);
+        __ bind(rewrite_not_inline);
+        if (rc == may_rewrite) {
+          patch_bytecode(Bytecodes::_fast_aputfield, bc, r19, true, byte_no);
+        }
+        __ b(Done);
+        // Implementation of the inline semantic
+        __ bind(is_inline);
+        __ null_check(r0);
+        __ test_field_is_inlined(flags2, r8 /*temp*/, isFlattened);
+        // Not inline case
+        pop_and_check_object(obj);
+        // Store into the field
+        do_oop_store(_masm, field, r0, IN_HEAP);
+        __ b(rewrite_inline);
+        __ bind(isFlattened);
+        pop_and_check_object(obj);
+        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);
+        __ bind(rewrite_inline);
+        if (rc == may_rewrite) {
+          patch_bytecode(Bytecodes::_fast_qputfield, bc, r19, true, byte_no);
+        }
+        __ b(Done);
+      }
+     }  // Valhalla
   }
 
   __ bind(notObj);
   __ cmp(flags, (u1)itos);
   __ br(Assembler::NE, notInt);
@@ -2921,10 +3141,11 @@
     __ push_ptr(r19);                 // put the object pointer back on tos
     // Save tos values before call_VM() clobbers them. Since we have
     // to do it for every data type, we use the saved values as the
     // jvalue object.
     switch (bytecode()) {          // load values into the jvalue object
+    case Bytecodes::_fast_qputfield: //fall through
     case Bytecodes::_fast_aputfield: __ push_ptr(r0); break;
     case Bytecodes::_fast_bputfield: // fall through
     case Bytecodes::_fast_zputfield: // fall through
     case Bytecodes::_fast_sputfield: // fall through
     case Bytecodes::_fast_cputfield: // fall through
@@ -2947,10 +3168,11 @@
                CAST_FROM_FN_PTR(address,
                                 InterpreterRuntime::post_field_modification),
                r19, c_rarg2, c_rarg3);
 
     switch (bytecode()) {             // restore tos values
+    case Bytecodes::_fast_qputfield: //fall through
     case Bytecodes::_fast_aputfield: __ pop_ptr(r0); break;
     case Bytecodes::_fast_bputfield: // fall through
     case Bytecodes::_fast_zputfield: // fall through
     case Bytecodes::_fast_sputfield: // fall through
     case Bytecodes::_fast_cputfield: // fall through
@@ -3000,10 +3222,23 @@
   // field address
   const Address field(r2, r1);
 
   // access field
   switch (bytecode()) {
+  case Bytecodes::_fast_qputfield: //fall through
+   {
+      Label isFlattened, done;
+      __ null_check(r0);
+      __ test_field_is_flattened(r3, r8 /* temp */, isFlattened);
+      // No Flattened case
+      do_oop_store(_masm, field, r0, IN_HEAP);
+      __ b(done);
+      __ bind(isFlattened);
+      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);
+      __ bind(done);
+    }
+    break;
   case Bytecodes::_fast_aputfield:
     do_oop_store(_masm, field, r0, IN_HEAP);
     break;
   case Bytecodes::_fast_lputfield:
     __ access_store_at(T_LONG, IN_HEAP, field, r0, noreg, noreg);
@@ -3097,10 +3332,36 @@
     __ bind(notVolatile);
   }
 
   // access field
   switch (bytecode()) {
+  case Bytecodes::_fast_qgetfield:
+    {
+       Label isFlattened, isInitialized, Done;
+       // FIXME: We don't need to reload registers multiple times, but stay close to x86 code
+       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
+       __ test_field_is_inlined(r9, r8 /* temp */, isFlattened);
+        // Non-flattened field case
+        __ mov(r9, r0);
+        __ load_heap_oop(r0, field);
+        __ cbnz(r0, isInitialized);
+          __ mov(r0, r9);
+          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
+          __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
+          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), r0, r9);
+        __ bind(isInitialized);
+        __ verify_oop(r0);
+        __ b(Done);
+      __ bind(isFlattened);
+        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));
+        __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);
+        __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));
+        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);
+        __ verify_oop(r0);
+      __ bind(Done);
+    }
+    break;
   case Bytecodes::_fast_agetfield:
     do_oop_load(_masm, field, r0, IN_HEAP);
     __ verify_oop(r0);
     break;
   case Bytecodes::_fast_lgetfield:
@@ -3652,10 +3913,34 @@
   __ bind(done);
   // Must prevent reordering of stores for object initialization with stores that publish the new object.
   __ membar(Assembler::StoreStore);
 }
 
+void TemplateTable::defaultvalue() {
+  transition(vtos, atos);
+  __ get_unsigned_2_byte_index_at_bcp(c_rarg2, 1);
+  __ get_constant_pool(c_rarg1);
+  call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::defaultvalue),
+          c_rarg1, c_rarg2);
+  __ verify_oop(r0);
+  // Must prevent reordering of stores for object initialization with stores that publish the new object.
+  __ membar(Assembler::StoreStore);
+}
+
+void TemplateTable::withfield() {
+  transition(vtos, atos);
+  resolve_cache_and_index(f2_byte, c_rarg1 /*cache*/, c_rarg2 /*index*/, sizeof(u2));
+
+  // n.b. unlike x86 cache is now rcpool plus the indexed offset
+  // so using rcpool to meet shared code expectations
+
+  call_VM(r1, CAST_FROM_FN_PTR(address, InterpreterRuntime::withfield), rcpool);
+  __ verify_oop(r1);
+  __ add(esp, esp, r0);
+  __ mov(r0, r1);
+}
+
 void TemplateTable::newarray() {
   transition(itos, atos);
   __ load_unsigned_byte(c_rarg1, at_bcp(1));
   __ mov(c_rarg2, r0);
   call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::newarray),
@@ -3723,16 +4008,31 @@
 
   // Come here on success
   __ bind(ok_is_subtype);
   __ mov(r0, r3); // Restore object in r3
 
+  __ b(done);
+  __ bind(is_null);
+
   // Collect counts on whether this test sees NULLs a lot or not.
   if (ProfileInterpreter) {
-    __ b(done);
-    __ bind(is_null);
-    __ profile_null_seen(r2);
-  } else {
+    __ profile_null_seen(r2);
+  }
+
+  if (EnableValhalla) {
+    // Get cpool & tags index
+    __ get_cpool_and_tags(r2, r3); // r2=cpool, r3=tags array
+    __ get_unsigned_2_byte_index_at_bcp(r19, 1); // r19=index
+     // See if bytecode has already been quicked
+    __ add(rscratch1, r3, Array<u1>::base_offset_in_bytes());
+    __ lea(r1, Address(rscratch1, r19));
+    __ ldarb(r1, r1);
+    // See if CP entry is a Q-descriptor
+    __ andr (r1, r1, JVM_CONSTANT_QDescBit);
+    __ cmp(r1, (u1) JVM_CONSTANT_QDescBit);
+    __ br(Assembler::NE, done);
+    __ b(ExternalAddress(Interpreter::_throw_NullPointerException_entry));
     __ bind(is_null);   // same as 'done'
   }
   __ bind(done);
 }
 
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -19,10 +19,11 @@
  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  * or visit www.oracle.com if you need additional information or have any
  * questions.
  *
  */
+
 #include "precompiled.hpp"
 #include "jvm.h"
 #include "aot/aotLoader.hpp"
 #include "classfile/classFileParser.hpp"
 #include "classfile/classFileStream.hpp"
@@ -47,10 +48,11 @@
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/annotations.hpp"
 #include "oops/constantPool.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "oops/instanceKlass.hpp"
 #include "oops/instanceMirrorKlass.hpp"
 #include "oops/klass.inline.hpp"
 #include "oops/klassVtable.hpp"
 #include "oops/metadata.hpp"
@@ -79,10 +81,11 @@
 #include "utilities/globalDefinitions.hpp"
 #include "utilities/growableArray.hpp"
 #include "utilities/macros.hpp"
 #include "utilities/ostream.hpp"
 #include "utilities/resourceHash.hpp"
+#include "utilities/stringUtils.hpp"
 #include "utilities/utf8.hpp"
 
 #if INCLUDE_CDS
 #include "classfile/systemDictionaryShared.hpp"
 #endif
@@ -132,10 +135,12 @@
 
 #define JAVA_15_VERSION                   59
 
 #define JAVA_16_VERSION                   60
 
+#define CONSTANT_CLASS_DESCRIPTORS        60
+
 void ClassFileParser::set_class_bad_constant_seen(short bad_constant) {
   assert((bad_constant == JVM_CONSTANT_Module ||
           bad_constant == JVM_CONSTANT_Package) && _major_version >= JAVA_9_VERSION,
          "Unexpected bad constant pool entry");
   if (_bad_constant_seen == 0) _bad_constant_seen = bad_constant;
@@ -171,11 +176,11 @@
     // Each of the following case guarantees one more byte in the stream
     // for the following tag or the access_flags following constant pool,
     // so we don't need bounds-check for reading tag.
     const u1 tag = cfs->get_u1_fast();
     switch (tag) {
-      case JVM_CONSTANT_Class : {
+      case JVM_CONSTANT_Class: {
         cfs->guarantee_more(3, CHECK);  // name_index, tag/access_flags
         const u2 name_index = cfs->get_u2_fast();
         cp->klass_index_at_put(index, name_index);
         break;
       }
@@ -501,11 +506,18 @@
       case JVM_CONSTANT_ClassIndex: {
         const int class_index = cp->klass_index_at(index);
         check_property(valid_symbol_at(class_index),
           "Invalid constant pool index %u in class file %s",
           class_index, CHECK);
-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);
+
+        Symbol* const name = cp->symbol_at(class_index);
+        const unsigned int name_len = name->utf8_length();
+        if (name->is_Q_signature()) {
+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);
+        } else {
+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);
+        }
         break;
       }
       case JVM_CONSTANT_StringIndex: {
         const int string_index = cp->string_index_at(index);
         check_property(valid_symbol_at(string_index),
@@ -755,18 +767,29 @@
             const int name_and_type_ref_index =
               cp->name_and_type_ref_index_at(ref_index);
             const int name_ref_index =
               cp->name_ref_index_at(name_and_type_ref_index);
             const Symbol* const name = cp->symbol_at(name_ref_index);
-            if (ref_kind == JVM_REF_newInvokeSpecial) {
-              if (name != vmSymbols::object_initializer_name()) {
+            if (name != vmSymbols::object_initializer_name()) {
+              if (ref_kind == JVM_REF_newInvokeSpecial) {
                 classfile_parse_error(
                   "Bad constructor name at constant pool index %u in class file %s",
                     name_ref_index, CHECK);
               }
             } else {
-              if (name == vmSymbols::object_initializer_name()) {
+              // The allowed invocation mode of <init> depends on its signature.
+              // This test corresponds to verify_invoke_instructions in the verifier.
+              const int signature_ref_index =
+                cp->signature_ref_index_at(name_and_type_ref_index);
+              const Symbol* const signature = cp->symbol_at(signature_ref_index);
+              if (signature->is_void_method_signature()
+                  && ref_kind == JVM_REF_newInvokeSpecial) {
+                // OK, could be a constructor call
+              } else if (!signature->is_void_method_signature()
+                         && ref_kind == JVM_REF_invokeStatic) {
+                // also OK, could be a static factory call
+              } else {
                 classfile_parse_error(
                   "Bad method name at constant pool index %u in class file %s",
                   name_ref_index, CHECK);
               }
             }
@@ -921,26 +944,33 @@
 
   return true;
 }
 
 // Side-effects: populates the _local_interfaces field
-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,
-                                       const int itfs_len,
-                                       ConstantPool* const cp,
+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,
+                                       int itfs_len,
+                                       ConstantPool* cp,
+                                       bool is_inline_type,
                                        bool* const has_nonstatic_concrete_methods,
+                                       // FIXME: lots of these functions
+                                       // declare their parameters as const,
+                                       // which adds only noise to the code.
+                                       // Remove the spurious const modifiers.
+                                       // Many are of the form "const int x"
+                                       // or "T* const x".
+                                       bool* const is_declared_atomic,
                                        TRAPS) {
   assert(stream != NULL, "invariant");
   assert(cp != NULL, "invariant");
   assert(has_nonstatic_concrete_methods != NULL, "invariant");
 
   if (itfs_len == 0) {
-    _local_interfaces = Universe::the_empty_instance_klass_array();
+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);
   } else {
     assert(itfs_len > 0, "only called for len>0");
-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);
-
-    int index;
+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);
+    int index = 0;
     for (index = 0; index < itfs_len; index++) {
       const u2 interface_index = stream->get_u2(CHECK);
       Klass* interf;
       check_property(
         valid_klass_reference_at(interface_index),
@@ -954,11 +984,11 @@
         // Don't need to check legal name because it's checked when parsing constant pool.
         // But need to make sure it's not an array type.
         guarantee_property(unresolved_klass->char_at(0) != JVM_SIGNATURE_ARRAY,
                            "Bad interface name in class file %s", CHECK);
 
-        // Call resolve_super so classcircularity is checked
+        // Call resolve_super so class circularity is checked
         interf = SystemDictionary::resolve_super_or_fail(
                                                   _class_name,
                                                   unresolved_klass,
                                                   Handle(THREAD, _loader_data->class_loader()),
                                                   _protection_domain,
@@ -972,14 +1002,33 @@
                           _class_name->as_klass_external_name(),
                           interf->external_name(),
                           interf->class_in_module_of_loader()));
       }
 
-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {
+      InstanceKlass* ik = InstanceKlass::cast(interf);
+      if (is_inline_type && ik->invalid_inline_super()) {
+        ResourceMark rm(THREAD);
+        Exceptions::fthrow(
+          THREAD_AND_LOCATION,
+          vmSymbols::java_lang_IncompatibleClassChangeError(),
+          "Inline type %s attempts to implement interface java.lang.IdentityObject",
+          _class_name->as_klass_external_name());
+        return;
+      }
+      if (ik->invalid_inline_super()) {
+        set_invalid_inline_super();
+      }
+      if (ik->has_nonstatic_concrete_methods()) {
         *has_nonstatic_concrete_methods = true;
       }
-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));
+      if (ik->is_declared_atomic()) {
+        *is_declared_atomic = true;
+      }
+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {
+        _implements_identityObject = true;
+      }
+      _temp_local_interfaces->append(ik);
     }
 
     if (!_need_verify || itfs_len <= 1) {
       return;
     }
@@ -993,11 +1042,11 @@
     bool dup = false;
     const Symbol* name = NULL;
     {
       debug_only(NoSafepointVerifier nsv;)
       for (index = 0; index < itfs_len; index++) {
-        const InstanceKlass* const k = _local_interfaces->at(index);
+        const InstanceKlass* const k = _temp_local_interfaces->at(index);
         name = k->name();
         // If no duplicates, add (name, NULL) in hashtable interface_names.
         if (!put_after_lookup(name, NULL, interface_names)) {
           dup = true;
           break;
@@ -1469,15 +1518,17 @@
   STATIC_OOP,           // Oops
   STATIC_BYTE,          // Boolean, Byte, char
   STATIC_SHORT,         // shorts
   STATIC_WORD,          // ints
   STATIC_DOUBLE,        // aligned long or double
+  STATIC_INLINE,        // inline type field
   NONSTATIC_OOP,
   NONSTATIC_BYTE,
   NONSTATIC_SHORT,
   NONSTATIC_WORD,
   NONSTATIC_DOUBLE,
+  NONSTATIC_INLINE,
   MAX_FIELD_ALLOCATION_TYPE,
   BAD_ALLOCATION_TYPE = -1
 };
 
 static FieldAllocationType _basic_type_to_atype[2 * (T_CONFLICT + 1)] = {
@@ -1493,16 +1544,17 @@
   NONSTATIC_SHORT,     // T_SHORT       =  9,
   NONSTATIC_WORD,      // T_INT         = 10,
   NONSTATIC_DOUBLE,    // T_LONG        = 11,
   NONSTATIC_OOP,       // T_OBJECT      = 12,
   NONSTATIC_OOP,       // T_ARRAY       = 13,
-  BAD_ALLOCATION_TYPE, // T_VOID        = 14,
-  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 15,
-  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 16,
-  BAD_ALLOCATION_TYPE, // T_METADATA    = 17,
-  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 18,
-  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 19,
+  NONSTATIC_OOP,       // T_INLINE_TYPE = 14,
+  BAD_ALLOCATION_TYPE, // T_VOID        = 15,
+  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 16,
+  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 17,
+  BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
+  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
+  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20,
   BAD_ALLOCATION_TYPE, // 0
   BAD_ALLOCATION_TYPE, // 1
   BAD_ALLOCATION_TYPE, // 2
   BAD_ALLOCATION_TYPE, // 3
   STATIC_BYTE ,        // T_BOOLEAN     =  4,
@@ -1513,22 +1565,26 @@
   STATIC_SHORT,        // T_SHORT       =  9,
   STATIC_WORD,         // T_INT         = 10,
   STATIC_DOUBLE,       // T_LONG        = 11,
   STATIC_OOP,          // T_OBJECT      = 12,
   STATIC_OOP,          // T_ARRAY       = 13,
-  BAD_ALLOCATION_TYPE, // T_VOID        = 14,
-  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 15,
-  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 16,
-  BAD_ALLOCATION_TYPE, // T_METADATA    = 17,
-  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 18,
-  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 19,
+  STATIC_OOP,          // T_INLINE_TYPE = 14,
+  BAD_ALLOCATION_TYPE, // T_VOID        = 15,
+  BAD_ALLOCATION_TYPE, // T_ADDRESS     = 16,
+  BAD_ALLOCATION_TYPE, // T_NARROWOOP   = 17,
+  BAD_ALLOCATION_TYPE, // T_METADATA    = 18,
+  BAD_ALLOCATION_TYPE, // T_NARROWKLASS = 19,
+  BAD_ALLOCATION_TYPE, // T_CONFLICT    = 20
 };
 
-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {
+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {
   assert(type >= T_BOOLEAN && type < T_VOID, "only allowable values");
   FieldAllocationType result = _basic_type_to_atype[type + (is_static ? (T_CONFLICT + 1) : 0)];
   assert(result != BAD_ALLOCATION_TYPE, "bad type");
+  if (is_inline_type) {
+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;
+  }
   return result;
 }
 
 class ClassFileParser::FieldAllocationCount : public ResourceObj {
  public:
@@ -1538,12 +1594,12 @@
     for (int i = 0; i < MAX_FIELD_ALLOCATION_TYPE; i++) {
       count[i] = 0;
     }
   }
 
-  FieldAllocationType update(bool is_static, BasicType type) {
-    FieldAllocationType atype = basic_type_to_atype(is_static, type);
+  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {
+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);
     if (atype != BAD_ALLOCATION_TYPE) {
       // Make sure there is no overflow with injected fields.
       assert(count[atype] < 0xFFFF, "More than 65535 fields");
       count[atype]++;
     }
@@ -1553,10 +1609,11 @@
 
 // Side-effects: populates the _fields, _fields_annotations,
 // _fields_type_annotations fields
 void ClassFileParser::parse_fields(const ClassFileStream* const cfs,
                                    bool is_interface,
+                                   bool is_inline_type,
                                    FieldAllocationCount* const fac,
                                    ConstantPool* cp,
                                    const int cp_size,
                                    u2* const java_fields_count_ptr,
                                    TRAPS) {
@@ -1575,11 +1632,15 @@
   *java_fields_count_ptr = length;
 
   int num_injected = 0;
   const InjectedField* const injected = JavaClasses::get_injected(_class_name,
                                                                   &num_injected);
-  const int total_fields = length + num_injected;
+
+  // two more slots are required for inline classes:
+  // one for the static field with a reference to the pre-allocated default value
+  // one for the field the JVM injects when detecting an empty inline class
+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);
 
   // The field array starts with tuples of shorts
   // [access, name index, sig index, initial value index, byte offset].
   // A generic signature slot only exists for field with generic
   // signature attribute. And the access flag is set with
@@ -1605,17 +1666,20 @@
                                               total_fields * (FieldInfo::field_slots + 1));
 
   // The generic signature slots start after all other fields' data.
   int generic_signature_slot = total_fields * FieldInfo::field_slots;
   int num_generic_signature = 0;
+  int instance_fields_count = 0;
   for (int n = 0; n < length; n++) {
     // access_flags, name_index, descriptor_index, attributes_count
     cfs->guarantee_more(8, CHECK);
 
+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;
+
+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;
+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);
     AccessFlags access_flags;
-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;
-    verify_legal_field_modifiers(flags, is_interface, CHECK);
     access_flags.set_flags(flags);
 
     const u2 name_index = cfs->get_u2_fast();
     check_property(valid_symbol_at(name_index),
       "Invalid constant pool index %u for field name in class file %s",
@@ -1627,10 +1691,11 @@
     check_property(valid_symbol_at(signature_index),
       "Invalid constant pool index %u for field signature in class file %s",
       signature_index, CHECK);
     const Symbol* const sig = cp->symbol_at(signature_index);
     verify_legal_field_signature(name, sig, CHECK);
+    if (!access_flags.is_static()) instance_fields_count++;
 
     u2 constantvalue_index = 0;
     bool is_synthetic = false;
     u2 generic_signature_index = 0;
     const bool is_static = access_flags.is_static();
@@ -1686,11 +1751,11 @@
                       signature_index,
                       constantvalue_index);
     const BasicType type = cp->basic_type_for_signature_at(signature_index);
 
     // Remember how many oops we encountered and compute allocation type
-    const FieldAllocationType atype = fac->update(is_static, type);
+    const FieldAllocationType atype = fac->update(is_static, type, type == T_INLINE_TYPE);
     field->set_allocation_type(atype);
 
     // After field is initialized with type, we can augment it with aux info
     if (parsed_annotations.has_any_annotations()) {
       parsed_annotations.apply_to(field);
@@ -1731,16 +1796,45 @@
                         0);
 
       const BasicType type = Signature::basic_type(injected[n].signature());
 
       // Remember how many oops we encountered and compute allocation type
-      const FieldAllocationType atype = fac->update(false, type);
+      const FieldAllocationType atype = fac->update(false, type, false);
       field->set_allocation_type(atype);
       index++;
     }
   }
 
+  if (is_inline_type) {
+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);
+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,
+                      vmSymbols::default_value_name_enum,
+                      vmSymbols::object_signature_enum,
+                      0);
+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());
+    const FieldAllocationType atype = fac->update(true, type, false);
+    field->set_allocation_type(atype);
+    index++;
+  }
+
+  if (is_inline_type && instance_fields_count == 0) {
+    _is_empty_inline_type = true;
+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);
+    field->initialize(JVM_ACC_FIELD_INTERNAL,
+        vmSymbols::empty_marker_name_enum,
+        vmSymbols::byte_signature_enum,
+        0);
+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());
+    const FieldAllocationType atype = fac->update(false, type, false);
+    field->set_allocation_type(atype);
+    index++;
+  }
+
+  if (instance_fields_count > 0) {
+    _has_nonstatic_fields = true;
+  }
+
   assert(NULL == _fields, "invariant");
 
   _fields =
     MetadataFactory::new_array<u2>(_loader_data,
                                    index * FieldInfo::field_slots + num_generic_signature,
@@ -2052,15 +2146,20 @@
                                             const Symbol* sig,
                                             TRAPS) const {
   assert(name != NULL, "invariant");
   assert(sig != NULL, "invariant");
 
+  const char* class_note = "";
+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {
+    class_note = " (an inline class)";
+  }
+
   ResourceMark rm(THREAD);
   Exceptions::fthrow(THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "%s \"%s\" in class %s has illegal signature \"%s\"", type,
-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());
+      "%s \"%s\" in class %s%s has illegal signature \"%s\"", type,
+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());
 }
 
 AnnotationCollector::ID
 AnnotationCollector::annotation_index(const ClassLoaderData* loader_data,
                                       const Symbol* name,
@@ -2321,10 +2420,11 @@
 // from the method back up to the containing klass. These flag values
 // are added to klass's access_flags.
 
 Method* ClassFileParser::parse_method(const ClassFileStream* const cfs,
                                       bool is_interface,
+                                      bool is_inline_type,
                                       const ConstantPool* cp,
                                       AccessFlags* const promoted_flags,
                                       TRAPS) {
   assert(cfs != NULL, "invariant");
   assert(cp != NULL, "invariant");
@@ -2361,15 +2461,60 @@
       flags &= JVM_ACC_STATIC | JVM_ACC_STRICT;
     } else {
       classfile_parse_error("Method <clinit> is not static in class file %s", CHECK_NULL);
     }
   } else {
-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);
-  }
-
-  if (name == vmSymbols::object_initializer_name() && is_interface) {
-    classfile_parse_error("Interface cannot have a method named <init>, class file %s", CHECK_NULL);
+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);
+  }
+
+  if (name == vmSymbols::object_initializer_name()) {
+    if (is_interface) {
+      classfile_parse_error("Interface cannot have a method named <init>, class file %s", CHECK_NULL);
+    } else if (!is_inline_type && signature->is_void_method_signature()) {
+      // OK, a constructor
+    } else if (is_inline_type && !signature->is_void_method_signature()) {
+      // also OK, a static factory, as long as the return value is good
+      bool ok = false;
+      SignatureStream ss((Symbol*) signature, true);
+      while (!ss.at_return_type())  ss.next();
+      if (ss.is_reference()) {
+        Symbol* ret = ss.as_symbol();
+        const Symbol* required = class_name();
+        if (is_hidden()) {
+          // The original class name in hidden classes gets changed.  So using
+          // the original name in the return type is no longer valid.
+          // Note that expecting the return type for inline hidden class factory
+          // methods to be java.lang.Object works around a JVM Spec issue for
+          // hidden classes.
+          required = vmSymbols::java_lang_Object();
+        }
+        ok = (ret == required);
+      }
+      if (!ok) {
+        throwIllegalSignature("Method", name, signature, CHECK_0);
+      }
+    } else {
+      // not OK, so throw the same error as in verify_legal_method_signature.
+      throwIllegalSignature("Method", name, signature, CHECK_0);
+    }
+    // A declared <init> method must always be either a non-static
+    // object constructor, with a void return, or else it must be a
+    // static factory method, with a non-void return.  No other
+    // definition of <init> is possible.
+    //
+    // The verifier (in verify_invoke_instructions) will inspect the
+    // signature of any attempt to invoke <init>, and ensures that it
+    // returns non-void if and only if it is being invoked by
+    // invokestatic, and void if and only if it is being invoked by
+    // invokespecial.
+    //
+    // When a symbolic reference to <init> is resolved for a
+    // particular invocation mode (special or static), the mode is
+    // matched to the JVM_ACC_STATIC modifier of the <init> method.
+    // Thus, it is impossible to statically invoke a constructor, and
+    // impossible to "new + invokespecial" a static factory, either
+    // through bytecode or through reflection.
   }
 
   int args_size = -1;  // only used when _need_verify is true
   if (_need_verify) {
     args_size = ((flags & JVM_ACC_STATIC) ? 0 : 1) +
@@ -2916,10 +3061,11 @@
 // from the methods back up to the containing klass. These flag values
 // are added to klass's access_flags.
 // Side-effects: populates the _methods field in the parser
 void ClassFileParser::parse_methods(const ClassFileStream* const cfs,
                                     bool is_interface,
+                                    bool is_inline_type,
                                     AccessFlags* promoted_flags,
                                     bool* has_final_method,
                                     bool* declares_nonstatic_concrete_methods,
                                     TRAPS) {
   assert(cfs != NULL, "invariant");
@@ -2940,10 +3086,11 @@
                                                    CHECK);
 
     for (int index = 0; index < length; index++) {
       Method* method = parse_method(cfs,
                                     is_interface,
+                                    is_inline_type,
                                     _cp,
                                     promoted_flags,
                                     CHECK);
 
       if (method->is_final()) {
@@ -3132,18 +3279,24 @@
       inner_name_index, CHECK_0);
     if (_need_verify) {
       guarantee_property(inner_class_info_index != outer_class_info_index,
                          "Class is both outer and inner class in class file %s", CHECK_0);
     }
-    // Access flags
-    jint flags;
+
+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;
     // JVM_ACC_MODULE is defined in JDK-9 and later.
     if (_major_version >= JAVA_9_VERSION) {
-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);
-    } else {
-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;
+      recognized_modifiers |= JVM_ACC_MODULE;
+    }
+    // JVM_ACC_INLINE is defined for class file version 55 and later
+    if (supports_inline_types()) {
+      recognized_modifiers |= JVM_ACC_INLINE;
     }
+
+    // Access flags
+    jint flags = cfs->get_u2_fast() & recognized_modifiers;
+
     if ((flags & JVM_ACC_INTERFACE) && _major_version < JAVA_6_VERSION) {
       // Set abstract bit for old class files for backward compatibility
       flags |= JVM_ACC_ABSTRACT;
     }
     verify_legal_class_modifiers(flags, CHECK_0);
@@ -3512,13 +3665,12 @@
                      "Bad length on BootstrapMethods in class file %s",
                      CHECK);
 }
 
 bool ClassFileParser::supports_sealed_types() {
-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&
-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&
-         Arguments::enable_preview();
+  // temporarily disable the sealed type preview feature check
+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;
 }
 
 bool ClassFileParser::supports_records() {
   return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&
          _minor_version == JAVA_PREVIEW_MINOR_VERSION &&
@@ -3999,11 +4151,12 @@
                                                         TRAPS) {
   assert(cp != NULL, "invariant");
   const InstanceKlass* super_klass = NULL;
 
   if (super_class_index == 0) {
-    check_property(_class_name == vmSymbols::java_lang_Object(),
+    check_property(_class_name == vmSymbols::java_lang_Object()
+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),
                    "Invalid superclass index %u in class file %s",
                    super_class_index,
                    CHECK_NULL);
   } else {
     check_property(valid_klass_reference_at(super_class_index),
@@ -4142,10 +4295,29 @@
 
 void OopMapBlocksBuilder::print_value_on(outputStream* st) const {
   print_on(st);
 }
 
+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,
+                                                const char* msg,
+                                                const Symbol* name,
+                                                const Symbol* sig) const {
+
+  ResourceMark rm(THREAD);
+  if (name == NULL || sig == NULL) {
+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,
+        vmSymbols::java_lang_ClassFormatError(),
+        "class: %s - %s", _class_name->as_C_string(), msg);
+  }
+  else {
+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,
+        vmSymbols::java_lang_ClassFormatError(),
+        "\"%s\" sig: \"%s\" class: %s - %s", name->as_C_string(), sig->as_C_string(),
+        _class_name->as_C_string(), msg);
+  }
+}
+
 void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   assert(ik != NULL, "invariant");
 
   const Klass* const super = ik->super();
 
@@ -4175,10 +4347,15 @@
 #endif
 
   // Check if this klass supports the java.lang.Cloneable interface
   if (SystemDictionary::Cloneable_klass_loaded()) {
     if (ik->is_subtype_of(SystemDictionary::Cloneable_klass())) {
+      if (ik->is_inline_klass()) {
+        Thread *THREAD = Thread::current();
+        throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support Cloneable");
+        return;
+      }
       ik->set_is_cloneable();
     }
   }
 
   // Check if this klass has a vanilla default constructor
@@ -4215,10 +4392,15 @@
     const jint lh = Klass::instance_layout_helper(ik->size_helper(), true);
     ik->set_layout_helper(lh);
   }
 }
 
+bool ClassFileParser::supports_inline_types() const {
+  // Inline types are only supported by class file version 55 and later
+  return _major_version >= JAVA_11_VERSION;
+}
+
 // utility methods for appending an array with check for duplicates
 
 static void append_interfaces(GrowableArray<InstanceKlass*>* result,
                               const Array<InstanceKlass*>* const ifs) {
   // iterate over new interfaces
@@ -4258,13 +4440,14 @@
     // no interfaces, use canonicalized array
     return Universe::the_empty_instance_klass_array();
   } else if (max_transitive_size == super_size) {
     // no new local interfaces added, share superklass' transitive interface array
     return super->transitive_interfaces();
-  } else if (max_transitive_size == local_size) {
-    // only local interfaces added, share local interface array
-    return local_ifs;
+    // The three lines below are commented to work around bug JDK-8245487
+//  } else if (max_transitive_size == local_size) {
+//    // only local interfaces added, share local interface array
+//    return local_ifs;
   } else {
     ResourceMark rm;
     GrowableArray<InstanceKlass*>* const result = new GrowableArray<InstanceKlass*>(max_transitive_size);
 
     // Copy down from superclass
@@ -4281,10 +4464,15 @@
     append_interfaces(result, local_ifs);
 
     // length will be less than the max_transitive_size if duplicates were removed
     const int length = result->length();
     assert(length <= max_transitive_size, "just checking");
+
+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {
+      return Universe::the_single_IdentityObject_klass_array();
+    }
+
     Array<InstanceKlass*>* const new_result =
       MetadataFactory::new_array<InstanceKlass*>(loader_data, length, CHECK_NULL);
     for (int i = 0; i < length; i++) {
       InstanceKlass* const e = result->at(i);
       assert(e != NULL, "just checking");
@@ -4513,21 +4701,33 @@
 
 // utility methods for format checking
 
 void ClassFileParser::verify_legal_class_modifiers(jint flags, TRAPS) const {
   const bool is_module = (flags & JVM_ACC_MODULE) != 0;
+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;
   assert(_major_version >= JAVA_9_VERSION || !is_module, "JVM_ACC_MODULE should not be set");
+  assert(supports_inline_types() || !is_inline_type, "JVM_ACC_INLINE should not be set");
   if (is_module) {
     ResourceMark rm(THREAD);
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_NoClassDefFoundError(),
       "%s is not a class because access_flag ACC_MODULE is set",
       _class_name->as_C_string());
     return;
   }
 
+  if (is_inline_type && !EnableValhalla) {
+    ResourceMark rm(THREAD);
+    Exceptions::fthrow(
+      THREAD_AND_LOCATION,
+      vmSymbols::java_lang_ClassFormatError(),
+      "Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla",
+      _class_name->as_C_string()
+    );
+  }
+
   if (!_need_verify) { return; }
 
   const bool is_interface  = (flags & JVM_ACC_INTERFACE)  != 0;
   const bool is_abstract   = (flags & JVM_ACC_ABSTRACT)   != 0;
   const bool is_final      = (flags & JVM_ACC_FINAL)      != 0;
@@ -4538,17 +4738,20 @@
   const bool major_gte_14  = _major_version >= JAVA_14_VERSION;
 
   if ((is_abstract && is_final) ||
       (is_interface && !is_abstract) ||
       (is_interface && major_gte_1_5 && (is_super || is_enum)) ||
-      (!is_interface && major_gte_1_5 && is_annotation)) {
+      (!is_interface && major_gte_1_5 && is_annotation) ||
+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {
     ResourceMark rm(THREAD);
+    const char* class_note = "";
+    if (is_inline_type)  class_note = " (an inline class)";
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "Illegal class modifiers in class %s: 0x%X",
-      _class_name->as_C_string(), flags
+      "Illegal class modifiers in class %s%s: 0x%X",
+      _class_name->as_C_string(), class_note, flags
     );
     return;
   }
 }
 
@@ -4623,10 +4826,11 @@
   }
 }
 
 void ClassFileParser::verify_legal_field_modifiers(jint flags,
                                                    bool is_interface,
+                                                   bool is_inline_type,
                                                    TRAPS) const {
   if (!_need_verify) { return; }
 
   const bool is_public    = (flags & JVM_ACC_PUBLIC)    != 0;
   const bool is_protected = (flags & JVM_ACC_PROTECTED) != 0;
@@ -4647,10 +4851,14 @@
       is_illegal = true;
     }
   } else { // not interface
     if (has_illegal_visibility(flags) || (is_final && is_volatile)) {
       is_illegal = true;
+    } else {
+      if (is_inline_type && !is_static && !is_final) {
+        is_illegal = true;
+      }
     }
   }
 
   if (is_illegal) {
     ResourceMark rm(THREAD);
@@ -4663,10 +4871,11 @@
   }
 }
 
 void ClassFileParser::verify_legal_method_modifiers(jint flags,
                                                     bool is_interface,
+                                                    bool is_inline_type,
                                                     const Symbol* name,
                                                     TRAPS) const {
   if (!_need_verify) { return; }
 
   const bool is_public       = (flags & JVM_ACC_PUBLIC)       != 0;
@@ -4683,10 +4892,12 @@
   const bool major_gte_8     = _major_version >= JAVA_8_VERSION;
   const bool is_initializer  = (name == vmSymbols::object_initializer_name());
 
   bool is_illegal = false;
 
+  const char* class_note = "";
+
   if (is_interface) {
     if (major_gte_8) {
       // Class file version is JAVA_8_VERSION or later Methods of
       // interfaces may set any of the flags except ACC_PROTECTED,
       // ACC_FINAL, ACC_NATIVE, and ACC_SYNCHRONIZED; they must
@@ -4717,19 +4928,33 @@
   } else { // not interface
     if (has_illegal_visibility(flags)) {
       is_illegal = true;
     } else {
       if (is_initializer) {
-        if (is_static || is_final || is_synchronized || is_native ||
+        if (is_final || is_synchronized || is_native ||
             is_abstract || (major_gte_1_5 && is_bridge)) {
           is_illegal = true;
         }
+        if (!is_static && !is_inline_type) {
+          // OK, an object constructor in a regular class
+        } else if (is_static && is_inline_type) {
+          // OK, a static init factory in an inline class
+        } else {
+          // but no other combinations are allowed
+          is_illegal = true;
+          class_note = (is_inline_type ? " (an inline class)" : " (not an inline class)");
+        }
       } else { // not initializer
-        if (is_abstract) {
-          if ((is_final || is_native || is_private || is_static ||
-              (major_gte_1_5 && (is_synchronized || is_strict)))) {
-            is_illegal = true;
+        if (is_inline_type && is_synchronized && !is_static) {
+          is_illegal = true;
+          class_note = " (an inline class)";
+        } else {
+          if (is_abstract) {
+            if ((is_final || is_native || is_private || is_static ||
+                (major_gte_1_5 && (is_synchronized || is_strict)))) {
+              is_illegal = true;
+            }
           }
         }
       }
     }
   }
@@ -4737,12 +4962,12 @@
   if (is_illegal) {
     ResourceMark rm(THREAD);
     Exceptions::fthrow(
       THREAD_AND_LOCATION,
       vmSymbols::java_lang_ClassFormatError(),
-      "Method %s in class %s has illegal modifiers: 0x%X",
-      name->as_C_string(), _class_name->as_C_string(), flags);
+      "Method %s in class %s%s has illegal modifiers: 0x%X",
+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);
     return;
   }
 }
 
 void ClassFileParser::verify_legal_utf8(const unsigned char* buffer,
@@ -4896,22 +5121,31 @@
     case JVM_SIGNATURE_INT:
     case JVM_SIGNATURE_FLOAT:
     case JVM_SIGNATURE_LONG:
     case JVM_SIGNATURE_DOUBLE:
       return signature + 1;
-    case JVM_SIGNATURE_CLASS: {
+    case JVM_SIGNATURE_INLINE_TYPE:
+      // Can't enable this check until JDK upgrades the bytecode generators
+      // if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {
+      //   classfile_parse_error("Class name contains illegal Q-signature "
+      //                                    "in descriptor in class file %s",
+      //                                    CHECK_0);
+      // }
+      // fall through
+    case JVM_SIGNATURE_CLASS:
+    {
       if (_major_version < JAVA_1_5_VERSION) {
         // Skip over the class name if one is there
         const char* const p = skip_over_field_name(signature + 1, true, --length);
 
         // The next character better be a semicolon
         if (p && (p - signature) > 1 && p[0] == JVM_SIGNATURE_ENDCLASS) {
           return p + 1;
         }
       }
       else {
-        // Skip leading 'L' and ignore first appearance of ';'
+        // Skip leading 'L' or 'Q' and ignore first appearance of ';'
         signature++;
         const char* c = (const char*) memchr(signature, JVM_SIGNATURE_ENDCLASS, length - 1);
         // Format check signature
         if (c != NULL) {
           int newlen = c - (char*) signature;
@@ -4962,10 +5196,13 @@
     } else if (_major_version < JAVA_1_5_VERSION) {
       if (bytes[0] != JVM_SIGNATURE_SPECIAL) {
         p = skip_over_field_name(bytes, true, length);
         legal = (p != NULL) && ((p - bytes) == (int)length);
       }
+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {
+      // Support for L...; and Q...; descriptors
+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);
     } else {
       // 4900761: relax the constraints based on JSR202 spec
       // Class names may be drawn from the entire Unicode character set.
       // Identifiers between '/' must be unqualified names.
       // The utf8 string has been verified when parsing cpool entries.
@@ -5111,14 +5348,30 @@
     }
     // The first non-signature thing better be a ')'
     if ((length > 0) && (*p++ == JVM_SIGNATURE_ENDFUNC)) {
       length--;
       if (name->utf8_length() > 0 && name->char_at(0) == JVM_SIGNATURE_SPECIAL) {
-        // All internal methods must return void
+        // All constructor methods must return void
         if ((length == 1) && (p[0] == JVM_SIGNATURE_VOID)) {
           return args_size;
         }
+        // All static init methods must return the current class
+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)
+            && name == vmSymbols::object_initializer_name()) {
+          nextp = skip_over_field_signature(p, true, length, CHECK_0);
+          if (nextp && ((int)length == (nextp - p))) {
+            // The actual class will be checked against current class
+            // when the method is defined (see parse_method).
+            // A reference to a static init with a bad return type
+            // will load and verify OK, but will fail to link.
+            return args_size;
+          }
+        }
+        // The distinction between static factory methods and
+        // constructors depends on the JVM_ACC_STATIC modifier.
+        // This distinction must be reflected in a void or non-void
+        // return. For declared methods, the check is in parse_method.
       } else {
         // Now we better just have a return value
         nextp = skip_over_field_signature(p, true, length, CHECK_0);
         if (nextp && ((int)length == (nextp - p))) {
           return args_size;
@@ -5272,13 +5525,53 @@
       log_info(class, fingerprint)("%s :  expected = " PTR64_FORMAT " actual = " PTR64_FORMAT,
                                  ik->external_name(), aot_fp, _stream->compute_fingerprint());
     }
   }
 
+  if (ik->is_inline_klass()) {
+    InlineKlass* vk = InlineKlass::cast(ik);
+    oop val = ik->allocate_instance(CHECK_NULL);
+    vk->set_default_value(val);
+  }
+
   return ik;
 }
 
+// Return true if the specified class is not a valid super class for an inline type.
+// A valid super class for an inline type is abstract, has no instance fields,
+// does not implement interface java.lang.IdentityObject (checked elsewhere), has
+// an empty body-less no-arg constructor, and no synchronized instance methods.
+// This function doesn't check if the class's super types are invalid.  Those checks
+// are done elsewhere.  The final determination of whether or not a class is an
+// invalid super type for an inline class is done in fill_instance_klass().
+bool ClassFileParser::is_invalid_super_for_inline_type() {
+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {
+    return true;
+  }
+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {
+    return false;
+  }
+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {
+    return true;
+  } else {
+    // Look at each method
+    for (int x = 0; x < _methods->length(); x++) {
+      const Method* const method = _methods->at(x);
+      if (method->is_synchronized() && !method->is_static()) {
+        return true;
+
+      } else if (method->name() == vmSymbols::object_initializer_name()) {
+        if (method->signature() != vmSymbols::void_method_signature() ||
+            !method->is_vanilla_constructor()) {
+          return true;
+        }
+      }
+    }
+  }
+  return false;
+}
+
 void ClassFileParser::fill_instance_klass(InstanceKlass* ik,
                                           bool changed_by_loadhook,
                                           const ClassInstanceInfo& cl_inst_info,
                                           TRAPS) {
   assert(ik != NULL, "invariant");
@@ -5307,12 +5600,27 @@
   ik->set_should_verify_class(_need_verify);
 
   // Not yet: supers are done below to support the new subtype-checking fields
   ik->set_nonstatic_field_size(_field_info->_nonstatic_field_size);
   ik->set_has_nonstatic_fields(_field_info->_has_nonstatic_fields);
+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {
+    ik->set_is_naturally_atomic();
+  }
+  if (_is_empty_inline_type) {
+    ik->set_is_empty_inline_type();
+  }
+
+  if (this->_invalid_inline_super) {
+    ik->set_invalid_inline_super();
+  }
+
+  if (_has_injected_identityObject) {
+    ik->set_has_injected_identityObject();
+  }
+
   assert(_fac != NULL, "invariant");
-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);
+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);
 
   // this transfers ownership of a lot of arrays from
   // the parser onto the InstanceKlass*
   apply_parsed_class_metadata(ik, _java_fields_count, CHECK);
 
@@ -5358,10 +5666,13 @@
 
   ik->set_minor_version(_minor_version);
   ik->set_major_version(_major_version);
   ik->set_has_nonstatic_concrete_methods(_has_nonstatic_concrete_methods);
   ik->set_declares_nonstatic_concrete_methods(_declares_nonstatic_concrete_methods);
+  if (_is_declared_atomic) {
+    ik->set_is_declared_atomic();
+  }
 
   if (_unsafe_anonymous_host != NULL) {
     assert (ik->is_unsafe_anonymous(), "should be the same");
     ik->set_unsafe_anonymous_host(_unsafe_anonymous_host);
   }
@@ -5469,10 +5780,40 @@
       // We won a potential race
       JvmtiExport::add_default_read_edges(module_handle, THREAD);
     }
   }
 
+  int nfields = ik->java_fields_count();
+  if (ik->is_inline_klass()) nfields++;
+  for (int i = 0; i < nfields; i++) {
+    if (ik->field_is_inline_type(i) && ((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {
+      Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);
+      // Inline classes for instance fields must have been pre-loaded
+      // Inline classes for static fields might not have been loaded yet
+      Klass* klass = SystemDictionary::find(klass_name,
+          Handle(THREAD, ik->class_loader()),
+          Handle(THREAD, ik->protection_domain()), CHECK);
+      if (klass != NULL) {
+        assert(klass->access_flags().is_inline_type(), "Inline type expected");
+        ik->set_inline_type_field_klass(i, klass);
+      }
+      klass_name->decrement_refcount();
+    } else
+      if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)
+        && ((ik->field_access_flags(i) & JVM_ACC_STATIC) != 0)) {
+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));
+    }
+  }
+
+  if (is_inline_type()) {
+    InlineKlass* vk = InlineKlass::cast(ik);
+    vk->set_alignment(_alignment);
+    vk->set_first_field_offset(_first_field_offset);
+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);
+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);
+  }
+
   ClassLoadingService::notify_class_loaded(ik, false /* not shared class */);
 
   if (!is_internal()) {
     if (log_is_enabled(Info, class, load)) {
       ResourceMark rm;
@@ -5518,10 +5859,14 @@
   set_klass_to_deallocate(NULL);
 
   // it's official
   set_klass(ik);
 
+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {
+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);
+  }
+
   debug_only(ik->verify();)
 }
 
 void ClassFileParser::update_class_name(Symbol* new_class_name) {
   // Decrement the refcount in the old name, since we're clobbering it.
@@ -5620,10 +5965,11 @@
   _inner_classes(NULL),
   _nest_members(NULL),
   _nest_host(0),
   _permitted_subclasses(NULL),
   _record_components(NULL),
+  _temp_local_interfaces(NULL),
   _local_interfaces(NULL),
   _transitive_interfaces(NULL),
   _combined_annotations(NULL),
   _class_annotations(NULL),
   _class_type_annotations(NULL),
@@ -5659,10 +6005,19 @@
   _relax_verify(false),
   _has_nonstatic_concrete_methods(false),
   _declares_nonstatic_concrete_methods(false),
   _has_final_method(false),
   _has_contended_fields(false),
+  _has_inline_type_fields(false),
+  _has_nonstatic_fields(false),
+  _is_empty_inline_type(false),
+  _is_naturally_atomic(false),
+  _is_declared_atomic(false),
+  _invalid_inline_super(false),
+  _invalid_identity_super(false),
+  _implements_identityObject(false),
+  _has_injected_identityObject(false),
   _has_finalizer(false),
   _has_empty_finalizer(false),
   _has_vanilla_constructor(false),
   _max_bootstrap_specifier_index(-1) {
 
@@ -5869,19 +6224,23 @@
   assert(cp_size == (const u2)cp->length(), "invariant");
 
   // ACCESS FLAGS
   stream->guarantee_more(8, CHECK);  // flags, this_class, super_class, infs_len
 
-  // Access flags
-  jint flags;
+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;
   // JVM_ACC_MODULE is defined in JDK-9 and later.
   if (_major_version >= JAVA_9_VERSION) {
-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);
-  } else {
-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;
+    recognized_modifiers |= JVM_ACC_MODULE;
+  }
+  // JVM_ACC_INLINE is defined for class file version 55 and later
+  if (supports_inline_types()) {
+    recognized_modifiers |= JVM_ACC_INLINE;
   }
 
+  // Access flags
+  jint flags = stream->get_u2_fast() & recognized_modifiers;
+
   if ((flags & JVM_ACC_INTERFACE) && _major_version < JAVA_6_VERSION) {
     // Set abstract bit for old class files for backward compatibility
     flags |= JVM_ACC_ABSTRACT;
   }
 
@@ -6035,19 +6394,22 @@
   // Interfaces
   _itfs_len = stream->get_u2_fast();
   parse_interfaces(stream,
                    _itfs_len,
                    cp,
+                   is_inline_type(),
                    &_has_nonstatic_concrete_methods,
+                   &_is_declared_atomic,
                    CHECK);
 
-  assert(_local_interfaces != NULL, "invariant");
+  assert(_temp_local_interfaces != NULL, "invariant");
 
   // Fields (offsets are filled in later)
   _fac = new FieldAllocationCount();
   parse_fields(stream,
-               _access_flags.is_interface(),
+               is_interface(),
+               is_inline_type(),
                _fac,
                cp,
                cp_size,
                &_java_fields_count,
                CHECK);
@@ -6055,11 +6417,12 @@
   assert(_fields != NULL, "invariant");
 
   // Methods
   AccessFlags promoted_flags;
   parse_methods(stream,
-                _access_flags.is_interface(),
+                is_interface(),
+                is_inline_type(),
                 &promoted_flags,
                 &_has_final_method,
                 &_declares_nonstatic_concrete_methods,
                 CHECK);
 
@@ -6126,18 +6489,18 @@
   assert(stream->at_eos(), "invariant");
   assert(cp != NULL, "invariant");
   assert(_loader_data != NULL, "invariant");
 
   if (_class_name == vmSymbols::java_lang_Object()) {
-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),
-                   "java.lang.Object cannot implement an interface in class file %s",
-                   CHECK);
+    check_property(_temp_local_interfaces->length() == 0,
+        "java.lang.Object cannot implement an interface in class file %s",
+        CHECK);
   }
   // We check super class after class file is parsed and format is checked
   if (_super_class_index > 0 && NULL ==_super_klass) {
     Symbol* const super_class_name = cp->klass_name_at(_super_class_index);
-    if (_access_flags.is_interface()) {
+    if (is_interface()) {
       // Before attempting to resolve the superclass, check for class format
       // errors not checked yet.
       guarantee_property(super_class_name == vmSymbols::java_lang_Object(),
         "Interfaces must have java.lang.Object as superclass in class file %s",
         CHECK);
@@ -6154,10 +6517,13 @@
 
   if (_super_klass != NULL) {
     if (_super_klass->has_nonstatic_concrete_methods()) {
       _has_nonstatic_concrete_methods = true;
     }
+    if (_super_klass->is_declared_atomic()) {
+      _is_declared_atomic = true;
+    }
 
     if (_super_klass->is_interface()) {
       ResourceMark rm(THREAD);
       Exceptions::fthrow(
         THREAD_AND_LOCATION,
@@ -6166,12 +6532,68 @@
         _class_name->as_klass_external_name(),
         _super_klass->external_name()
       );
       return;
     }
+
+    // For an inline class, only java/lang/Object or special abstract classes
+    // are acceptable super classes.
+    if (is_inline_type()) {
+      const InstanceKlass* super_ik = _super_klass;
+      if (super_ik->invalid_inline_super()) {
+        ResourceMark rm(THREAD);
+        Exceptions::fthrow(
+          THREAD_AND_LOCATION,
+          vmSymbols::java_lang_IncompatibleClassChangeError(),
+          "inline class %s has an invalid super class %s",
+          _class_name->as_klass_external_name(),
+          _super_klass->external_name());
+        return;
+      }
+    }
+  }
+
+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {
+    // This is the original source of this condition.
+    // It propagates by inheritance, as if testing "instanceof NonTearable".
+    _is_declared_atomic = true;
+  } else if (*ForceNonTearable != '\0') {
+    // Allow a command line switch to force the same atomicity property:
+    const char* class_name_str = _class_name->as_C_string();
+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {
+      _is_declared_atomic = true;
+    }
   }
 
+  // Set ik->invalid_inline_super field to TRUE if already marked as invalid,
+  // if super is marked invalid, or if is_invalid_super_for_inline_type()
+  // returns true
+  if (invalid_inline_super() ||
+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||
+      is_invalid_super_for_inline_type()) {
+    set_invalid_inline_super();
+  }
+
+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())
+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {
+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());
+    _has_injected_identityObject = true;
+  }
+  int itfs_len = _temp_local_interfaces->length();
+  if (itfs_len == 0) {
+    _local_interfaces = Universe::the_empty_instance_klass_array();
+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {
+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();
+  } else {
+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);
+    for (int i = 0; i < itfs_len; i++) {
+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));
+    }
+  }
+  _temp_local_interfaces = NULL;
+  assert(_local_interfaces != NULL, "invariant");
+
   // Compute the transitive list of all unique interfaces implemented by this class
   _transitive_interfaces =
     compute_transitive_interfaces(_super_klass,
                                   _local_interfaces,
                                   _loader_data,
@@ -6196,24 +6618,42 @@
                                                     _class_name,
                                                     _local_interfaces,
                                                     CHECK);
 
   // Size of Java itable (in words)
-  _itable_size = _access_flags.is_interface() ? 0 :
+  _itable_size = is_interface() ? 0 :
     klassItable::compute_itable_size(_transitive_interfaces);
 
   assert(_fac != NULL, "invariant");
   assert(_parsed_annotations != NULL, "invariant");
 
+
+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {
+      // Pre-load inline class
+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,
+          Handle(THREAD, _loader_data->class_loader()),
+          _protection_domain, true, CHECK);
+      assert(klass != NULL, "Sanity check");
+      assert(klass->access_flags().is_inline_type(), "Value type expected");
+    }
+  }
+
   _field_info = new FieldLayoutInfo();
   FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
-                        _parsed_annotations->is_contended(), _field_info);
-  lb.build_layout();
+      _parsed_annotations->is_contended(), is_inline_type(),
+      loader_data(), _protection_domain, _field_info);
+  lb.build_layout(CHECK);
+  if (is_inline_type()) {
+    _alignment = lb.get_alignment();
+    _first_field_offset = lb.get_first_field_offset();
+    _exact_size_in_bytes = lb.get_exact_size_in_byte();
+  }
+  _has_inline_type_fields = _field_info->_has_inline_fields;
 
-  // Compute reference typ
+  // Compute reference type
   _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();
-
 }
 
 void ClassFileParser::set_klass(InstanceKlass* klass) {
 
 #ifdef ASSERT
@@ -6241,10 +6681,11 @@
 const ClassFileStream* ClassFileParser::clone_stream() const {
   assert(_stream != NULL, "invariant");
 
   return _stream->clone();
 }
+
 // ----------------------------------------------------------------------------
 // debugging
 
 #ifdef ASSERT
 
diff a/src/hotspot/share/classfile/classFileParser.hpp b/src/hotspot/share/classfile/classFileParser.hpp
--- a/src/hotspot/share/classfile/classFileParser.hpp
+++ b/src/hotspot/share/classfile/classFileParser.hpp
@@ -73,10 +73,12 @@
   OopMapBlocksBuilder* oop_map_blocks;
   int _instance_size;
   int _nonstatic_field_size;
   int _static_field_size;
   bool  _has_nonstatic_fields;
+  bool  _is_naturally_atomic;
+  bool _has_inline_fields;
 };
 
 // Parser for for .class files
 //
 // The bytes describing the class file structure is read from a Stream object
@@ -132,10 +134,11 @@
   Array<u2>* _inner_classes;
   Array<u2>* _nest_members;
   u2 _nest_host;
   Array<u2>* _permitted_subclasses;
   Array<RecordComponent*>* _record_components;
+  GrowableArray<InstanceKlass*>* _temp_local_interfaces;
   Array<InstanceKlass*>* _local_interfaces;
   Array<InstanceKlass*>* _transitive_interfaces;
   Annotations* _combined_annotations;
   AnnotationArray* _class_annotations;
   AnnotationArray* _class_type_annotations;
@@ -157,10 +160,14 @@
   int _vtable_size;
   int _itable_size;
 
   int _num_miranda_methods;
 
+  int _alignment;
+  int _first_field_offset;
+  int _exact_size_in_bytes;
+
   ReferenceType _rt;
   Handle _protection_domain;
   AccessFlags _access_flags;
 
   // for tracing and notifications
@@ -195,10 +202,20 @@
   bool _has_nonstatic_concrete_methods;
   bool _declares_nonstatic_concrete_methods;
   bool _has_final_method;
   bool _has_contended_fields;
 
+  bool _has_inline_type_fields;
+  bool _has_nonstatic_fields;
+  bool _is_empty_inline_type;
+  bool _is_naturally_atomic;
+  bool _is_declared_atomic;
+  bool _invalid_inline_super;   // if true, invalid super type for an inline type.
+  bool _invalid_identity_super; // if true, invalid super type for an identity type.
+  bool _implements_identityObject;
+  bool _has_injected_identityObject;
+
   // precomputed flags
   bool _has_finalizer;
   bool _has_empty_finalizer;
   bool _has_vanilla_constructor;
   int _max_bootstrap_specifier_index;  // detects BSS values
@@ -244,11 +261,13 @@
 
   // Interface parsing
   void parse_interfaces(const ClassFileStream* const stream,
                         const int itfs_len,
                         ConstantPool* const cp,
+                        bool is_inline_type,
                         bool* has_nonstatic_concrete_methods,
+                        bool* is_declared_atomic,
                         TRAPS);
 
   const InstanceKlass* parse_super_class(ConstantPool* const cp,
                                          const int super_class_index,
                                          const bool need_verify,
@@ -265,25 +284,28 @@
                               FieldAnnotationCollector* parsed_annotations,
                               TRAPS);
 
   void parse_fields(const ClassFileStream* const cfs,
                     bool is_interface,
+                    bool is_inline_type,
                     FieldAllocationCount* const fac,
                     ConstantPool* cp,
                     const int cp_size,
                     u2* const java_fields_count_ptr,
                     TRAPS);
 
   // Method parsing
   Method* parse_method(const ClassFileStream* const cfs,
                        bool is_interface,
+                       bool is_inline_type,
                        const ConstantPool* cp,
                        AccessFlags* const promoted_flags,
                        TRAPS);
 
   void parse_methods(const ClassFileStream* const cfs,
                      bool is_interface,
+                     bool is_inline_type,
                      AccessFlags* const promoted_flags,
                      bool* const has_final_method,
                      bool* const declares_nonstatic_concrete_methods,
                      TRAPS);
 
@@ -441,10 +463,15 @@
   void throwIllegalSignature(const char* type,
                              const Symbol* name,
                              const Symbol* sig,
                              TRAPS) const;
 
+  void throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,
+                                 const char* msg,
+                                 const Symbol* name = NULL,
+                                 const Symbol* sig  = NULL) const;
+
   void verify_constantvalue(const ConstantPool* const cp,
                             int constantvalue_index,
                             int signature_index,
                             TRAPS) const;
 
@@ -459,13 +486,17 @@
   int  verify_legal_method_signature(const Symbol* methodname,
                                      const Symbol* signature,
                                      TRAPS) const;
 
   void verify_legal_class_modifiers(jint flags, TRAPS) const;
-  void verify_legal_field_modifiers(jint flags, bool is_interface, TRAPS) const;
+  void verify_legal_field_modifiers(jint flags,
+                                    bool is_interface,
+                                    bool is_inline_type,
+                                    TRAPS) const;
   void verify_legal_method_modifiers(jint flags,
                                      bool is_interface,
+                                     bool is_inline_type,
                                      const Symbol* name,
                                      TRAPS) const;
 
   const char* skip_over_field_signature(const char* signature,
                                         bool void_ok,
@@ -533,10 +564,13 @@
                                int annotation_default_length,
                                TRAPS);
 
   void update_class_name(Symbol* new_name);
 
+  // Check if the class file supports inline types
+  bool supports_inline_types() const;
+
  public:
   ClassFileParser(ClassFileStream* stream,
                   Symbol* name,
                   ClassLoaderData* loader_data,
                   const ClassLoadInfo* cl_info,
@@ -561,10 +595,20 @@
   u2 this_class_index() const { return _this_class_index; }
 
   bool is_unsafe_anonymous() const { return _unsafe_anonymous_host != NULL; }
   bool is_hidden() const { return _is_hidden; }
   bool is_interface() const { return _access_flags.is_interface(); }
+  bool is_inline_type() const { return _access_flags.is_inline_type(); }
+  bool is_value_capable_class() const;
+  bool has_inline_fields() const { return _has_inline_type_fields; }
+  bool invalid_inline_super() const { return _invalid_inline_super; }
+  void set_invalid_inline_super() { _invalid_inline_super = true; }
+  bool invalid_identity_super() const { return _invalid_identity_super; }
+  void set_invalid_identity_super() { _invalid_identity_super = true; }
+  bool is_invalid_super_for_inline_type();
+
+  u2 java_fields_count() const { return _java_fields_count; }
 
   const InstanceKlass* unsafe_anonymous_host() const { return _unsafe_anonymous_host; }
   const GrowableArray<Handle>* cp_patches() const { return _cp_patches; }
   ClassLoaderData* loader_data() const { return _loader_data; }
   const Symbol* class_name() const { return _class_name; }
diff a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -52,19 +52,21 @@
 #define VM_SYMBOLS_DO(template, do_alias)                                                         \
   /* commonly used class, package, module names */                                                \
   template(java_base,                                 "java.base")                                \
   template(java_lang_System,                          "java/lang/System")                         \
   template(java_lang_Object,                          "java/lang/Object")                         \
+  template(java_lang_IdentityObject,                  "java/lang/IdentityObject")                 \
   template(java_lang_Class,                           "java/lang/Class")                          \
   template(java_lang_Package,                         "java/lang/Package")                        \
   template(java_lang_Module,                          "java/lang/Module")                         \
   template(java_lang_String,                          "java/lang/String")                         \
   template(java_lang_StringLatin1,                    "java/lang/StringLatin1")                   \
   template(java_lang_StringUTF16,                     "java/lang/StringUTF16")                    \
   template(java_lang_Thread,                          "java/lang/Thread")                         \
   template(java_lang_ThreadGroup,                     "java/lang/ThreadGroup")                    \
   template(java_lang_Cloneable,                       "java/lang/Cloneable")                      \
+  template(java_lang_NonTearable,                     "java/lang/NonTearable")                    \
   template(java_lang_Throwable,                       "java/lang/Throwable")                      \
   template(java_lang_ClassLoader,                     "java/lang/ClassLoader")                    \
   template(java_lang_ThreadDeath,                     "java/lang/ThreadDeath")                    \
   template(java_lang_Boolean,                         "java/lang/Boolean")                        \
   template(java_lang_Character,                       "java/lang/Character")                      \
@@ -327,12 +329,13 @@
   template(setTargetVolatile_name,                    "setTargetVolatile")                        \
   template(setTarget_signature,                       "(Ljava/lang/invoke/MethodHandle;)V")       \
   template(DEFAULT_CONTEXT_name,                      "DEFAULT_CONTEXT")                          \
   NOT_LP64(  do_alias(intptr_signature,               int_signature)  )                           \
   LP64_ONLY( do_alias(intptr_signature,               long_signature) )                           \
-                                                                                                                                      \
-  /* Support for JVMCI */                                                                                                             \
+                                                                                                  \
+                                                                                                  \
+  /* Support for JVMCI */                                                                         \
   JVMCI_VM_SYMBOLS_DO(template, do_alias)                                                         \
                                                                                                   \
   template(java_lang_StackWalker,                     "java/lang/StackWalker")                    \
   template(java_lang_StackFrameInfo,                  "java/lang/StackFrameInfo")                 \
   template(java_lang_LiveStackFrameInfo,              "java/lang/LiveStackFrameInfo")             \
@@ -452,10 +455,12 @@
   template(java_lang_Boolean_signature,               "Ljava/lang/Boolean;")                      \
   template(url_code_signer_array_void_signature,      "(Ljava/net/URL;[Ljava/security/CodeSigner;)V") \
   template(module_entry_name,                         "module_entry")                             \
   template(resolved_references_name,                  "<resolved_references>")                    \
   template(init_lock_name,                            "<init_lock>")                              \
+  template(default_value_name,                        ".default")                                 \
+  template(empty_marker_name,                         ".empty")                                   \
   template(address_size_name,                         "ADDRESS_SIZE0")                            \
   template(page_size_name,                            "PAGE_SIZE")                                \
   template(big_endian_name,                           "BIG_ENDIAN")                               \
   template(use_unaligned_access_name,                 "UNALIGNED_ACCESS")                         \
   template(data_cache_line_flush_size_name,           "DATA_CACHE_LINE_FLUSH_SIZE")               \
@@ -521,10 +526,11 @@
   template(thread_void_signature,                     "(Ljava/lang/Thread;)V")                                    \
   template(threadgroup_runnable_void_signature,       "(Ljava/lang/ThreadGroup;Ljava/lang/Runnable;)V")           \
   template(threadgroup_string_void_signature,         "(Ljava/lang/ThreadGroup;Ljava/lang/String;)V")             \
   template(string_class_signature,                    "(Ljava/lang/String;)Ljava/lang/Class;")                    \
   template(object_object_object_signature,            "(Ljava/lang/Object;Ljava/lang/Object;)Ljava/lang/Object;") \
+  template(object_object_boolean_signature,           "(Ljava/lang/Object;Ljava/lang/Object;)Z") \
   template(string_string_string_signature,            "(Ljava/lang/String;Ljava/lang/String;)Ljava/lang/String;") \
   template(string_string_signature,                   "(Ljava/lang/String;)Ljava/lang/String;")                   \
   template(classloader_string_long_signature,         "(Ljava/lang/ClassLoader;Ljava/lang/String;)J")             \
   template(byte_array_void_signature,                 "([B)V")                                                    \
   template(char_array_void_signature,                 "([C)V")                                                    \
@@ -666,10 +672,15 @@
   template(jdk_internal_loader_ClassLoaders,       "jdk/internal/loader/ClassLoaders")                            \
   template(toFileURL_name,                         "toFileURL")                                                   \
   template(toFileURL_signature,                    "(Ljava/lang/String;)Ljava/net/URL;")                          \
   template(url_void_signature,                     "(Ljava/net/URL;)V")                                           \
                                                                                                                   \
+  template(java_lang_invoke_ValueBootstrapMethods, "java/lang/invoke/ValueBootstrapMethods")                      \
+  template(isSubstitutable_name,                   "isSubstitutable0")                                            \
+  template(inlineObjectHashCode_name,              "inlineObjectHashCode")                                        \
+                                                                                                                  \
+  template(jdk_internal_vm_jni_SubElementSelector, "jdk/internal/vm/jni/SubElementSelector")                      \
   /*end*/
 
 // Here are all the intrinsics known to the runtime and the CI.
 // Each intrinsic consists of a public enum name (like _hashCode),
 // followed by a specification of its klass, name, and signature:
@@ -1157,39 +1168,49 @@
   do_signature(putLong_signature,         "(Ljava/lang/Object;JJ)V")                                                    \
   do_signature(getFloat_signature,        "(Ljava/lang/Object;J)F")                                                     \
   do_signature(putFloat_signature,        "(Ljava/lang/Object;JF)V")                                                    \
   do_signature(getDouble_signature,       "(Ljava/lang/Object;J)D")                                                     \
   do_signature(putDouble_signature,       "(Ljava/lang/Object;JD)V")                                                    \
+  do_signature(getValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;)Ljava/lang/Object;")                   \
+  do_signature(putValue_signature,        "(Ljava/lang/Object;JLjava/lang/Class;Ljava/lang/Object;)V")                  \
                                                                                                                         \
   do_name(getReference_name,"getReference")     do_name(putReference_name,"putReference")                               \
   do_name(getBoolean_name,"getBoolean")         do_name(putBoolean_name,"putBoolean")                                   \
   do_name(getByte_name,"getByte")               do_name(putByte_name,"putByte")                                         \
   do_name(getShort_name,"getShort")             do_name(putShort_name,"putShort")                                       \
   do_name(getChar_name,"getChar")               do_name(putChar_name,"putChar")                                         \
   do_name(getInt_name,"getInt")                 do_name(putInt_name,"putInt")                                           \
   do_name(getLong_name,"getLong")               do_name(putLong_name,"putLong")                                         \
   do_name(getFloat_name,"getFloat")             do_name(putFloat_name,"putFloat")                                       \
   do_name(getDouble_name,"getDouble")           do_name(putDouble_name,"putDouble")                                     \
+  do_name(getValue_name,"getValue")             do_name(putValue_name,"putValue")                                       \
+  do_name(makePrivateBuffer_name,"makePrivateBuffer")                                                                   \
+  do_name(finishPrivateBuffer_name,"finishPrivateBuffer")                                                               \
                                                                                                                         \
   do_intrinsic(_getReference,       jdk_internal_misc_Unsafe,     getReference_name, getReference_signature,     F_RN)  \
   do_intrinsic(_getBoolean,         jdk_internal_misc_Unsafe,     getBoolean_name, getBoolean_signature,         F_RN)  \
   do_intrinsic(_getByte,            jdk_internal_misc_Unsafe,     getByte_name, getByte_signature,               F_RN)  \
   do_intrinsic(_getShort,           jdk_internal_misc_Unsafe,     getShort_name, getShort_signature,             F_RN)  \
   do_intrinsic(_getChar,            jdk_internal_misc_Unsafe,     getChar_name, getChar_signature,               F_RN)  \
   do_intrinsic(_getInt,             jdk_internal_misc_Unsafe,     getInt_name, getInt_signature,                 F_RN)  \
   do_intrinsic(_getLong,            jdk_internal_misc_Unsafe,     getLong_name, getLong_signature,               F_RN)  \
   do_intrinsic(_getFloat,           jdk_internal_misc_Unsafe,     getFloat_name, getFloat_signature,             F_RN)  \
   do_intrinsic(_getDouble,          jdk_internal_misc_Unsafe,     getDouble_name, getDouble_signature,           F_RN)  \
+  do_intrinsic(_getValue,           jdk_internal_misc_Unsafe,     getValue_name, getValue_signature,             F_RN)  \
   do_intrinsic(_putReference,       jdk_internal_misc_Unsafe,     putReference_name, putReference_signature,     F_RN)  \
   do_intrinsic(_putBoolean,         jdk_internal_misc_Unsafe,     putBoolean_name, putBoolean_signature,         F_RN)  \
   do_intrinsic(_putByte,            jdk_internal_misc_Unsafe,     putByte_name, putByte_signature,               F_RN)  \
   do_intrinsic(_putShort,           jdk_internal_misc_Unsafe,     putShort_name, putShort_signature,             F_RN)  \
   do_intrinsic(_putChar,            jdk_internal_misc_Unsafe,     putChar_name, putChar_signature,               F_RN)  \
   do_intrinsic(_putInt,             jdk_internal_misc_Unsafe,     putInt_name, putInt_signature,                 F_RN)  \
   do_intrinsic(_putLong,            jdk_internal_misc_Unsafe,     putLong_name, putLong_signature,               F_RN)  \
   do_intrinsic(_putFloat,           jdk_internal_misc_Unsafe,     putFloat_name, putFloat_signature,             F_RN)  \
   do_intrinsic(_putDouble,          jdk_internal_misc_Unsafe,     putDouble_name, putDouble_signature,           F_RN)  \
+  do_intrinsic(_putValue,           jdk_internal_misc_Unsafe,     putValue_name, putValue_signature,             F_RN)  \
+                                                                                                                        \
+  do_intrinsic(_makePrivateBuffer,  jdk_internal_misc_Unsafe,     makePrivateBuffer_name, object_object_signature, F_RN)   \
+  do_intrinsic(_finishPrivateBuffer,  jdk_internal_misc_Unsafe,   finishPrivateBuffer_name, object_object_signature, F_RN) \
                                                                                                                         \
   do_name(getReferenceVolatile_name,"getReferenceVolatile")   do_name(putReferenceVolatile_name,"putReferenceVolatile") \
   do_name(getBooleanVolatile_name,"getBooleanVolatile")       do_name(putBooleanVolatile_name,"putBooleanVolatile")     \
   do_name(getByteVolatile_name,"getByteVolatile")             do_name(putByteVolatile_name,"putByteVolatile")           \
   do_name(getShortVolatile_name,"getShortVolatile")           do_name(putShortVolatile_name,"putShortVolatile")         \
diff a/src/hotspot/share/gc/shared/collectedHeap.hpp b/src/hotspot/share/gc/shared/collectedHeap.hpp
--- a/src/hotspot/share/gc/shared/collectedHeap.hpp
+++ b/src/hotspot/share/gc/shared/collectedHeap.hpp
@@ -257,10 +257,11 @@
     _gc_cause = v;
   }
   GCCause::Cause gc_cause() { return _gc_cause; }
 
   oop obj_allocate(Klass* klass, int size, TRAPS);
+  oop obj_buffer_allocate(Klass* klass, int size, TRAPS); // doesn't clear memory
   virtual oop array_allocate(Klass* klass, int size, int length, bool do_zero, TRAPS);
   oop class_allocate(Klass* klass, int size, TRAPS);
 
   // Utilities for turning raw memory into filler objects.
   //
diff a/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp b/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp
--- a/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp
+++ b/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp
@@ -266,18 +266,18 @@
   Raw::clone(src, dst, size);
 }
 
 template <DecoratorSet decorators, typename BarrierSetT>
 template <typename T>
-bool ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
+void ShenandoahBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,
                                                                                          arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,
                                                                                          size_t length) {
   ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();
   bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),
                         arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),
                         length);
-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);
+  Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);
 }
 
 template <class T, bool HAS_FWD, bool EVAC, bool ENQUEUE>
 void ShenandoahBarrierSet::arraycopy_work(T* src, size_t count) {
   assert(HAS_FWD == _heap->has_forwarded_objects(), "Forwarded object status is sane");
diff a/src/hotspot/share/memory/universe.cpp b/src/hotspot/share/memory/universe.cpp
--- a/src/hotspot/share/memory/universe.cpp
+++ b/src/hotspot/share/memory/universe.cpp
@@ -115,10 +115,12 @@
 LatestMethodCache* Universe::_finalizer_register_cache = NULL;
 LatestMethodCache* Universe::_loader_addClass_cache    = NULL;
 LatestMethodCache* Universe::_throw_illegal_access_error_cache = NULL;
 LatestMethodCache* Universe::_throw_no_such_method_error_cache = NULL;
 LatestMethodCache* Universe::_do_stack_walk_cache     = NULL;
+LatestMethodCache* Universe::_is_substitutable_cache  = NULL;
+LatestMethodCache* Universe::_inline_type_hash_code_cache = NULL;
 oop Universe::_out_of_memory_error_java_heap          = NULL;
 oop Universe::_out_of_memory_error_metaspace          = NULL;
 oop Universe::_out_of_memory_error_class_metaspace    = NULL;
 oop Universe::_out_of_memory_error_array_size         = NULL;
 oop Universe::_out_of_memory_error_gc_overhead_limit  = NULL;
@@ -137,10 +139,11 @@
 
 Array<int>* Universe::_the_empty_int_array            = NULL;
 Array<u2>* Universe::_the_empty_short_array           = NULL;
 Array<Klass*>* Universe::_the_empty_klass_array     = NULL;
 Array<InstanceKlass*>* Universe::_the_empty_instance_klass_array  = NULL;
+Array<InstanceKlass*>* Universe::_the_single_IdentityObject_klass_array = NULL;
 Array<Method*>* Universe::_the_empty_method_array   = NULL;
 
 // These variables are guarded by FullGCALot_lock.
 debug_only(objArrayOop Universe::_fullgc_alot_dummy_array = NULL;)
 debug_only(int Universe::_fullgc_alot_dummy_next      = 0;)
@@ -223,16 +226,19 @@
   it->push(&_the_empty_short_array);
   it->push(&_the_empty_klass_array);
   it->push(&_the_empty_instance_klass_array);
   it->push(&_the_empty_method_array);
   it->push(&_the_array_interfaces_array);
+  it->push(&_the_single_IdentityObject_klass_array);
 
   _finalizer_register_cache->metaspace_pointers_do(it);
   _loader_addClass_cache->metaspace_pointers_do(it);
   _throw_illegal_access_error_cache->metaspace_pointers_do(it);
   _throw_no_such_method_error_cache->metaspace_pointers_do(it);
   _do_stack_walk_cache->metaspace_pointers_do(it);
+  _is_substitutable_cache->metaspace_pointers_do(it);
+  _inline_type_hash_code_cache->metaspace_pointers_do(it);
 }
 
 #define ASSERT_MIRROR_NULL(m) \
   assert(m == NULL, "archived mirrors should be NULL");
 
@@ -260,15 +266,18 @@
   f->do_ptr((void**)&_the_empty_int_array);
   f->do_ptr((void**)&_the_empty_short_array);
   f->do_ptr((void**)&_the_empty_method_array);
   f->do_ptr((void**)&_the_empty_klass_array);
   f->do_ptr((void**)&_the_empty_instance_klass_array);
+  f->do_ptr((void**)&_the_single_IdentityObject_klass_array);
   _finalizer_register_cache->serialize(f);
   _loader_addClass_cache->serialize(f);
   _throw_illegal_access_error_cache->serialize(f);
   _throw_no_such_method_error_cache->serialize(f);
   _do_stack_walk_cache->serialize(f);
+  _is_substitutable_cache->serialize(f);
+  _inline_type_hash_code_cache->serialize(f);
 }
 
 void Universe::check_alignment(uintx size, uintx alignment, const char* name) {
   if (size < alignment || size % alignment != 0) {
     vm_exit_during_initialization(
@@ -312,11 +321,11 @@
           _typeArrayKlassObjs[i] = TypeArrayKlass::create_klass((BasicType)i, CHECK);
         }
 
         ClassLoaderData* null_cld = ClassLoaderData::the_null_class_loader_data();
 
-        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 2, NULL, CHECK);
+        _the_array_interfaces_array     = MetadataFactory::new_array<Klass*>(null_cld, 3, NULL, CHECK);
         _the_empty_int_array            = MetadataFactory::new_array<int>(null_cld, 0, CHECK);
         _the_empty_short_array          = MetadataFactory::new_array<u2>(null_cld, 0, CHECK);
         _the_empty_method_array         = MetadataFactory::new_array<Method*>(null_cld, 0, CHECK);
         _the_empty_klass_array          = MetadataFactory::new_array<Klass*>(null_cld, 0, CHECK);
         _the_empty_instance_klass_array = MetadataFactory::new_array<InstanceKlass*>(null_cld, 0, CHECK);
@@ -337,16 +346,22 @@
       // Verify shared interfaces array.
       assert(_the_array_interfaces_array->at(0) ==
              SystemDictionary::Cloneable_klass(), "u3");
       assert(_the_array_interfaces_array->at(1) ==
              SystemDictionary::Serializable_klass(), "u3");
+      assert(_the_array_interfaces_array->at(2) ==
+                   SystemDictionary::IdentityObject_klass(), "u3");
+
+      assert(_the_single_IdentityObject_klass_array->at(0) ==
+          SystemDictionary::IdentityObject_klass(), "u3");
     } else
 #endif
     {
       // Set up shared interfaces array.  (Do this before supers are set up.)
       _the_array_interfaces_array->at_put(0, SystemDictionary::Cloneable_klass());
       _the_array_interfaces_array->at_put(1, SystemDictionary::Serializable_klass());
+      _the_array_interfaces_array->at_put(2, SystemDictionary::IdentityObject_klass());
     }
 
     initialize_basic_type_klass(boolArrayKlassObj(), CHECK);
     initialize_basic_type_klass(charArrayKlassObj(), CHECK);
     initialize_basic_type_klass(floatArrayKlassObj(), CHECK);
@@ -464,10 +479,18 @@
     _mirrors[T_VOID]    = _void_mirror;
   //_mirrors[T_OBJECT]  = _object_klass->java_mirror();
   //_mirrors[T_ARRAY]   = _object_klass->java_mirror();
 }
 
+void Universe::initialize_the_single_IdentityObject_klass_array(InstanceKlass* ik, TRAPS) {
+    assert(_the_single_IdentityObject_klass_array == NULL, "Must not be initialized twice");
+    assert(ik->name() == vmSymbols::java_lang_IdentityObject(), "Must be");
+    Array<InstanceKlass*>* array = MetadataFactory::new_array<InstanceKlass*>(ik->class_loader_data(), 1, NULL, CHECK);
+    array->at_put(0, ik);
+    _the_single_IdentityObject_klass_array = array;
+  }
+
 void Universe::fixup_mirrors(TRAPS) {
   // Bootstrap problem: all classes gets a mirror (java.lang.Class instance) assigned eagerly,
   // but we cannot do that for classes created before java.lang.Class is loaded. Here we simply
   // walk over permanent objects created so far (mostly classes) and fixup their mirrors. Note
   // that the number of objects allocated at this point is very small.
@@ -662,11 +685,10 @@
   }
 
   Universe::initialize_tlab();
 
   Metaspace::global_initialize();
-
   // Initialize performance counters for metaspaces
   MetaspaceCounters::initialize_performance_counters();
   CompressedClassSpaceCounters::initialize_performance_counters();
 
   AOTLoader::universe_init();
@@ -685,10 +707,12 @@
   Universe::_finalizer_register_cache = new LatestMethodCache();
   Universe::_loader_addClass_cache    = new LatestMethodCache();
   Universe::_throw_illegal_access_error_cache = new LatestMethodCache();
   Universe::_throw_no_such_method_error_cache = new LatestMethodCache();
   Universe::_do_stack_walk_cache = new LatestMethodCache();
+  Universe::_is_substitutable_cache = new LatestMethodCache();
+  Universe::_inline_type_hash_code_cache = new LatestMethodCache();
 
 #if INCLUDE_CDS
   if (UseSharedSpaces) {
     // Read the data structures supporting the shared spaces (shared
     // system dictionary, symbol table, etc.).  After that, access to
@@ -836,10 +860,21 @@
   // Set up method for stack walking
   initialize_known_method(_do_stack_walk_cache,
                           SystemDictionary::AbstractStackWalker_klass(),
                           "doStackWalk",
                           vmSymbols::doStackWalk_signature(), false, CHECK);
+
+  // Set up substitutability testing
+  ResourceMark rm;
+  initialize_known_method(_is_substitutable_cache,
+                          SystemDictionary::ValueBootstrapMethods_klass(),
+                          vmSymbols::isSubstitutable_name()->as_C_string(),
+                          vmSymbols::object_object_boolean_signature(), true, CHECK);
+  initialize_known_method(_inline_type_hash_code_cache,
+                          SystemDictionary::ValueBootstrapMethods_klass(),
+                          vmSymbols::inlineObjectHashCode_name()->as_C_string(),
+                          vmSymbols::object_int_signature(), true, CHECK);
 }
 
 void universe2_init() {
   EXCEPTION_MARK;
   Universe::genesis(CATCH);
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -63,10 +63,11 @@
 #include "oops/klass.inline.hpp"
 #include "oops/method.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/recordComponent.hpp"
 #include "oops/symbol.hpp"
+#include "oops/inlineKlass.hpp"
 #include "prims/jvmtiExport.hpp"
 #include "prims/jvmtiRedefineClasses.hpp"
 #include "prims/jvmtiThreadState.hpp"
 #include "prims/methodComparator.hpp"
 #include "runtime/atomic.hpp"
@@ -153,10 +154,12 @@
     }
   }
   return false;
 }
 
+bool InstanceKlass::field_is_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_INLINE_TYPE; }
+
 // private: called to verify that k is a static member of this nest.
 // We know that k is an instance class in the same package and hence the
 // same classloader.
 bool InstanceKlass::has_nest_member(InstanceKlass* k, TRAPS) const {
   assert(!is_hidden(), "unexpected hidden class");
@@ -471,11 +474,13 @@
   const int size = InstanceKlass::size(parser.vtable_size(),
                                        parser.itable_size(),
                                        nonstatic_oop_map_size(parser.total_oop_map_count()),
                                        parser.is_interface(),
                                        parser.is_unsafe_anonymous(),
-                                       should_store_fingerprint(is_hidden_or_anonymous));
+                                       should_store_fingerprint(is_hidden_or_anonymous),
+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,
+                                       parser.is_inline_type());
 
   const Symbol* const class_name = parser.class_name();
   assert(class_name != NULL, "invariant");
   ClassLoaderData* loader_data = parser.loader_data();
   assert(loader_data != NULL, "invariant");
@@ -485,14 +490,16 @@
   // Allocation
   if (REF_NONE == parser.reference_type()) {
     if (class_name == vmSymbols::java_lang_Class()) {
       // mirror
       ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);
-    }
-    else if (is_class_loader(class_name, parser)) {
+    } else if (is_class_loader(class_name, parser)) {
       // class loader
       ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);
+    } else if (parser.is_inline_type()) {
+      // inline type
+      ik = new (loader_data, size, THREAD) InlineKlass(parser);
     } else {
       // normal
       ik = new (loader_data, size, THREAD) InstanceKlass(parser, InstanceKlass::_kind_other);
     }
   } else {
@@ -504,13 +511,43 @@
   // class count.  Can get OOM here.
   if (HAS_PENDING_EXCEPTION) {
     return NULL;
   }
 
+#ifdef ASSERT
+  assert(ik->size() == size, "");
+  ik->bounds_check((address) ik->start_of_vtable(), false, size);
+  ik->bounds_check((address) ik->start_of_itable(), false, size);
+  ik->bounds_check((address) ik->end_of_itable(), true, size);
+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);
+#endif //ASSERT
   return ik;
 }
 
+#ifndef PRODUCT
+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {
+  const char* bad = NULL;
+  address end = NULL;
+  if (addr < (address)this) {
+    bad = "before";
+  } else if (addr == (address)this) {
+    if (edge_ok)  return true;
+    bad = "just before";
+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {
+    if (edge_ok)  return true;
+    bad = "just after";
+  } else if (addr > end) {
+    bad = "after";
+  } else {
+    return true;
+  }
+  tty->print_cr("%s object bounds: " INTPTR_FORMAT " [" INTPTR_FORMAT ".." INTPTR_FORMAT "]",
+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);
+  Verbose = WizardMode = true; this->print(); //@@
+  return false;
+}
+#endif //PRODUCT
 
 // copy method ordering from resource area to Metaspace
 void InstanceKlass::copy_method_ordering(const intArray* m, TRAPS) {
   if (m != NULL) {
     // allocate a new array and copy contents (memcpy?)
@@ -541,29 +578,38 @@
   _nonstatic_oop_map_size(nonstatic_oop_map_size(parser.total_oop_map_count())),
   _itable_len(parser.itable_size()),
   _nest_host_index(0),
   _init_state(allocated),
   _reference_type(parser.reference_type()),
-  _init_thread(NULL)
+  _init_thread(NULL),
+  _inline_type_field_klasses(NULL),
+  _adr_inlineklass_fixed_block(NULL)
 {
   set_vtable_length(parser.vtable_size());
   set_kind(kind);
   set_access_flags(parser.access_flags());
   if (parser.is_hidden()) set_is_hidden();
   set_is_unsafe_anonymous(parser.is_unsafe_anonymous());
   set_layout_helper(Klass::instance_layout_helper(parser.layout_size(),
                                                     false));
+    if (parser.has_inline_fields()) {
+      set_has_inline_type_fields();
+    }
+    _java_fields_count = parser.java_fields_count();
 
-  assert(NULL == _methods, "underlying memory not zeroed?");
-  assert(is_instance_klass(), "is layout incorrect?");
-  assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
+    assert(NULL == _methods, "underlying memory not zeroed?");
+    assert(is_instance_klass(), "is layout incorrect?");
+    assert(size_helper() == parser.layout_size(), "incorrect size_helper?");
 
   // Set biased locking bit for all instances of this class; it will be
   // cleared if revocation occurs too often for this type
   if (UseBiasedLocking && BiasedLocking::enabled()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
+  if (has_inline_type_fields()) {
+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();
+  }
 }
 
 void InstanceKlass::deallocate_methods(ClassLoaderData* loader_data,
                                        Array<Method*>* methods) {
   if (methods != NULL && methods != Universe::the_empty_method_array() &&
@@ -589,18 +635,20 @@
   Array<InstanceKlass*>* ti = transitive_interfaces;
   if (ti != Universe::the_empty_instance_klass_array() && ti != local_interfaces) {
     // check that the interfaces don't come from super class
     Array<InstanceKlass*>* sti = (super_klass == NULL) ? NULL :
                     InstanceKlass::cast(super_klass)->transitive_interfaces();
-    if (ti != sti && ti != NULL && !ti->is_shared()) {
+    if (ti != sti && ti != NULL && !ti->is_shared() &&
+        ti != Universe::the_single_IdentityObject_klass_array()) {
       MetadataFactory::free_array<InstanceKlass*>(loader_data, ti);
     }
   }
 
   // local interfaces can be empty
   if (local_interfaces != Universe::the_empty_instance_klass_array() &&
-      local_interfaces != NULL && !local_interfaces->is_shared()) {
+      local_interfaces != NULL && !local_interfaces->is_shared() &&
+      local_interfaces != Universe::the_single_IdentityObject_klass_array()) {
     MetadataFactory::free_array<InstanceKlass*>(loader_data, local_interfaces);
   }
 }
 
 void InstanceKlass::deallocate_record_components(ClassLoaderData* loader_data,
@@ -924,10 +972,66 @@
   for (int index = 0; index < num_interfaces; index++) {
     InstanceKlass* interk = interfaces->at(index);
     interk->link_class_impl(CHECK_false);
   }
 
+
+  // If a class declares a method that uses an inline class as an argument
+  // type or return inline type, this inline class must be loaded during the
+  // linking of this class because size and properties of the inline class
+  // must be known in order to be able to perform inline type optimizations.
+  // The implementation below is an approximation of this rule, the code
+  // iterates over all methods of the current class (including overridden
+  // methods), not only the methods declared by this class. This
+  // approximation makes the code simpler, and doesn't change the semantic
+  // because classes declaring methods overridden by the current class are
+  // linked (and have performed their own pre-loading) before the linking
+  // of the current class.
+
+
+  // Note:
+  // Inline class types are loaded during
+  // the loading phase (see ClassFileParser::post_process_parsed_stream()).
+  // Inline class types used as element types for array creation
+  // are not pre-loaded. Their loading is triggered by either anewarray
+  // or multianewarray bytecodes.
+
+  // Could it be possible to do the following processing only if the
+  // class uses inline types?
+  {
+    ResourceMark rm(THREAD);
+    for (int i = 0; i < methods()->length(); i++) {
+      Method* m = methods()->at(i);
+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {
+        if (ss.is_reference()) {
+          if (ss.is_array()) {
+            ss.skip_array_prefix();
+          }
+          if (ss.type() == T_INLINE_TYPE) {
+            Symbol* symb = ss.as_symbol();
+
+            oop loader = class_loader();
+            oop protection_domain = this->protection_domain();
+            Klass* klass = SystemDictionary::resolve_or_fail(symb,
+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,
+                                                             CHECK_false);
+            if (klass == NULL) {
+              THROW_(vmSymbols::java_lang_LinkageError(), false);
+            }
+            if (!klass->is_inline_klass()) {
+              Exceptions::fthrow(
+                THREAD_AND_LOCATION,
+                vmSymbols::java_lang_IncompatibleClassChangeError(),
+                "class %s is not an inline type",
+                klass->external_name());
+            }
+          }
+        }
+      }
+    }
+  }
+
   // in case the class is linked in the process of linking its superclasses
   if (is_linked()) {
     return true;
   }
 
@@ -995,10 +1099,11 @@
 #ifdef ASSERT
       vtable().verify(tty, true);
       // In case itable verification is ever added.
       // itable().verify(tty, true);
 #endif
+
       set_init_state(linked);
       if (JvmtiExport::should_post_class_prepare()) {
         Thread *thread = THREAD;
         assert(thread->is_Java_thread(), "thread->is_Java_thread()");
         JvmtiExport::post_class_prepare((JavaThread *) thread, this);
@@ -1148,15 +1253,44 @@
       DTRACE_CLASSINIT_PROBE_WAIT(super__failed, -1, wait);
       THROW_OOP(e());
     }
   }
 
+  // Step 8
+  // Initialize classes of inline fields
+  {
+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {
+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {
+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());
+        if (fs.access_flags().is_static() && klass == NULL) {
+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),
+              Handle(THREAD, class_loader()),
+              Handle(THREAD, protection_domain()),
+              true, CHECK);
+          if (klass == NULL) {
+            THROW(vmSymbols::java_lang_NoClassDefFoundError());
+          }
+          if (!klass->is_inline_klass()) {
+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
+          }
+          set_inline_type_field_klass(fs.index(), klass);
+        }
+        InstanceKlass::cast(klass)->initialize(CHECK);
+        if (fs.access_flags().is_static()) {
+          if (java_mirror()->obj_field(fs.offset()) == NULL) {
+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());
+          }
+        }
+      }
+    }
+  }
+
 
   // Look for aot compiled methods for this klass, including class initializer.
   AOTLoader::load_for_klass(this, THREAD);
 
-  // Step 8
+  // Step 9
   {
     DTRACE_CLASSINIT_PROBE_WAIT(clinit, -1, wait);
     if (class_initializer() != NULL) {
       // Timer includes any side effects of class initialization (resolution,
       // etc), but not recursive entry into call_class_initializer().
@@ -1174,19 +1308,19 @@
       }
       call_class_initializer(THREAD);
     }
   }
 
-  // Step 9
+  // Step 10
   if (!HAS_PENDING_EXCEPTION) {
     set_initialization_state_and_notify(fully_initialized, CHECK);
     {
       debug_only(vtable().verify(tty, true);)
     }
   }
   else {
-    // Step 10 and 11
+    // Step 11 and 12
     Handle e(THREAD, PENDING_EXCEPTION);
     CLEAR_PENDING_EXCEPTION;
     // JVMTI has already reported the pending exception
     // JVMTI internal flag reset is needed in order to report ExceptionInInitializerError
     JvmtiExport::clear_detected_exception(jt);
@@ -1470,11 +1604,11 @@
 static int call_class_initializer_counter = 0;   // for debugging
 
 Method* InstanceKlass::class_initializer() const {
   Method* clinit = find_method(
       vmSymbols::class_initializer_name(), vmSymbols::void_method_signature());
-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {
+  if (clinit != NULL && clinit->is_class_initializer()) {
     return clinit;
   }
   return NULL;
 }
 
@@ -1508,11 +1642,11 @@
   InterpreterOopMap* entry_for) {
   // Lazily create the _oop_map_cache at first request
   // Lock-free access requires load_acquire.
   OopMapCache* oop_map_cache = Atomic::load_acquire(&_oop_map_cache);
   if (oop_map_cache == NULL) {
-    MutexLocker x(OopMapCacheAlloc_lock);
+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);
     // Check if _oop_map_cache was allocated while we were waiting for this lock
     if ((oop_map_cache = _oop_map_cache) == NULL) {
       oop_map_cache = new OopMapCache();
       // Ensure _oop_map_cache is stable, since it is examined without a lock
       Atomic::release_store(&_oop_map_cache, oop_map_cache);
@@ -1520,15 +1654,10 @@
   }
   // _oop_map_cache is constant after init; lookup below does its own locking.
   oop_map_cache->lookup(method, bci, entry_for);
 }
 
-bool InstanceKlass::contains_field_offset(int offset) {
-  fieldDescriptor fd;
-  return find_field_from_offset(offset, false, &fd);
-}
-
 bool InstanceKlass::find_local_field(Symbol* name, Symbol* sig, fieldDescriptor* fd) const {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
     Symbol* f_name = fs.name();
     Symbol* f_sig  = fs.signature();
     if (f_name == name && f_sig == sig) {
@@ -1595,10 +1724,19 @@
   }
   // 4) otherwise field lookup fails
   return NULL;
 }
 
+bool InstanceKlass::contains_field_offset(int offset) {
+  if (this->is_inline_klass()) {
+    InlineKlass* vk = InlineKlass::cast(this);
+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());
+  } else {
+    fieldDescriptor fd;
+    return find_field_from_offset(offset, false, &fd);
+  }
+}
 
 bool InstanceKlass::find_local_field_from_offset(int offset, bool is_static, fieldDescriptor* fd) const {
   for (JavaFieldStream fs(this); !fs.done(); fs.next()) {
     if (fs.offset() == offset) {
       fd->reinitialize(const_cast<InstanceKlass*>(this), fs.index());
@@ -1979,10 +2117,13 @@
                                                                         find_static,
                                                                         private_mode);
     if (method != NULL) {
       return method;
     }
+    if (name == vmSymbols::object_initializer_name()) {
+      break;  // <init> is never inherited, not even as a static factory
+    }
     klass = klass->super();
     overpass_local_mode = skip_overpass;   // Always ignore overpass methods in superclasses
   }
   return NULL;
 }
@@ -2487,10 +2628,16 @@
   }
 
   it->push(&_nest_members);
   it->push(&_permitted_subclasses);
   it->push(&_record_components);
+
+  if (has_inline_type_fields()) {
+    for (int i = 0; i < java_fields_count(); i++) {
+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);
+    }
+  }
 }
 
 void InstanceKlass::remove_unshareable_info() {
   Klass::remove_unshareable_info();
 
@@ -2522,10 +2669,18 @@
   // do array classes also.
   if (array_klasses() != NULL) {
     array_klasses()->remove_unshareable_info();
   }
 
+  if (has_inline_type_fields()) {
+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {
+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {
+        reset_inline_type_field_klass(fs.index());
+      }
+    }
+  }
+
   // These are not allocated from metaspace. They are safe to set to NULL.
   _source_debug_extension = NULL;
   _dep_context = NULL;
   _osr_nmethods_head = NULL;
 #if INCLUDE_JVMTI
@@ -2561,10 +2716,14 @@
   // sure the current state is <loaded.
   assert(!is_loaded(), "invalid init state");
   set_package(loader_data, pkg_entry, CHECK);
   Klass::restore_unshareable_info(loader_data, protection_domain, CHECK);
 
+  if (is_inline_klass()) {
+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);
+  }
+
   Array<Method*>* methods = this->methods();
   int num_methods = methods->length();
   for (int index = 0; index < num_methods; ++index) {
     methods->at(index)->restore_unshareable_info(CHECK);
   }
@@ -2586,11 +2745,11 @@
     // --> see ArrayKlass::complete_create_array_klass()
     array_klasses()->restore_unshareable_info(ClassLoaderData::the_null_class_loader_data(), Handle(), CHECK);
   }
 
   // Initialize current biased locking state.
-  if (UseBiasedLocking && BiasedLocking::enabled()) {
+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_inline_klass()) {
     set_prototype_header(markWord::biased_locking_prototype());
   }
 }
 
 void InstanceKlass::set_shared_class_loader_type(s2 loader_type) {
@@ -2751,13 +2910,13 @@
   const char* src = (const char*) (name()->as_C_string());
   const int src_length = (int)strlen(src);
 
   char* dest = NEW_RESOURCE_ARRAY(char, src_length + hash_len + 3);
 
-  // Add L as type indicator
+  // Add L or Q as type indicator
   int dest_index = 0;
-  dest[dest_index++] = JVM_SIGNATURE_CLASS;
+  dest[dest_index++] = is_inline_klass() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;
 
   // Add the actual class name
   for (int src_index = 0; src_index < src_length; ) {
     dest[dest_index++] = src[src_index++];
   }
@@ -3323,33 +3482,69 @@
 
 static const char* state_names[] = {
   "allocated", "loaded", "linked", "being_initialized", "fully_initialized", "initialization_error"
 };
 
-static void print_vtable(intptr_t* start, int len, outputStream* st) {
+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {
+  ResourceMark rm;
+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);
+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;
   for (int i = 0; i < len; i++) {
     intptr_t e = start[i];
     st->print("%d : " INTPTR_FORMAT, i, e);
+    if (forward_refs[i] != 0) {
+      int from = forward_refs[i];
+      int off = (int) start[from];
+      st->print(" (offset %d <= [%d])", off, from);
+    }
     if (MetaspaceObj::is_valid((Metadata*)e)) {
       st->print(" ");
       ((Metadata*)e)->print_value_on(st);
+    } else if (self != NULL && e > 0 && e < 0x10000) {
+      address location = self + e;
+      int index = (int)((intptr_t*)location - start);
+      st->print(" (offset %d => [%d])", (int)e, index);
+      if (index >= 0 && index < len)
+        forward_refs[index] = i;
     }
     st->cr();
   }
 }
 
 static void print_vtable(vtableEntry* start, int len, outputStream* st) {
-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);
+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);
+}
+
+template<typename T>
+ static void print_array_on(outputStream* st, Array<T>* array) {
+   if (array == NULL) { st->print_cr("NULL"); return; }
+   array->print_value_on(st); st->cr();
+   if (Verbose || WizardMode) {
+     for (int i = 0; i < array->length(); i++) {
+       st->print("%d : ", i); array->at(i)->print_value_on(st); st->cr();
+     }
+   }
+ }
+
+static void print_array_on(outputStream* st, Array<int>* array) {
+  if (array == NULL) { st->print_cr("NULL"); return; }
+  array->print_value_on(st); st->cr();
+  if (Verbose || WizardMode) {
+    for (int i = 0; i < array->length(); i++) {
+      st->print("%d : %d", i, array->at(i)); st->cr();
+    }
+  }
 }
 
 void InstanceKlass::print_on(outputStream* st) const {
   assert(is_klass(), "must be klass");
   Klass::print_on(st);
 
   st->print(BULLET"instance size:     %d", size_helper());                        st->cr();
   st->print(BULLET"klass size:        %d", size());                               st->cr();
   st->print(BULLET"access:            "); access_flags().print_on(st);            st->cr();
+  st->print(BULLET"misc flags:        0x%x", _misc_flags);                        st->cr();
   st->print(BULLET"state:             "); st->print_cr("%s", state_names[_init_state]);
   st->print(BULLET"name:              "); name()->print_value_on(st);             st->cr();
   st->print(BULLET"super:             "); Metadata::print_value_on_maybe_null(st, super()); st->cr();
   st->print(BULLET"sub:               ");
   Klass* sub = subklass();
@@ -3372,30 +3567,18 @@
       st->cr();
     }
   }
 
   st->print(BULLET"arrays:            "); Metadata::print_value_on_maybe_null(st, array_klasses()); st->cr();
-  st->print(BULLET"methods:           "); methods()->print_value_on(st);                  st->cr();
-  if (Verbose || WizardMode) {
-    Array<Method*>* method_array = methods();
-    for (int i = 0; i < method_array->length(); i++) {
-      st->print("%d : ", i); method_array->at(i)->print_value(); st->cr();
-    }
-  }
-  st->print(BULLET"method ordering:   "); method_ordering()->print_value_on(st);      st->cr();
-  st->print(BULLET"default_methods:   "); default_methods()->print_value_on(st);      st->cr();
-  if (Verbose && default_methods() != NULL) {
-    Array<Method*>* method_array = default_methods();
-    for (int i = 0; i < method_array->length(); i++) {
-      st->print("%d : ", i); method_array->at(i)->print_value(); st->cr();
-    }
-  }
+  st->print(BULLET"methods:           "); print_array_on(st, methods());
+  st->print(BULLET"method ordering:   "); print_array_on(st, method_ordering());
+  st->print(BULLET"default_methods:   "); print_array_on(st, default_methods());
   if (default_vtable_indices() != NULL) {
-    st->print(BULLET"default vtable indices:   "); default_vtable_indices()->print_value_on(st);       st->cr();
+    st->print(BULLET"default vtable indices:   "); print_array_on(st, default_vtable_indices());
   }
-  st->print(BULLET"local interfaces:  "); local_interfaces()->print_value_on(st);      st->cr();
-  st->print(BULLET"trans. interfaces: "); transitive_interfaces()->print_value_on(st); st->cr();
+  st->print(BULLET"local interfaces:  "); print_array_on(st, local_interfaces());
+  st->print(BULLET"trans. interfaces: "); print_array_on(st, transitive_interfaces());
   st->print(BULLET"constants:         "); constants()->print_value_on(st);         st->cr();
   if (class_loader_data() != NULL) {
     st->print(BULLET"class loader data:  ");
     class_loader_data()->print_value_on(st);
     st->cr();
@@ -3448,11 +3631,11 @@
     st->print_cr(BULLET"java mirror:       NULL");
   }
   st->print(BULLET"vtable length      %d  (start addr: " INTPTR_FORMAT ")", vtable_length(), p2i(start_of_vtable())); st->cr();
   if (vtable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_vtable(), vtable_length(), st);
   st->print(BULLET"itable length      %d (start addr: " INTPTR_FORMAT ")", itable_length(), p2i(start_of_itable())); st->cr();
-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);
+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);
   st->print_cr(BULLET"---- static fields (%d words):", static_field_size());
   FieldPrinter print_static_field(st);
   ((InstanceKlass*)this)->do_local_static_fields(&print_static_field);
   st->print_cr(BULLET"---- non-static fields (%d words):", nonstatic_field_size());
   FieldPrinter print_nonstatic_field(st);
@@ -4184,5 +4367,10 @@
 
 unsigned char * InstanceKlass::get_cached_class_file_bytes() {
   return VM_RedefineClasses::get_cached_class_file_bytes(_cached_class_file);
 }
 #endif
+
+#define THROW_DVT_ERROR(s) \
+  Exceptions::fthrow(THREAD_AND_LOCATION, vmSymbols::java_lang_IncompatibleClassChangeError(), \
+      "ValueCapableClass class '%s' %s", external_name(),(s)); \
+      return
diff a/src/hotspot/share/oops/symbol.cpp b/src/hotspot/share/oops/symbol.cpp
--- a/src/hotspot/share/oops/symbol.cpp
+++ b/src/hotspot/share/oops/symbol.cpp
@@ -102,10 +102,85 @@
   assert(SafepointSynchronize::is_at_safepoint(), "must be at a safepoint");
   _hash_and_refcount =  pack_hash_and_refcount(extract_hash(_hash_and_refcount), PERM_REFCOUNT);
 }
 #endif
 
+bool Symbol::is_Q_signature() const {
+  int len = utf8_length();
+  return len > 2 && char_at(0) == JVM_SIGNATURE_INLINE_TYPE && char_at(len - 1) == JVM_SIGNATURE_ENDCLASS;
+}
+
+bool Symbol::is_Q_array_signature() const {
+  int l = utf8_length();
+  if (l < 2 || char_at(0) != JVM_SIGNATURE_ARRAY || char_at(l - 1) != JVM_SIGNATURE_ENDCLASS) {
+    return false;
+  }
+  for (int i = 1; i < (l - 2); i++) {
+    char c = char_at(i);
+    if (c == JVM_SIGNATURE_INLINE_TYPE) {
+      return true;
+    }
+    if (c != JVM_SIGNATURE_ARRAY) {
+      return false;
+    }
+  }
+  return false;
+}
+
+bool Symbol::is_Q_method_signature() const {
+  assert(SignatureVerifier::is_valid_method_signature(this), "must be");
+  int len = utf8_length();
+  if (len > 4 && char_at(0) == JVM_SIGNATURE_FUNC) {
+    for (int i=1; i<len-3; i++) { // Must end with ")Qx;", where x is at least one character or more.
+      if (char_at(i) == JVM_SIGNATURE_ENDFUNC && char_at(i+1) == JVM_SIGNATURE_INLINE_TYPE) {
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
+Symbol* Symbol::fundamental_name(TRAPS) {
+  if ((char_at(0) == JVM_SIGNATURE_INLINE_TYPE || char_at(0) == JVM_SIGNATURE_CLASS) && ends_with(JVM_SIGNATURE_ENDCLASS)) {
+    return SymbolTable::new_symbol(this, 1, utf8_length() - 1);
+  } else {
+    // reference count is incremented to be consistent with the behavior with
+    // the SymbolTable::new_symbol() call above
+    this->increment_refcount();
+    return this;
+  }
+}
+
+bool Symbol::is_same_fundamental_type(Symbol* s) const {
+  if (this == s) return true;
+  if (utf8_length() < 3) return false;
+  int offset1, offset2, len;
+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {
+    if (char_at(0) != JVM_SIGNATURE_INLINE_TYPE && char_at(0) != JVM_SIGNATURE_CLASS) return false;
+    offset1 = 1;
+    len = utf8_length() - 2;
+  } else {
+    offset1 = 0;
+    len = utf8_length();
+  }
+  if (ends_with(JVM_SIGNATURE_ENDCLASS)) {
+    if (s->char_at(0) != JVM_SIGNATURE_INLINE_TYPE && s->char_at(0) != JVM_SIGNATURE_CLASS) return false;
+    offset2 = 1;
+  } else {
+    offset2 = 0;
+  }
+  if ((offset2 + len) > s->utf8_length()) return false;
+  if ((utf8_length() - offset1 * 2) != (s->utf8_length() - offset2 * 2))
+    return false;
+  int l = len;
+  while (l-- > 0) {
+    if (char_at(offset1 + l) != s->char_at(offset2 + l))
+      return false;
+  }
+  return true;
+}
+
 // ------------------------------------------------------------------
 // Symbol::index_of
 //
 // Finds if the given string is a substring of this symbol's utf8 bytes.
 // Return -1 on failure.  Otherwise return the first index where str occurs.
@@ -406,7 +481,19 @@
 
   jbyte* bytes = (jbyte*) s->bytes();
   return os::is_readable_range(bytes, bytes + len);
 }
 
+void Symbol::print_Qvalue_on(outputStream* st) const {
+  if (this == NULL) {
+    st->print("NULL");
+  } else {
+    st->print("'Q");
+    for (int i = 0; i < utf8_length(); i++) {
+      st->print("%c", char_at(i));
+    }
+    st->print(";'");
+  }
+}
+
 // SymbolTable prints this in its statistics
 NOT_PRODUCT(size_t Symbol::_total_count = 0;)
diff a/src/hotspot/share/oops/symbol.hpp b/src/hotspot/share/oops/symbol.hpp
--- a/src/hotspot/share/oops/symbol.hpp
+++ b/src/hotspot/share/oops/symbol.hpp
@@ -193,11 +193,11 @@
     return contains_utf8_at(0, prefix, len);
   }
   bool starts_with(const char* prefix) const {
     return starts_with(prefix, (int) strlen(prefix));
   }
-  bool starts_with(int prefix_char) const {
+  bool starts_with(char prefix_char) const {
     return contains_byte_at(0, prefix_char);
   }
   // Tests if the symbol ends with the given suffix.
   bool ends_with(const char* suffix, int len) const {
     return contains_utf8_at(utf8_length() - len, suffix, len);
@@ -223,10 +223,22 @@
     if (position < 0)  return false;  // can happen with ends_with
     if (position >= utf8_length()) return false;
     return code_byte == char_at(position);
   }
 
+  // True if this is a descriptor for a method with void return.
+  // (Assumes it is a valid descriptor.)
+  bool is_void_method_signature() const {
+    return starts_with('(') && ends_with('V');
+  }
+
+  bool is_Q_signature() const;
+  bool is_Q_array_signature() const;
+  bool is_Q_method_signature() const;
+  Symbol* fundamental_name(TRAPS);
+  bool is_same_fundamental_type(Symbol*) const;
+
   // Tests if the symbol starts with the given prefix.
   int index_of_at(int i, const char* str, int len) const;
 
   // Three-way compare for sorting; returns -1/0/1 if receiver is </==/> than arg
   // note that the ordering is not alfabetical
@@ -266,10 +278,11 @@
   // Printing
   void print_symbol_on(outputStream* st = NULL) const;
   void print_utf8_on(outputStream* st) const;
   void print_on(outputStream* st) const;         // First level print
   void print_value_on(outputStream* st) const;   // Second level print.
+  void print_Qvalue_on(outputStream* st) const;  // Second level print for Q-types.
 
   // printing on default output stream
   void print() const;
   void print_value() const;
 
diff a/src/hotspot/share/opto/output.cpp b/src/hotspot/share/opto/output.cpp
--- a/src/hotspot/share/opto/output.cpp
+++ b/src/hotspot/share/opto/output.cpp
@@ -240,16 +240,23 @@
     _code_offsets(),
     _node_bundling_limit(0),
     _node_bundling_base(NULL),
     _orig_pc_slot(0),
     _orig_pc_slot_offset_in_bytes(0),
+    _sp_inc_slot(0),
+    _sp_inc_slot_offset_in_bytes(0),
     _buf_sizes(),
     _block(NULL),
     _index(0) {
   C->set_output(this);
   if (C->stub_name() == NULL) {
-    _orig_pc_slot = C->fixed_slots() - (sizeof(address) / VMRegImpl::stack_slot_size);
+    int fixed_slots = C->fixed_slots();
+    if (C->needs_stack_repair()) {
+      fixed_slots -= 2;
+      _sp_inc_slot = fixed_slots;
+    }
+    _orig_pc_slot = fixed_slots - (sizeof(address) / VMRegImpl::stack_slot_size);
   }
 }
 
 PhaseOutput::~PhaseOutput() {
   C->set_output(NULL);
@@ -284,28 +291,38 @@
   Block *broot = C->cfg()->get_root_block();
 
   const StartNode *start = entry->head()->as_Start();
 
   // Replace StartNode with prolog
-  MachPrologNode *prolog = new MachPrologNode();
+  Label verified_entry;
+  MachPrologNode* prolog = new MachPrologNode(&verified_entry);
   entry->map_node(prolog, 0);
   C->cfg()->map_node_to_block(prolog, entry);
   C->cfg()->unmap_node_from_block(start); // start is no longer in any block
 
   // Virtual methods need an unverified entry point
-
-  if( C->is_osr_compilation() ) {
-    if( PoisonOSREntry ) {
+  if (C->is_osr_compilation()) {
+    if (PoisonOSREntry) {
       // TODO: Should use a ShouldNotReachHereNode...
       C->cfg()->insert( broot, 0, new MachBreakpointNode() );
     }
   } else {
-    if( C->method() && !C->method()->flags().is_static() ) {
-      // Insert unvalidated entry point
-      C->cfg()->insert( broot, 0, new MachUEPNode() );
+    if (C->method()) {
+      if (C->method()->has_scalarized_args()) {
+        // Add entry point to unpack all inline type arguments
+        C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, /* verified */ true, /* receiver_only */ false));
+        if (!C->method()->is_static()) {
+          // Add verified/unverified entry points to only unpack inline type receiver at interface calls
+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, /* verified */ false, /* receiver_only */ false));
+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, /* verified */ true,  /* receiver_only */ true));
+          C->cfg()->insert(broot, 0, new MachVEPNode(&verified_entry, /* verified */ false, /* receiver_only */ true));
+        }
+      } else if (!C->method()->is_static()) {
+        // Insert unvalidated entry point
+        C->cfg()->insert(broot, 0, new MachUEPNode());
+      }
     }
-
   }
 
   // Break before main entry point
   if ((C->method() && C->directive()->BreakAtExecuteOption) ||
       (OptoBreakpoint && C->is_method_compilation())       ||
@@ -341,10 +358,35 @@
   // Must be done before ScheduleAndBundle due to SPARC delay slots
   uint* blk_starts = NEW_RESOURCE_ARRAY(uint, C->cfg()->number_of_blocks() + 1);
   blk_starts[0] = 0;
   shorten_branches(blk_starts);
 
+  if (!C->is_osr_compilation() && C->has_scalarized_args()) {
+    // Compute the offsets of the entry points required by the inline type calling convention
+    if (!C->method()->is_static()) {
+      // We have entries at the beginning of the method, implemented by the first 4 nodes.
+      // Entry                     (unverified) @ offset 0
+      // Verified_Inline_Entry_RO
+      // Inline_Entry              (unverified)
+      // Verified_Inline_Entry
+      uint offset = 0;
+      _code_offsets.set_value(CodeOffsets::Entry, offset);
+
+      offset += ((MachVEPNode*)broot->get_node(0))->size(C->regalloc());
+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, offset);
+
+      offset += ((MachVEPNode*)broot->get_node(1))->size(C->regalloc());
+      _code_offsets.set_value(CodeOffsets::Inline_Entry, offset);
+
+      offset += ((MachVEPNode*)broot->get_node(2))->size(C->regalloc());
+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, offset);
+    } else {
+      _code_offsets.set_value(CodeOffsets::Entry, -1); // will be patched later
+      _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, 0);
+    }
+  }
+
   ScheduleAndBundle();
   if (C->failing()) {
     return;
   }
 
@@ -498,11 +540,13 @@
           reloc_size += CallStubImpl::reloc_call_trampoline();
 
           MachCallNode *mcall = mach->as_MachCall();
           // This destination address is NOT PC-relative
 
-          mcall->method_set((intptr_t)mcall->entry_point());
+          if (mcall->entry_point() != NULL) {
+            mcall->method_set((intptr_t)mcall->entry_point());
+          }
 
           if (mcall->is_MachCallJava() && mcall->as_MachCallJava()->_method) {
             stub_size  += CompiledStaticCall::to_interp_stub_size();
             reloc_size += CompiledStaticCall::reloc_to_interp_stub();
 #if INCLUDE_AOT
@@ -931,10 +975,11 @@
   MachCallNode      *mcall;
 
   int safepoint_pc_offset = current_offset;
   bool is_method_handle_invoke = false;
   bool return_oop = false;
+  bool return_vt = false;
 
   // Add the safepoint in the DebugInfoRecorder
   if( !mach->is_MachCall() ) {
     mcall = NULL;
     C->debug_info()->add_safepoint(safepoint_pc_offset, sfn->_oop_map);
@@ -948,13 +993,16 @@
         is_method_handle_invoke = true;
       }
     }
 
     // Check if a call returns an object.
-    if (mcall->returns_pointer()) {
+    if (mcall->returns_pointer() || mcall->returns_vt()) {
       return_oop = true;
     }
+    if (mcall->returns_vt()) {
+      return_vt = true;
+    }
     safepoint_pc_offset += mcall->ret_addr_offset();
     C->debug_info()->add_safepoint(safepoint_pc_offset, mcall->_oop_map);
   }
 
   // Loop over the JVMState list to add scope information
@@ -1065,11 +1113,11 @@
     assert(jvms->bci() >= InvocationEntryBci && jvms->bci() <= 0x10000, "must be a valid or entry BCI");
     assert(!jvms->should_reexecute() || depth == max_depth, "reexecute allowed only for the youngest");
     // Now we can describe the scope.
     methodHandle null_mh;
     bool rethrow_exception = false;
-    C->debug_info()->describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms->bci(), jvms->should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, locvals, expvals, monvals);
+    C->debug_info()->describe_scope(safepoint_pc_offset, null_mh, scope_method, jvms->bci(), jvms->should_reexecute(), rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);
   } // End jvms loop
 
   // Mark the end of the scope set.
   C->debug_info()->end_safepoint(safepoint_pc_offset);
 }
@@ -1170,10 +1218,14 @@
 
   // Compute the byte offset where we can store the deopt pc.
   if (C->fixed_slots() != 0) {
     _orig_pc_slot_offset_in_bytes = C->regalloc()->reg2offset(OptoReg::stack2reg(_orig_pc_slot));
   }
+  if (C->needs_stack_repair()) {
+    // Compute the byte offset of the stack increment value
+    _sp_inc_slot_offset_in_bytes = C->regalloc()->reg2offset(OptoReg::stack2reg(_sp_inc_slot));
+  }
 
   // Compute prolog code size
   _method_size = 0;
   _frame_slots = OptoReg::reg2stack(C->matcher()->_old_SP) + C->regalloc()->_framesize;
 #if defined(IA64) && !defined(AIX)
@@ -1446,12 +1498,14 @@
 
         // Remember the start of the last call in a basic block
         if (is_mcall) {
           MachCallNode *mcall = mach->as_MachCall();
 
-          // This destination address is NOT PC-relative
-          mcall->method_set((intptr_t)mcall->entry_point());
+          if (mcall->entry_point() != NULL) {
+            // This destination address is NOT PC-relative
+            mcall->method_set((intptr_t)mcall->entry_point());
+          }
 
           // Save the return address
           call_returns[block->_pre_order] = current_offset + mcall->ret_addr_offset();
 
           if (mcall->is_MachCallLeaf()) {
@@ -3178,10 +3232,16 @@
     }
 
     ResourceMark rm;
     _scratch_const_size = const_size;
     int size = C2Compiler::initial_code_buffer_size(const_size);
+#ifdef ASSERT
+    if (C->has_scalarized_args()) {
+      // Oop verification for loading object fields from scalarized inline types in the new entry point requires lots of space
+      size += 5120;
+    }
+#endif
     blob = BufferBlob::create("Compile::scratch_buffer", size);
     // Record the buffer blob for next time.
     set_scratch_buffer_blob(blob);
     // Have we run out of code space?
     if (scratch_buffer_blob() == NULL) {
@@ -3242,18 +3302,30 @@
   if (is_branch) {
     MacroAssembler masm(&buf);
     masm.bind(fakeL);
     n->as_MachBranch()->save_label(&saveL, &save_bnum);
     n->as_MachBranch()->label_set(&fakeL, 0);
+  } else if (n->is_MachProlog()) {
+    saveL = ((MachPrologNode*)n)->_verified_entry;
+    ((MachPrologNode*)n)->_verified_entry = &fakeL;
+  } else if (n->is_MachVEP()) {
+    saveL = ((MachVEPNode*)n)->_verified_entry;
+    ((MachVEPNode*)n)->_verified_entry = &fakeL;
   }
   n->emit(buf, C->regalloc());
 
   // Emitting into the scratch buffer should not fail
   assert (!C->failing(), "Must not have pending failure. Reason is: %s", C->failure_reason());
 
-  if (is_branch) // Restore label.
+  // Restore label.
+  if (is_branch) {
     n->as_MachBranch()->label_set(saveL, save_bnum);
+  } else if (n->is_MachProlog()) {
+    ((MachPrologNode*)n)->_verified_entry = saveL;
+  } else if (n->is_MachVEP()) {
+    ((MachVEPNode*)n)->_verified_entry = saveL;
+  }
 
   // End scratch_emit_size section.
   set_in_scratch_emit_size(false);
 
   return buf.insts_size();
@@ -3294,26 +3366,35 @@
     if (C->is_osr_compilation()) {
       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
     } else {
       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry) == -1) {
+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry, _first_block_size);
+      }
+      if (_code_offsets.value(CodeOffsets::Verified_Inline_Entry_RO) == -1) {
+        _code_offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, _first_block_size);
+      }
+      if (_code_offsets.value(CodeOffsets::Entry) == -1) {
+        _code_offsets.set_value(CodeOffsets::Entry, _first_block_size);
+      }
       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
     }
 
     C->env()->register_method(target,
-                                     entry_bci,
-                                     &_code_offsets,
-                                     _orig_pc_slot_offset_in_bytes,
-                                     code_buffer(),
-                                     frame_size_in_words(),
-                                     oop_map_set(),
-                                     &_handler_table,
-                                     inc_table(),
-                                     compiler,
-                                     has_unsafe_access,
-                                     SharedRuntime::is_wide_vector(C->max_vector_size()),
-                                     C->rtm_state());
+                              entry_bci,
+                              &_code_offsets,
+                              _orig_pc_slot_offset_in_bytes,
+                              code_buffer(),
+                              frame_size_in_words(),
+                              _oop_map_set,
+                              &_handler_table,
+                              &_inc_table,
+                              compiler,
+                              has_unsafe_access,
+                              SharedRuntime::is_wide_vector(C->max_vector_size()),
+                              C->rtm_state());
 
     if (C->log() != NULL) { // Print code cache state into compiler log
       C->log()->code_cache_state();
     }
   }
diff a/src/hotspot/share/opto/stringopts.cpp b/src/hotspot/share/opto/stringopts.cpp
--- a/src/hotspot/share/opto/stringopts.cpp
+++ b/src/hotspot/share/opto/stringopts.cpp
@@ -320,41 +320,41 @@
 }
 
 
 void StringConcat::eliminate_call(CallNode* call) {
   Compile* C = _stringopts->C;
-  CallProjections projs;
-  call->extract_projections(&projs, false);
-  if (projs.fallthrough_catchproj != NULL) {
-    C->gvn_replace_by(projs.fallthrough_catchproj, call->in(TypeFunc::Control));
+  CallProjections* projs = call->extract_projections(false);
+  if (projs->fallthrough_catchproj != NULL) {
+    C->gvn_replace_by(projs->fallthrough_catchproj, call->in(TypeFunc::Control));
   }
-  if (projs.fallthrough_memproj != NULL) {
-    C->gvn_replace_by(projs.fallthrough_memproj, call->in(TypeFunc::Memory));
+  if (projs->fallthrough_memproj != NULL) {
+    C->gvn_replace_by(projs->fallthrough_memproj, call->in(TypeFunc::Memory));
   }
-  if (projs.catchall_memproj != NULL) {
-    C->gvn_replace_by(projs.catchall_memproj, C->top());
+  if (projs->catchall_memproj != NULL) {
+    C->gvn_replace_by(projs->catchall_memproj, C->top());
   }
-  if (projs.fallthrough_ioproj != NULL) {
-    C->gvn_replace_by(projs.fallthrough_ioproj, call->in(TypeFunc::I_O));
+  if (projs->fallthrough_ioproj != NULL) {
+    C->gvn_replace_by(projs->fallthrough_ioproj, call->in(TypeFunc::I_O));
   }
-  if (projs.catchall_ioproj != NULL) {
-    C->gvn_replace_by(projs.catchall_ioproj, C->top());
+  if (projs->catchall_ioproj != NULL) {
+    C->gvn_replace_by(projs->catchall_ioproj, C->top());
   }
-  if (projs.catchall_catchproj != NULL) {
+  if (projs->catchall_catchproj != NULL) {
     // EA can't cope with the partially collapsed graph this
     // creates so put it on the worklist to be collapsed later.
-    for (SimpleDUIterator i(projs.catchall_catchproj); i.has_next(); i.next()) {
+    for (SimpleDUIterator i(projs->catchall_catchproj); i.has_next(); i.next()) {
       Node *use = i.get();
       int opc = use->Opcode();
       if (opc == Op_CreateEx || opc == Op_Region) {
         _stringopts->record_dead_node(use);
       }
     }
-    C->gvn_replace_by(projs.catchall_catchproj, C->top());
+    C->gvn_replace_by(projs->catchall_catchproj, C->top());
   }
-  if (projs.resproj != NULL) {
-    C->gvn_replace_by(projs.resproj, C->top());
+  if (projs->resproj[0] != NULL) {
+    assert(projs->nb_resproj == 1, "unexpected number of results");
+    C->gvn_replace_by(projs->resproj[0], C->top());
   }
   C->gvn_replace_by(call, C->top());
 }
 
 void StringConcat::eliminate_initialize(InitializeNode* init) {
diff a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -2610,11 +2610,12 @@
   int id_index = (result_count - 1);
 
   for (FilteredFieldStream src_st(ik, true, true); !src_st.eos(); src_st.next()) {
     result_list[id_index--] = jfieldIDWorkaround::to_jfieldID(
                                             ik, src_st.offset(),
-                                            src_st.access_flags().is_static());
+                                            src_st.access_flags().is_static(),
+                                            src_st.field_descriptor().is_inlined());
   }
   assert(id_index == -1, "just checking");
   // Fill in the results
   *field_count_ptr = result_count;
   *fields_ptr = result_list;
@@ -2647,20 +2648,25 @@
       *interface_count_ptr = 0;
       *interfaces_ptr = (jclass*) jvmtiMalloc(0 * sizeof(jclass));
       return JVMTI_ERROR_NONE;
     }
 
-    Array<InstanceKlass*>* interface_list = InstanceKlass::cast(k)->local_interfaces();
-    const int result_length = (interface_list == NULL ? 0 : interface_list->length());
+    InstanceKlass* ik = InstanceKlass::cast(k);
+    Array<InstanceKlass*>* interface_list = ik->local_interfaces();
+    int result_length = (interface_list == NULL ? 0 : interface_list->length());
+    if (ik->has_injected_identityObject()) result_length--;
     jclass* result_list = (jclass*) jvmtiMalloc(result_length * sizeof(jclass));
+    int cursor = 0;
     for (int i_index = 0; i_index < result_length; i_index += 1) {
       InstanceKlass* klass_at = interface_list->at(i_index);
       assert(klass_at->is_klass(), "interfaces must be Klass*s");
       assert(klass_at->is_interface(), "interfaces must be interfaces");
-      oop mirror_at = klass_at->java_mirror();
-      Handle handle_at = Handle(current_thread, mirror_at);
-      result_list[i_index] = (jclass) jni_reference(handle_at);
+      if (klass_at != SystemDictionary::IdentityObject_klass() || !ik->has_injected_identityObject()) {
+        oop mirror_at = klass_at->java_mirror();
+        Handle handle_at = Handle(current_thread, mirror_at);
+        result_list[cursor++] = (jclass) jni_reference(handle_at);
+      }
     }
     *interface_count_ptr = result_length;
     *interfaces_ptr = result_list;
   }
 
diff a/src/hotspot/share/prims/jvmtiEnvBase.cpp b/src/hotspot/share/prims/jvmtiEnvBase.cpp
--- a/src/hotspot/share/prims/jvmtiEnvBase.cpp
+++ b/src/hotspot/share/prims/jvmtiEnvBase.cpp
@@ -962,12 +962,17 @@
   };
 
   uint32_t debug_bits = 0;
   // first derive the object's owner and entry_count (if any)
   {
-    // Revoke any biases before querying the mark word
-    BiasedLocking::revoke_at_safepoint(hobj);
+    // Inline types instances don't support synchronization operations
+    // they are marked as always locked and no attempt to remove a
+    // potential bias (which cannot exist) should be made
+    if (!hobj()->mark().is_always_locked()) {
+      // Revoke any biases before querying the mark word
+      BiasedLocking::revoke_at_safepoint(hobj);
+    }
 
     address owner = NULL;
     {
       markWord mark = hobj()->mark();
 
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -30,17 +30,23 @@
 #include "classfile/javaClasses.inline.hpp"
 #include "classfile/vmSymbols.hpp"
 #include "jfr/jfrEvents.hpp"
 #include "memory/allocation.inline.hpp"
 #include "memory/resourceArea.hpp"
+#include "logging/log.hpp"
+#include "logging/logStream.hpp"
 #include "oops/access.inline.hpp"
 #include "oops/fieldStreams.inline.hpp"
+#include "oops/flatArrayKlass.hpp"
+#include "oops/flatArrayOop.inline.hpp"
+#include "oops/inlineKlass.inline.hpp"
 #include "oops/instanceKlass.inline.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "prims/unsafe.hpp"
+#include "runtime/fieldDescriptor.inline.hpp"
 #include "runtime/globals.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
 #include "runtime/jniHandles.inline.hpp"
 #include "runtime/orderAccess.hpp"
@@ -145,11 +151,10 @@
 }
 jlong Unsafe_field_offset_from_byte_offset(jlong byte_offset) {
   return byte_offset;
 }
 
-
 ///// Data read/writes on the Java heap and in native (off-heap) memory
 
 /**
  * Helper class to wrap memory accesses in JavaThread::doing_unsafe_access()
  */
@@ -230,15 +235,15 @@
   void put(T x) {
     if (_obj == NULL) {
       GuardUnsafeAccess guard(_thread);
       RawAccess<>::store(addr(), normalize_for_write(x));
     } else {
+      assert(!_obj->is_inline_type() || _obj->mark().is_larval_state(), "must be an object instance or a larval inline type");
       HeapAccess<>::store_at(_obj, _offset, normalize_for_write(x));
     }
   }
 
-
   T get_volatile() {
     if (_obj == NULL) {
       GuardUnsafeAccess guard(_thread);
       volatile T ret = RawAccess<MO_SEQ_CST>::load(addr());
       return normalize_for_read(ret);
@@ -256,10 +261,72 @@
       HeapAccess<MO_SEQ_CST>::store_at(_obj, _offset, normalize_for_write(x));
     }
   }
 };
 
+#ifdef ASSERT
+/*
+ * Get the field descriptor of the field of the given object at the given offset.
+ */
+static bool get_field_descriptor(oop p, jlong offset, fieldDescriptor* fd) {
+  bool found = false;
+  Klass* k = p->klass();
+  if (k->is_instance_klass()) {
+    InstanceKlass* ik = InstanceKlass::cast(k);
+    found = ik->find_field_from_offset((int)offset, false, fd);
+    if (!found && ik->is_mirror_instance_klass()) {
+      Klass* k2 = java_lang_Class::as_Klass(p);
+      if (k2->is_instance_klass()) {
+        ik = InstanceKlass::cast(k2);
+        found = ik->find_field_from_offset((int)offset, true, fd);
+      }
+    }
+  }
+  return found;
+}
+#endif // ASSERT
+
+static void assert_and_log_unsafe_value_access(oop p, jlong offset, InlineKlass* vk) {
+  Klass* k = p->klass();
+#ifdef ASSERT
+  if (k->is_instance_klass()) {
+    assert_field_offset_sane(p, offset);
+    fieldDescriptor fd;
+    bool found = get_field_descriptor(p, offset, &fd);
+    if (found) {
+      assert(found, "value field not found");
+      assert(fd.is_inlined(), "field not flat");
+    } else {
+      if (log_is_enabled(Trace, valuetypes)) {
+        log_trace(valuetypes)("not a field in %s at offset " SIZE_FORMAT_HEX,
+                              p->klass()->external_name(), offset);
+      }
+    }
+  } else if (k->is_flatArray_klass()) {
+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);
+    int index = (offset - vak->array_header_in_bytes()) / vak->element_byte_size();
+    address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());
+    assert(dest == (cast_from_oop<address>(p) + offset), "invalid offset");
+  } else {
+    ShouldNotReachHere();
+  }
+#endif // ASSERT
+  if (log_is_enabled(Trace, valuetypes)) {
+    if (k->is_flatArray_klass()) {
+      FlatArrayKlass* vak = FlatArrayKlass::cast(k);
+      int index = (offset - vak->array_header_in_bytes()) / vak->element_byte_size();
+      address dest = (address)((flatArrayOop)p)->value_at_addr(index, vak->layout_helper());
+      log_trace(valuetypes)("%s array type %s index %d element size %d offset " SIZE_FORMAT_HEX " at " INTPTR_FORMAT,
+                            p->klass()->external_name(), vak->external_name(),
+                            index, vak->element_byte_size(), offset, p2i(dest));
+    } else {
+      log_trace(valuetypes)("%s field type %s at offset " SIZE_FORMAT_HEX,
+                            p->klass()->external_name(), vk->external_name(), offset);
+    }
+  }
+}
+
 // These functions allow a null base pointer with an arbitrary address.
 // But if the base pointer is non-null, the offset should make some sense.
 // That is, it should be in the range [0, MAX_OBJECT_SIZE].
 UNSAFE_ENTRY(jobject, Unsafe_GetReference(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) {
   oop p = JNIHandles::resolve(obj);
@@ -270,13 +337,72 @@
 
 UNSAFE_ENTRY(void, Unsafe_PutReference(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jobject x_h)) {
   oop x = JNIHandles::resolve(x_h);
   oop p = JNIHandles::resolve(obj);
   assert_field_offset_sane(p, offset);
+  assert(!p->is_inline_type() || p->mark().is_larval_state(), "must be an object instance or a larval inline type");
   HeapAccess<ON_UNKNOWN_OOP_REF>::oop_store_at(p, offset, x);
 } UNSAFE_END
 
+UNSAFE_ENTRY(jlong, Unsafe_ValueHeaderSize(JNIEnv *env, jobject unsafe, jclass c)) {
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));
+  InlineKlass* vk = InlineKlass::cast(k);
+  return vk->first_field_offset();
+} UNSAFE_END
+
+UNSAFE_ENTRY(jboolean, Unsafe_IsFlattenedArray(JNIEnv *env, jobject unsafe, jclass c)) {
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(c));
+  return k->is_flatArray_klass();
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_UninitializedDefaultValue(JNIEnv *env, jobject unsafe, jclass vc)) {
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
+  InlineKlass* vk = InlineKlass::cast(k);
+  oop v = vk->default_value();
+  return JNIHandles::make_local(env, v);
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_GetValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc)) {
+  oop base = JNIHandles::resolve(obj);
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
+  InlineKlass* vk = InlineKlass::cast(k);
+  assert_and_log_unsafe_value_access(base, offset, vk);
+  Handle base_h(THREAD, base);
+  oop v = vk->read_inlined_field(base_h(), offset, CHECK_NULL);
+  return JNIHandles::make_local(env, v);
+} UNSAFE_END
+
+UNSAFE_ENTRY(void, Unsafe_PutValue(JNIEnv *env, jobject unsafe, jobject obj, jlong offset, jclass vc, jobject value)) {
+  oop base = JNIHandles::resolve(obj);
+  Klass* k = java_lang_Class::as_Klass(JNIHandles::resolve_non_null(vc));
+  InlineKlass* vk = InlineKlass::cast(k);
+  assert(!base->is_inline_type() || base->mark().is_larval_state(), "must be an object instance or a larval inline type");
+  assert_and_log_unsafe_value_access(base, offset, vk);
+  oop v = JNIHandles::resolve(value);
+  vk->write_inlined_field(base, offset, v, CHECK);
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_MakePrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {
+  oop v = JNIHandles::resolve_non_null(value);
+  assert(v->is_inline_type(), "must be an inline type instance");
+  Handle vh(THREAD, v);
+  InlineKlass* vk = InlineKlass::cast(v->klass());
+  instanceOop new_value = vk->allocate_instance_buffer(CHECK_NULL);
+  vk->inline_copy_oop_to_new_oop(vh(),  new_value);
+  markWord mark = new_value->mark();
+  new_value->set_mark(mark.enter_larval_state());
+  return JNIHandles::make_local(env, new_value);
+} UNSAFE_END
+
+UNSAFE_ENTRY(jobject, Unsafe_FinishPrivateBuffer(JNIEnv *env, jobject unsafe, jobject value)) {
+  oop v = JNIHandles::resolve(value);
+  assert(v->mark().is_larval_state(), "must be a larval value");
+  markWord mark = v->mark();
+  v->set_mark(mark.exit_larval_state());
+  return JNIHandles::make_local(env, v);
+} UNSAFE_END
+
 UNSAFE_ENTRY(jobject, Unsafe_GetReferenceVolatile(JNIEnv *env, jobject unsafe, jobject obj, jlong offset)) {
   oop p = JNIHandles::resolve(obj);
   assert_field_offset_sane(p, offset);
   oop v = HeapAccess<MO_SEQ_CST | ON_UNKNOWN_OOP_REF>::oop_load_at(p, offset);
   return JNIHandles::make_local(env, v);
@@ -609,10 +735,15 @@
   } else if (k->is_typeArray_klass()) {
     TypeArrayKlass* tak = TypeArrayKlass::cast(k);
     base  = tak->array_header_in_bytes();
     assert(base == arrayOopDesc::base_offset_in_bytes(tak->element_type()), "array_header_size semantics ok");
     scale = (1 << tak->log2_element_size());
+  } else if (k->is_flatArray_klass()) {
+    FlatArrayKlass* vak = FlatArrayKlass::cast(k);
+    InlineKlass* vklass = vak->element_klass();
+    base = vak->array_header_in_bytes();
+    scale = vak->element_byte_size();
   } else {
     ShouldNotReachHere();
   }
 }
 
@@ -644,10 +775,16 @@
   // but requires it to be linear in byte offset.
   return field_offset_from_byte_offset(scale) - field_offset_from_byte_offset(0);
 } UNSAFE_END
 
 
+UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))
+  oop p = JNIHandles::resolve(obj);
+  return p->size() * HeapWordSize;
+UNSAFE_END
+
+
 static inline void throw_new(JNIEnv *env, const char *ename) {
   jclass cls = env->FindClass(ename);
   if (env->ExceptionCheck()) {
     env->ExceptionClear();
     tty->print_cr("Unsafe: cannot throw %s because FindClass has failed", ename);
@@ -773,10 +910,12 @@
 // the anonymous class is loaded.  This is useful in dynamic languages, in which
 // various kinds of metaobjects must be introduced as constants into bytecode.
 // Note the cast (Object), which tells the verifier to expect an arbitrary object,
 // not just a literal string.  For such ldc instructions, the verifier uses the
 // type Object instead of String, if the loaded constant is not in fact a String.
+//
+// An anonymous class cannot be an inline type.
 
 static InstanceKlass*
 Unsafe_DefineAnonymousClass_impl(JNIEnv *env,
                                  jclass host_class, jbyteArray data, jobjectArray cp_patches_jh,
                                  u1** temp_alloc,
@@ -868,10 +1007,12 @@
                                                 CHECK_NULL);
   if (anonk == NULL) {
     return NULL;
   }
 
+  assert(!anonk->is_inline_klass(), "unsafe anonymous class cannot be inline class");
+
   return InstanceKlass::cast(anonk);
 }
 
 UNSAFE_ENTRY(jclass, Unsafe_DefineAnonymousClass0(JNIEnv *env, jobject unsafe, jclass host_class, jbyteArray data, jobjectArray cp_patches_jh)) {
   ResourceMark rm(THREAD);
@@ -1070,22 +1211,30 @@
 
 #define CC (char*)  /*cast a literal from (const char*)*/
 #define FN_PTR(f) CAST_FROM_FN_PTR(void*, &f)
 
 #define DECLARE_GETPUTOOP(Type, Desc) \
-    {CC "get" #Type,      CC "(" OBJ "J)" #Desc,       FN_PTR(Unsafe_Get##Type)}, \
-    {CC "put" #Type,      CC "(" OBJ "J" #Desc ")V",   FN_PTR(Unsafe_Put##Type)}, \
-    {CC "get" #Type "Volatile",      CC "(" OBJ "J)" #Desc,       FN_PTR(Unsafe_Get##Type##Volatile)}, \
-    {CC "put" #Type "Volatile",      CC "(" OBJ "J" #Desc ")V",   FN_PTR(Unsafe_Put##Type##Volatile)}
+    {CC "get"  #Type,      CC "(" OBJ "J)" #Desc,                 FN_PTR(Unsafe_Get##Type)}, \
+    {CC "put"  #Type,      CC "(" OBJ "J" #Desc ")V",             FN_PTR(Unsafe_Put##Type)}, \
+    {CC "get"  #Type "Volatile",      CC "(" OBJ "J)" #Desc,      FN_PTR(Unsafe_Get##Type##Volatile)}, \
+    {CC "put"  #Type "Volatile",      CC "(" OBJ "J" #Desc ")V",  FN_PTR(Unsafe_Put##Type##Volatile)}
 
 
 static JNINativeMethod jdk_internal_misc_Unsafe_methods[] = {
     {CC "getReference",         CC "(" OBJ "J)" OBJ "",   FN_PTR(Unsafe_GetReference)},
     {CC "putReference",         CC "(" OBJ "J" OBJ ")V",  FN_PTR(Unsafe_PutReference)},
     {CC "getReferenceVolatile", CC "(" OBJ "J)" OBJ,      FN_PTR(Unsafe_GetReferenceVolatile)},
     {CC "putReferenceVolatile", CC "(" OBJ "J" OBJ ")V",  FN_PTR(Unsafe_PutReferenceVolatile)},
 
+    {CC "isFlattenedArray", CC "(" CLS ")Z",                     FN_PTR(Unsafe_IsFlattenedArray)},
+    {CC "getValue",         CC "(" OBJ "J" CLS ")" OBJ,          FN_PTR(Unsafe_GetValue)},
+    {CC "putValue",         CC "(" OBJ "J" CLS OBJ ")V",         FN_PTR(Unsafe_PutValue)},
+    {CC "uninitializedDefaultValue", CC "(" CLS ")" OBJ,         FN_PTR(Unsafe_UninitializedDefaultValue)},
+    {CC "makePrivateBuffer",     CC "(" OBJ ")" OBJ,             FN_PTR(Unsafe_MakePrivateBuffer)},
+    {CC "finishPrivateBuffer",   CC "(" OBJ ")" OBJ,             FN_PTR(Unsafe_FinishPrivateBuffer)},
+    {CC "valueHeaderSize",       CC "(" CLS ")J",                FN_PTR(Unsafe_ValueHeaderSize)},
+
     {CC "getUncompressedObject", CC "(" ADR ")" OBJ,  FN_PTR(Unsafe_GetUncompressedObject)},
 
     DECLARE_GETPUTOOP(Boolean, Z),
     DECLARE_GETPUTOOP(Byte, B),
     DECLARE_GETPUTOOP(Short, S),
@@ -1104,10 +1253,11 @@
     {CC "staticFieldOffset0", CC "(" FLD ")J",           FN_PTR(Unsafe_StaticFieldOffset0)},
     {CC "staticFieldBase0",   CC "(" FLD ")" OBJ,        FN_PTR(Unsafe_StaticFieldBase0)},
     {CC "ensureClassInitialized0", CC "(" CLS ")V",      FN_PTR(Unsafe_EnsureClassInitialized0)},
     {CC "arrayBaseOffset0",   CC "(" CLS ")I",           FN_PTR(Unsafe_ArrayBaseOffset0)},
     {CC "arrayIndexScale0",   CC "(" CLS ")I",           FN_PTR(Unsafe_ArrayIndexScale0)},
+    {CC "getObjectSize0",     CC "(Ljava/lang/Object;)J", FN_PTR(Unsafe_GetObjectSize0)},
 
     {CC "defineClass0",       CC "(" DC_Args ")" CLS,    FN_PTR(Unsafe_DefineClass0)},
     {CC "allocateInstance",   CC "(" CLS ")" OBJ,        FN_PTR(Unsafe_AllocateInstance)},
     {CC "throwException",     CC "(" THR ")V",           FN_PTR(Unsafe_ThrowException)},
     {CC "compareAndSetReference",CC "(" OBJ "J" OBJ "" OBJ ")Z", FN_PTR(Unsafe_CompareAndSetReference)},
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -44,21 +44,23 @@
 #include "logging/log.hpp"
 #include "memory/filemap.hpp"
 #include "memory/heapShared.inline.hpp"
 #include "memory/metaspaceShared.hpp"
 #include "memory/metadataFactory.hpp"
-#include "memory/iterator.hpp"
+#include "memory/iterator.inline.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "memory/oopFactory.hpp"
 #include "oops/array.hpp"
 #include "oops/compressedOops.hpp"
+#include "oops/compressedOops.inline.hpp"
 #include "oops/constantPool.inline.hpp"
 #include "oops/method.inline.hpp"
 #include "oops/objArrayKlass.hpp"
 #include "oops/objArrayOop.inline.hpp"
 #include "oops/oop.inline.hpp"
+#include "oops/objArrayOop.inline.hpp"
 #include "oops/typeArrayOop.inline.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "prims/wbtestmethods/parserTests.hpp"
 #include "prims/whitebox.inline.hpp"
 #include "runtime/arguments.hpp"
@@ -1843,10 +1845,102 @@
 
 WB_ENTRY(jint, WB_ConstantPoolEncodeIndyIndex(JNIEnv* env, jobject wb, jint index))
   return ConstantPool::encode_invokedynamic_index(index);
 WB_END
 
+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))
+  oop aoop = JNIHandles::resolve(thing);
+  if (!aoop->is_instance()) {
+    return NULL;
+  }
+  instanceHandle ih(THREAD, (instanceOop) aoop);
+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());
+  if (klass->nonstatic_oop_map_count() == 0) {
+    return NULL;
+  }
+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();
+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();
+  int oop_count = 0;
+  while (map < end) {
+    oop_count += map->count();
+    map++;
+  }
+
+  objArrayOop result_array =
+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);
+  map = klass->start_of_nonstatic_oop_maps();
+  instanceOop ioop = ih();
+  int index = 0;
+  while (map < end) {
+    int offset = map->offset();
+    for (unsigned int j = 0; j < map->count(); j++) {
+      result_array->obj_at_put(index++, ioop->obj_field(offset));
+      offset += heapOopSize;
+    }
+    map++;
+  }
+  return (jobjectArray)JNIHandles::make_local(env, result_array);
+WB_END
+
+class CollectOops : public BasicOopIterateClosure {
+ public:
+  GrowableArray<Handle>* array;
+
+  objArrayOop create_results(TRAPS) {
+    objArrayOop result_array =
+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);
+    for (int i = 0 ; i < array->length(); i++) {
+      result_array->obj_at_put(i, array->at(i)());
+    }
+    return result_array;
+  }
+
+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {
+    return (jobjectArray)JNIHandles::make_local(env, create_results(THREAD));
+  }
+
+  void add_oop(oop o) {
+    // Value might be oop, but JLS can't see as Object, just iterate through it...
+    if (o != NULL && o->is_inline_type()) {
+      o->oop_iterate(this);
+    } else {
+      array->append(Handle(Thread::current(), o));
+    }
+  }
+
+  void do_oop(oop* o) { add_oop(*o); }
+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }
+};
+
+
+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))
+  ResourceMark rm(THREAD);
+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);
+  CollectOops collectOops;
+  collectOops.array = array;
+
+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);
+
+  return collectOops.create_jni_result(env, THREAD);
+WB_END
+
+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))
+  ResourceMark rm(THREAD);
+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);
+  CollectOops collectOops;
+  collectOops.array = array;
+  StackFrameStream sfs(thread);
+  while (depth > 0) { // Skip the native WB API frame
+    sfs.next();
+    frame* f = sfs.current();
+    f->oops_do(&collectOops, NULL, sfs.register_map());
+    depth--;
+  }
+  return collectOops.create_jni_result(env, THREAD);
+WB_END
+
+
 WB_ENTRY(void, WB_ClearInlineCaches(JNIEnv* env, jobject wb, jboolean preserve_static_stubs))
   VM_ClearICs clear_ics(preserve_static_stubs == JNI_TRUE);
   VMThread::execute(&clear_ics);
 WB_END
 
@@ -2475,10 +2569,16 @@
   {CC"getConstantPoolCacheLength0", CC"(Ljava/lang/Class;)I",  (void*)&WB_GetConstantPoolCacheLength},
   {CC"remapInstructionOperandFromCPCache0",
       CC"(Ljava/lang/Class;I)I",                      (void*)&WB_ConstantPoolRemapInstructionOperandFromCache},
   {CC"encodeConstantPoolIndyIndex0",
       CC"(I)I",                      (void*)&WB_ConstantPoolEncodeIndyIndex},
+  {CC"getObjectsViaKlassOopMaps0",
+      CC"(Ljava/lang/Object;)[Ljava/lang/Object;",    (void*)&WB_getObjectsViaKlassOopMaps},
+  {CC"getObjectsViaOopIterator0",
+          CC"(Ljava/lang/Object;)[Ljava/lang/Object;",(void*)&WB_getObjectsViaOopIterator},
+  {CC"getObjectsViaFrameOopIterator",
+      CC"(I)[Ljava/lang/Object;",                     (void*)&WB_getObjectsViaFrameOopIterator},
   {CC"getMethodBooleanOption",
       CC"(Ljava/lang/reflect/Executable;Ljava/lang/String;)Ljava/lang/Boolean;",
                                                       (void*)&WB_GetMethodBooleaneOption},
   {CC"getMethodIntxOption",
       CC"(Ljava/lang/reflect/Executable;Ljava/lang/String;)Ljava/lang/Long;",
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -2153,10 +2153,20 @@
   }
 #endif
 
   status = status && GCArguments::check_args_consistency();
 
+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {
+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);
+    warning("InlineTypePassFieldsAsArgs is not supported on this platform");
+  }
+
+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {
+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);
+    warning("InlineTypeReturnedAsFields is not supported on this platform");
+  }
+
   return status;
 }
 
 bool Arguments::is_bad_option(const JavaVMOption* option, jboolean ignore,
   const char* option_type) {
@@ -3075,10 +3085,28 @@
     } else if (is_bad_option(option, args->ignoreUnrecognized)) {
       return JNI_ERR;
     }
   }
 
+  if (EnableValhalla) {
+    // create_property("valhalla.enableValhalla", "true", InternalProperty)
+    const char* prop_name = "valhalla.enableValhalla";
+    const char* prop_value = "true";
+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;
+    char* property = AllocateHeap(prop_len, mtArguments);
+    int ret = jio_snprintf(property, prop_len, "%s=%s", prop_name, prop_value);
+    if (ret < 0 || ret >= (int)prop_len) {
+      FreeHeap(property);
+      return JNI_ENOMEM;
+    }
+    bool added = add_property(property, UnwriteableProperty, InternalProperty);
+    FreeHeap(property);
+    if (!added) {
+      return JNI_ENOMEM;
+    }
+  }
+
   // PrintSharedArchiveAndExit will turn on
   //   -Xshare:on
   //   -Xlog:class+path=info
   if (PrintSharedArchiveAndExit) {
     if (FLAG_SET_CMDLINE(UseSharedSpaces, true) != JVMFlag::SUCCESS) {
@@ -4164,10 +4192,15 @@
   // verification is not as if both were enabled.
   if (BytecodeVerificationLocal && !BytecodeVerificationRemote) {
     log_info(verification)("Turning on remote verification because local verification is on");
     FLAG_SET_DEFAULT(BytecodeVerificationRemote, true);
   }
+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {
+    // Disable calling convention optimizations if inline types are not supported
+    InlineTypePassFieldsAsArgs = false;
+    InlineTypeReturnedAsFields = false;
+  }
 
 #ifndef PRODUCT
   if (!LogVMOutput && FLAG_IS_DEFAULT(LogVMOutput)) {
     if (use_vm_log()) {
       LogVMOutput = true;
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -760,10 +760,28 @@
           "Use SSE2 MOVQ instruction for Arraycopy")                        \
                                                                             \
   notproduct(bool, PrintFieldLayout, false,                                 \
           "Print field layout for each class")                              \
                                                                             \
+  notproduct(bool, PrintInlineLayout, false,                                \
+          "Print field layout for each inline type")                        \
+                                                                            \
+  notproduct(bool, PrintFlatArrayLayout, false,                             \
+          "Print array layout for each inline type array")                  \
+                                                                            \
+  product(intx, FlatArrayElementMaxSize, -1,                                \
+          "Max size for flattening inline array elements, <0 no limit")     \
+                                                                            \
+  product(intx, InlineFieldMaxFlatSize, 128,                                \
+          "Max size for flattening inline type fields, <0 no limit")        \
+                                                                            \
+  product(intx, FlatArrayElementMaxOops, 4,                                 \
+          "Max nof embedded object references in an inline type to flatten, <0 no limit")  \
+                                                                            \
+  product(bool, InlineArrayAtomicAccess, false,                             \
+          "Atomic inline array accesses by-default, for all inline arrays") \
+                                                                            \
   /* Need to limit the extent of the padding to reasonable size.          */\
   /* 8K is well beyond the reasonable HW cache line size, even with       */\
   /* aggressive prefetching, while still leaving the room for segregating */\
   /* among the distinct pages.                                            */\
   product(intx, ContendedPaddingWidth, 128,                                 \
@@ -775,11 +793,11 @@
           "Enable @Contended annotation support")                           \
                                                                             \
   product(bool, RestrictContended, true,                                    \
           "Restrict @Contended to trusted classes")                         \
                                                                             \
-  product(bool, UseBiasedLocking, false,                                    \
+  product(bool, UseBiasedLocking, true,                                     \
           "(Deprecated) Enable biased locking in JVM")                      \
                                                                             \
   product(intx, BiasedLockingStartupDelay, 0,                               \
           "(Deprecated) Number of milliseconds to wait before enabling "    \
           "biased locking")                                                 \
@@ -2450,16 +2468,37 @@
           "Start flight recording with options"))                           \
                                                                             \
   experimental(bool, UseFastUnorderedTimeStamps, false,                     \
           "Use platform unstable time where supported for timestamps only") \
                                                                             \
+  product(bool, EnableValhalla, true,                                       \
+          "Enable experimental Valhalla features")                          \
+                                                                            \
+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \
+          "Pass each inline type field as an argument at calls")            \
+                                                                            \
+  product_pd(bool, InlineTypeReturnedAsFields,                              \
+          "Return fields instead of an inline type reference")              \
+                                                                            \
+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \
+          "Stress return of fields instead of an inline type reference")    \
+                                                                            \
+  develop(bool, ScalarizeInlineTypes, true,                                 \
+          "Scalarize inline types in compiled code")                        \
+                                                                            \
+  diagnostic(ccstrlist, ForceNonTearable, "",                               \
+          "List of inline classes which are forced to be atomic "           \
+          "(whitespace and commas separate names, "                         \
+          "and leading and trailing stars '*' are wildcards)")              \
+                                                                            \
   product(bool, UseEmptySlotsInSupers, true,                                \
                 "Allow allocating fields in empty slots of super-classes")  \
                                                                             \
   diagnostic(bool, DeoptimizeNMethodBarriersALot, false,                    \
                 "Make nmethod barriers deoptimise a lot.")                  \
 
+
 // Interface macros
 #define DECLARE_PRODUCT_FLAG(type, name, value, doc)      extern "C" type name;
 #define DECLARE_PD_PRODUCT_FLAG(type, name, doc)          extern "C" type name;
 #define DECLARE_DIAGNOSTIC_FLAG(type, name, value, doc)   extern "C" type name;
 #define DECLARE_PD_DIAGNOSTIC_FLAG(type, name, doc)       extern "C" type name;
diff a/src/hotspot/share/runtime/init.cpp b/src/hotspot/share/runtime/init.cpp
--- a/src/hotspot/share/runtime/init.cpp
+++ b/src/hotspot/share/runtime/init.cpp
@@ -110,21 +110,21 @@
   bytecodes_init();
   classLoader_init1();
   compilationPolicy_init();
   codeCache_init();
   VM_Version_init();
+  VMRegImpl::set_regName();  // need this before generate_stubs (for printing oop maps).
   stubRoutines_init1();
   jint status = universe_init();  // dependent on codeCache_init and
                                   // stubRoutines_init1 and metaspace_init.
   if (status != JNI_OK)
     return status;
 
   gc_barrier_stubs_init();  // depends on universe_init, must be before interpreter_init
   interpreter_init_stub();  // before methods get loaded
   accessFlags_init();
   InterfaceSupport_init();
-  VMRegImpl::set_regName(); // need this before generate_stubs (for printing oop maps).
   SharedRuntime::generate_stubs();
   universe2_init();  // dependent on codeCache_init and stubRoutines_init1
   javaClasses_init();// must happen after vtable initialization, before referenceProcessor_init
   interpreter_init_code();  // after javaClasses_init and before any method gets linked
   referenceProcessor_init();
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -45,10 +45,11 @@
 #include "logging/logStream.hpp"
 #include "memory/resourceArea.hpp"
 #include "memory/universe.hpp"
 #include "oops/oop.inline.hpp"
 #include "oops/symbol.hpp"
+#include "oops/inlineKlass.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/deoptimization.hpp"
 #include "runtime/frame.inline.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/interfaceSupport.inline.hpp"
@@ -992,29 +993,56 @@
   // has already had the effect of causing the return to occur, so the execution
   // will continue immediately after the call. In addition, the oopmap at the
   // return point does not mark the return value as an oop (if it is), so
   // it needs a handle here to be updated.
   if( nm->is_at_poll_return(real_return_addr) ) {
+    ResourceMark rm;
     // See if return type is an oop.
-    bool return_oop = nm->method()->is_returning_oop();
-    Handle return_value;
+    Method* method = nm->method();
+    bool return_oop = method->is_returning_oop();
+
+    GrowableArray<Handle> return_values;
+    InlineKlass* vk = NULL;
+
+    if (return_oop && InlineTypeReturnedAsFields) {
+      SignatureStream ss(method->signature());
+      while (!ss.at_return_type()) {
+        ss.next();
+      }
+      if (ss.type() == T_INLINE_TYPE) {
+        // Check if inline type is returned as fields
+        vk = InlineKlass::returned_inline_klass(map);
+        if (vk != NULL) {
+          // We're at a safepoint at the return of a method that returns
+          // multiple values. We must make sure we preserve the oop values
+          // across the safepoint.
+          assert(vk == method->returned_inline_type(thread()), "bad inline klass");
+          vk->save_oop_fields(map, return_values);
+          return_oop = false;
+        }
+      }
+    }
+
     if (return_oop) {
       // The oop result has been saved on the stack together with all
       // the other registers. In order to preserve it over GCs we need
       // to keep it in a handle.
       oop result = caller_fr.saved_oop_result(&map);
       assert(oopDesc::is_oop_or_null(result), "must be oop");
-      return_value = Handle(thread(), result);
+      return_values.push(Handle(thread(), result));
       assert(Universe::heap()->is_in_or_null(result), "must be heap pointer");
     }
 
     // Block the thread
     SafepointMechanism::block_if_requested(thread());
 
     // restore oop result, if any
     if (return_oop) {
-      caller_fr.set_saved_oop_result(&map, return_value());
+      assert(return_values.length() == 1, "only one return value");
+      caller_fr.set_saved_oop_result(&map, return_values.pop()());
+    } else if (vk != NULL) {
+      vk->restore_oop_results(map, return_values);
     }
   }
 
   // This is a safepoint poll. Verify the return address and block.
   else {
diff a/src/hotspot/share/runtime/synchronizer.cpp b/src/hotspot/share/runtime/synchronizer.cpp
--- a/src/hotspot/share/runtime/synchronizer.cpp
+++ b/src/hotspot/share/runtime/synchronizer.cpp
@@ -157,10 +157,23 @@
   int _wait_count;    // # on wait_list
   DEFINE_PAD_MINUS_SIZE(7, OM_CACHE_LINE_SIZE, sizeof(int));
 };
 static ObjectMonitorListGlobals om_list_globals;
 
+#define CHECK_THROW_NOSYNC_IMSE(obj)  \
+  if ((obj)->mark().is_always_locked()) {  \
+    ResourceMark rm(THREAD);                \
+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \
+  }
+
+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \
+    if ((obj)->mark().is_always_locked()) {  \
+    ResourceMark rm(THREAD);                  \
+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \
+  }
+
+
 #define CHAINMARKER (cast_to_oop<intptr_t>(-1))
 
 
 // =====================> Spin-lock functions
 
@@ -434,10 +447,11 @@
   assert(!SafepointSynchronize::is_at_safepoint(), "invariant");
   assert(self->is_Java_thread(), "invariant");
   assert(((JavaThread *) self)->thread_state() == _thread_in_Java, "invariant");
   NoSafepointVerifier nsv;
   if (obj == NULL) return false;  // slow-path for invalid obj
+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), "monitor op on inline type");
   const markWord mark = obj->mark();
 
   if (mark.has_locker() && self->is_lock_owned((address)mark.locker())) {
     // Degenerate notify
     // stack-locked by caller so by definition the implied waitset is empty.
@@ -484,11 +498,11 @@
   assert(!SafepointSynchronize::is_at_safepoint(), "invariant");
   assert(self->is_Java_thread(), "invariant");
   assert(((JavaThread *) self)->thread_state() == _thread_in_Java, "invariant");
   NoSafepointVerifier nsv;
   if (obj == NULL) return false;       // Need to throw NPE
-
+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), "monitor op on inline type");
   const markWord mark = obj->mark();
 
   if (mark.has_monitor()) {
     ObjectMonitor* const m = mark.monitor();
     // An async deflation can race us before we manage to make the
@@ -542,10 +556,11 @@
 // The interpreter and compiler assembly code tries to lock using the fast path
 // of this algorithm. Make sure to update that code if the following function is
 // changed. The implementation is extremely sensitive to race condition. Be careful.
 
 void ObjectSynchronizer::enter(Handle obj, BasicLock* lock, TRAPS) {
+  CHECK_THROW_NOSYNC_IMSE(obj);
   if (UseBiasedLocking) {
     if (!SafepointSynchronize::is_at_safepoint()) {
       BiasedLocking::revoke(obj, THREAD);
     } else {
       BiasedLocking::revoke_at_safepoint(obj);
@@ -587,10 +602,14 @@
   }
 }
 
 void ObjectSynchronizer::exit(oop object, BasicLock* lock, TRAPS) {
   markWord mark = object->mark();
+  if (EnableValhalla && mark.is_always_locked()) {
+    return;
+  }
+  assert(!EnableValhalla || !object->klass()->is_inline_klass(), "monitor op on inline type");
   // We cannot check for Biased Locking if we are racing an inflation.
   assert(mark == markWord::INFLATING() ||
          !mark.has_bias_pattern(), "should not see bias pattern here");
 
   markWord dhw = lock->displaced_header();
@@ -650,10 +669,11 @@
 //  3) when notified on lock2, unlock lock2
 //  4) reenter lock1 with original recursion count
 //  5) lock lock2
 // NOTE: must use heavy weight monitor to handle complete_exit/reenter()
 intx ObjectSynchronizer::complete_exit(Handle obj, TRAPS) {
+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), "monitor op on inline type");
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
 
@@ -664,10 +684,11 @@
   return ret_code;
 }
 
 // NOTE: must use heavy weight monitor to handle complete_exit/reenter()
 void ObjectSynchronizer::reenter(Handle obj, intx recursions, TRAPS) {
+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), "monitor op on inline type");
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
 
@@ -686,10 +707,11 @@
 // -----------------------------------------------------------------------------
 // JNI locks on java objects
 // NOTE: must use heavy weight monitor to handle jni monitor enter
 void ObjectSynchronizer::jni_enter(Handle obj, TRAPS) {
   // the current locking is from JNI instead of Java code
+  CHECK_THROW_NOSYNC_IMSE(obj);
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
   THREAD->set_current_pending_monitor_is_from_java(false);
@@ -705,10 +727,11 @@
   THREAD->set_current_pending_monitor_is_from_java(true);
 }
 
 // NOTE: must use heavy weight monitor to handle jni monitor exit
 void ObjectSynchronizer::jni_exit(oop obj, Thread* THREAD) {
+  CHECK_THROW_NOSYNC_IMSE(obj);
   if (UseBiasedLocking) {
     Handle h_obj(THREAD, obj);
     BiasedLocking::revoke(h_obj, THREAD);
     obj = h_obj();
   }
@@ -748,10 +771,11 @@
 
 // -----------------------------------------------------------------------------
 //  Wait/Notify/NotifyAll
 // NOTE: must use heavy weight monitor to handle wait()
 int ObjectSynchronizer::wait(Handle obj, jlong millis, TRAPS) {
+  CHECK_THROW_NOSYNC_IMSE_0(obj);
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
   if (millis < 0) {
@@ -772,10 +796,11 @@
   int ret_code = dtrace_waited_probe(monitor, obj, THREAD);
   return ret_code;
 }
 
 void ObjectSynchronizer::wait_uninterruptibly(Handle obj, jlong millis, TRAPS) {
+  CHECK_THROW_NOSYNC_IMSE(obj);
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
   if (millis < 0) {
@@ -787,10 +812,11 @@
   ObjectMonitor* monitor = inflate(THREAD, obj(), inflate_cause_wait);
   monitor->wait(millis, false, THREAD);
 }
 
 void ObjectSynchronizer::notify(Handle obj, TRAPS) {
+  CHECK_THROW_NOSYNC_IMSE(obj);
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
 
@@ -804,10 +830,11 @@
   monitor->notify(THREAD);
 }
 
 // NOTE: see comment of notify()
 void ObjectSynchronizer::notifyall(Handle obj, TRAPS) {
+  CHECK_THROW_NOSYNC_IMSE(obj);
   if (UseBiasedLocking) {
     BiasedLocking::revoke(obj, THREAD);
     assert(!obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
 
@@ -978,10 +1005,14 @@
   assert(value != markWord::no_hash, "invariant");
   return value;
 }
 
 intptr_t ObjectSynchronizer::FastHashCode(Thread* self, oop obj) {
+  if (EnableValhalla && obj->klass()->is_inline_klass()) {
+    // VM should be calling bootstrap method
+    ShouldNotReachHere();
+  }
   if (UseBiasedLocking) {
     // NOTE: many places throughout the JVM do not expect a safepoint
     // to be taken here. However, we only ever bias Java instances and all
     // of the call sites of identity_hash that might revoke biases have
     // been checked to make sure they can handle a safepoint. The
@@ -1119,19 +1150,16 @@
     // We finally get the hash.
     return hash;
   }
 }
 
-// Deprecated -- use FastHashCode() instead.
-
-intptr_t ObjectSynchronizer::identity_hash_value_for(Handle obj) {
-  return FastHashCode(Thread::current(), obj());
-}
-
 
 bool ObjectSynchronizer::current_thread_holds_lock(JavaThread* thread,
                                                    Handle h_obj) {
+  if (EnableValhalla && h_obj->mark().is_always_locked()) {
+    return false;
+  }
   if (UseBiasedLocking) {
     BiasedLocking::revoke(h_obj, thread);
     assert(!h_obj->mark().has_bias_pattern(), "biases should be revoked by now");
   }
 
@@ -1780,10 +1808,14 @@
   // Inflate mutates the heap ...
   // Relaxing assertion for bug 6320749.
   assert(Universe::verify_in_progress() ||
          !SafepointSynchronize::is_at_safepoint(), "invariant");
 
+  if (EnableValhalla) {
+    guarantee(!object->klass()->is_inline_klass(), "Attempt to inflate inline type");
+  }
+
   EventJavaMonitorInflate event;
 
   for (;;) {
     const markWord mark = object->mark();
     assert(!mark.has_bias_pattern(), "invariant");
diff a/src/hotspot/share/runtime/synchronizer.hpp b/src/hotspot/share/runtime/synchronizer.hpp
--- a/src/hotspot/share/runtime/synchronizer.hpp
+++ b/src/hotspot/share/runtime/synchronizer.hpp
@@ -105,11 +105,11 @@
   static void inflate_helper(oop obj);
   static const char* inflate_cause_name(const InflateCause cause);
 
   // Returns the identity hash value for an oop
   // NOTE: It may cause monitor inflation
-  static intptr_t identity_hash_value_for(Handle obj);
+  static intptr_t FastHashCode(Thread* self, oop obj);
   static intptr_t FastHashCode(Thread* self, oop obj);
 
   // java.lang.Thread support
   static bool current_thread_holds_lock(JavaThread* thread, Handle h_obj);
   static LockOwnership query_lock_ownership(JavaThread* self, Handle h_obj);
diff a/src/hotspot/share/runtime/vmOperations.cpp b/src/hotspot/share/runtime/vmOperations.cpp
--- a/src/hotspot/share/runtime/vmOperations.cpp
+++ b/src/hotspot/share/runtime/vmOperations.cpp
@@ -502,6 +502,10 @@
 
 #if INCLUDE_SERVICES
 void VM_PrintClassHierarchy::doit() {
   KlassHierarchy::print_class_hierarchy(_out, _print_interfaces, _print_subclasses, _classname);
 }
+
+void VM_PrintClassLayout::doit() {
+  PrintClassLayout::print_class_layout(_out, _class_name);
+}
 #endif
diff a/src/hotspot/share/runtime/vmOperations.hpp b/src/hotspot/share/runtime/vmOperations.hpp
--- a/src/hotspot/share/runtime/vmOperations.hpp
+++ b/src/hotspot/share/runtime/vmOperations.hpp
@@ -113,10 +113,11 @@
   template(ICBufferFull)                          \
   template(ScavengeMonitors)                      \
   template(PrintMetadata)                         \
   template(GTestExecuteAtSafepoint)               \
   template(JFROldObject)                          \
+  template(ClassPrintLayout)                      \
 
 class VM_Operation : public StackObj {
  public:
   enum VMOp_Type {
     VM_OPS_DO(VM_OP_ENUM)
@@ -414,10 +415,20 @@
   VM_PrintCompileQueue(outputStream* st) : _out(st) {}
   VMOp_Type type() const { return VMOp_PrintCompileQueue; }
   void doit();
 };
 
+class VM_PrintClassLayout: public VM_Operation {
+ private:
+  outputStream* _out;
+  char* _class_name;
+ public:
+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}
+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }
+  void doit();
+};
+
 #if INCLUDE_SERVICES
 class VM_PrintClassHierarchy: public VM_Operation {
  private:
   outputStream* _out;
   bool _print_interfaces;
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -2394,10 +2394,16 @@
          *                              <a href="MethodHandles.Lookup.html#secmgr">refuses access</a>
          * @throws NullPointerException if any argument is null
          */
         public MethodHandle findStatic(Class<?> refc, String name, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             MemberName method = resolveOrFail(REF_invokeStatic, refc, name, type);
+            // resolveOrFail could return a non-static <init> method if present
+            // detect and throw NSME before producing a MethodHandle
+            if (!method.isStatic() && name.equals("<init>")) {
+                throw new NoSuchMethodException("illegal method name: " + name);
+            }
+
             return getDirectMethod(REF_invokeStatic, refc, method, findBoundCallerLookup(method));
         }
 
         /**
          * Produces a method handle for a virtual method.
@@ -2539,10 +2545,17 @@
   ProcessBuilder.class, methodType(void.class, String[].class));
 ProcessBuilder pb = (ProcessBuilder)
   MH_newProcessBuilder.invoke("x", "y", "z");
 assertEquals("[x, y, z]", pb.command().toString());
          * }</pre></blockquote>
+         *
+         * @apiNote
+         * This method does not find a static {@code <init>} factory method as it is invoked
+         * via {@code invokestatic} bytecode as opposed to {@code invokespecial} for an
+         * object constructor.  To look up static {@code <init>} factory method, use
+         * the {@link #findStatic(Class, String, MethodType) findStatic} method.
+         *
          * @param refc the class or interface from which the method is accessed
          * @param type the type of the method, with the receiver argument omitted, and a void return type
          * @return the desired method handle
          * @throws NoSuchMethodException if the constructor does not exist
          * @throws IllegalAccessException if access checking fails
@@ -2554,10 +2567,13 @@
          */
         public MethodHandle findConstructor(Class<?> refc, MethodType type) throws NoSuchMethodException, IllegalAccessException {
             if (refc.isArray()) {
                 throw new NoSuchMethodException("no constructor for array class: " + refc.getName());
             }
+            if (type.returnType() != void.class) {
+                throw new NoSuchMethodException("Constructors must have void return type: " + refc.getName());
+            }
             String name = "<init>";
             MemberName ctor = resolveOrFail(REF_newInvokeSpecial, refc, name, type);
             return getDirectConstructor(refc, ctor);
         }
 
@@ -3229,14 +3245,22 @@
          *                                is set and {@code asVarargsCollector} fails
          * @throws NullPointerException if the argument is null
          */
         public MethodHandle unreflectConstructor(Constructor<?> c) throws IllegalAccessException {
             MemberName ctor = new MemberName(c);
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructorOrStaticInitMethod());
             @SuppressWarnings("deprecation")
             Lookup lookup = c.isAccessible() ? IMPL_LOOKUP : this;
-            return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            if (ctor.isObjectConstructor()) {
+                assert(ctor.getReturnType() == void.class);
+                return lookup.getDirectConstructorNoSecurityManager(ctor.getDeclaringClass(), ctor);
+            } else {
+                // static init factory is a static method
+                assert(ctor.isMethod() && ctor.getReturnType() == ctor.getDeclaringClass() && ctor.getReferenceKind() == REF_invokeStatic);
+                assert(!MethodHandleNatives.isCallerSensitive(ctor));  // must not be caller-sensitive
+                return lookup.getDirectMethodNoSecurityManager(ctor.getReferenceKind(), ctor.getDeclaringClass(), ctor, lookup);
+            }
         }
 
         /**
          * Produces a method handle giving read access to a reflected field.
          * The type of the method handle will have a return type of the field's
@@ -3485,15 +3509,17 @@
             return caller == null || VerifyAccess.isClassAccessible(refc, caller, prevLookupClass, allowedModes);
         }
 
         /** Check name for an illegal leading "&lt;" character. */
         void checkMethodName(byte refKind, String name) throws NoSuchMethodException {
-            if (name.startsWith("<") && refKind != REF_newInvokeSpecial)
-                throw new NoSuchMethodException("illegal method name: "+name);
+            // "<init>" can only be invoked via invokespecial or it's a static init factory
+            if (name.startsWith("<") && refKind != REF_newInvokeSpecial &&
+                    !(refKind == REF_invokeStatic && name.equals("<init>"))) {
+                    throw new NoSuchMethodException("illegal method name: " + name);
+            }
         }
 
-
         /**
          * Find my trustable caller class if m is a caller sensitive method.
          * If this lookup object has full privilege access, then the caller class is the lookupClass.
          * Otherwise, if m is caller-sensitive, throw IllegalAccessException.
          */
@@ -3592,11 +3618,11 @@
         }
 
         void checkMethod(byte refKind, Class<?> refc, MemberName m) throws IllegalAccessException {
             boolean wantStatic = (refKind == REF_invokeStatic);
             String message;
-            if (m.isConstructor())
+            if (m.isObjectConstructor())
                 message = "expected a method, not a constructor";
             else if (!m.isMethod())
                 message = "expected a method";
             else if (wantStatic != m.isStatic())
                 message = wantStatic ? "expected a static method" : "expected a non-static method";
@@ -3891,11 +3917,11 @@
             return getDirectConstructorCommon(refc, ctor, checkSecurity);
         }
         /** Common code for all constructors; do not call directly except from immediately above. */
         private MethodHandle getDirectConstructorCommon(Class<?> refc, MemberName ctor,
                                                   boolean checkSecurity) throws IllegalAccessException {
-            assert(ctor.isConstructor());
+            assert(ctor.isObjectConstructor());
             checkAccess(REF_newInvokeSpecial, refc, ctor);
             // Optionally check with the security manager; this isn't needed for unreflect* calls.
             if (checkSecurity)
                 checkSecurityManager(refc, ctor);
             assert(!MethodHandleNatives.isCallerSensitive(ctor));  // maybeBindCaller not relevant here
@@ -4081,10 +4107,13 @@
      * @throws NullPointerException if the argument is null
      * @throws IllegalArgumentException if arrayClass is not an array type
      * @jvms 6.5 {@code aastore} Instruction
      */
     public static MethodHandle arrayElementSetter(Class<?> arrayClass) throws IllegalArgumentException {
+        if (arrayClass.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        }
         return MethodHandleImpl.makeArrayElementAccessor(arrayClass, MethodHandleImpl.ArrayAccess.SET);
     }
 
     /**
      * Produces a VarHandle giving access to elements of an array of type
@@ -4841,11 +4870,17 @@
      * @see MethodHandles#explicitCastArguments
      * @since 9
      */
     public static MethodHandle zero(Class<?> type) {
         Objects.requireNonNull(type);
-        return type.isPrimitive() ?  zero(Wrapper.forPrimitiveType(type), type) : zero(Wrapper.OBJECT, type);
+        if (type.isPrimitive()) {
+            return zero(Wrapper.forPrimitiveType(type), type);
+        } else if (type.isInlineClass()) {
+            throw new UnsupportedOperationException();
+        } else {
+            return zero(Wrapper.OBJECT, type);
+        }
     }
 
     private static MethodHandle identityOrVoid(Class<?> type) {
         return type == void.class ? zero(type) : identity(type);
     }
@@ -4871,11 +4906,11 @@
         return dropArguments(zero(type.returnType()), 0, type.parameterList());
     }
 
     private static final MethodHandle[] IDENTITY_MHS = new MethodHandle[Wrapper.COUNT];
     private static MethodHandle makeIdentity(Class<?> ptype) {
-        MethodType mtype = methodType(ptype, ptype);
+        MethodType mtype = MethodType.methodType(ptype, ptype);
         LambdaForm lform = LambdaForm.identityForm(BasicType.basicType(ptype));
         return MethodHandleImpl.makeIntrinsic(mtype, lform, Intrinsic.IDENTITY);
     }
 
     private static MethodHandle zero(Wrapper btw, Class<?> rtype) {
diff a/src/java.base/share/classes/module-info.java b/src/java.base/share/classes/module-info.java
--- a/src/java.base/share/classes/module-info.java
+++ b/src/java.base/share/classes/module-info.java
@@ -127,11 +127,10 @@
     exports javax.security.auth.login;
     exports javax.security.auth.spi;
     exports javax.security.auth.x500;
     exports javax.security.cert;
 
-
     // additional qualified exports may be inserted at build time
     // see make/gensrc/GenModuleInfo.gmk
 
     exports sun.invoke.util to
         jdk.compiler,
@@ -341,11 +340,10 @@
         java.logging,
         java.prefs;
     exports sun.util.resources to
         jdk.localedata;
 
-
     // the service types defined by the APIs in this module
 
     uses java.lang.System.LoggerFinder;
     uses java.net.ContentHandlerFactory;
     uses java.net.spi.URLStreamHandlerProvider;
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
@@ -39,10 +39,11 @@
 import com.sun.tools.javac.code.Scope.NamedImportScope;
 import com.sun.tools.javac.code.Scope.StarImportScope;
 import com.sun.tools.javac.code.Scope.WriteableScope;
 import com.sun.tools.javac.code.Source.Feature;
 import com.sun.tools.javac.comp.Annotate.AnnotationTypeMetadata;
+import com.sun.tools.javac.jvm.Target;
 import com.sun.tools.javac.tree.*;
 import com.sun.tools.javac.util.*;
 import com.sun.tools.javac.util.DefinedBy.Api;
 
 import com.sun.tools.javac.code.Symbol.*;
@@ -676,10 +677,11 @@
             ClassSymbol sym = tree.sym;
             ClassType ct = (ClassType)sym.type;
             // Determine supertype.
             Type supertype;
             JCExpression extending;
+            final boolean isValueType = (tree.mods.flags & Flags.VALUE) != 0;
 
             if (tree.extending != null) {
                 extending = clearTypeParams(tree.extending);
                 supertype = attr.attribBase(extending, baseEnv, true, false, true);
                 if (supertype == syms.recordType) {
@@ -727,10 +729,19 @@
             }  else {
                 ct.interfaces_field = interfaces.toList();
                 ct.all_interfaces_field = (all_interfaces == null)
                         ? ct.interfaces_field : all_interfaces.toList();
             }
+            if (ct.isValue()) {
+                ClassSymbol cSym = (ClassSymbol) ct.tsym;
+                if (cSym.projection != null) {
+                    ClassType projectedType = (ClassType) cSym.projection.type;
+                    projectedType.supertype_field = ct.supertype_field;
+                    projectedType.interfaces_field = ct.interfaces_field;
+                    projectedType.all_interfaces_field = ct.all_interfaces_field;
+                }
+            }
 
             /* it could be that there are already some symbols in the permitted list, for the case
              * where there are subtypes in the same compilation unit but the permits list is empty
              * so don't overwrite the permitted list if it is not empty
              */
@@ -1071,10 +1082,13 @@
                     tree.defs.diff(alreadyEntered) : tree.defs;
             memberEnter.memberEnter(defsToEnter, env);
             if (isRecord) {
                 addRecordMembersIfNeeded(tree, env);
             }
+            if ((tree.mods.flags & (Flags.VALUE | Flags.INTERFACE)) == Flags.VALUE && !tree.sym.type.hasTag(ERROR)) {
+                addValueMembers(tree, env);
+            }
             if (tree.sym.isAnnotationType()) {
                 Assert.check(tree.sym.isCompleted());
                 tree.sym.setAnnotationTypeMetadata(new AnnotationTypeMetadata(tree.sym, annotate.annotationTypeSourceCompleter()));
             }
         }
@@ -1145,10 +1159,59 @@
                           null,
                           null);
             memberEnter.memberEnter(valueOf, env);
         }
 
+        /** Add the implicit members for a value type to the parse tree and the symbol table.
+         */
+        private void addValueMembers(JCClassDecl tree, Env<AttrContext> env) {
+
+            boolean requireToString = true;
+
+            for (JCTree def : tree.defs) {
+                if (def.getTag() == METHODDEF) {
+                    JCMethodDecl methodDecl = (JCMethodDecl)def;
+                    if (methodDecl.sym != null
+                            && methodDecl.sym.type != null
+                            && !methodDecl.sym.type.isErroneous()
+                            && (methodDecl.sym.flags() & STATIC) == 0) {
+                        final List<Type> parameterTypes = methodDecl.sym.type.getParameterTypes();
+                        if (parameterTypes.size() == 0) {
+                            String name = methodDecl.name.toString();
+                            if (name.equals("toString")) {
+                                requireToString = false;
+                            }
+                        }
+                    }
+                }
+            }
+
+            if (requireToString) {
+                make.at(tree.pos);
+                // Make a body comprising { throw new RuntimeException(""Internal error: This method must have been replaced by javac"); }
+                JCBlock body = make.Block(Flags.SYNTHETIC, List.of(make.Throw(
+                        make.NewClass(null,
+                                null,
+                                make.Ident(names.fromString("RuntimeException")),
+                                List.of(make.Literal(CLASS, "Internal error: This method must have been replaced by javac")),
+                                null))));
+                // public String toString() { throw new RuntimeException(message); }
+                JCMethodDecl toString = make.
+                        MethodDef(make.Modifiers(Flags.PUBLIC | Flags.FINAL),
+                                names.toString,
+                                make.Ident(names.fromString("String")),
+                                List.nil(),
+                                List.nil(),
+                                List.nil(), // thrown
+                                body,
+                                null);
+                memberEnter.memberEnter(toString, env);
+                tree.defs = tree.defs.append(toString);
+            }
+
+        }
+
         JCMethodDecl getCanonicalConstructorDecl(JCClassDecl tree) {
             // let's check if there is a constructor with exactly the same arguments as the record components
             List<Type> recordComponentErasedTypes = types.erasure(TreeInfo.recordFields(tree).map(vd -> vd.sym.type));
             JCMethodDecl canonicalDecl = null;
             for (JCTree def : tree.defs) {
diff a/test/jtreg-ext/requires/VMProps.java b/test/jtreg-ext/requires/VMProps.java
--- a/test/jtreg-ext/requires/VMProps.java
+++ b/test/jtreg-ext/requires/VMProps.java
@@ -322,10 +322,11 @@
         vmOptFinalFlag(map, "ClassUnloading");
         vmOptFinalFlag(map, "ClassUnloadingWithConcurrentMark");
         vmOptFinalFlag(map, "UseCompressedOops");
         vmOptFinalFlag(map, "EnableJVMCI");
         vmOptFinalFlag(map, "EliminateAllocations");
+        vmOptFinalFlag(map, "TieredCompilation");
     }
 
     /**
      * @return "true" if VM has a serviceability agent.
      */
@@ -363,10 +364,13 @@
 
     /**
      * @return true if VM supports AOT and false otherwise
      */
     protected String vmAOT() {
+        if (WB.getBooleanVMFlag("EnableValhalla").booleanValue()) {
+            return "false";
+        }
         // builds with aot have jaotc in <JDK>/bin
         Path bin = Paths.get(System.getProperty("java.home"))
                         .resolve("bin");
         Path jaotc;
         if (Platform.isWindows()) {

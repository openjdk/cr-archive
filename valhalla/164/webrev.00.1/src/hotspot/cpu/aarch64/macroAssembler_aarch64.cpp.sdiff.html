<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIRAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;

  49 #include &quot;runtime/thread.hpp&quot;
  50 #include &quot;utilities/powerOfTwo.hpp&quot;
  51 #ifdef COMPILER1
  52 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  53 #endif
  54 #ifdef COMPILER2
  55 #include &quot;oops/oop.hpp&quot;
  56 #include &quot;opto/compile.hpp&quot;
  57 #include &quot;opto/node.hpp&quot;
  58 #include &quot;opto/output.hpp&quot;
  59 #endif
  60 
  61 #ifdef PRODUCT
  62 #define BLOCK_COMMENT(str) /* nothing */
  63 #else
  64 #define BLOCK_COMMENT(str) block_comment(str)
  65 #endif
  66 #define STOP(str) stop(str);
  67 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  68 
</pre>
<hr />
<pre>
1297   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1298   subs(zr, scratch, InstanceKlass::fully_initialized);
1299   br(Assembler::EQ, *L_fast_path);
1300 
1301   // Fast path check: current thread is initializer thread
1302   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1303   cmp(rthread, scratch);
1304 
1305   if (L_slow_path == &amp;L_fallthrough) {
1306     br(Assembler::EQ, *L_fast_path);
1307     bind(*L_slow_path);
1308   } else if (L_fast_path == &amp;L_fallthrough) {
1309     br(Assembler::NE, *L_slow_path);
1310     bind(*L_fast_path);
1311   } else {
1312     Unimplemented();
1313   }
1314 }
1315 
1316 void MacroAssembler::verify_oop(Register reg, const char* s) {
<span class="line-modified">1317   if (!VerifyOops) return;</span>




1318 
1319   // Pass register number to verify_oop_subroutine
1320   const char* b = NULL;
1321   {
1322     ResourceMark rm;
1323     stringStream ss;
1324     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1325     b = code_string(ss.as_string());
1326   }
1327   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1328 
1329   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1330   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1331 
1332   mov(r0, reg);
1333   movptr(rscratch1, (uintptr_t)(address)b);
1334 
1335   // call indirectly to solve generation ordering problem
1336   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1337   ldr(rscratch2, Address(rscratch2));
1338   blr(rscratch2);
1339 
1340   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1341   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1342 
1343   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1344 }
1345 
1346 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
<span class="line-modified">1347   if (!VerifyOops) return;</span>




1348 
1349   const char* b = NULL;
1350   {
1351     ResourceMark rm;
1352     stringStream ss;
1353     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1354     b = code_string(ss.as_string());
1355   }
1356   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1357 
1358   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1359   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1360 
1361   // addr may contain sp so we will have to adjust it based on the
1362   // pushes that we just did.
1363   if (addr.uses(sp)) {
1364     lea(r0, addr);
1365     ldr(r0, Address(r0, 4 * wordSize));
1366   } else {
1367     ldr(r0, addr);
</pre>
<hr />
<pre>
1420 
1421 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1422   pass_arg0(this, arg_0);
1423   call_VM_leaf_base(entry_point, 1);
1424 }
1425 
1426 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1427   pass_arg0(this, arg_0);
1428   pass_arg1(this, arg_1);
1429   call_VM_leaf_base(entry_point, 2);
1430 }
1431 
1432 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1433                                   Register arg_1, Register arg_2) {
1434   pass_arg0(this, arg_0);
1435   pass_arg1(this, arg_1);
1436   pass_arg2(this, arg_2);
1437   call_VM_leaf_base(entry_point, 3);
1438 }
1439 




1440 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1441   pass_arg0(this, arg_0);
1442   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1443 }
1444 
1445 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1446 
1447   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1448   pass_arg1(this, arg_1);
1449   pass_arg0(this, arg_0);
1450   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1451 }
1452 
1453 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1454   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1455   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1456   pass_arg2(this, arg_2);
1457   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1458   pass_arg1(this, arg_1);
1459   pass_arg0(this, arg_0);
</pre>
<hr />
<pre>
1469   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1470   pass_arg2(this, arg_2);
1471   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1472   pass_arg1(this, arg_1);
1473   pass_arg0(this, arg_0);
1474   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1475 }
1476 
1477 void MacroAssembler::null_check(Register reg, int offset) {
1478   if (needs_explicit_null_check(offset)) {
1479     // provoke OS NULL exception if reg = NULL by
1480     // accessing M[reg] w/o changing any registers
1481     // NOTE: this is plenty to provoke a segv
1482     ldr(zr, Address(reg));
1483   } else {
1484     // nothing to do, (later) access of M[reg + offset]
1485     // will provoke OS NULL exception if reg = NULL
1486   }
1487 }
1488 

































1489 // MacroAssembler protected routines needed to implement
1490 // public methods
1491 
1492 void MacroAssembler::mov(Register r, Address dest) {
1493   code_section()-&gt;relocate(pc(), dest.rspec());
1494   uint64_t imm64 = (uint64_t)dest.target();
1495   movptr(r, imm64);
1496 }
1497 
1498 // Move a constant pointer into r.  In AArch64 mode the virtual
1499 // address space is 48 bits in size, so we only need three
1500 // instructions to create a patchable instruction sequence that can
1501 // reach anywhere.
1502 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1503 #ifndef PRODUCT
1504   {
1505     char buffer[64];
1506     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1507     block_comment(buffer);
1508   }
</pre>
<hr />
<pre>
3700   ldr(rscratch1, Address(rscratch1, offset));
3701   cmp(src1, rscratch1);
3702 }
3703 
3704 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3705   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3706   bs-&gt;obj_equals(this, obj1, obj2);
3707 }
3708 
3709 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
3710   load_method_holder(rresult, rmethod);
3711   ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
3712 }
3713 
3714 void MacroAssembler::load_method_holder(Register holder, Register method) {
3715   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3716   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3717   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3718 }
3719 
<span class="line-modified">3720 void MacroAssembler::load_klass(Register dst, Register src) {</span>
3721   if (UseCompressedClassPointers) {
3722     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));
<span class="line-removed">3723     decode_klass_not_null(dst);</span>
3724   } else {
3725     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3726   }
3727 }
3728 










3729 // ((OopHandle)result).resolve();
3730 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3731   // OopHandle::resolve is an indirection.
3732   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3733 }
3734 
3735 // ((WeakHandle)result).resolve();
3736 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
3737   assert_different_registers(rresult, rtmp);
3738   Label resolved;
3739 
3740   // A null weak handle resolves to null.
3741   cbz(rresult, resolved);
3742 
3743   // Only 64 bit platforms support GCs that require a tmp register
3744   // Only IN_HEAP loads require a thread_tmp register
3745   // WeakHandle::resolve is an indirection like jweak.
3746   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
3747                  rresult, Address(rresult), rtmp, /*tmp_thread*/noreg);
3748   bind(resolved);
3749 }
3750 
3751 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3752   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3753   ldr(dst, Address(rmethod, Method::const_offset()));
3754   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3755   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3756   ldr(dst, Address(dst, mirror_offset));
3757   resolve_oop_handle(dst, tmp);
3758 }
3759 









3760 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3761   if (UseCompressedClassPointers) {
3762     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3763     if (CompressedKlassPointers::base() == NULL) {
3764       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3765       return;
3766     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3767                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3768       // Only the bottom 32 bits matter
3769       cmpw(trial_klass, tmp);
3770       return;
3771     }
3772     decode_klass_not_null(tmp);
3773   } else {
3774     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3775   }
3776   cmp(trial_klass, tmp);
3777 }
3778 
3779 void MacroAssembler::load_prototype_header(Register dst, Register src) {
</pre>
<hr />
<pre>
4077   narrowKlass nk = CompressedKlassPointers::encode(k);
4078   movz(dst, (nk &gt;&gt; 16), 16);
4079   movk(dst, nk &amp; 0xffff);
4080 }
4081 
4082 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4083                                     Register dst, Address src,
4084                                     Register tmp1, Register thread_tmp) {
4085   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4086   decorators = AccessInternal::decorator_fixup(decorators);
4087   bool as_raw = (decorators &amp; AS_RAW) != 0;
4088   if (as_raw) {
4089     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4090   } else {
4091     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4092   }
4093 }
4094 
4095 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4096                                      Address dst, Register src,
<span class="line-modified">4097                                      Register tmp1, Register thread_tmp) {</span>

4098   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4099   decorators = AccessInternal::decorator_fixup(decorators);
4100   bool as_raw = (decorators &amp; AS_RAW) != 0;
4101   if (as_raw) {
<span class="line-modified">4102     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);</span>
4103   } else {
<span class="line-modified">4104     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp);</span>
4105   }
4106 }
4107 
4108 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4109   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4110   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4111     decorators |= ACCESS_READ | ACCESS_WRITE;
4112   }
4113   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4114   return bs-&gt;resolve(this, decorators, obj);
4115 }
4116 
4117 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4118                                    Register thread_tmp, DecoratorSet decorators) {
4119   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4120 }
4121 
4122 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4123                                             Register thread_tmp, DecoratorSet decorators) {
4124   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4125 }
4126 
4127 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
<span class="line-modified">4128                                     Register thread_tmp, DecoratorSet decorators) {</span>
<span class="line-modified">4129   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);</span>
4130 }
4131 
4132 // Used for storing NULLs.
4133 void MacroAssembler::store_heap_oop_null(Address dst) {
<span class="line-modified">4134   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);</span>
4135 }
4136 
4137 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4138   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4139   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4140   RelocationHolder rspec = metadata_Relocation::spec(index);
4141   return Address((address)obj, rspec);
4142 }
4143 
4144 // Move an oop into a register.  immediate is true if we want
4145 // immediate instructions and nmethod entry barriers are not enabled.
4146 // i.e. we are not going to patch this instruction while the code is being
4147 // executed by another thread.
4148 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4149   int oop_index;
4150   if (obj == NULL) {
4151     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4152   } else {
4153 #ifdef ASSERT
4154     {
</pre>
<hr />
<pre>
5187 // get_thread() can be called anywhere inside generated code so we
5188 // need to save whatever non-callee save context might get clobbered
5189 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5190 // the call setup code.
5191 //
5192 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5193 //
5194 void MacroAssembler::get_thread(Register dst) {
5195   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5196   push(saved_regs, sp);
5197 
5198   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5199   blr(lr);
5200   if (dst != c_rarg0) {
5201     mov(dst, c_rarg0);
5202   }
5203 
5204   pop(saved_regs, sp);
5205 }
5206 





































































































































































































































































































































































































5207 void MacroAssembler::cache_wb(Address line) {
5208   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5209   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5210   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5211   // would like to assert this
5212   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5213   if (VM_Version::supports_dcpop()) {
5214     // writeback using clear virtual address to point of persistence
5215     dc(Assembler::CVAP, line.base());
5216   } else {
5217     // no need to generate anything as Unsafe.writebackMemory should
5218     // never invoke this stub
5219   }
5220 }
5221 
5222 void MacroAssembler::cache_wbsync(bool is_pre) {
5223   // we only need a barrier post sync
5224   if (!is_pre) {
5225     membar(Assembler::AnyAny);
5226   }
</pre>
</td>
<td>
<hr />
<pre>
  29 #include &quot;jvm.h&quot;
  30 #include &quot;asm/assembler.hpp&quot;
  31 #include &quot;asm/assembler.inline.hpp&quot;
  32 #include &quot;gc/shared/barrierSet.hpp&quot;
  33 #include &quot;gc/shared/cardTable.hpp&quot;
  34 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  35 #include &quot;gc/shared/cardTableBarrierSet.hpp&quot;
  36 #include &quot;interpreter/interpreter.hpp&quot;
  37 #include &quot;compiler/disassembler.hpp&quot;
  38 #include &quot;memory/resourceArea.hpp&quot;
  39 #include &quot;memory/universe.hpp&quot;
  40 #include &quot;nativeInst_aarch64.hpp&quot;
  41 #include &quot;oops/accessDecorators.hpp&quot;
  42 #include &quot;oops/compressedOops.inline.hpp&quot;
  43 #include &quot;oops/klass.inline.hpp&quot;
  44 #include &quot;runtime/biasedLocking.hpp&quot;
  45 #include &quot;runtime/icache.hpp&quot;
  46 #include &quot;runtime/interfaceSupport.inline.hpp&quot;
  47 #include &quot;runtime/jniHandles.inline.hpp&quot;
  48 #include &quot;runtime/sharedRuntime.hpp&quot;
<span class="line-added">  49 #include &quot;runtime/signature_cc.hpp&quot;</span>
  50 #include &quot;runtime/thread.hpp&quot;
  51 #include &quot;utilities/powerOfTwo.hpp&quot;
  52 #ifdef COMPILER1
  53 #include &quot;c1/c1_LIRAssembler.hpp&quot;
  54 #endif
  55 #ifdef COMPILER2
  56 #include &quot;oops/oop.hpp&quot;
  57 #include &quot;opto/compile.hpp&quot;
  58 #include &quot;opto/node.hpp&quot;
  59 #include &quot;opto/output.hpp&quot;
  60 #endif
  61 
  62 #ifdef PRODUCT
  63 #define BLOCK_COMMENT(str) /* nothing */
  64 #else
  65 #define BLOCK_COMMENT(str) block_comment(str)
  66 #endif
  67 #define STOP(str) stop(str);
  68 #define BIND(label) bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  69 
</pre>
<hr />
<pre>
1298   ldrb(scratch, Address(klass, InstanceKlass::init_state_offset()));
1299   subs(zr, scratch, InstanceKlass::fully_initialized);
1300   br(Assembler::EQ, *L_fast_path);
1301 
1302   // Fast path check: current thread is initializer thread
1303   ldr(scratch, Address(klass, InstanceKlass::init_thread_offset()));
1304   cmp(rthread, scratch);
1305 
1306   if (L_slow_path == &amp;L_fallthrough) {
1307     br(Assembler::EQ, *L_fast_path);
1308     bind(*L_slow_path);
1309   } else if (L_fast_path == &amp;L_fallthrough) {
1310     br(Assembler::NE, *L_slow_path);
1311     bind(*L_fast_path);
1312   } else {
1313     Unimplemented();
1314   }
1315 }
1316 
1317 void MacroAssembler::verify_oop(Register reg, const char* s) {
<span class="line-modified">1318   if (!VerifyOops || VerifyAdapterSharing) {</span>
<span class="line-added">1319     // Below address of the code string confuses VerifyAdapterSharing</span>
<span class="line-added">1320     // because it may differ between otherwise equivalent adapters.</span>
<span class="line-added">1321     return;</span>
<span class="line-added">1322   }</span>
1323 
1324   // Pass register number to verify_oop_subroutine
1325   const char* b = NULL;
1326   {
1327     ResourceMark rm;
1328     stringStream ss;
1329     ss.print(&quot;verify_oop: %s: %s&quot;, reg-&gt;name(), s);
1330     b = code_string(ss.as_string());
1331   }
1332   BLOCK_COMMENT(&quot;verify_oop {&quot;);
1333 
1334   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1335   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1336 
1337   mov(r0, reg);
1338   movptr(rscratch1, (uintptr_t)(address)b);
1339 
1340   // call indirectly to solve generation ordering problem
1341   lea(rscratch2, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()));
1342   ldr(rscratch2, Address(rscratch2));
1343   blr(rscratch2);
1344 
1345   ldp(rscratch2, lr, Address(post(sp, 2 * wordSize)));
1346   ldp(r0, rscratch1, Address(post(sp, 2 * wordSize)));
1347 
1348   BLOCK_COMMENT(&quot;} verify_oop&quot;);
1349 }
1350 
1351 void MacroAssembler::verify_oop_addr(Address addr, const char* s) {
<span class="line-modified">1352   if (!VerifyOops || VerifyAdapterSharing) {</span>
<span class="line-added">1353     // Below address of the code string confuses VerifyAdapterSharing</span>
<span class="line-added">1354     // because it may differ between otherwise equivalent adapters.</span>
<span class="line-added">1355     return;</span>
<span class="line-added">1356   }</span>
1357 
1358   const char* b = NULL;
1359   {
1360     ResourceMark rm;
1361     stringStream ss;
1362     ss.print(&quot;verify_oop_addr: %s&quot;, s);
1363     b = code_string(ss.as_string());
1364   }
1365   BLOCK_COMMENT(&quot;verify_oop_addr {&quot;);
1366 
1367   stp(r0, rscratch1, Address(pre(sp, -2 * wordSize)));
1368   stp(rscratch2, lr, Address(pre(sp, -2 * wordSize)));
1369 
1370   // addr may contain sp so we will have to adjust it based on the
1371   // pushes that we just did.
1372   if (addr.uses(sp)) {
1373     lea(r0, addr);
1374     ldr(r0, Address(r0, 4 * wordSize));
1375   } else {
1376     ldr(r0, addr);
</pre>
<hr />
<pre>
1429 
1430 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0) {
1431   pass_arg0(this, arg_0);
1432   call_VM_leaf_base(entry_point, 1);
1433 }
1434 
1435 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1436   pass_arg0(this, arg_0);
1437   pass_arg1(this, arg_1);
1438   call_VM_leaf_base(entry_point, 2);
1439 }
1440 
1441 void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0,
1442                                   Register arg_1, Register arg_2) {
1443   pass_arg0(this, arg_0);
1444   pass_arg1(this, arg_1);
1445   pass_arg2(this, arg_2);
1446   call_VM_leaf_base(entry_point, 3);
1447 }
1448 
<span class="line-added">1449 void MacroAssembler::super_call_VM_leaf(address entry_point) {</span>
<span class="line-added">1450   MacroAssembler::call_VM_leaf_base(entry_point, 1);</span>
<span class="line-added">1451 }</span>
<span class="line-added">1452 </span>
1453 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0) {
1454   pass_arg0(this, arg_0);
1455   MacroAssembler::call_VM_leaf_base(entry_point, 1);
1456 }
1457 
1458 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1) {
1459 
1460   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1461   pass_arg1(this, arg_1);
1462   pass_arg0(this, arg_0);
1463   MacroAssembler::call_VM_leaf_base(entry_point, 2);
1464 }
1465 
1466 void MacroAssembler::super_call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2) {
1467   assert(arg_0 != c_rarg2, &quot;smashed arg&quot;);
1468   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1469   pass_arg2(this, arg_2);
1470   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1471   pass_arg1(this, arg_1);
1472   pass_arg0(this, arg_0);
</pre>
<hr />
<pre>
1482   assert(arg_1 != c_rarg2, &quot;smashed arg&quot;);
1483   pass_arg2(this, arg_2);
1484   assert(arg_0 != c_rarg1, &quot;smashed arg&quot;);
1485   pass_arg1(this, arg_1);
1486   pass_arg0(this, arg_0);
1487   MacroAssembler::call_VM_leaf_base(entry_point, 4);
1488 }
1489 
1490 void MacroAssembler::null_check(Register reg, int offset) {
1491   if (needs_explicit_null_check(offset)) {
1492     // provoke OS NULL exception if reg = NULL by
1493     // accessing M[reg] w/o changing any registers
1494     // NOTE: this is plenty to provoke a segv
1495     ldr(zr, Address(reg));
1496   } else {
1497     // nothing to do, (later) access of M[reg + offset]
1498     // will provoke OS NULL exception if reg = NULL
1499   }
1500 }
1501 
<span class="line-added">1502 void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label&amp; is_value) {</span>
<span class="line-added">1503   ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));</span>
<span class="line-added">1504   andr(temp_reg, temp_reg, JVM_ACC_INLINE);</span>
<span class="line-added">1505   cbnz(temp_reg, is_value);</span>
<span class="line-added">1506 }</span>
<span class="line-added">1507 </span>
<span class="line-added">1508 void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label&amp; is_inline) {</span>
<span class="line-added">1509   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1510   tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);</span>
<span class="line-added">1511 }</span>
<span class="line-added">1512 </span>
<span class="line-added">1513 void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label&amp; not_inline) {</span>
<span class="line-added">1514   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1515   tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);</span>
<span class="line-added">1516 }</span>
<span class="line-added">1517 </span>
<span class="line-added">1518 void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label&amp; is_flattened) {</span>
<span class="line-added">1519   (void) temp_reg; // keep signature uniform with x86</span>
<span class="line-added">1520   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);</span>
<span class="line-added">1521 }</span>
<span class="line-added">1522 </span>
<span class="line-added">1523 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label&amp; is_flattened_array) {</span>
<span class="line-added">1524   load_storage_props(temp_reg, oop);</span>
<span class="line-added">1525   andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);</span>
<span class="line-added">1526   cbnz(temp_reg, is_flattened_array);</span>
<span class="line-added">1527 }</span>
<span class="line-added">1528 </span>
<span class="line-added">1529 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp; is_null_free_array) {</span>
<span class="line-added">1530   load_storage_props(temp_reg, oop);</span>
<span class="line-added">1531   andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);</span>
<span class="line-added">1532   cbnz(temp_reg, is_null_free_array);</span>
<span class="line-added">1533 }</span>
<span class="line-added">1534 </span>
1535 // MacroAssembler protected routines needed to implement
1536 // public methods
1537 
1538 void MacroAssembler::mov(Register r, Address dest) {
1539   code_section()-&gt;relocate(pc(), dest.rspec());
1540   uint64_t imm64 = (uint64_t)dest.target();
1541   movptr(r, imm64);
1542 }
1543 
1544 // Move a constant pointer into r.  In AArch64 mode the virtual
1545 // address space is 48 bits in size, so we only need three
1546 // instructions to create a patchable instruction sequence that can
1547 // reach anywhere.
1548 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1549 #ifndef PRODUCT
1550   {
1551     char buffer[64];
1552     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1553     block_comment(buffer);
1554   }
</pre>
<hr />
<pre>
3746   ldr(rscratch1, Address(rscratch1, offset));
3747   cmp(src1, rscratch1);
3748 }
3749 
3750 void MacroAssembler::cmpoop(Register obj1, Register obj2) {
3751   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
3752   bs-&gt;obj_equals(this, obj1, obj2);
3753 }
3754 
3755 void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {
3756   load_method_holder(rresult, rmethod);
3757   ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));
3758 }
3759 
3760 void MacroAssembler::load_method_holder(Register holder, Register method) {
3761   ldr(holder, Address(method, Method::const_offset()));                      // ConstMethod*
3762   ldr(holder, Address(holder, ConstMethod::constants_offset()));             // ConstantPool*
3763   ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); // InstanceKlass*
3764 }
3765 
<span class="line-modified">3766 void MacroAssembler::load_metadata(Register dst, Register src) {</span>
3767   if (UseCompressedClassPointers) {
3768     ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));

3769   } else {
3770     ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));
3771   }
3772 }
3773 
<span class="line-added">3774 void MacroAssembler::load_klass(Register dst, Register src) {</span>
<span class="line-added">3775   load_metadata(dst, src);</span>
<span class="line-added">3776   if (UseCompressedClassPointers) {</span>
<span class="line-added">3777     andr(dst, dst, oopDesc::compressed_klass_mask());</span>
<span class="line-added">3778     decode_klass_not_null(dst);</span>
<span class="line-added">3779   } else {</span>
<span class="line-added">3780     ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);</span>
<span class="line-added">3781   }</span>
<span class="line-added">3782 }</span>
<span class="line-added">3783 </span>
3784 // ((OopHandle)result).resolve();
3785 void MacroAssembler::resolve_oop_handle(Register result, Register tmp) {
3786   // OopHandle::resolve is an indirection.
3787   access_load_at(T_OBJECT, IN_NATIVE, result, Address(result, 0), tmp, noreg);
3788 }
3789 
3790 // ((WeakHandle)result).resolve();
3791 void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {
3792   assert_different_registers(rresult, rtmp);
3793   Label resolved;
3794 
3795   // A null weak handle resolves to null.
3796   cbz(rresult, resolved);
3797 
3798   // Only 64 bit platforms support GCs that require a tmp register
3799   // Only IN_HEAP loads require a thread_tmp register
3800   // WeakHandle::resolve is an indirection like jweak.
3801   access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,
3802                  rresult, Address(rresult), rtmp, /*tmp_thread*/noreg);
3803   bind(resolved);
3804 }
3805 
3806 void MacroAssembler::load_mirror(Register dst, Register method, Register tmp) {
3807   const int mirror_offset = in_bytes(Klass::java_mirror_offset());
3808   ldr(dst, Address(rmethod, Method::const_offset()));
3809   ldr(dst, Address(dst, ConstMethod::constants_offset()));
3810   ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));
3811   ldr(dst, Address(dst, mirror_offset));
3812   resolve_oop_handle(dst, tmp);
3813 }
3814 
<span class="line-added">3815 void MacroAssembler::load_storage_props(Register dst, Register src) {</span>
<span class="line-added">3816   load_metadata(dst, src);</span>
<span class="line-added">3817   if (UseCompressedClassPointers) {</span>
<span class="line-added">3818     asrw(dst, dst, oopDesc::narrow_storage_props_shift);</span>
<span class="line-added">3819   } else {</span>
<span class="line-added">3820     asr(dst, dst, oopDesc::wide_storage_props_shift);</span>
<span class="line-added">3821   }</span>
<span class="line-added">3822 }</span>
<span class="line-added">3823 </span>
3824 void MacroAssembler::cmp_klass(Register oop, Register trial_klass, Register tmp) {
3825   if (UseCompressedClassPointers) {
3826     ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3827     if (CompressedKlassPointers::base() == NULL) {
3828       cmp(trial_klass, tmp, LSL, CompressedKlassPointers::shift());
3829       return;
3830     } else if (((uint64_t)CompressedKlassPointers::base() &amp; 0xffffffff) == 0
3831                &amp;&amp; CompressedKlassPointers::shift() == 0) {
3832       // Only the bottom 32 bits matter
3833       cmpw(trial_klass, tmp);
3834       return;
3835     }
3836     decode_klass_not_null(tmp);
3837   } else {
3838     ldr(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));
3839   }
3840   cmp(trial_klass, tmp);
3841 }
3842 
3843 void MacroAssembler::load_prototype_header(Register dst, Register src) {
</pre>
<hr />
<pre>
4141   narrowKlass nk = CompressedKlassPointers::encode(k);
4142   movz(dst, (nk &gt;&gt; 16), 16);
4143   movk(dst, nk &amp; 0xffff);
4144 }
4145 
4146 void MacroAssembler::access_load_at(BasicType type, DecoratorSet decorators,
4147                                     Register dst, Address src,
4148                                     Register tmp1, Register thread_tmp) {
4149   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4150   decorators = AccessInternal::decorator_fixup(decorators);
4151   bool as_raw = (decorators &amp; AS_RAW) != 0;
4152   if (as_raw) {
4153     bs-&gt;BarrierSetAssembler::load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4154   } else {
4155     bs-&gt;load_at(this, decorators, type, dst, src, tmp1, thread_tmp);
4156   }
4157 }
4158 
4159 void MacroAssembler::access_store_at(BasicType type, DecoratorSet decorators,
4160                                      Address dst, Register src,
<span class="line-modified">4161                                      Register tmp1, Register thread_tmp, Register tmp3) {</span>
<span class="line-added">4162 </span>
4163   BarrierSetAssembler *bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4164   decorators = AccessInternal::decorator_fixup(decorators);
4165   bool as_raw = (decorators &amp; AS_RAW) != 0;
4166   if (as_raw) {
<span class="line-modified">4167     bs-&gt;BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);</span>
4168   } else {
<span class="line-modified">4169     bs-&gt;store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);</span>
4170   }
4171 }
4172 
4173 void MacroAssembler::resolve(DecoratorSet decorators, Register obj) {
4174   // Use stronger ACCESS_WRITE|ACCESS_READ by default.
4175   if ((decorators &amp; (ACCESS_READ | ACCESS_WRITE)) == 0) {
4176     decorators |= ACCESS_READ | ACCESS_WRITE;
4177   }
4178   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
4179   return bs-&gt;resolve(this, decorators, obj);
4180 }
4181 
4182 void MacroAssembler::load_heap_oop(Register dst, Address src, Register tmp1,
4183                                    Register thread_tmp, DecoratorSet decorators) {
4184   access_load_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);
4185 }
4186 
4187 void MacroAssembler::load_heap_oop_not_null(Register dst, Address src, Register tmp1,
4188                                             Register thread_tmp, DecoratorSet decorators) {
4189   access_load_at(T_OBJECT, IN_HEAP | IS_NOT_NULL | decorators, dst, src, tmp1, thread_tmp);
4190 }
4191 
4192 void MacroAssembler::store_heap_oop(Address dst, Register src, Register tmp1,
<span class="line-modified">4193                                     Register thread_tmp, Register tmp3, DecoratorSet decorators) {</span>
<span class="line-modified">4194   access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);</span>
4195 }
4196 
4197 // Used for storing NULLs.
4198 void MacroAssembler::store_heap_oop_null(Address dst) {
<span class="line-modified">4199   access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);</span>
4200 }
4201 
4202 Address MacroAssembler::allocate_metadata_address(Metadata* obj) {
4203   assert(oop_recorder() != NULL, &quot;this assembler needs a Recorder&quot;);
4204   int index = oop_recorder()-&gt;allocate_metadata_index(obj);
4205   RelocationHolder rspec = metadata_Relocation::spec(index);
4206   return Address((address)obj, rspec);
4207 }
4208 
4209 // Move an oop into a register.  immediate is true if we want
4210 // immediate instructions and nmethod entry barriers are not enabled.
4211 // i.e. we are not going to patch this instruction while the code is being
4212 // executed by another thread.
4213 void MacroAssembler::movoop(Register dst, jobject obj, bool immediate) {
4214   int oop_index;
4215   if (obj == NULL) {
4216     oop_index = oop_recorder()-&gt;allocate_oop_index(obj);
4217   } else {
4218 #ifdef ASSERT
4219     {
</pre>
<hr />
<pre>
5252 // get_thread() can be called anywhere inside generated code so we
5253 // need to save whatever non-callee save context might get clobbered
5254 // by the call to JavaThread::aarch64_get_thread_helper() or, indeed,
5255 // the call setup code.
5256 //
5257 // aarch64_get_thread_helper() clobbers only r0, r1, and flags.
5258 //
5259 void MacroAssembler::get_thread(Register dst) {
5260   RegSet saved_regs = RegSet::range(r0, r1) + lr - dst;
5261   push(saved_regs, sp);
5262 
5263   mov(lr, CAST_FROM_FN_PTR(address, JavaThread::aarch64_get_thread_helper));
5264   blr(lr);
5265   if (dst != c_rarg0) {
5266     mov(dst, c_rarg0);
5267   }
5268 
5269   pop(saved_regs, sp);
5270 }
5271 
<span class="line-added">5272 // C2 compiled method&#39;s prolog code</span>
<span class="line-added">5273 // Moved here from aarch64.ad to support Valhalla code belows</span>
<span class="line-added">5274 void MacroAssembler::verified_entry(Compile* C, int sp_inc) {</span>
<span class="line-added">5275 </span>
<span class="line-added">5276 // n.b. frame size includes space for return pc and rfp</span>
<span class="line-added">5277   const long framesize = C-&gt;frame_size_in_bytes();</span>
<span class="line-added">5278   assert(framesize % (2 * wordSize) == 0, &quot;must preserve 2 * wordSize alignment&quot;);</span>
<span class="line-added">5279 </span>
<span class="line-added">5280   // insert a nop at the start of the prolog so we can patch in a</span>
<span class="line-added">5281   // branch if we need to invalidate the method later</span>
<span class="line-added">5282   nop();</span>
<span class="line-added">5283 </span>
<span class="line-added">5284   int bangsize = C-&gt;bang_size_in_bytes();</span>
<span class="line-added">5285   if (C-&gt;need_stack_bang(bangsize) &amp;&amp; UseStackBanging)</span>
<span class="line-added">5286      generate_stack_overflow_check(bangsize);</span>
<span class="line-added">5287 </span>
<span class="line-added">5288   build_frame(framesize);</span>
<span class="line-added">5289 </span>
<span class="line-added">5290   if (VerifyStackAtCalls) {</span>
<span class="line-added">5291     Unimplemented();</span>
<span class="line-added">5292   }</span>
<span class="line-added">5293 }</span>
<span class="line-added">5294 </span>
<span class="line-added">5295 int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {</span>
<span class="line-added">5296   // An inline type might be returned. If fields are in registers we</span>
<span class="line-added">5297   // need to allocate an inline type instance and initialize it with</span>
<span class="line-added">5298   // the value of the fields.</span>
<span class="line-added">5299   Label skip;</span>
<span class="line-added">5300   // We only need a new buffered inline type if a new one is not returned</span>
<span class="line-added">5301   cmp(r0, (u1) 1);</span>
<span class="line-added">5302   br(Assembler::EQ, skip);</span>
<span class="line-added">5303   int call_offset = -1;</span>
<span class="line-added">5304 </span>
<span class="line-added">5305   Label slow_case;</span>
<span class="line-added">5306 </span>
<span class="line-added">5307   // Try to allocate a new buffered inline type (from the heap)</span>
<span class="line-added">5308   if (UseTLAB) {</span>
<span class="line-added">5309 </span>
<span class="line-added">5310     if (vk != NULL) {</span>
<span class="line-added">5311       // Called from C1, where the return type is statically known.</span>
<span class="line-added">5312       mov(r1, (intptr_t)vk-&gt;get_InlineKlass());</span>
<span class="line-added">5313       jint lh = vk-&gt;layout_helper();</span>
<span class="line-added">5314       assert(lh != Klass::_lh_neutral_value, &quot;inline class in return type must have been resolved&quot;);</span>
<span class="line-added">5315       mov(r14, lh);</span>
<span class="line-added">5316     } else {</span>
<span class="line-added">5317        // Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)</span>
<span class="line-added">5318        andr(r1, r0, -2);</span>
<span class="line-added">5319        // get obj size</span>
<span class="line-added">5320        ldrw(r14, Address(rscratch1 /*klass*/, Klass::layout_helper_offset()));</span>
<span class="line-added">5321     }</span>
<span class="line-added">5322 </span>
<span class="line-added">5323      ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));</span>
<span class="line-added">5324 </span>
<span class="line-added">5325      // check whether we have space in TLAB,</span>
<span class="line-added">5326      // rscratch1 contains pointer to just allocated obj</span>
<span class="line-added">5327       lea(r14, Address(r13, r14));</span>
<span class="line-added">5328       ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));</span>
<span class="line-added">5329 </span>
<span class="line-added">5330       cmp(r14, rscratch1);</span>
<span class="line-added">5331       br(Assembler::GT, slow_case);</span>
<span class="line-added">5332 </span>
<span class="line-added">5333       // OK we have room in TLAB,</span>
<span class="line-added">5334       // Set new TLAB top</span>
<span class="line-added">5335       str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));</span>
<span class="line-added">5336 </span>
<span class="line-added">5337       // Set new class always locked</span>
<span class="line-added">5338       mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());</span>
<span class="line-added">5339       str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));</span>
<span class="line-added">5340 </span>
<span class="line-added">5341       store_klass_gap(r13, zr);  // zero klass gap for compressed oops</span>
<span class="line-added">5342       if (vk == NULL) {</span>
<span class="line-added">5343         // store_klass corrupts rbx, so save it in rax for later use (interpreter case only).</span>
<span class="line-added">5344          mov(r0, r1);</span>
<span class="line-added">5345       }</span>
<span class="line-added">5346 </span>
<span class="line-added">5347       store_klass(r13, r1);  // klass</span>
<span class="line-added">5348 </span>
<span class="line-added">5349       if (vk != NULL) {</span>
<span class="line-added">5350         // FIXME -- do the packing in-line to avoid the runtime call</span>
<span class="line-added">5351         mov(r0, r13);</span>
<span class="line-added">5352         far_call(RuntimeAddress(vk-&gt;pack_handler())); // no need for call info as this will not safepoint.</span>
<span class="line-added">5353       } else {</span>
<span class="line-added">5354 </span>
<span class="line-added">5355         // We have our new buffered inline type, initialize its fields with an inline class specific handler</span>
<span class="line-added">5356         ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));</span>
<span class="line-added">5357         ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));</span>
<span class="line-added">5358 </span>
<span class="line-added">5359         // Mov new class to r0 and call pack_handler</span>
<span class="line-added">5360         mov(r0, r13);</span>
<span class="line-added">5361         blr(r1);</span>
<span class="line-added">5362       }</span>
<span class="line-added">5363       b(skip);</span>
<span class="line-added">5364   }</span>
<span class="line-added">5365 </span>
<span class="line-added">5366   bind(slow_case);</span>
<span class="line-added">5367   // We failed to allocate a new inline type, fall back to a runtime</span>
<span class="line-added">5368   // call. Some oop field may be live in some registers but we can&#39;t</span>
<span class="line-added">5369   // tell. That runtime call will take care of preserving them</span>
<span class="line-added">5370   // across a GC if there&#39;s one.</span>
<span class="line-added">5371 </span>
<span class="line-added">5372 </span>
<span class="line-added">5373   if (from_interpreter) {</span>
<span class="line-added">5374     super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());</span>
<span class="line-added">5375   } else {</span>
<span class="line-added">5376     ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));</span>
<span class="line-added">5377     blr(rscratch1);</span>
<span class="line-added">5378     call_offset = offset();</span>
<span class="line-added">5379   }</span>
<span class="line-added">5380 </span>
<span class="line-added">5381   bind(skip);</span>
<span class="line-added">5382   return call_offset;</span>
<span class="line-added">5383 }</span>
<span class="line-added">5384 </span>
<span class="line-added">5385 // Move a value between registers/stack slots and update the reg_state</span>
<span class="line-added">5386 bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5387   if (reg_state[to-&gt;value()] == reg_written) {</span>
<span class="line-added">5388     return true; // Already written</span>
<span class="line-added">5389   }</span>
<span class="line-added">5390 </span>
<span class="line-added">5391   if (from != to &amp;&amp; bt != T_VOID) {</span>
<span class="line-added">5392     if (reg_state[to-&gt;value()] == reg_readonly) {</span>
<span class="line-added">5393       return false; // Not yet writable</span>
<span class="line-added">5394     }</span>
<span class="line-added">5395     if (from-&gt;is_reg()) {</span>
<span class="line-added">5396       if (to-&gt;is_reg()) {</span>
<span class="line-added">5397         mov(to-&gt;as_Register(), from-&gt;as_Register());</span>
<span class="line-added">5398       } else {</span>
<span class="line-added">5399         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5400         Address to_addr = Address(sp, st_off);</span>
<span class="line-added">5401         if (from-&gt;is_FloatRegister()) {</span>
<span class="line-added">5402           if (bt == T_DOUBLE) {</span>
<span class="line-added">5403              strd(from-&gt;as_FloatRegister(), to_addr);</span>
<span class="line-added">5404           } else {</span>
<span class="line-added">5405              assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5406              strs(from-&gt;as_FloatRegister(), to_addr);</span>
<span class="line-added">5407           }</span>
<span class="line-added">5408         } else {</span>
<span class="line-added">5409           str(from-&gt;as_Register(), to_addr);</span>
<span class="line-added">5410         }</span>
<span class="line-added">5411       }</span>
<span class="line-added">5412     } else {</span>
<span class="line-added">5413       Address from_addr = Address(sp, from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);</span>
<span class="line-added">5414       if (to-&gt;is_reg()) {</span>
<span class="line-added">5415         if (to-&gt;is_FloatRegister()) {</span>
<span class="line-added">5416           if (bt == T_DOUBLE) {</span>
<span class="line-added">5417              ldrd(to-&gt;as_FloatRegister(), from_addr);</span>
<span class="line-added">5418           } else {</span>
<span class="line-added">5419             assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5420             ldrs(to-&gt;as_FloatRegister(), from_addr);</span>
<span class="line-added">5421           }</span>
<span class="line-added">5422         } else {</span>
<span class="line-added">5423           ldr(to-&gt;as_Register(), from_addr);</span>
<span class="line-added">5424         }</span>
<span class="line-added">5425       } else {</span>
<span class="line-added">5426         int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5427         ldr(rscratch1, from_addr);</span>
<span class="line-added">5428         str(rscratch1, Address(sp, st_off));</span>
<span class="line-added">5429       }</span>
<span class="line-added">5430     }</span>
<span class="line-added">5431   }</span>
<span class="line-added">5432 </span>
<span class="line-added">5433   // Update register states</span>
<span class="line-added">5434   reg_state[from-&gt;value()] = reg_writable;</span>
<span class="line-added">5435   reg_state[to-&gt;value()] = reg_written;</span>
<span class="line-added">5436   return true;</span>
<span class="line-added">5437 }</span>
<span class="line-added">5438 </span>
<span class="line-added">5439 // Read all fields from an inline type oop and store the values in registers/stack slots</span>
<span class="line-added">5440 bool MacroAssembler::unpack_inline_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, VMReg from, VMRegPair* regs_to,</span>
<span class="line-added">5441                                           int&amp; to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5442   Register fromReg = from-&gt;is_reg() ? from-&gt;as_Register() : noreg;</span>
<span class="line-added">5443   assert(sig-&gt;at(sig_index)._bt == T_VOID, &quot;should be at end delimiter&quot;);</span>
<span class="line-added">5444 </span>
<span class="line-added">5445 </span>
<span class="line-added">5446   int vt = 1;</span>
<span class="line-added">5447   bool done = true;</span>
<span class="line-added">5448   bool mark_done = true;</span>
<span class="line-added">5449   do {</span>
<span class="line-added">5450     sig_index--;</span>
<span class="line-added">5451     BasicType bt = sig-&gt;at(sig_index)._bt;</span>
<span class="line-added">5452     if (bt == T_INLINE_TYPE) {</span>
<span class="line-added">5453       vt--;</span>
<span class="line-added">5454     } else if (bt == T_VOID &amp;&amp;</span>
<span class="line-added">5455                sig-&gt;at(sig_index-1)._bt != T_LONG &amp;&amp;</span>
<span class="line-added">5456                sig-&gt;at(sig_index-1)._bt != T_DOUBLE) {</span>
<span class="line-added">5457       vt++;</span>
<span class="line-added">5458     } else if (SigEntry::is_reserved_entry(sig, sig_index)) {</span>
<span class="line-added">5459       to_index--; // Ignore this</span>
<span class="line-added">5460     } else {</span>
<span class="line-added">5461       assert(to_index &gt;= 0, &quot;invalid to_index&quot;);</span>
<span class="line-added">5462       VMRegPair pair_to = regs_to[to_index--];</span>
<span class="line-added">5463       VMReg to = pair_to.first();</span>
<span class="line-added">5464 </span>
<span class="line-added">5465       if (bt == T_VOID) continue;</span>
<span class="line-added">5466 </span>
<span class="line-added">5467       int idx = (int) to-&gt;value();</span>
<span class="line-added">5468       if (reg_state[idx] == reg_readonly) {</span>
<span class="line-added">5469          if (idx != from-&gt;value()) {</span>
<span class="line-added">5470            mark_done = false;</span>
<span class="line-added">5471          }</span>
<span class="line-added">5472          done = false;</span>
<span class="line-added">5473          continue;</span>
<span class="line-added">5474       } else if (reg_state[idx] == reg_written) {</span>
<span class="line-added">5475         continue;</span>
<span class="line-added">5476       } else {</span>
<span class="line-added">5477         assert(reg_state[idx] == reg_writable, &quot;must be writable&quot;);</span>
<span class="line-added">5478         reg_state[idx] = reg_written;</span>
<span class="line-added">5479       }</span>
<span class="line-added">5480 </span>
<span class="line-added">5481       if (fromReg == noreg) {</span>
<span class="line-added">5482         int st_off = from-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5483         ldr(rscratch2, Address(sp, st_off));</span>
<span class="line-added">5484         fromReg = rscratch2;</span>
<span class="line-added">5485       }</span>
<span class="line-added">5486 </span>
<span class="line-added">5487       int off = sig-&gt;at(sig_index)._offset;</span>
<span class="line-added">5488       assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">5489       bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);</span>
<span class="line-added">5490 </span>
<span class="line-added">5491       Address fromAddr = Address(fromReg, off);</span>
<span class="line-added">5492       bool is_signed = (bt != T_CHAR) &amp;&amp; (bt != T_BOOLEAN);</span>
<span class="line-added">5493 </span>
<span class="line-added">5494       if (!to-&gt;is_FloatRegister()) {</span>
<span class="line-added">5495 </span>
<span class="line-added">5496         Register dst = to-&gt;is_stack() ? rscratch1 : to-&gt;as_Register();</span>
<span class="line-added">5497 </span>
<span class="line-added">5498         if (is_oop) {</span>
<span class="line-added">5499           load_heap_oop(dst, fromAddr);</span>
<span class="line-added">5500         } else {</span>
<span class="line-added">5501           load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);</span>
<span class="line-added">5502         }</span>
<span class="line-added">5503         if (to-&gt;is_stack()) {</span>
<span class="line-added">5504           int st_off = to-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5505           str(dst, Address(sp, st_off));</span>
<span class="line-added">5506         }</span>
<span class="line-added">5507       } else {</span>
<span class="line-added">5508         if (bt == T_DOUBLE) {</span>
<span class="line-added">5509           ldrd(to-&gt;as_FloatRegister(), fromAddr);</span>
<span class="line-added">5510         } else {</span>
<span class="line-added">5511           assert(bt == T_FLOAT, &quot;must be float&quot;);</span>
<span class="line-added">5512           ldrs(to-&gt;as_FloatRegister(), fromAddr);</span>
<span class="line-added">5513         }</span>
<span class="line-added">5514      }</span>
<span class="line-added">5515 </span>
<span class="line-added">5516     }</span>
<span class="line-added">5517 </span>
<span class="line-added">5518   } while (vt != 0);</span>
<span class="line-added">5519 </span>
<span class="line-added">5520   if (mark_done &amp;&amp; reg_state[from-&gt;value()] != reg_written) {</span>
<span class="line-added">5521     // This is okay because no one else will write to that slot</span>
<span class="line-added">5522     reg_state[from-&gt;value()] = reg_writable;</span>
<span class="line-added">5523   }</span>
<span class="line-added">5524   return done;</span>
<span class="line-added">5525 }</span>
<span class="line-added">5526 </span>
<span class="line-added">5527 // Pack fields back into an inline type oop</span>
<span class="line-added">5528 bool MacroAssembler::pack_inline_helper(const GrowableArray&lt;SigEntry&gt;* sig, int&amp; sig_index, int vtarg_index,</span>
<span class="line-added">5529                                         VMReg to, VMRegPair* regs_from, int regs_from_count, int&amp; from_index, RegState reg_state[],</span>
<span class="line-added">5530                                         int ret_off, int extra_stack_offset) {</span>
<span class="line-added">5531   assert(sig-&gt;at(sig_index)._bt == T_INLINE_TYPE, &quot;should be at end delimiter&quot;);</span>
<span class="line-added">5532   assert(to-&gt;is_valid(), &quot;must be&quot;);</span>
<span class="line-added">5533 </span>
<span class="line-added">5534   if (reg_state[to-&gt;value()] == reg_written) {</span>
<span class="line-added">5535     skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5536     return true; // Already written</span>
<span class="line-added">5537   }</span>
<span class="line-added">5538 </span>
<span class="line-added">5539   Register val_array = r0;</span>
<span class="line-added">5540   Register val_obj_tmp = r11;</span>
<span class="line-added">5541   Register from_reg_tmp = r10;</span>
<span class="line-added">5542   Register tmp1 = r14;</span>
<span class="line-added">5543   Register tmp2 = r13;</span>
<span class="line-added">5544   Register tmp3 = r1;</span>
<span class="line-added">5545   Register val_obj = to-&gt;is_stack() ? val_obj_tmp : to-&gt;as_Register();</span>
<span class="line-added">5546 </span>
<span class="line-added">5547   if (reg_state[to-&gt;value()] == reg_readonly) {</span>
<span class="line-added">5548     if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {</span>
<span class="line-added">5549       skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5550       return false; // Not yet writable</span>
<span class="line-added">5551     }</span>
<span class="line-added">5552     val_obj = val_obj_tmp;</span>
<span class="line-added">5553   }</span>
<span class="line-added">5554 </span>
<span class="line-added">5555   int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);</span>
<span class="line-added">5556   load_heap_oop(val_obj, Address(val_array, index));</span>
<span class="line-added">5557 </span>
<span class="line-added">5558   ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);</span>
<span class="line-added">5559   VMRegPair from_pair;</span>
<span class="line-added">5560   BasicType bt;</span>
<span class="line-added">5561 </span>
<span class="line-added">5562   while (stream.next(from_pair, bt)) {</span>
<span class="line-added">5563     int off = sig-&gt;at(stream.sig_cc_index())._offset;</span>
<span class="line-added">5564     assert(off &gt; 0, &quot;offset in object should be positive&quot;);</span>
<span class="line-added">5565     bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);</span>
<span class="line-added">5566     size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;</span>
<span class="line-added">5567 </span>
<span class="line-added">5568     VMReg from_r1 = from_pair.first();</span>
<span class="line-added">5569     VMReg from_r2 = from_pair.second();</span>
<span class="line-added">5570 </span>
<span class="line-added">5571     // Pack the scalarized field into the value object.</span>
<span class="line-added">5572     Address dst(val_obj, off);</span>
<span class="line-added">5573 </span>
<span class="line-added">5574     if (!from_r1-&gt;is_FloatRegister()) {</span>
<span class="line-added">5575       Register from_reg;</span>
<span class="line-added">5576       if (from_r1-&gt;is_stack()) {</span>
<span class="line-added">5577         from_reg = from_reg_tmp;</span>
<span class="line-added">5578         int ld_off = from_r1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;</span>
<span class="line-added">5579         load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, /* is_signed */ false);</span>
<span class="line-added">5580       } else {</span>
<span class="line-added">5581         from_reg = from_r1-&gt;as_Register();</span>
<span class="line-added">5582       }</span>
<span class="line-added">5583 </span>
<span class="line-added">5584       if (is_oop) {</span>
<span class="line-added">5585         DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;</span>
<span class="line-added">5586         store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);</span>
<span class="line-added">5587       } else {</span>
<span class="line-added">5588         store_sized_value(dst, from_reg, size_in_bytes);</span>
<span class="line-added">5589       }</span>
<span class="line-added">5590     } else {</span>
<span class="line-added">5591       if (from_r2-&gt;is_valid()) {</span>
<span class="line-added">5592         strd(from_r1-&gt;as_FloatRegister(), dst);</span>
<span class="line-added">5593       } else {</span>
<span class="line-added">5594         strs(from_r1-&gt;as_FloatRegister(), dst);</span>
<span class="line-added">5595       }</span>
<span class="line-added">5596     }</span>
<span class="line-added">5597 </span>
<span class="line-added">5598     reg_state[from_r1-&gt;value()] = reg_writable;</span>
<span class="line-added">5599   }</span>
<span class="line-added">5600   sig_index = stream.sig_cc_index();</span>
<span class="line-added">5601   from_index = stream.regs_cc_index();</span>
<span class="line-added">5602 </span>
<span class="line-added">5603   assert(reg_state[to-&gt;value()] == reg_writable, &quot;must have already been read&quot;);</span>
<span class="line-added">5604   bool success = move_helper(val_obj-&gt;as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);</span>
<span class="line-added">5605   assert(success, &quot;to register must be writeable&quot;);</span>
<span class="line-added">5606 </span>
<span class="line-added">5607   return true;</span>
<span class="line-added">5608 }</span>
<span class="line-added">5609 </span>
<span class="line-added">5610 // Unpack all inline type arguments passed as oops</span>
<span class="line-added">5611 void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {</span>
<span class="line-added">5612   int sp_inc = unpack_inline_args_common(C, receiver_only);</span>
<span class="line-added">5613   // Emit code for verified entry and save increment for stack repair on return</span>
<span class="line-added">5614   verified_entry(C, sp_inc);</span>
<span class="line-added">5615 }</span>
<span class="line-added">5616 </span>
<span class="line-added">5617 int MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,</span>
<span class="line-added">5618                                         BasicType* sig_bt, const GrowableArray&lt;SigEntry&gt;* sig_cc,</span>
<span class="line-added">5619                                         int args_passed, int args_on_stack, VMRegPair* regs,            // from</span>
<span class="line-added">5620                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { // to</span>
<span class="line-added">5621   // Check if we need to extend the stack for packing/unpacking</span>
<span class="line-added">5622   int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;</span>
<span class="line-added">5623   if (sp_inc &gt; 0) {</span>
<span class="line-added">5624     sp_inc = align_up(sp_inc, StackAlignmentInBytes);</span>
<span class="line-added">5625     if (!is_packing) {</span>
<span class="line-added">5626       // Save the return address, adjust the stack (make sure it is properly</span>
<span class="line-added">5627       // 16-byte aligned) and copy the return address to the new top of the stack.</span>
<span class="line-added">5628       // (Note: C1 does this in C1_MacroAssembler::scalarized_entry).</span>
<span class="line-added">5629       // FIXME: We need not to preserve return address on aarch64</span>
<span class="line-added">5630       pop(rscratch1);</span>
<span class="line-added">5631       sub(sp, sp, sp_inc);</span>
<span class="line-added">5632       push(rscratch1);</span>
<span class="line-added">5633     }</span>
<span class="line-added">5634   } else {</span>
<span class="line-added">5635     // The scalarized calling convention needs less stack space than the unscalarized one.</span>
<span class="line-added">5636     // No need to extend the stack, the caller will take care of these adjustments.</span>
<span class="line-added">5637     sp_inc = 0;</span>
<span class="line-added">5638   }</span>
<span class="line-added">5639 </span>
<span class="line-added">5640   int ret_off; // make sure we don&#39;t overwrite the return address</span>
<span class="line-added">5641   if (is_packing) {</span>
<span class="line-added">5642     // For C1 code, the VIEP doesn&#39;t have reserved slots, so we store the returned address at</span>
<span class="line-added">5643     // rsp[0] during shuffling.</span>
<span class="line-added">5644     ret_off = 0;</span>
<span class="line-added">5645   } else {</span>
<span class="line-added">5646     // C2 code ensures that sp_inc is a reserved slot.</span>
<span class="line-added">5647     ret_off = sp_inc;</span>
<span class="line-added">5648   }</span>
<span class="line-added">5649 </span>
<span class="line-added">5650   return shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,</span>
<span class="line-added">5651                                     sig_bt, sig_cc,</span>
<span class="line-added">5652                                     args_passed, args_on_stack, regs,</span>
<span class="line-added">5653                                     args_passed_to, args_on_stack_to, regs_to,</span>
<span class="line-added">5654                                     sp_inc, ret_off);</span>
<span class="line-added">5655 }</span>
<span class="line-added">5656 </span>
<span class="line-added">5657 VMReg MacroAssembler::spill_reg_for(VMReg reg) {</span>
<span class="line-added">5658   return (reg-&gt;is_FloatRegister()) ? v0-&gt;as_VMReg() : r14-&gt;as_VMReg();</span>
<span class="line-added">5659 }</span>
<span class="line-added">5660 </span>
5661 void MacroAssembler::cache_wb(Address line) {
5662   assert(line.getMode() == Address::base_plus_offset, &quot;mode should be base_plus_offset&quot;);
5663   assert(line.index() == noreg, &quot;index should be noreg&quot;);
5664   assert(line.offset() == 0, &quot;offset should be 0&quot;);
5665   // would like to assert this
5666   // assert(line._ext.shift == 0, &quot;shift should be zero&quot;);
5667   if (VM_Version::supports_dcpop()) {
5668     // writeback using clear virtual address to point of persistence
5669     dc(Assembler::CVAP, line.base());
5670   } else {
5671     // no need to generate anything as Unsafe.writebackMemory should
5672     // never invoke this stub
5673   }
5674 }
5675 
5676 void MacroAssembler::cache_wbsync(bool is_pre) {
5677   // we only need a barrier post sync
5678   if (!is_pre) {
5679     membar(Assembler::AnyAny);
5680   }
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIRAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
diff a/.hgtags b/.hgtags
--- a/.hgtags
+++ b/.hgtags
@@ -649,8 +649,8 @@
 06c9f89459daba98395fad726100feb44f89ba71 jdk-15+28
 bcbe7b8a77b8971bc221c0be1bd2abb6fb68c2d0 jdk-16+2
 b58fc60580550a4a587cab729d8fd87223ad6932 jdk-15+29
 76810b3a88c8c641ae3850a8dfd7c40c984aea9d jdk-16+3
 6909e4a1f25bfe9a2727026f5845fc1fc44a36aa jdk-15+30
-78c07dd7240412e60d8694e9dbfd46e57bd42ee0 jdk-16+4
-78c07dd7240412e60d8694e9dbfd46e57bd42ee0 jdk-16+4
+e2622818f0bd30e736252eba101fe7d2c27f400b jdk-16+4
+a32f58c6b8be81877411767de7ba9c4cf087c1b5 jdk-15+31
 e2622818f0bd30e736252eba101fe7d2c27f400b jdk-16+4
diff a/make/conf/jib-profiles.js b/make/conf/jib-profiles.js
--- a/make/conf/jib-profiles.js
+++ b/make/conf/jib-profiles.js
@@ -958,11 +958,11 @@
  */
 var getJibProfilesDependencies = function (input, common) {
 
     var devkit_platform_revisions = {
         linux_x64: "gcc9.2.0-OL6.4+1.0",
-        macosx_x64: "Xcode10.1-MacOSX10.14+1.0",
+        macosx_x64: "Xcode11.3.1-MacOSX10.15+1.0",
         windows_x64: "VS2019-16.5.3+1.0",
         linux_aarch64: "gcc9.2.0-OL7.6+1.0",
         linux_arm: "gcc8.2.0-Fedora27+1.0",
         linux_ppc64le: "gcc8.2.0-Fedora27+1.0",
         linux_s390x: "gcc8.2.0-Fedora27+1.0"
diff a/src/hotspot/cpu/aarch64/aarch64.ad b/src/hotspot/cpu/aarch64/aarch64.ad
--- a/src/hotspot/cpu/aarch64/aarch64.ad
+++ b/src/hotspot/cpu/aarch64/aarch64.ad
@@ -1955,20 +1955,24 @@
   C2_MacroAssembler _masm(&cbuf);
 
   int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
   int reg    = ra_->get_encode(this);
 
-  if (Assembler::operand_valid_for_add_sub_immediate(offset)) {
-    __ add(as_Register(reg), sp, offset);
-  } else {
-    ShouldNotReachHere();
-  }
+  // This add will handle any 24-bit signed offset. 24 bits allows an
+  // 8 megabyte stack frame.
+  __ add(as_Register(reg), sp, offset);
 }
 
 uint BoxLockNode::size(PhaseRegAlloc *ra_) const {
   // BoxLockNode is not a MachNode, so we can't just call MachNode::size(ra_).
-  return 4;
+  int offset = ra_->reg2offset(in_RegMask(0).find_first_elem());
+
+  if (Assembler::operand_valid_for_add_sub_immediate(offset)) {
+    return NativeInstruction::instruction_size;
+  } else {
+    return 2 * NativeInstruction::instruction_size;
+  }
 }
 
 ///=============================================================================
 #ifndef PRODUCT
 void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const
@@ -3157,11 +3161,11 @@
 
   /// mov envcodings
 
   enc_class aarch64_enc_movw_imm(iRegI dst, immI src) %{
     C2_MacroAssembler _masm(&cbuf);
-    u_int32_t con = (u_int32_t)$src$$constant;
+    uint32_t con = (uint32_t)$src$$constant;
     Register dst_reg = as_Register($dst$$reg);
     if (con == 0) {
       __ movw(dst_reg, zr);
     } else {
       __ movw(dst_reg, con);
@@ -3169,11 +3173,11 @@
   %}
 
   enc_class aarch64_enc_mov_imm(iRegL dst, immL src) %{
     C2_MacroAssembler _masm(&cbuf);
     Register dst_reg = as_Register($dst$$reg);
-    u_int64_t con = (u_int64_t)$src$$constant;
+    uint64_t con = (uint64_t)$src$$constant;
     if (con == 0) {
       __ mov(dst_reg, zr);
     } else {
       __ mov(dst_reg, con);
     }
@@ -3211,11 +3215,11 @@
   %}
 
   enc_class aarch64_enc_mov_p1(iRegP dst, immP_1 src) %{
     C2_MacroAssembler _masm(&cbuf);
     Register dst_reg = as_Register($dst$$reg);
-    __ mov(dst_reg, (u_int64_t)1);
+    __ mov(dst_reg, (uint64_t)1);
   %}
 
   enc_class aarch64_enc_mov_byte_map_base(iRegP dst, immByteMapBase src) %{
     C2_MacroAssembler _masm(&cbuf);
     __ load_byte_map_base($dst$$Register);
@@ -3336,11 +3340,11 @@
   %}
 
   enc_class aarch64_enc_cmpw_imm(iRegI src1, immI src2) %{
     C2_MacroAssembler _masm(&cbuf);
     Register reg1 = as_Register($src1$$reg);
-    u_int32_t val = (u_int32_t)$src2$$constant;
+    uint32_t val = (uint32_t)$src2$$constant;
     __ movw(rscratch1, val);
     __ cmpw(reg1, rscratch1);
   %}
 
   enc_class aarch64_enc_cmp(iRegL src1, iRegL src2) %{
@@ -3358,19 +3362,19 @@
       __ subs(zr, reg, val);
     } else if (val != -val) {
       __ adds(zr, reg, -val);
     } else {
     // aargh, Long.MIN_VALUE is a special case
-      __ orr(rscratch1, zr, (u_int64_t)val);
+      __ orr(rscratch1, zr, (uint64_t)val);
       __ subs(zr, reg, rscratch1);
     }
   %}
 
   enc_class aarch64_enc_cmp_imm(iRegL src1, immL src2) %{
     C2_MacroAssembler _masm(&cbuf);
     Register reg1 = as_Register($src1$$reg);
-    u_int64_t val = (u_int64_t)$src2$$constant;
+    uint64_t val = (uint64_t)$src2$$constant;
     __ mov(rscratch1, val);
     __ cmp(reg1, rscratch1);
   %}
 
   enc_class aarch64_enc_cmpp(iRegP src1, iRegP src2) %{
@@ -13933,14 +13937,31 @@
 %{
   match(Set dummy (ClearArray (Binary cnt base) val));
   effect(USE_KILL cnt, USE_KILL base, KILL cr);
 
   ins_cost(4 * INSN_COST);
-  format %{ "ClearArray $cnt, $base, $val" %}
+  format %{ "ClearArray $cnt, $base" %}
+
+  ins_encode %{
+    __ zero_words($base$$Register, $cnt$$Register);
+  %}
+
+  ins_pipe(pipe_class_memory);
+%}
+
+instruct clearArray_imm_reg(immL cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)
+%{
+  predicate((uint64_t)n->in(2)->get_long()
+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));
+  match(Set dummy (ClearArray cnt base));
+  effect(USE_KILL base);
+
+  ins_cost(4 * INSN_COST);
+  format %{ "ClearArray $cnt, $base" %}
 
   ins_encode %{
-    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);
+    __ zero_words($base$$Register, (uint64_t)$cnt$$constant);
   %}
 
   ins_pipe(pipe_class_memory);
 %}
 
diff a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/c1_LIRAssembler_aarch64.cpp
@@ -2182,11 +2182,11 @@
       ShouldNotReachHere();
     }
   } else if (code == lir_cmp_l2i) {
     Label done;
     __ cmp(left->as_register_lo(), right->as_register_lo());
-    __ mov(dst->as_register(), (u_int64_t)-1L);
+    __ mov(dst->as_register(), (uint64_t)-1L);
     __ br(Assembler::LT, done);
     __ csinc(dst->as_register(), zr, zr, Assembler::EQ);
     __ bind(done);
   } else {
     ShouldNotReachHere();
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp
@@ -92,11 +92,11 @@
   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
     // PC-rel. addressing
     offset = target-branch;
     int shift = Instruction_aarch64::extract(insn, 31, 31);
     if (shift) {
-      u_int64_t dest = (u_int64_t)target;
+      uint64_t dest = (uint64_t)target;
       uint64_t pc_page = (uint64_t)branch >> 12;
       uint64_t adr_page = (uint64_t)target >> 12;
       unsigned offset_lo = dest & 0xfff;
       offset = adr_page - pc_page;
 
@@ -145,11 +145,11 @@
     int offset_lo = offset & 3;
     offset >>= 2;
     Instruction_aarch64::spatch(branch, 23, 5, offset);
     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
-    u_int64_t dest = (u_int64_t)target;
+    uint64_t dest = (uint64_t)target;
     // Move wide constant
     assert(nativeInstruction_at(branch+4)->is_movk(), "wrong insns in patch");
     assert(nativeInstruction_at(branch+8)->is_movk(), "wrong insns in patch");
     Instruction_aarch64::patch(branch, 20, 5, dest & 0xffff);
     Instruction_aarch64::patch(branch+4, 20, 5, (dest >>= 16) & 0xffff);
@@ -271,17 +271,17 @@
       }
     } else {
       ShouldNotReachHere();
     }
   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
-    u_int32_t *insns = (u_int32_t *)insn_addr;
+    uint32_t *insns = (uint32_t *)insn_addr;
     // Move wide constant: movz, movk, movk.  See movptr().
     assert(nativeInstruction_at(insns+1)->is_movk(), "wrong insns in patch");
     assert(nativeInstruction_at(insns+2)->is_movk(), "wrong insns in patch");
-    return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))
-                   + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)
-                   + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));
+    return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))
+                   + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)
+                   + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));
   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &&
              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
     return 0;
   } else {
     ShouldNotReachHere();
@@ -1535,11 +1535,11 @@
 // MacroAssembler protected routines needed to implement
 // public methods
 
 void MacroAssembler::mov(Register r, Address dest) {
   code_section()->relocate(pc(), dest.rspec());
-  u_int64_t imm64 = (u_int64_t)dest.target();
+  uint64_t imm64 = (uint64_t)dest.target();
   movptr(r, imm64);
 }
 
 // Move a constant pointer into r.  In AArch64 mode the virtual
 // address space is 48 bits in size, so we only need three
@@ -1568,24 +1568,24 @@
 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
 //   T1D/T2D: invalid
-void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {
+void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {
   assert(T != T1D && T != T2D, "invalid arrangement");
   if (T == T8B || T == T16B) {
     assert((imm32 & ~0xff) == 0, "extraneous bits in unsigned imm32 (T8B/T16B)");
     movi(Vd, T, imm32 & 0xff, 0);
     return;
   }
-  u_int32_t nimm32 = ~imm32;
+  uint32_t nimm32 = ~imm32;
   if (T == T4H || T == T8H) {
     assert((imm32  & ~0xffff) == 0, "extraneous bits in unsigned imm32 (T4H/T8H)");
     imm32 &= 0xffff;
     nimm32 &= 0xffff;
   }
-  u_int32_t x = imm32;
+  uint32_t x = imm32;
   int movi_cnt = 0;
   int movn_cnt = 0;
   while (x) { if (x & 0xff) movi_cnt++; x >>= 8; }
   x = nimm32;
   while (x) { if (x & 0xff) movn_cnt++; x >>= 8; }
@@ -1605,11 +1605,11 @@
       orri(Vd, T, imm32 & 0xff, lsl);
     lsl += 8; imm32 >>= 8;
   }
 }
 
-void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)
+void MacroAssembler::mov_immediate64(Register dst, uint64_t imm64)
 {
 #ifndef PRODUCT
   {
     char buffer[64];
     snprintf(buffer, sizeof(buffer), "0x%" PRIX64, imm64);
@@ -1619,11 +1619,11 @@
   if (operand_valid_for_logical_immediate(false, imm64)) {
     orr(dst, zr, imm64);
   } else {
     // we can use a combination of MOVZ or MOVN with
     // MOVK to build up the constant
-    u_int64_t imm_h[4];
+    uint64_t imm_h[4];
     int zero_count = 0;
     int neg_count = 0;
     int i;
     for (i = 0; i < 4; i++) {
       imm_h[i] = ((imm64 >> (i * 16)) & 0xffffL);
@@ -1640,89 +1640,89 @@
       // one MOVN will do
       movn(dst, 0);
     } else if (zero_count == 3) {
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0L) {
-          movz(dst, (u_int32_t)imm_h[i], (i << 4));
+          movz(dst, (uint32_t)imm_h[i], (i << 4));
           break;
         }
       }
     } else if (neg_count == 3) {
       // one MOVN will do
       for (int i = 0; i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i << 4));
+          movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i << 4));
           break;
         }
       }
     } else if (zero_count == 2) {
       // one MOVZ and one MOVK will do
       for (i = 0; i < 3; i++) {
         if (imm_h[i] != 0L) {
-          movz(dst, (u_int32_t)imm_h[i], (i << 4));
+          movz(dst, (uint32_t)imm_h[i], (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0L) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else if (neg_count == 2) {
       // one MOVN and one MOVK will do
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i << 4));
+          movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else if (zero_count == 1) {
       // one MOVZ and two MOVKs will do
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0L) {
-          movz(dst, (u_int32_t)imm_h[i], (i << 4));
+          movz(dst, (uint32_t)imm_h[i], (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0x0L) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else if (neg_count == 1) {
       // one MOVN and two MOVKs will do
       for (i = 0; i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i << 4));
+          movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i << 4));
           i++;
           break;
         }
       }
       for (;i < 4; i++) {
         if (imm_h[i] != 0xffffL) {
-          movk(dst, (u_int32_t)imm_h[i], (i << 4));
+          movk(dst, (uint32_t)imm_h[i], (i << 4));
         }
       }
     } else {
       // use a MOVZ and 3 MOVKs (makes it easier to debug)
-      movz(dst, (u_int32_t)imm_h[0], 0);
+      movz(dst, (uint32_t)imm_h[0], 0);
       for (i = 1; i < 4; i++) {
-        movk(dst, (u_int32_t)imm_h[i], (i << 4));
+        movk(dst, (uint32_t)imm_h[i], (i << 4));
       }
     }
   }
 }
 
-void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)
+void MacroAssembler::mov_immediate32(Register dst, uint32_t imm32)
 {
 #ifndef PRODUCT
     {
       char buffer[64];
       snprintf(buffer, sizeof(buffer), "0x%" PRIX32, imm32);
@@ -1732,11 +1732,11 @@
   if (operand_valid_for_logical_immediate(true, imm32)) {
     orrw(dst, zr, imm32);
   } else {
     // we can use MOVZ, MOVN or two calls to MOVK to build up the
     // constant
-    u_int32_t imm_h[2];
+    uint32_t imm_h[2];
     imm_h[0] = imm32 & 0xffff;
     imm_h[1] = ((imm32 >> 16) & 0xffff);
     if (imm_h[0] == 0) {
       movzw(dst, imm_h[1], 16);
     } else if (imm_h[0] == 0xffff) {
@@ -4898,11 +4898,11 @@
 }
 
 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
 // cnt:          Immediate count in HeapWords.
 #define SmallArraySize (18 * BytesPerLong)
-void MacroAssembler::zero_words(Register base, u_int64_t cnt)
+void MacroAssembler::zero_words(Register base, uint64_t cnt)
 {
   BLOCK_COMMENT("zero_words {");
   int i = cnt & 1;  // store any odd word to start
   if (i) str(zr, Address(base));
 
diff a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
--- a/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
+++ b/src/hotspot/cpu/aarch64/macroAssembler_aarch64.hpp
@@ -459,12 +459,12 @@
   // macro assembly operations needed for aarch64
 
   // first two private routines for loading 32 bit or 64 bit constants
 private:
 
-  void mov_immediate64(Register dst, u_int64_t imm64);
-  void mov_immediate32(Register dst, u_int32_t imm32);
+  void mov_immediate64(Register dst, uint64_t imm64);
+  void mov_immediate32(Register dst, uint32_t imm32);
 
   int push(unsigned int bitset, Register stack);
   int pop(unsigned int bitset, Register stack);
 
   int push_fp(unsigned int bitset, Register stack);
@@ -489,26 +489,26 @@
   // now mov instructions for loading absolute addresses and 32 or
   // 64 bit integers
 
   inline void mov(Register dst, address addr)
   {
-    mov_immediate64(dst, (u_int64_t)addr);
+    mov_immediate64(dst, (uint64_t)addr);
   }
 
-  inline void mov(Register dst, u_int64_t imm64)
+  inline void mov(Register dst, uint64_t imm64)
   {
     mov_immediate64(dst, imm64);
   }
 
-  inline void movw(Register dst, u_int32_t imm32)
+  inline void movw(Register dst, uint32_t imm32)
   {
     mov_immediate32(dst, imm32);
   }
 
   inline void mov(Register dst, long l)
   {
-    mov(dst, (u_int64_t)l);
+    mov(dst, (uint64_t)l);
   }
 
   inline void mov(Register dst, int i)
   {
     mov(dst, (long)i);
@@ -521,11 +521,11 @@
       mov(dst, src.as_constant());
   }
 
   void movptr(Register r, uintptr_t imm64);
 
-  void mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32);
+  void mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32);
 
   void mov(FloatRegister Vd, SIMD_Arrangement T, FloatRegister Vn) {
     orr(Vd, T, Vn, Vn);
   }
 
@@ -1284,13 +1284,13 @@
 
   void string_equals(Register a1, Register a2, Register result, Register cnt1,
                      int elem_size);
 
   void fill_words(Register base, Register cnt, Register value);
-  void fill_words(Register base, u_int64_t cnt, Register value);
+  void fill_words(Register base, uint64_t cnt, Register value);
 
-  void zero_words(Register base, u_int64_t cnt);
+  void zero_words(Register base, uint64_t cnt);
   void zero_words(Register ptr, Register cnt);
   void zero_dcache_blocks(Register base, Register cnt);
 
   static const int zero_words_block_size;
 
diff a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
--- a/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
+++ b/src/hotspot/cpu/aarch64/templateTable_aarch64.cpp
@@ -1778,11 +1778,11 @@
 {
   transition(ltos, itos);
   Label done;
   __ pop_l(r1);
   __ cmp(r1, r0);
-  __ mov(r0, (u_int64_t)-1L);
+  __ mov(r0, (uint64_t)-1L);
   __ br(Assembler::LT, done);
   // __ mov(r0, 1UL);
   // __ csel(r0, r0, zr, Assembler::NE);
   // and here is a faster way
   __ csinc(r0, zr, zr, Assembler::EQ);
@@ -1802,11 +1802,11 @@
     __ fcmpd(v1, v0);
   }
   if (unordered_result < 0) {
     // we want -1 for unordered or less than, 0 for equal and 1 for
     // greater than.
-    __ mov(r0, (u_int64_t)-1L);
+    __ mov(r0, (uint64_t)-1L);
     // for FP LT tests less than or unordered
     __ br(Assembler::LT, done);
     // install 0 for EQ otherwise 1
     __ csinc(r0, zr, zr, Assembler::EQ);
   } else {
diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -4179,47 +4179,10 @@
     }
   }
   return super_klass;
 }
 
-#ifndef PRODUCT
-static void print_field_layout(const Symbol* name,
-                               Array<u2>* fields,
-                               ConstantPool* cp,
-                               int instance_size,
-                               int instance_fields_start,
-                               int instance_fields_end,
-                               int static_fields_end) {
-
-  assert(name != NULL, "invariant");
-
-  tty->print("%s: field layout\n", name->as_klass_external_name());
-  tty->print("  @%3d %s\n", instance_fields_start, "--- instance fields start ---");
-  for (AllFieldStream fs(fields, cp); !fs.done(); fs.next()) {
-    if (!fs.access_flags().is_static()) {
-      tty->print("  @%3d \"%s\" %s\n",
-        fs.offset(),
-        fs.name()->as_klass_external_name(),
-        fs.signature()->as_klass_external_name());
-    }
-  }
-  tty->print("  @%3d %s\n", instance_fields_end, "--- instance fields end ---");
-  tty->print("  @%3d %s\n", instance_size * wordSize, "--- instance ends ---");
-  tty->print("  @%3d %s\n", InstanceMirrorKlass::offset_of_static_fields(), "--- static fields start ---");
-  for (AllFieldStream fs(fields, cp); !fs.done(); fs.next()) {
-    if (fs.access_flags().is_static()) {
-      tty->print("  @%3d \"%s\" %s\n",
-        fs.offset(),
-        fs.name()->as_klass_external_name(),
-        fs.signature()->as_klass_external_name());
-    }
-  }
-  tty->print("  @%3d %s\n", static_fields_end, "--- static fields end ---");
-  tty->print("\n");
-}
-#endif
-
 OopMapBlocksBuilder::OopMapBlocksBuilder(unsigned int max_blocks) {
   _max_nonstatic_oop_maps = max_blocks;
   _nonstatic_oop_map_count = 0;
   if (max_blocks == 0) {
     _nonstatic_oop_maps = NULL;
@@ -4351,622 +4314,10 @@
         "\"%s\" sig: \"%s\" class: %s - %s", name->as_C_string(), sig->as_C_string(),
         _class_name->as_C_string(), msg);
   }
 }
 
-// Layout fields and fill in FieldLayoutInfo.  Could use more refactoring!
-void ClassFileParser::layout_fields(ConstantPool* cp,
-                                    const FieldAllocationCount* fac,
-                                    const ClassAnnotationCollector* parsed_annotations,
-                                    FieldLayoutInfo* info,
-                                    TRAPS) {
-
-  assert(cp != NULL, "invariant");
-
-  // Field size and offset computation
-  int nonstatic_field_size = _super_klass == NULL ? 0 :
-                               _super_klass->nonstatic_field_size();
-  int next_nonstatic_inline_type_offset = 0;
-  int first_nonstatic_inline_type_offset = 0;
-
-  // Fields that are inline types are handled differently depending if they are static or not:
-  // - static fields are oops
-  // - non-static fields are embedded
-
-  // Count the contended fields by type.
-  //
-  // We ignore static fields, because @Contended is not supported for them.
-  // The layout code below will also ignore the static fields.
-  int nonstatic_contended_count = 0;
-  FieldAllocationCount fac_contended;
-  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-    FieldAllocationType atype = (FieldAllocationType) fs.allocation_type();
-    if (fs.is_contended()) {
-      fac_contended.count[atype]++;
-      if (!fs.access_flags().is_static()) {
-        nonstatic_contended_count++;
-      }
-    }
-  }
-
-
-  // Calculate the starting byte offsets
-  int next_static_oop_offset    = InstanceMirrorKlass::offset_of_static_fields();
-  // Inline types in static fields are not embedded, they are handled with oops
-  int next_static_double_offset = next_static_oop_offset +
-                                  ((fac->count[STATIC_OOP] + fac->count[STATIC_INLINE]) * heapOopSize);
-  if (fac->count[STATIC_DOUBLE]) {
-    next_static_double_offset = align_up(next_static_double_offset, BytesPerLong);
-  }
-
-  int next_static_word_offset   = next_static_double_offset +
-                                    ((fac->count[STATIC_DOUBLE]) * BytesPerLong);
-  int next_static_short_offset  = next_static_word_offset +
-                                    ((fac->count[STATIC_WORD]) * BytesPerInt);
-  int next_static_byte_offset   = next_static_short_offset +
-                                  ((fac->count[STATIC_SHORT]) * BytesPerShort);
-
-  int nonstatic_fields_start  = instanceOopDesc::base_offset_in_bytes() +
-                                nonstatic_field_size * heapOopSize;
-
-  // First field of inline types is aligned on a long boundary in order to ease
-  // in-lining of inline types (with header removal) in packed arrays and
-  // inlined fields
-  int initial_inline_type_padding = 0;
-  if (is_inline_type()) {
-    int old = nonstatic_fields_start;
-    nonstatic_fields_start = align_up(nonstatic_fields_start, BytesPerLong);
-    initial_inline_type_padding = nonstatic_fields_start - old;
-  }
-
-  int next_nonstatic_field_offset = nonstatic_fields_start;
-
-  const bool is_contended_class     = parsed_annotations->is_contended();
-
-  // Class is contended, pad before all the fields
-  if (is_contended_class) {
-    next_nonstatic_field_offset += ContendedPaddingWidth;
-  }
-
-  // Temporary inline types restrictions
-  if (is_inline_type()) {
-    if (is_contended_class) {
-      throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support @Contended annotation yet");
-      return;
-    }
-  }
-
-  // Compute the non-contended fields count.
-  // The packing code below relies on these counts to determine if some field
-  // can be squeezed into the alignment gap. Contended fields are obviously
-  // exempt from that.
-  unsigned int nonstatic_double_count = fac->count[NONSTATIC_DOUBLE] - fac_contended.count[NONSTATIC_DOUBLE];
-  unsigned int nonstatic_word_count   = fac->count[NONSTATIC_WORD]   - fac_contended.count[NONSTATIC_WORD];
-  unsigned int nonstatic_short_count  = fac->count[NONSTATIC_SHORT]  - fac_contended.count[NONSTATIC_SHORT];
-  unsigned int nonstatic_byte_count   = fac->count[NONSTATIC_BYTE]   - fac_contended.count[NONSTATIC_BYTE];
-  unsigned int nonstatic_oop_count    = fac->count[NONSTATIC_OOP]    - fac_contended.count[NONSTATIC_OOP];
-
-  int static_inline_type_count = 0;
-  int nonstatic_inline_type_count = 0;
-  int* nonstatic_inline_type_indexes = NULL;
-  Klass** nonstatic_inline_type_klasses = NULL;
-  unsigned int inline_type_oop_map_count = 0;
-  int inline_types_not_inlined = 0;
-  int not_atomic_inline_types = 0;
-
-  int max_nonstatic_inline_type = fac->count[NONSTATIC_INLINE] + 1;
-
-  nonstatic_inline_type_indexes = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, int,
-                                                               max_nonstatic_inline_type);
-  for (int i = 0; i < max_nonstatic_inline_type; i++) {
-    nonstatic_inline_type_indexes[i] = -1;
-  }
-  nonstatic_inline_type_klasses = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, Klass*,
-                                                               max_nonstatic_inline_type);
-
-  for (AllFieldStream fs(_fields, _cp); !fs.done(); fs.next()) {
-    if (fs.allocation_type() == STATIC_INLINE) {
-      ResourceMark rm;
-      if (!fs.signature()->is_Q_signature()) {
-        THROW(vmSymbols::java_lang_ClassFormatError());
-      }
-      static_inline_type_count++;
-    } else if (fs.allocation_type() == NONSTATIC_INLINE) {
-      // Pre-resolve the inline field and check for inline type circularity issues.
-      ResourceMark rm;
-      if (!fs.signature()->is_Q_signature()) {
-        THROW(vmSymbols::java_lang_ClassFormatError());
-      }
-      Klass* klass =
-        SystemDictionary::resolve_inline_type_field_or_fail(&fs,
-                                                            Handle(THREAD, _loader_data->class_loader()),
-                                                            _protection_domain, true, CHECK);
-      assert(klass != NULL, "Sanity check");
-      if (!klass->access_flags().is_inline_type()) {
-        THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
-      }
-      InlineKlass* vk = InlineKlass::cast(klass);
-      // Conditions to apply flattening or not should be defined in a single place
-      bool too_big_to_allocate_inline = (InlineFieldMaxFlatSize >= 0 &&
-                                 (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
-      bool too_atomic_to_allocate_inline = vk->is_declared_atomic();
-      bool too_volatile_to_allocate_inline = fs.access_flags().is_volatile();
-      if (vk->is_naturally_atomic()) {
-        too_atomic_to_allocate_inline = false;
-        // too_volatile_to_allocate_inline = false; //FIXME
-        // volatile fields are currently never inlined, this could change in the future
-      }
-      if (!(too_big_to_allocate_inline | too_atomic_to_allocate_inline | too_volatile_to_allocate_inline)) {
-        nonstatic_inline_type_indexes[nonstatic_inline_type_count] = fs.index();
-        nonstatic_inline_type_klasses[nonstatic_inline_type_count] = klass;
-        nonstatic_inline_type_count++;
-
-        InlineKlass* vklass = InlineKlass::cast(klass);
-        if (vklass->contains_oops()) {
-          inline_type_oop_map_count += vklass->nonstatic_oop_map_count();
-        }
-        fs.set_inlined(true);
-        if (!vk->is_atomic()) {  // flat and non-atomic: take note
-          not_atomic_inline_types++;
-        }
-      } else {
-        inline_types_not_inlined++;
-        fs.set_inlined(false);
-      }
-    }
-  }
-
-  // Adjusting non_static_oop_count to take into account inline types fields not inlined;
-  nonstatic_oop_count += inline_types_not_inlined;
-
-  // Total non-static fields count, including every contended field
-  unsigned int nonstatic_fields_count = fac->count[NONSTATIC_DOUBLE] + fac->count[NONSTATIC_WORD] +
-                                        fac->count[NONSTATIC_SHORT] + fac->count[NONSTATIC_BYTE] +
-                                        fac->count[NONSTATIC_OOP] + fac->count[NONSTATIC_INLINE];
-
-  const bool super_has_nonstatic_fields =
-          (_super_klass != NULL && _super_klass->has_nonstatic_fields());
-  const bool has_nonstatic_fields =
-    super_has_nonstatic_fields || (nonstatic_fields_count != 0);
-  const bool has_nonstatic_inline_fields = nonstatic_inline_type_count > 0;
-
-  if (is_inline_type() && (!has_nonstatic_fields)) {
-    // There are a number of fixes required throughout the type system and JIT
-    throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support zero instance size yet");
-    return;
-  }
-
-  // Prepare list of oops for oop map generation.
-  //
-  // "offset" and "count" lists are describing the set of contiguous oop
-  // regions. offset[i] is the start of the i-th region, which then has
-  // count[i] oops following. Before we know how many regions are required,
-  // we pessimistically allocate the maps to fit all the oops into the
-  // distinct regions.
-  //
-  int super_oop_map_count = (_super_klass == NULL) ? 0 :_super_klass->nonstatic_oop_map_count();
-  int max_oop_map_count =
-      super_oop_map_count +
-      fac->count[NONSTATIC_OOP] +
-      inline_type_oop_map_count +
-      inline_types_not_inlined;
-
-  OopMapBlocksBuilder* nonstatic_oop_maps = new OopMapBlocksBuilder(max_oop_map_count);
-  if (super_oop_map_count > 0) {
-    nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),
-                                                    _super_klass->nonstatic_oop_map_count());
-  }
-
-  int first_nonstatic_oop_offset = 0; // will be set for first oop field
-
-  bool compact_fields  = true;
-  bool allocate_oops_first = false;
-
-  int next_nonstatic_oop_offset = 0;
-  int next_nonstatic_double_offset = 0;
-
-  // Rearrange fields for a given allocation style
-  if (allocate_oops_first) {
-    // Fields order: oops, longs/doubles, ints, shorts/chars, bytes, padded fields
-    next_nonstatic_oop_offset    = next_nonstatic_field_offset;
-    next_nonstatic_double_offset = next_nonstatic_oop_offset +
-                                    (nonstatic_oop_count * heapOopSize);
-  } else {
-    // Fields order: longs/doubles, ints, shorts/chars, bytes, oops, padded fields
-    next_nonstatic_double_offset = next_nonstatic_field_offset;
-  }
-
-  int nonstatic_oop_space_count   = 0;
-  int nonstatic_word_space_count  = 0;
-  int nonstatic_short_space_count = 0;
-  int nonstatic_byte_space_count  = 0;
-  int nonstatic_oop_space_offset = 0;
-  int nonstatic_word_space_offset = 0;
-  int nonstatic_short_space_offset = 0;
-  int nonstatic_byte_space_offset = 0;
-
-  // Try to squeeze some of the fields into the gaps due to
-  // long/double alignment.
-  if (nonstatic_double_count > 0) {
-    int offset = next_nonstatic_double_offset;
-    next_nonstatic_double_offset = align_up(offset, BytesPerLong);
-    if (compact_fields && offset != next_nonstatic_double_offset) {
-      // Allocate available fields into the gap before double field.
-      int length = next_nonstatic_double_offset - offset;
-      assert(length == BytesPerInt, "");
-      nonstatic_word_space_offset = offset;
-      if (nonstatic_word_count > 0) {
-        nonstatic_word_count      -= 1;
-        nonstatic_word_space_count = 1; // Only one will fit
-        length -= BytesPerInt;
-        offset += BytesPerInt;
-      }
-      nonstatic_short_space_offset = offset;
-      while (length >= BytesPerShort && nonstatic_short_count > 0) {
-        nonstatic_short_count       -= 1;
-        nonstatic_short_space_count += 1;
-        length -= BytesPerShort;
-        offset += BytesPerShort;
-      }
-      nonstatic_byte_space_offset = offset;
-      while (length > 0 && nonstatic_byte_count > 0) {
-        nonstatic_byte_count       -= 1;
-        nonstatic_byte_space_count += 1;
-        length -= 1;
-      }
-      // Allocate oop field in the gap if there are no other fields for that.
-      nonstatic_oop_space_offset = offset;
-      if (length >= heapOopSize && nonstatic_oop_count > 0 &&
-          !allocate_oops_first) { // when oop fields not first
-        nonstatic_oop_count      -= 1;
-        nonstatic_oop_space_count = 1; // Only one will fit
-        length -= heapOopSize;
-        offset += heapOopSize;
-      }
-    }
-  }
-
-  int next_nonstatic_word_offset = next_nonstatic_double_offset +
-                                     (nonstatic_double_count * BytesPerLong);
-  int next_nonstatic_short_offset = next_nonstatic_word_offset +
-                                      (nonstatic_word_count * BytesPerInt);
-  int next_nonstatic_byte_offset = next_nonstatic_short_offset +
-                                     (nonstatic_short_count * BytesPerShort);
-  int next_nonstatic_padded_offset = next_nonstatic_byte_offset +
-                                       nonstatic_byte_count;
-
-  // let oops jump before padding with this allocation style
-  if (!allocate_oops_first) {
-    next_nonstatic_oop_offset = next_nonstatic_padded_offset;
-    if( nonstatic_oop_count > 0 ) {
-      next_nonstatic_oop_offset = align_up(next_nonstatic_oop_offset, heapOopSize);
-    }
-    next_nonstatic_padded_offset = next_nonstatic_oop_offset + (nonstatic_oop_count * heapOopSize);
-  }
-
-  // Aligning embedded inline types
-  // bug below, the current algorithm to layout embedded inline types always put them at the
-  // end of the layout, which doesn't match the different allocation policies the VM is
-  // supposed to provide => FixMe
-  // Note also that the current alignment policy is to make each inline type starting on a
-  // 64 bits boundary. This could be optimized later. For instance, it could be nice to
-  // align inline types according to their most constrained internal type.
-  next_nonstatic_inline_type_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
-  int next_inline_type_index = 0;
-
-  // Iterate over fields again and compute correct offsets.
-  // The field allocation type was temporarily stored in the offset slot.
-  // oop fields are located before non-oop fields (static and non-static).
-  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-
-    // skip already laid out fields
-    if (fs.is_offset_set()) continue;
-
-    // contended instance fields are handled below
-    if (fs.is_contended() && !fs.access_flags().is_static()) continue;
-
-    int real_offset = 0;
-    const FieldAllocationType atype = (const FieldAllocationType) fs.allocation_type();
-
-    // pack the rest of the fields
-    switch (atype) {
-      // Inline types in static fields are handled with oops
-      case STATIC_INLINE:   // Fallthrough
-      case STATIC_OOP:
-        real_offset = next_static_oop_offset;
-        next_static_oop_offset += heapOopSize;
-        break;
-      case STATIC_BYTE:
-        real_offset = next_static_byte_offset;
-        next_static_byte_offset += 1;
-        break;
-      case STATIC_SHORT:
-        real_offset = next_static_short_offset;
-        next_static_short_offset += BytesPerShort;
-        break;
-      case STATIC_WORD:
-        real_offset = next_static_word_offset;
-        next_static_word_offset += BytesPerInt;
-        break;
-      case STATIC_DOUBLE:
-        real_offset = next_static_double_offset;
-        next_static_double_offset += BytesPerLong;
-        break;
-      case NONSTATIC_INLINE:
-        if (fs.is_inlined()) {
-          Klass* klass = nonstatic_inline_type_klasses[next_inline_type_index];
-          assert(klass != NULL, "Klass should have been loaded and resolved earlier");
-          assert(klass->access_flags().is_inline_type(),"Must be an inline type");
-          InlineKlass* vklass = InlineKlass::cast(klass);
-          real_offset = next_nonstatic_inline_type_offset;
-          next_nonstatic_inline_type_offset += (vklass->size_helper()) * wordSize - vklass->first_field_offset();
-          // aligning next inline type on a 64 bits boundary
-          next_nonstatic_inline_type_offset = align_up(next_nonstatic_inline_type_offset, BytesPerLong);
-          next_inline_type_index += 1;
-
-          if (vklass->contains_oops()) { // add flatten oop maps
-            int diff = real_offset - vklass->first_field_offset();
-            const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
-            const OopMapBlock* const last_map = map + vklass->nonstatic_oop_map_count();
-            while (map < last_map) {
-              nonstatic_oop_maps->add(map->offset() + diff, map->count());
-              map++;
-            }
-          }
-          break;
-        } else {
-          // Fall through
-        }
-      case NONSTATIC_OOP:
-        if( nonstatic_oop_space_count > 0 ) {
-          real_offset = nonstatic_oop_space_offset;
-          nonstatic_oop_space_offset += heapOopSize;
-          nonstatic_oop_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_oop_offset;
-          next_nonstatic_oop_offset += heapOopSize;
-        }
-        nonstatic_oop_maps->add(real_offset, 1);
-        break;
-      case NONSTATIC_BYTE:
-        if( nonstatic_byte_space_count > 0 ) {
-          real_offset = nonstatic_byte_space_offset;
-          nonstatic_byte_space_offset += 1;
-          nonstatic_byte_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_byte_offset;
-          next_nonstatic_byte_offset += 1;
-        }
-        break;
-      case NONSTATIC_SHORT:
-        if( nonstatic_short_space_count > 0 ) {
-          real_offset = nonstatic_short_space_offset;
-          nonstatic_short_space_offset += BytesPerShort;
-          nonstatic_short_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_short_offset;
-          next_nonstatic_short_offset += BytesPerShort;
-        }
-        break;
-      case NONSTATIC_WORD:
-        if( nonstatic_word_space_count > 0 ) {
-          real_offset = nonstatic_word_space_offset;
-          nonstatic_word_space_offset += BytesPerInt;
-          nonstatic_word_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_word_offset;
-          next_nonstatic_word_offset += BytesPerInt;
-        }
-        break;
-      case NONSTATIC_DOUBLE:
-        real_offset = next_nonstatic_double_offset;
-        next_nonstatic_double_offset += BytesPerLong;
-        break;
-      default:
-        ShouldNotReachHere();
-    }
-    fs.set_offset(real_offset);
-  }
-
-
-  // Handle the contended cases.
-  //
-  // Each contended field should not intersect the cache line with another contended field.
-  // In the absence of alignment information, we end up with pessimistically separating
-  // the fields with full-width padding.
-  //
-  // Additionally, this should not break alignment for the fields, so we round the alignment up
-  // for each field.
-  if (nonstatic_contended_count > 0) {
-
-    // if there is at least one contended field, we need to have pre-padding for them
-    next_nonstatic_padded_offset += ContendedPaddingWidth;
-
-    // collect all contended groups
-    ResourceBitMap bm(cp->size());
-    for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-      // skip already laid out fields
-      if (fs.is_offset_set()) continue;
-
-      if (fs.is_contended()) {
-        bm.set_bit(fs.contended_group());
-      }
-    }
-
-    int current_group = -1;
-    while ((current_group = (int)bm.get_next_one_offset(current_group + 1)) != (int)bm.size()) {
-
-      for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-
-        // skip already laid out fields
-        if (fs.is_offset_set()) continue;
-
-        // skip non-contended fields and fields from different group
-        if (!fs.is_contended() || (fs.contended_group() != current_group)) continue;
-
-        // handle statics below
-        if (fs.access_flags().is_static()) continue;
-
-        int real_offset = 0;
-        FieldAllocationType atype = (FieldAllocationType) fs.allocation_type();
-
-        switch (atype) {
-          case NONSTATIC_BYTE:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, 1);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += 1;
-            break;
-
-          case NONSTATIC_SHORT:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerShort);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += BytesPerShort;
-            break;
-
-          case NONSTATIC_WORD:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerInt);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += BytesPerInt;
-            break;
-
-          case NONSTATIC_DOUBLE:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += BytesPerLong;
-            break;
-
-            // Inline types in static fields are handled with oops
-          case NONSTATIC_INLINE:
-            throwInlineTypeLimitation(THREAD_AND_LOCATION,
-                                      "@Contended annotation not supported for inline types yet", fs.name(), fs.signature());
-            return;
-
-          case NONSTATIC_OOP:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, heapOopSize);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += heapOopSize;
-            nonstatic_oop_maps->add(real_offset, 1);
-            break;
-
-          default:
-            ShouldNotReachHere();
-        }
-
-        if (fs.contended_group() == 0) {
-          // Contended group defines the equivalence class over the fields:
-          // the fields within the same contended group are not inter-padded.
-          // The only exception is default group, which does not incur the
-          // equivalence, and so requires intra-padding.
-          next_nonstatic_padded_offset += ContendedPaddingWidth;
-        }
-
-        fs.set_offset(real_offset);
-      } // for
-
-      // Start laying out the next group.
-      // Note that this will effectively pad the last group in the back;
-      // this is expected to alleviate memory contention effects for
-      // subclass fields and/or adjacent object.
-      // If this was the default group, the padding is already in place.
-      if (current_group != 0) {
-        next_nonstatic_padded_offset += ContendedPaddingWidth;
-      }
-    }
-
-    // handle static fields
-  }
-
-  // Entire class is contended, pad in the back.
-  // This helps to alleviate memory contention effects for subclass fields
-  // and/or adjacent object.
-  if (is_contended_class) {
-    assert(!is_inline_type(), "@Contended not supported for inline types yet");
-    next_nonstatic_padded_offset += ContendedPaddingWidth;
-  }
-
-  int notaligned_nonstatic_fields_end;
-  if (nonstatic_inline_type_count != 0) {
-    notaligned_nonstatic_fields_end = next_nonstatic_inline_type_offset;
-  } else {
-    notaligned_nonstatic_fields_end = next_nonstatic_padded_offset;
-  }
-
-  int nonstatic_field_sz_align = heapOopSize;
-  if (is_inline_type()) {
-    if ((notaligned_nonstatic_fields_end - nonstatic_fields_start) > heapOopSize) {
-      nonstatic_field_sz_align = BytesPerLong; // value copy of fields only uses jlong copy
-    }
-  }
-  int nonstatic_fields_end      = align_up(notaligned_nonstatic_fields_end, nonstatic_field_sz_align);
-  int instance_end              = align_up(notaligned_nonstatic_fields_end, wordSize);
-  int static_fields_end         = align_up(next_static_byte_offset, wordSize);
-
-  int static_field_size         = (static_fields_end -
-                                   InstanceMirrorKlass::offset_of_static_fields()) / wordSize;
-  nonstatic_field_size          = nonstatic_field_size +
-                                  (nonstatic_fields_end - nonstatic_fields_start) / heapOopSize;
-
-  int instance_size             = align_object_size(instance_end / wordSize);
-
-  assert(instance_size == align_object_size(align_up(
-         (instanceOopDesc::base_offset_in_bytes() + nonstatic_field_size*heapOopSize)
-         + initial_inline_type_padding, wordSize) / wordSize), "consistent layout helper value");
-
-
-  // Invariant: nonstatic_field end/start should only change if there are
-  // nonstatic fields in the class, or if the class is contended. We compare
-  // against the non-aligned value, so that end alignment will not fail the
-  // assert without actually having the fields.
-  assert((notaligned_nonstatic_fields_end == nonstatic_fields_start) ||
-         is_contended_class ||
-         (nonstatic_fields_count > 0), "double-check nonstatic start/end");
-
-  // Number of non-static oop map blocks allocated at end of klass.
-  nonstatic_oop_maps->compact();
-
-#ifndef PRODUCT
-  if ((PrintFieldLayout && !is_inline_type()) ||
-      (PrintInlineLayout && (is_inline_type() || has_nonstatic_inline_fields))) {
-    print_field_layout(_class_name,
-          _fields,
-          cp,
-          instance_size,
-          nonstatic_fields_start,
-          nonstatic_fields_end,
-          static_fields_end);
-    nonstatic_oop_maps->print_on(tty);
-    tty->print("\n");
-    tty->print_cr("Instance size = %d", instance_size);
-    tty->print_cr("Nonstatic_field_size = %d", nonstatic_field_size);
-    tty->print_cr("Static_field_size = %d", static_field_size);
-    tty->print_cr("Has nonstatic fields = %d", has_nonstatic_fields);
-    tty->print_cr("---");
-  }
-
-#endif
-  // Pass back information needed for InstanceKlass creation
-  info->oop_map_blocks = nonstatic_oop_maps;
-  info->_instance_size = instance_size;
-  info->_static_field_size = static_field_size;
-  info->_nonstatic_field_size = nonstatic_field_size;
-  info->_has_nonstatic_fields = has_nonstatic_fields;
-  info->_has_inline_fields = nonstatic_inline_type_count > 0;
-
-  // An inline type is naturally atomic if it has just one field, and
-  // that field is simple enough.
-  info->_is_naturally_atomic = (is_inline_type() &&
-                                !super_has_nonstatic_fields &&
-                                (nonstatic_fields_count <= 1) &&
-                                (not_atomic_inline_types == 0) &&
-                                (nonstatic_contended_count == 0));
-  // This may be too restrictive, since if all the fields fit in 64
-  // bits we could make the decision to align instances of this class
-  // to 64-bit boundaries, and load and store them as single words.
-  // And on machines which supported larger atomics we could similarly
-  // allow larger values to be atomic, if properly aligned.
-}
-
 void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   assert(ik != NULL, "invariant");
 
   const Klass* const super = ik->super();
 
@@ -6453,17 +5804,13 @@
     }
   }
 
   if (is_inline_type()) {
     InlineKlass* vk = InlineKlass::cast(ik);
-    if (UseNewFieldLayout) {
-      vk->set_alignment(_alignment);
-      vk->set_first_field_offset(_first_field_offset);
-      vk->set_exact_size_in_bytes(_exact_size_in_bytes);
-    } else {
-      vk->set_first_field_offset(vk->first_field_offset_old());
-    }
+    vk->set_alignment(_alignment);
+    vk->set_first_field_offset(_first_field_offset);
+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);
     InlineKlass::cast(ik)->initialize_calling_convention(CHECK);
   }
 
   ClassLoadingService::notify_class_loaded(ik, false /* not shared class */);
 
@@ -7290,22 +6637,18 @@
       assert(klass->access_flags().is_inline_type(), "Value type expected");
     }
   }
 
   _field_info = new FieldLayoutInfo();
-  if (UseNewFieldLayout) {
-    FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
-        _parsed_annotations->is_contended(), is_inline_type(),
-        loader_data(), _protection_domain, _field_info);
-    lb.build_layout(CHECK);
-    if (is_inline_type()) {
-      _alignment = lb.get_alignment();
-      _first_field_offset = lb.get_first_field_offset();
-      _exact_size_in_bytes = lb.get_exact_size_in_byte();
-    }
-  } else {
-    layout_fields(cp, _fac, _parsed_annotations, _field_info, CHECK);
+  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
+      _parsed_annotations->is_contended(), is_inline_type(),
+      loader_data(), _protection_domain, _field_info);
+  lb.build_layout(CHECK);
+  if (is_inline_type()) {
+    _alignment = lb.get_alignment();
+    _first_field_offset = lb.get_first_field_offset();
+    _exact_size_in_bytes = lb.get_exact_size_in_byte();
   }
   _has_inline_type_fields = _field_info->_has_inline_fields;
 
   // Compute reference type
   _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();
diff a/src/hotspot/share/classfile/classFileParser.hpp b/src/hotspot/share/classfile/classFileParser.hpp
--- a/src/hotspot/share/classfile/classFileParser.hpp
+++ b/src/hotspot/share/classfile/classFileParser.hpp
@@ -562,17 +562,10 @@
                                int runtime_invisible_type_annotations_length,
                                const u1* annotation_default,
                                int annotation_default_length,
                                TRAPS);
 
-  // lays out fields in class and returns the total oopmap count
-  void layout_fields(ConstantPool* cp,
-                     const FieldAllocationCount* fac,
-                     const ClassAnnotationCollector* parsed_annotations,
-                     FieldLayoutInfo* info,
-                     TRAPS);
-
   void update_class_name(Symbol* new_name);
 
   // Check if the class file supports inline types
   bool supports_inline_types() const;
 
diff a/src/hotspot/share/classfile/vmSymbols.hpp b/src/hotspot/share/classfile/vmSymbols.hpp
--- a/src/hotspot/share/classfile/vmSymbols.hpp
+++ b/src/hotspot/share/classfile/vmSymbols.hpp
@@ -127,11 +127,11 @@
   template(java_lang_AssertionStatusDirectives,       "java/lang/AssertionStatusDirectives")      \
   template(getBootClassPathEntryForClass_name,        "getBootClassPathEntryForClass")            \
   template(jdk_internal_vm_PostVMInitHook,            "jdk/internal/vm/PostVMInitHook")           \
   template(sun_net_www_ParseUtil,                     "sun/net/www/ParseUtil")                    \
   template(java_util_Iterator,                        "java/util/Iterator")                       \
-  template(java_lang_Record,                          "java/lang/Record")                       \
+  template(java_lang_Record,                          "java/lang/Record")                         \
                                                                                                   \
   template(jdk_internal_loader_NativeLibraries,       "jdk/internal/loader/NativeLibraries")      \
   template(jdk_internal_loader_ClassLoaders_AppClassLoader,      "jdk/internal/loader/ClassLoaders$AppClassLoader")      \
   template(jdk_internal_loader_ClassLoaders_PlatformClassLoader, "jdk/internal/loader/ClassLoaders$PlatformClassLoader") \
                                                                                                   \
diff a/src/hotspot/share/gc/shared/collectedHeap.hpp b/src/hotspot/share/gc/shared/collectedHeap.hpp
--- a/src/hotspot/share/gc/shared/collectedHeap.hpp
+++ b/src/hotspot/share/gc/shared/collectedHeap.hpp
@@ -491,12 +491,10 @@
 
   // Deduplicate the string, iff the GC supports string deduplication.
   virtual void deduplicate_string(oop str);
 
   virtual bool is_oop(oop object) const;
-
-  virtual size_t obj_size(oop obj) const;
 
   // Non product verification and debugging.
 #ifndef PRODUCT
   // Support for PromotionFailureALot.  Return true if it's time to cause a
   // promotion failure.  The no-argument version uses
diff a/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp b/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp
--- a/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp
+++ b/src/hotspot/share/gc/shenandoah/shenandoahBarrierSet.inline.hpp
@@ -80,11 +80,11 @@
   assert(_satb_mark_queue_set.is_active(), "only get here when SATB active");
 
   // Filter marked objects before hitting the SATB queues. The same predicate would
   // be used by SATBMQ::filter to eliminate already marked objects downstream, but
   // filtering here helps to avoid wasteful SATB queueing work to begin with.
-  if (!_heap->requires_marking<false>(obj)) return;
+  if (!_heap->requires_marking(obj)) return;
 
   ShenandoahThreadLocalData::satb_mark_queue(Thread::current()).enqueue_known_active(obj);
 }
 
 template <DecoratorSet decorators, typename T>
diff a/src/hotspot/share/memory/universe.cpp b/src/hotspot/share/memory/universe.cpp
--- a/src/hotspot/share/memory/universe.cpp
+++ b/src/hotspot/share/memory/universe.cpp
@@ -62,11 +62,10 @@
 #include "oops/typeArrayKlass.hpp"
 #include "prims/resolvedMethodTable.hpp"
 #include "runtime/arguments.hpp"
 #include "runtime/atomic.hpp"
 #include "runtime/deoptimization.hpp"
-#include "runtime/flags/flagSetting.hpp"
 #include "runtime/flags/jvmFlagConstraintList.hpp"
 #include "runtime/handles.inline.hpp"
 #include "runtime/init.hpp"
 #include "runtime/java.hpp"
 #include "runtime/javaCalls.hpp"
@@ -75,10 +74,11 @@
 #include "runtime/thread.inline.hpp"
 #include "runtime/timerTrace.hpp"
 #include "runtime/vmOperations.hpp"
 #include "services/memoryService.hpp"
 #include "utilities/align.hpp"
+#include "utilities/autoRestore.hpp"
 #include "utilities/copy.hpp"
 #include "utilities/debug.hpp"
 #include "utilities/events.hpp"
 #include "utilities/formatBuffer.hpp"
 #include "utilities/hashtable.inline.hpp"
@@ -305,11 +305,11 @@
 }
 
 void Universe::genesis(TRAPS) {
   ResourceMark rm(THREAD);
 
-  { FlagSetting fs(_bootstrapping, true);
+  { AutoModifyRestore<bool> temporarily(_bootstrapping, true);
 
     { MutexLocker mc(THREAD, Compile_lock);
 
       java_lang_Class::allocate_fixup_lists();
 
diff a/src/hotspot/share/oops/instanceKlass.cpp b/src/hotspot/share/oops/instanceKlass.cpp
--- a/src/hotspot/share/oops/instanceKlass.cpp
+++ b/src/hotspot/share/oops/instanceKlass.cpp
@@ -2982,11 +2982,21 @@
   // not needed for shared class since CDS does not archive prohibited classes.
   if (!is_shared()) {
     check_prohibited_package(name(), loader_data, CHECK);
   }
 
-  TempNewSymbol pkg_name = pkg_entry != NULL ? pkg_entry->name() : ClassLoader::package_from_class_name(name());
+  // ClassLoader::package_from_class_name has already incremented the refcount of the symbol
+  // it returns, so we need to decrement it when the current function exits.
+  TempNewSymbol from_class_name =
+      (pkg_entry != NULL) ? NULL : ClassLoader::package_from_class_name(name());
+
+  Symbol* pkg_name;
+  if (pkg_entry != NULL) {
+    pkg_name = pkg_entry->name();
+  } else {
+    pkg_name = from_class_name;
+  }
 
   if (pkg_name != NULL && loader_data != NULL) {
 
     // Find in class loader's package entry table.
     _package_entry = pkg_entry != NULL ? pkg_entry : loader_data->packages()->lookup_only(pkg_name);
diff a/src/hotspot/share/oops/symbol.cpp b/src/hotspot/share/oops/symbol.cpp
--- a/src/hotspot/share/oops/symbol.cpp
+++ b/src/hotspot/share/oops/symbol.cpp
@@ -50,13 +50,11 @@
 
 Symbol::Symbol(const u1* name, int length, int refcount) {
   _hash_and_refcount =  pack_hash_and_refcount((short)os::random(), refcount);
   _length = length;
   _body[0] = 0;  // in case length == 0
-  for (int i = 0; i < length; i++) {
-    byte_at_put(i, name[i]);
-  }
+  memcpy(_body, name, length);
 }
 
 void* Symbol::operator new(size_t sz, int len) throw() {
 #if INCLUDE_CDS
  if (DumpSharedSpaces) {
diff a/src/hotspot/share/oops/symbol.hpp b/src/hotspot/share/oops/symbol.hpp
--- a/src/hotspot/share/oops/symbol.hpp
+++ b/src/hotspot/share/oops/symbol.hpp
@@ -123,15 +123,10 @@
   static int size(int length) {
     // minimum number of natural words needed to hold these bits (no non-heap version)
     return (int)heap_word_size(byte_size(length));
   }
 
-  void byte_at_put(int index, u1 value) {
-    assert(index >=0 && index < length(), "symbol index overflow");
-    _body[index] = value;
-  }
-
   Symbol(const u1* name, int length, int refcount);
   void* operator new(size_t size, int len) throw();
   void* operator new(size_t size, int len, Arena* arena) throw();
 
   void  operator delete(void* p);
diff a/src/hotspot/share/opto/output.cpp b/src/hotspot/share/opto/output.cpp
--- a/src/hotspot/share/opto/output.cpp
+++ b/src/hotspot/share/opto/output.cpp
@@ -2073,12 +2073,12 @@
 
   // Don't optimize this if scheduling is disabled
   if (!C->do_scheduling())
     return;
 
-  // Scheduling code works only with pairs (16 bytes) maximum.
-  if (C->max_vector_size() > 16)
+  // Scheduling code works only with pairs (8 bytes) maximum.
+  if (C->max_vector_size() > 8)
     return;
 
   Compile::TracePhase tp("isched", &timers[_t_instrSched]);
 
   // Create a data structure for all the scheduling information
diff a/src/hotspot/share/opto/stringopts.cpp b/src/hotspot/share/opto/stringopts.cpp
--- a/src/hotspot/share/opto/stringopts.cpp
+++ b/src/hotspot/share/opto/stringopts.cpp
@@ -538,10 +538,19 @@
                (cnode->method()->signature()->as_symbol() == string_sig ||
                 cnode->method()->signature()->as_symbol() == char_sig ||
                 cnode->method()->signature()->as_symbol() == int_sig)) {
       sc->add_control(cnode);
       Node* arg = cnode->in(TypeFunc::Parms + 1);
+      if (arg == NULL || arg->is_top()) {
+#ifndef PRODUCT
+        if (PrintOptimizeStringConcat) {
+          tty->print("giving up because the call is effectively dead");
+          cnode->jvms()->dump_spec(tty); tty->cr();
+        }
+#endif
+        break;
+      }
       if (cnode->method()->signature()->as_symbol() == int_sig) {
         sc->push_int(arg);
       } else if (cnode->method()->signature()->as_symbol() == char_sig) {
         sc->push_char(arg);
       } else {
diff a/src/hotspot/share/prims/jvmtiEnv.cpp b/src/hotspot/share/prims/jvmtiEnv.cpp
--- a/src/hotspot/share/prims/jvmtiEnv.cpp
+++ b/src/hotspot/share/prims/jvmtiEnv.cpp
@@ -484,11 +484,11 @@
 // size_ptr - pre-checked for NULL
 jvmtiError
 JvmtiEnv::GetObjectSize(jobject object, jlong* size_ptr) {
   oop mirror = JNIHandles::resolve_external_guard(object);
   NULL_CHECK(mirror, JVMTI_ERROR_INVALID_OBJECT);
-  *size_ptr = (jlong)Universe::heap()->obj_size(mirror) * wordSize;
+  *size_ptr = (jlong)mirror->size() * wordSize;
   return JVMTI_ERROR_NONE;
 } /* end GetObjectSize */
 
   //
   // Method functions
@@ -1533,19 +1533,18 @@
 jvmtiError
 JvmtiEnv::GetStackTrace(JavaThread* java_thread, jint start_depth, jint max_frame_count, jvmtiFrameInfo* frame_buffer, jint* count_ptr) {
   jvmtiError err = JVMTI_ERROR_NONE;
 
   // It is only safe to perform the direct operation on the current
-  // thread. All other usage needs to use a vm-safepoint-op for safety.
+  // thread. All other usage needs to use a direct handshake for safety.
   if (java_thread == JavaThread::current()) {
     err = get_stack_trace(java_thread, start_depth, max_frame_count, frame_buffer, count_ptr);
   } else {
-    // JVMTI get stack trace at safepoint. Do not require target thread to
-    // be suspended.
-    VM_GetStackTrace op(this, java_thread, start_depth, max_frame_count, frame_buffer, count_ptr);
-    VMThread::execute(&op);
-    err = op.result();
+    // Get stack trace with handshake.
+    GetStackTraceClosure op(this, start_depth, max_frame_count, frame_buffer, count_ptr);
+    bool executed = Handshake::execute_direct(&op, java_thread);
+    err = executed ? op.result() : JVMTI_ERROR_THREAD_NOT_ALIVE;
   }
 
   return err;
 } /* end GetStackTrace */
 
@@ -1573,16 +1572,35 @@
 // max_frame_count - pre-checked to be greater than or equal to 0
 // stack_info_ptr - pre-checked for NULL
 jvmtiError
 JvmtiEnv::GetThreadListStackTraces(jint thread_count, const jthread* thread_list, jint max_frame_count, jvmtiStackInfo** stack_info_ptr) {
   jvmtiError err = JVMTI_ERROR_NONE;
-  // JVMTI get stack traces at safepoint.
-  VM_GetThreadListStackTraces op(this, thread_count, thread_list, max_frame_count);
-  VMThread::execute(&op);
-  err = op.result();
-  if (err == JVMTI_ERROR_NONE) {
-    *stack_info_ptr = op.stack_info();
+
+  if (thread_count == 1) {
+    // Use direct handshake if we need to get only one stack trace.
+    JavaThread *current_thread = JavaThread::current();
+    ThreadsListHandle tlh(current_thread);
+    JavaThread *java_thread;
+    err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), *thread_list, &java_thread, NULL);
+    if (err != JVMTI_ERROR_NONE) {
+      return err;
+    }
+
+    GetSingleStackTraceClosure op(this, current_thread, *thread_list, max_frame_count);
+    bool executed = Handshake::execute_direct(&op, java_thread);
+    err = executed ? op.result() : JVMTI_ERROR_THREAD_NOT_ALIVE;
+    if (err == JVMTI_ERROR_NONE) {
+      *stack_info_ptr = op.stack_info();
+    }
+  } else {
+    // JVMTI get stack traces at safepoint.
+    VM_GetThreadListStackTraces op(this, thread_count, thread_list, max_frame_count);
+    VMThread::execute(&op);
+    err = op.result();
+    if (err == JVMTI_ERROR_NONE) {
+      *stack_info_ptr = op.stack_info();
+    }
   }
   return err;
 } /* end GetThreadListStackTraces */
 
 
diff a/src/hotspot/share/prims/jvmtiEnvBase.cpp b/src/hotspot/share/prims/jvmtiEnvBase.cpp
--- a/src/hotspot/share/prims/jvmtiEnvBase.cpp
+++ b/src/hotspot/share/prims/jvmtiEnvBase.cpp
@@ -814,17 +814,18 @@
                               jint start_depth, jint max_count,
                               jvmtiFrameInfo* frame_buffer, jint* count_ptr) {
 #ifdef ASSERT
   uint32_t debug_bits = 0;
 #endif
-  assert((SafepointSynchronize::is_at_safepoint() ||
-          java_thread->is_thread_fully_suspended(false, &debug_bits)),
-         "at safepoint or target thread is suspended");
+  Thread *current_thread = Thread::current();
+  assert(current_thread == java_thread ||
+         SafepointSynchronize::is_at_safepoint() ||
+         current_thread == java_thread->active_handshaker(),
+         "call by myself / at safepoint / at handshake");
   int count = 0;
   if (java_thread->has_last_Java_frame()) {
     RegisterMap reg_map(java_thread);
-    Thread* current_thread = Thread::current();
     ResourceMark rm(current_thread);
     javaVFrame *jvf = java_thread->last_java_vframe(&reg_map);
     HandleMark hm(current_thread);
     if (start_depth != 0) {
       if (start_depth > 0) {
@@ -1157,12 +1158,18 @@
 // buffer for the frame information, both allocated as resource objects.
 // Fill in both the jvmtiStackInfo and the jvmtiFrameInfo.
 // Note that either or both of thr and thread_oop
 // may be null if the thread is new or has exited.
 void
-VM_GetMultipleStackTraces::fill_frames(jthread jt, JavaThread *thr, oop thread_oop) {
-  assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
+MultipleStackTracesCollector::fill_frames(jthread jt, JavaThread *thr, oop thread_oop) {
+#ifdef ASSERT
+  Thread *current_thread = Thread::current();
+  assert(current_thread == thr ||
+         SafepointSynchronize::is_at_safepoint() ||
+         current_thread == thr->active_handshaker(),
+         "call by myself / at safepoint / at handshake");
+#endif
 
   jint state = 0;
   struct StackInfoNode *node = NEW_RESOURCE_OBJ(struct StackInfoNode);
   jvmtiStackInfo *infop = &(node->info);
   node->next = head();
@@ -1202,11 +1209,11 @@
 }
 
 // Based on the stack information in the linked list, allocate memory
 // block to return and fill it from the info in the linked list.
 void
-VM_GetMultipleStackTraces::allocate_and_fill_stacks(jint thread_count) {
+MultipleStackTracesCollector::allocate_and_fill_stacks(jint thread_count) {
   // do I need to worry about alignment issues?
   jlong alloc_size =  thread_count       * sizeof(jvmtiStackInfo)
                     + _frame_count_total * sizeof(jvmtiFrameInfo);
   env()->allocate(alloc_size, (unsigned char **)&_stack_info);
 
@@ -1251,18 +1258,31 @@
     if (err != JVMTI_ERROR_NONE) {
       // We got an error code so we don't have a JavaThread *, but
       // only return an error from here if we didn't get a valid
       // thread_oop.
       if (thread_oop == NULL) {
-        set_result(err);
+        _collector.set_result(err);
         return;
       }
       // We have a valid thread_oop.
     }
-    fill_frames(jt, java_thread, thread_oop);
+    _collector.fill_frames(jt, java_thread, thread_oop);
+  }
+  _collector.allocate_and_fill_stacks(_thread_count);
+}
+
+void
+GetSingleStackTraceClosure::do_thread(Thread *target) {
+  assert(target->is_Java_thread(), "just checking");
+  JavaThread *jt = (JavaThread *)target;
+  oop thread_oop = jt->threadObj();
+
+  if (!jt->is_exiting() && thread_oop != NULL) {
+    ResourceMark rm;
+    _collector.fill_frames(_jthread, jt, thread_oop);
+    _collector.allocate_and_fill_stacks(1);
   }
-  allocate_and_fill_stacks(_thread_count);
 }
 
 void
 VM_GetAllStackTraces::doit() {
   assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
@@ -1275,15 +1295,15 @@
         !jt->is_exiting() &&
         java_lang_Thread::is_alive(thread_oop) &&
         !jt->is_hidden_from_external_view()) {
       ++_final_thread_count;
       // Handle block of the calling thread is used to create local refs.
-      fill_frames((jthread)JNIHandles::make_local(_calling_thread, thread_oop),
-                  jt, thread_oop);
+      _collector.fill_frames((jthread)JNIHandles::make_local(_calling_thread, thread_oop),
+                             jt, thread_oop);
     }
   }
-  allocate_and_fill_stacks(_final_thread_count);
+  _collector.allocate_and_fill_stacks(_final_thread_count);
 }
 
 // Verifies that the top frame is a java frame in an expected state.
 // Deoptimizes frame if needed.
 // Checks that the frame method signature matches the return type (tos).
@@ -1531,16 +1551,15 @@
                                                                     _owned_monitor_ptr);
   }
 }
 
 void
-VM_GetStackTrace::doit() {
-  _result = JVMTI_ERROR_THREAD_NOT_ALIVE;
-  ThreadsListHandle tlh;
-  if (_java_thread != NULL && tlh.includes(_java_thread)
-      && !_java_thread->is_exiting() && _java_thread->threadObj() != NULL) {
-    _result = ((JvmtiEnvBase *)_env)->get_stack_trace(_java_thread,
+GetStackTraceClosure::do_thread(Thread *target) {
+  assert(target->is_Java_thread(), "just checking");
+  JavaThread *jt = (JavaThread *)target;
+  if (!jt->is_exiting() && jt->threadObj() != NULL) {
+    _result = ((JvmtiEnvBase *)_env)->get_stack_trace(jt,
                                                       _start_depth, _max_count,
                                                       _frame_buffer, _count_ptr);
   }
 }
 
diff a/src/hotspot/share/prims/unsafe.cpp b/src/hotspot/share/prims/unsafe.cpp
--- a/src/hotspot/share/prims/unsafe.cpp
+++ b/src/hotspot/share/prims/unsafe.cpp
@@ -777,11 +777,11 @@
 } UNSAFE_END
 
 
 UNSAFE_ENTRY(jlong, Unsafe_GetObjectSize0(JNIEnv* env, jobject o, jobject obj))
   oop p = JNIHandles::resolve(obj);
-  return Universe::heap()->obj_size(p) * HeapWordSize;
+  return p->size() * HeapWordSize;
 UNSAFE_END
 
 
 static inline void throw_new(JNIEnv *env, const char *ename) {
   jclass cls = env->FindClass(ename);
diff a/src/hotspot/share/prims/whitebox.cpp b/src/hotspot/share/prims/whitebox.cpp
--- a/src/hotspot/share/prims/whitebox.cpp
+++ b/src/hotspot/share/prims/whitebox.cpp
@@ -387,11 +387,11 @@
   return !gch->is_in_young(p);
 WB_END
 
 WB_ENTRY(jlong, WB_GetObjectSize(JNIEnv* env, jobject o, jobject obj))
   oop p = JNIHandles::resolve(obj);
-  return Universe::heap()->obj_size(p) * HeapWordSize;
+  return p->size() * HeapWordSize;
 WB_END
 
 WB_ENTRY(jlong, WB_GetHeapSpaceAlignment(JNIEnv* env, jobject o))
   return (jlong)SpaceAlignment;
 WB_END
diff a/src/hotspot/share/runtime/arguments.cpp b/src/hotspot/share/runtime/arguments.cpp
--- a/src/hotspot/share/runtime/arguments.cpp
+++ b/src/hotspot/share/runtime/arguments.cpp
@@ -520,11 +520,10 @@
   { "MinRAMFraction",               JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "InitialRAMFraction",           JDK_Version::jdk(10),  JDK_Version::undefined(), JDK_Version::undefined() },
   { "UseMembar",                    JDK_Version::jdk(10), JDK_Version::jdk(12), JDK_Version::undefined() },
   { "AllowRedefinitionToAddDeleteMethods", JDK_Version::jdk(13), JDK_Version::undefined(), JDK_Version::undefined() },
   { "FlightRecorder",               JDK_Version::jdk(13), JDK_Version::undefined(), JDK_Version::undefined() },
-  { "UseNewFieldLayout",            JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "ForceNUMA",                    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "UseBiasedLocking",             JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "BiasedLockingStartupDelay",    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "PrintBiasedLockingStatistics", JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "BiasedLockingBulkRebiasThreshold",    JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
@@ -550,10 +549,11 @@
 #ifdef BSD
   { "UseBsdPosixThreadCPUClocks",    JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },
   { "UseOprofile",                   JDK_Version::undefined(), JDK_Version::jdk(16), JDK_Version::jdk(17) },
 #endif
   { "PrintVMQWaitTime",              JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
+  { "UseNewFieldLayout",             JDK_Version::jdk(15), JDK_Version::jdk(16), JDK_Version::jdk(17) },
 
 #ifdef TEST_VERIFY_SPECIAL_JVM_FLAGS
   // These entries will generate build errors.  Their purpose is to test the macros.
   { "dep > obs",                    JDK_Version::jdk(9), JDK_Version::jdk(8), JDK_Version::undefined() },
   { "dep > exp ",                   JDK_Version::jdk(9), JDK_Version::undefined(), JDK_Version::jdk(8) },
diff a/src/hotspot/share/runtime/globals.hpp b/src/hotspot/share/runtime/globals.hpp
--- a/src/hotspot/share/runtime/globals.hpp
+++ b/src/hotspot/share/runtime/globals.hpp
@@ -686,13 +686,10 @@
                                                                             \
   experimental(bool, DisablePrimordialThreadGuardPages, false,              \
                "Disable the use of stack guard pages if the JVM is loaded " \
                "on the primordial process thread")                          \
                                                                             \
-  diagnostic(bool, AsyncDeflateIdleMonitors, true,                          \
-          "Deflate idle monitors using the ServiceThread.")                 \
-                                                                            \
   /* notice: the max range value here is max_jint, not max_intx  */         \
   /* because of overflow issue                                   */         \
   diagnostic(intx, AsyncDeflationInterval, 250,                             \
           "Async deflate idle monitors every so many milliseconds when "    \
           "MonitorUsedDeflationThreshold is exceeded (0 is off).")          \
@@ -2491,13 +2488,10 @@
   diagnostic(ccstrlist, ForceNonTearable, "",                               \
           "List of inline classes which are forced to be atomic "           \
           "(whitespace and commas separate names, "                         \
           "and leading and trailing stars '*' are wildcards)")              \
                                                                             \
-  product(bool, UseNewFieldLayout, true,                                    \
-                "(Deprecated) Use new algorithm to compute field layouts")  \
-                                                                            \
   product(bool, UseEmptySlotsInSupers, true,                                \
                 "Allow allocating fields in empty slots of super-classes")  \
                                                                             \
   diagnostic(bool, DeoptimizeNMethodBarriersALot, false,                    \
                 "Make nmethod barriers deoptimise a lot.")                  \
diff a/src/hotspot/share/runtime/init.cpp b/src/hotspot/share/runtime/init.cpp
--- a/src/hotspot/share/runtime/init.cpp
+++ b/src/hotspot/share/runtime/init.cpp
@@ -168,14 +168,12 @@
   if (!destructorsCalled) {
     destructorsCalled = true;
     if (log_is_enabled(Info, monitorinflation)) {
       // The ObjectMonitor subsystem uses perf counters so
       // do this before perfMemory_exit().
-      // These other two audit_and_print_stats() calls are done at the
+      // This other audit_and_print_stats() call is done at the
       // Debug level at a safepoint:
-      // - for safepoint based deflation auditing:
-      //   ObjectSynchronizer::finish_deflate_idle_monitors()
       // - for async deflation auditing:
       //   ObjectSynchronizer::do_safepoint_work()
       ObjectSynchronizer::audit_and_print_stats(true /* on_exit */);
     }
     perfMemory_exit();
diff a/src/hotspot/share/runtime/safepoint.cpp b/src/hotspot/share/runtime/safepoint.cpp
--- a/src/hotspot/share/runtime/safepoint.cpp
+++ b/src/hotspot/share/runtime/safepoint.cpp
@@ -489,65 +489,35 @@
 
   post_safepoint_end_event(event, safepoint_id());
 }
 
 bool SafepointSynchronize::is_cleanup_needed() {
-  // Need a cleanup safepoint if there are too many monitors in use
-  // and the monitor deflation needs to be done at a safepoint.
-  if (ObjectSynchronizer::is_safepoint_deflation_needed()) return true;
   // Need a safepoint if some inline cache buffers is non-empty
   if (!InlineCacheBuffer::is_empty()) return true;
   if (StringTable::needs_rehashing()) return true;
   if (SymbolTable::needs_rehashing()) return true;
   return false;
 }
 
-class ParallelSPCleanupThreadClosure : public ThreadClosure {
-private:
-  DeflateMonitorCounters* _counters;
-
-public:
-  ParallelSPCleanupThreadClosure(DeflateMonitorCounters* counters) :
-    _counters(counters) {}
-
-  void do_thread(Thread* thread) {
-    // deflate_thread_local_monitors() handles or requests deflation of
-    // this thread's idle monitors. If !AsyncDeflateIdleMonitors or if
-    // there is a special cleanup request, deflation is handled now.
-    // Otherwise, async deflation is requested via a flag.
-    ObjectSynchronizer::deflate_thread_local_monitors(thread, _counters);
-  }
-};
-
 class ParallelSPCleanupTask : public AbstractGangTask {
 private:
   SubTasksDone _subtasks;
-  ParallelSPCleanupThreadClosure _cleanup_threads_cl;
-  uint _num_workers;
-  DeflateMonitorCounters* _counters;
+  uint _num_workers;
 public:
   ParallelSPCleanupTask(uint num_workers, DeflateMonitorCounters* counters) :
     AbstractGangTask("Parallel Safepoint Cleanup"),
     _subtasks(SafepointSynchronize::SAFEPOINT_CLEANUP_NUM_TASKS),
-    _cleanup_threads_cl(ParallelSPCleanupThreadClosure(counters)),
-    _num_workers(num_workers),
-    _counters(counters) {}
+    _num_workers(num_workers) {}
 
   void work(uint worker_id) {
     uint64_t safepoint_id = SafepointSynchronize::safepoint_id();
-    // All threads deflate monitors and mark nmethods (if necessary).
-    Threads::possibly_parallel_threads_do(true, &_cleanup_threads_cl);
 
     if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_DEFLATE_MONITORS)) {
-      const char* name = "deflating global idle monitors";
+      const char* name = "deflating idle monitors";
       EventSafepointCleanupTask event;
       TraceTime timer(name, TRACETIME_LOG(Info, safepoint, cleanup));
-      // AsyncDeflateIdleMonitors only uses DeflateMonitorCounters
-      // when a special cleanup has been requested.
-      // Note: This logging output will include global idle monitor
-      // elapsed times, but not global idle monitor deflation count.
-      ObjectSynchronizer::do_safepoint_work(_counters);
+      ObjectSynchronizer::do_safepoint_work();
 
       post_safepoint_cleanup_task_event(event, safepoint_id, name);
     }
 
     if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES)) {
@@ -614,27 +584,21 @@
 // Various cleaning tasks that should be done periodically at safepoints.
 void SafepointSynchronize::do_cleanup_tasks() {
 
   TraceTime timer("safepoint cleanup tasks", TRACETIME_LOG(Info, safepoint, cleanup));
 
-  // Prepare for monitor deflation.
-  DeflateMonitorCounters deflate_counters;
-  ObjectSynchronizer::prepare_deflate_idle_monitors(&deflate_counters);
-
   CollectedHeap* heap = Universe::heap();
   assert(heap != NULL, "heap not initialized yet?");
   WorkGang* cleanup_workers = heap->get_safepoint_workers();
   if (cleanup_workers != NULL) {
     // Parallel cleanup using GC provided thread pool.
     uint num_cleanup_workers = cleanup_workers->active_workers();
-    ParallelSPCleanupTask cleanup(num_cleanup_workers, &deflate_counters);
-    StrongRootsScope srs(num_cleanup_workers);
+    ParallelSPCleanupTask cleanup(num_cleanup_workers);
     cleanup_workers->run_task(&cleanup);
   } else {
     // Serial cleanup using VMThread.
-    ParallelSPCleanupTask cleanup(1, &deflate_counters);
-    StrongRootsScope srs(1);
+    ParallelSPCleanupTask cleanup(1);
     cleanup.work(0);
   }
 
   // Needs to be done single threaded by the VMThread.  This walks
   // the thread stacks looking for references to metadata before
@@ -643,13 +607,10 @@
     const char* name = "cleanup live ClassLoaderData metaspaces";
     TraceTime timer(name, TRACETIME_LOG(Info, safepoint, cleanup));
     ClassLoaderDataGraph::walk_metadata_and_clean_metaspaces();
   }
 
-  // Finish monitor deflation.
-  ObjectSynchronizer::finish_deflate_idle_monitors(&deflate_counters);
-
   assert(InlineCacheBuffer::is_empty(), "should have cleaned up ICBuffer");
 }
 
 // Methods for determining if a JavaThread is safepoint safe.
 
diff a/src/hotspot/share/runtime/synchronizer.cpp b/src/hotspot/share/runtime/synchronizer.cpp
--- a/src/hotspot/share/runtime/synchronizer.cpp
+++ b/src/hotspot/share/runtime/synchronizer.cpp
@@ -503,19 +503,15 @@
   assert(!EnableValhalla || !obj->klass()->is_inline_klass(), "monitor op on inline type");
   const markWord mark = obj->mark();
 
   if (mark.has_monitor()) {
     ObjectMonitor* const m = mark.monitor();
-    if (AsyncDeflateIdleMonitors) {
-      // An async deflation can race us before we manage to make the
-      // ObjectMonitor busy by setting the owner below. If we detect
-      // that race we just bail out to the slow-path here.
-      if (m->object() == NULL) {
-        return false;
-      }
-    } else {
-      assert(m->object() == obj, "invariant");
+    // An async deflation can race us before we manage to make the
+    // ObjectMonitor busy by setting the owner below. If we detect
+    // that race we just bail out to the slow-path here.
+    if (m->object() == NULL) {
+      return false;
     }
     Thread* const owner = (Thread *) m->_owner;
 
     // Lock contention and Transactional Lock Elision (TLE) diagnostics
     // and observability
@@ -1015,13 +1011,12 @@
     // VM should be calling bootstrap method
     ShouldNotReachHere();
   }
   if (UseBiasedLocking) {
     // NOTE: many places throughout the JVM do not expect a safepoint
-    // to be taken here, in particular most operations on perm gen
-    // objects. However, we only ever bias Java instances and all of
-    // the call sites of identity_hash that might revoke biases have
+    // to be taken here. However, we only ever bias Java instances and all
+    // of the call sites of identity_hash that might revoke biases have
     // been checked to make sure they can handle a safepoint. The
     // added check of the bias pattern is to avoid useless calls to
     // thread-local storage.
     if (obj->mark().has_bias_pattern()) {
       // Handle for oop obj in case of STW safepoint
@@ -1219,12 +1214,10 @@
     return self->is_lock_owned((address)mark.locker()) ?
       owner_self : owner_other;
   }
 
   // CASE: inflated. Mark (tagged pointer) points to an ObjectMonitor.
-  // The Object:ObjectMonitor relationship is stable as long as we're
-  // not at a safepoint and AsyncDeflateIdleMonitors is false.
   if (mark.has_monitor()) {
     // The first stage of async deflation does not affect any field
     // used by this comparison so the ObjectMonitor* is usable here.
     ObjectMonitor* monitor = mark.monitor();
     void* owner = monitor->owner();
@@ -1320,13 +1313,10 @@
   }
   return false;
 }
 
 bool ObjectSynchronizer::is_async_deflation_needed() {
-  if (!AsyncDeflateIdleMonitors) {
-    return false;
-  }
   if (is_async_deflation_requested()) {
     // Async deflation request.
     return true;
   }
   if (AsyncDeflationInterval > 0 &&
@@ -1339,52 +1329,37 @@
     return true;
   }
   return false;
 }
 
-bool ObjectSynchronizer::is_safepoint_deflation_needed() {
-  return !AsyncDeflateIdleMonitors &&
-         monitors_used_above_threshold();  // Too many monitors in use.
-}
-
 bool ObjectSynchronizer::request_deflate_idle_monitors() {
   bool is_JavaThread = Thread::current()->is_Java_thread();
   bool ret_code = false;
 
-  if (AsyncDeflateIdleMonitors) {
-    jlong last_time = last_async_deflation_time_ns();
-    set_is_async_deflation_requested(true);
-    {
-      MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);
-      ml.notify_all();
-    }
-    const int N_CHECKS = 5;
-    for (int i = 0; i < N_CHECKS; i++) {  // sleep for at most 5 seconds
-      if (last_async_deflation_time_ns() > last_time) {
-        log_info(monitorinflation)("Async Deflation happened after %d check(s).", i);
-        ret_code = true;
-        break;
-      }
-      if (is_JavaThread) {
-        // JavaThread has to honor the blocking protocol.
-        ThreadBlockInVM tbivm(JavaThread::current());
-        os::naked_short_sleep(999);  // sleep for almost 1 second
-      } else {
-        os::naked_short_sleep(999);  // sleep for almost 1 second
-      }
+  jlong last_time = last_async_deflation_time_ns();
+  set_is_async_deflation_requested(true);
+  {
+    MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);
+    ml.notify_all();
+  }
+  const int N_CHECKS = 5;
+  for (int i = 0; i < N_CHECKS; i++) {  // sleep for at most 5 seconds
+    if (last_async_deflation_time_ns() > last_time) {
+      log_info(monitorinflation)("Async Deflation happened after %d check(s).", i);
+      ret_code = true;
+      break;
     }
-    if (!ret_code) {
-      log_info(monitorinflation)("Async Deflation DID NOT happen after %d checks.", N_CHECKS);
+    if (is_JavaThread) {
+      // JavaThread has to honor the blocking protocol.
+      ThreadBlockInVM tbivm(JavaThread::current());
+      os::naked_short_sleep(999);  // sleep for almost 1 second
+    } else {
+      os::naked_short_sleep(999);  // sleep for almost 1 second
     }
-  } else {
-    // Only need to force this safepoint if we are not using async
-    // deflation. The VMThread won't call this function before the
-    // final safepoint if we are not using async deflation so we
-    // don't have to reason about the VMThread executing a VM-op here.
-    VM_ForceSafepoint force_safepoint_op;
-    VMThread::execute(&force_safepoint_op);
-    ret_code = true;
+  }
+  if (!ret_code) {
+    log_info(monitorinflation)("Async Deflation DID NOT happen after %d checks.", N_CHECKS);
   }
 
   return ret_code;
 }
 
@@ -1422,13 +1397,13 @@
 
 // -----------------------------------------------------------------------------
 // ObjectMonitor Lifecycle
 // -----------------------
 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
-// free list and associates them with objects. Deflation -- which occurs at
-// STW-time or asynchronously -- disassociates idle monitors from objects.
-// Such scavenged monitors are returned to the om_list_globals._free_list.
+// free list and associates them with objects. Async deflation disassociates
+// idle monitors from objects. Such scavenged monitors are returned to the
+// om_list_globals._free_list.
 //
 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
 //
 // Lifecycle:
 // --   unassigned and on the om_list_globals._free_list
@@ -1437,11 +1412,11 @@
 //      to the ObjectMonitor.
 
 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
   // A large MAXPRIVATE value reduces both list lock contention
   // and list coherency traffic, but also tends to increase the
-  // number of ObjectMonitors in circulation as well as the STW
+  // number of ObjectMonitors in circulation as well as the
   // scavenge costs.  As usual, we lean toward time in space-time
   // tradeoffs.
   const int MAXPRIVATE = 1024;
   NoSafepointVerifier nsv;
 
@@ -1473,27 +1448,25 @@
         ObjectMonitor* take = take_from_start_of_global_free_list();
         if (take == NULL) {
           break;  // No more are available.
         }
         guarantee(take->object() == NULL, "invariant");
-        if (AsyncDeflateIdleMonitors) {
-          // We allowed 3 field values to linger during async deflation.
-          // Clear or restore them as appropriate.
-          take->set_header(markWord::zero());
-          // DEFLATER_MARKER is the only non-NULL value we should see here.
-          take->try_set_owner_from(DEFLATER_MARKER, NULL);
-          if (take->contentions() < 0) {
-            // Add back max_jint to restore the contentions field to its
-            // proper value.
-            take->add_to_contentions(max_jint);
+        // We allowed 3 field values to linger during async deflation.
+        // Clear or restore them as appropriate.
+        take->set_header(markWord::zero());
+        // DEFLATER_MARKER is the only non-NULL value we should see here.
+        take->try_set_owner_from(DEFLATER_MARKER, NULL);
+        if (take->contentions() < 0) {
+          // Add back max_jint to restore the contentions field to its
+          // proper value.
+          take->add_to_contentions(max_jint);
 
 #ifdef ASSERT
-            jint l_contentions = take->contentions();
+          jint l_contentions = take->contentions();
+          assert(l_contentions >= 0, "must not be negative: l_contentions=%d, contentions=%d",
+                 l_contentions, take->contentions());
 #endif
-            assert(l_contentions >= 0, "must not be negative: l_contentions=%d, contentions=%d",
-                   l_contentions, take->contentions());
-          }
         }
         take->Recycle();
         // Since we're taking from the global free-list, take must be Free.
         // om_release() also sets the allocation state to Free because it
         // is called from other code paths.
@@ -1555,12 +1528,12 @@
 // a CAS attempt failed. This doesn't allow unbounded #s of monitors to
 // accumulate on a thread's free list.
 //
 // Key constraint: all ObjectMonitors on a thread's free list and the global
 // free list must have their object field set to null. This prevents the
-// scavenger -- deflate_monitor_list() or deflate_monitor_list_using_JT()
-// -- from reclaiming them while we are trying to release them.
+// scavenger -- deflate_monitor_list_using_JT() -- from reclaiming them
+// while we are trying to release them.
 
 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
                                     bool from_per_thread_alloc) {
   guarantee(m->header().value() == 0, "invariant");
   guarantee(m->object() == NULL, "invariant");
@@ -1665,19 +1638,17 @@
 // lists to the appropriate global lists. The ObjectMonitors on the
 // per-thread in-use list may still be in use by other threads.
 //
 // We currently call om_flush() from Threads::remove() before the
 // thread has been excised from the thread list and is no longer a
-// mutator. This means that om_flush() cannot run concurrently with
-// a safepoint and interleave with deflate_idle_monitors(). In
-// particular, this ensures that the thread's in-use monitors are
-// scanned by a GC safepoint, either via Thread::oops_do() (before
-// om_flush() is called) or via ObjectSynchronizer::oops_do() (after
-// om_flush() is called).
+// mutator. In particular, this ensures that the thread's in-use
+// monitors are scanned by a GC safepoint, either via Thread::oops_do()
+// (before om_flush() is called) or via ObjectSynchronizer::oops_do()
+// (after om_flush() is called).
 //
-// With AsyncDeflateIdleMonitors, deflate_global_idle_monitors_using_JT()
-// and deflate_per_thread_idle_monitors_using_JT() (in another thread) can
+// deflate_global_idle_monitors_using_JT() and
+// deflate_per_thread_idle_monitors_using_JT() (in another thread) can
 // run at the same time as om_flush() so we have to follow a careful
 // protocol to prevent list corruption.
 
 void ObjectSynchronizer::om_flush(Thread* self) {
   // Process the per-thread in-use list first to be consistent.
@@ -1727,13 +1698,15 @@
       in_use_tail = cur_om;
       in_use_count++;
       cur_om = unmarked_next(cur_om);
     }
     guarantee(in_use_tail != NULL, "invariant");
+#ifdef ASSERT
     int l_om_in_use_count = Atomic::load(&self->om_in_use_count);
-    ADIM_guarantee(l_om_in_use_count == in_use_count, "in-use counts don't match: "
-                   "l_om_in_use_count=%d, in_use_count=%d", l_om_in_use_count, in_use_count);
+    assert(l_om_in_use_count == in_use_count, "in-use counts don't match: "
+           "l_om_in_use_count=%d, in_use_count=%d", l_om_in_use_count, in_use_count);
+#endif
     Atomic::store(&self->om_in_use_count, 0);
     // Clear the in-use list head (which also unlocks it):
     Atomic::store(&self->om_in_use_list, (ObjectMonitor*)NULL);
     om_unlock(in_use_list);
   }
@@ -1770,13 +1743,15 @@
         stringStream ss;
         fatal("must be !is_busy: %s", s->is_busy_to_string(&ss));
       }
     }
     guarantee(free_tail != NULL, "invariant");
+#ifdef ASSERT
     int l_om_free_count = Atomic::load(&self->om_free_count);
-    ADIM_guarantee(l_om_free_count == free_count, "free counts don't match: "
-                   "l_om_free_count=%d, free_count=%d", l_om_free_count, free_count);
+    assert(l_om_free_count == free_count, "free counts don't match: "
+           "l_om_free_count=%d, free_count=%d", l_om_free_count, free_count);
+#endif
     Atomic::store(&self->om_free_count, 0);
     Atomic::store(&self->om_free_list, (ObjectMonitor*)NULL);
     om_unlock(free_list);
   }
 
@@ -1855,11 +1830,10 @@
     // CASE: inflated
     if (mark.has_monitor()) {
       ObjectMonitor* inf = mark.monitor();
       markWord dmw = inf->header();
       assert(dmw.is_neutral(), "invariant: header=" INTPTR_FORMAT, dmw.value());
-      assert(AsyncDeflateIdleMonitors || inf->object() == object, "invariant");
       assert(ObjectSynchronizer::verify_objmon_isinpool(inf), "monitor is invalid");
       return inf;
     }
 
     // CASE: inflation in progress - inflating over a stack-lock.
@@ -1941,25 +1915,21 @@
       // object is in the mark.  Furthermore the owner can't complete
       // an unlock on the object, either.
       markWord dmw = mark.displaced_mark_helper();
       // Catch if the object's header is not neutral (not locked and
       // not marked is what we care about here).
-      ADIM_guarantee(dmw.is_neutral(), "invariant: header=" INTPTR_FORMAT, dmw.value());
+      assert(dmw.is_neutral(), "invariant: header=" INTPTR_FORMAT, dmw.value());
 
       // Setup monitor fields to proper values -- prepare the monitor
       m->set_header(dmw);
 
       // Optimization: if the mark.locker stack address is associated
       // with this thread we could simply set m->_owner = self.
       // Note that a thread can inflate an object
       // that it has stack-locked -- as might happen in wait() -- directly
       // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
-      if (AsyncDeflateIdleMonitors) {
-        m->set_owner_from(NULL, DEFLATER_MARKER, mark.locker());
-      } else {
-        m->set_owner_from(NULL, mark.locker());
-      }
+      m->set_owner_from(NULL, DEFLATER_MARKER, mark.locker());
       m->set_object(object);
       // TODO-FIXME: assert BasicLock->dhw != 0.
 
       // Must preserve store ordering. The monitor state must
       // be stable at the time of publishing the monitor address.
@@ -1995,19 +1965,17 @@
     // to inflate and then CAS() again to try to swing _owner from NULL to self.
     // An inflateTry() method that we could call from enter() would be useful.
 
     // Catch if the object's header is not neutral (not locked and
     // not marked is what we care about here).
-    ADIM_guarantee(mark.is_neutral(), "invariant: header=" INTPTR_FORMAT, mark.value());
+    assert(mark.is_neutral(), "invariant: header=" INTPTR_FORMAT, mark.value());
     ObjectMonitor* m = om_alloc(self);
     // prepare m for installation - set monitor to initial state
     m->Recycle();
     m->set_header(mark);
-    if (AsyncDeflateIdleMonitors) {
-      // DEFLATER_MARKER is the only non-NULL value we should see here.
-      m->try_set_owner_from(DEFLATER_MARKER, NULL);
-    }
+    // DEFLATER_MARKER is the only non-NULL value we should see here.
+    m->try_set_owner_from(DEFLATER_MARKER, NULL);
     m->set_object(object);
     m->_Responsible  = NULL;
     m->_SpinDuration = ObjectMonitor::Knob_SpinLimit;       // consider: keep metastats by type/class
 
     if (object->cas_set_mark(markWord::encode(m), mark) != mark) {
@@ -2043,137 +2011,28 @@
     return m;
   }
 }
 
 
-// We maintain a list of in-use monitors for each thread.
-//
-// For safepoint based deflation:
-// deflate_thread_local_monitors() scans a single thread's in-use list, while
-// deflate_idle_monitors() scans only a global list of in-use monitors which
-// is populated only as a thread dies (see om_flush()).
-//
-// These operations are called at all safepoints, immediately after mutators
-// are stopped, but before any objects have moved. Collectively they traverse
-// the population of in-use monitors, deflating where possible. The scavenged
-// monitors are returned to the global monitor free list.
-//
-// Beware that we scavenge at *every* stop-the-world point. Having a large
-// number of monitors in-use could negatively impact performance. We also want
-// to minimize the total # of monitors in circulation, as they incur a small
-// footprint penalty.
-//
-// Perversely, the heap size -- and thus the STW safepoint rate --
-// typically drives the scavenge rate.  Large heaps can mean infrequent GC,
-// which in turn can mean large(r) numbers of ObjectMonitors in circulation.
-// This is an unfortunate aspect of this design.
-//
-// For async deflation:
-// If a special deflation request is made, then the safepoint based
-// deflation mechanism is used. Otherwise, an async deflation request
-// is registered with the ServiceThread and it is notified.
-
-void ObjectSynchronizer::do_safepoint_work(DeflateMonitorCounters* counters) {
+// An async deflation request is registered with the ServiceThread
+// and it is notified.
+void ObjectSynchronizer::do_safepoint_work() {
   assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
 
-  // The per-thread in-use lists are handled in
-  // ParallelSPCleanupThreadClosure::do_thread().
-
-  if (!AsyncDeflateIdleMonitors) {
-    // Use the older mechanism for the global in-use list.
-    ObjectSynchronizer::deflate_idle_monitors(counters);
-    return;
-  }
-
   log_debug(monitorinflation)("requesting async deflation of idle monitors.");
   // Request deflation of idle monitors by the ServiceThread:
   set_is_async_deflation_requested(true);
   MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);
   ml.notify_all();
 
   if (log_is_enabled(Debug, monitorinflation)) {
     // exit_globals()'s call to audit_and_print_stats() is done
     // at the Info level and not at a safepoint.
-    // For safepoint based deflation, audit_and_print_stats() is called
-    // in ObjectSynchronizer::finish_deflate_idle_monitors() at the
-    // Debug level at a safepoint.
     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
   }
 }
 
-// Deflate a single monitor if not in-use
-// Return true if deflated, false if in-use
-bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid, oop obj,
-                                         ObjectMonitor** free_head_p,
-                                         ObjectMonitor** free_tail_p) {
-  bool deflated;
-  // Normal case ... The monitor is associated with obj.
-  const markWord mark = obj->mark();
-  guarantee(mark == markWord::encode(mid), "should match: mark="
-            INTPTR_FORMAT ", encoded mid=" INTPTR_FORMAT, mark.value(),
-            markWord::encode(mid).value());
-  // Make sure that mark.monitor() and markWord::encode() agree:
-  guarantee(mark.monitor() == mid, "should match: monitor()=" INTPTR_FORMAT
-            ", mid=" INTPTR_FORMAT, p2i(mark.monitor()), p2i(mid));
-  const markWord dmw = mid->header();
-  guarantee(dmw.is_neutral(), "invariant: header=" INTPTR_FORMAT, dmw.value());
-
-  if (mid->is_busy()) {
-    // Easy checks are first - the ObjectMonitor is busy so no deflation.
-    deflated = false;
-  } else {
-    // Deflate the monitor if it is no longer being used
-    // It's idle - scavenge and return to the global free list
-    // plain old deflation ...
-    if (log_is_enabled(Trace, monitorinflation)) {
-      ResourceMark rm;
-      log_trace(monitorinflation)("deflate_monitor: "
-                                  "object=" INTPTR_FORMAT ", mark="
-                                  INTPTR_FORMAT ", type='%s'", p2i(obj),
-                                  mark.value(), obj->klass()->external_name());
-    }
-
-    // Restore the header back to obj
-    obj->release_set_mark(dmw);
-    if (AsyncDeflateIdleMonitors) {
-      // clear() expects the owner field to be NULL.
-      // DEFLATER_MARKER is the only non-NULL value we should see here.
-      mid->try_set_owner_from(DEFLATER_MARKER, NULL);
-    }
-    mid->clear();
-
-    assert(mid->object() == NULL, "invariant: object=" INTPTR_FORMAT,
-           p2i(mid->object()));
-    assert(mid->is_free(), "invariant");
-
-    // Move the deflated ObjectMonitor to the working free list
-    // defined by free_head_p and free_tail_p.
-    if (*free_head_p == NULL) *free_head_p = mid;
-    if (*free_tail_p != NULL) {
-      // We append to the list so the caller can use mid->_next_om
-      // to fix the linkages in its context.
-      ObjectMonitor* prevtail = *free_tail_p;
-      // Should have been cleaned up by the caller:
-      // Note: Should not have to lock prevtail here since we're at a
-      // safepoint and ObjectMonitors on the local free list should
-      // not be accessed in parallel.
-#ifdef ASSERT
-      ObjectMonitor* l_next_om = prevtail->next_om();
-#endif
-      assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
-      prevtail->set_next_om(mid);
-    }
-    *free_tail_p = mid;
-    // At this point, mid->_next_om still refers to its current
-    // value and another ObjectMonitor's _next_om field still
-    // refers to this ObjectMonitor. Those linkages have to be
-    // cleaned up by the caller who has the complete context.
-    deflated = true;
-  }
-  return deflated;
-}
-
 // Deflate the specified ObjectMonitor if not in-use using a JavaThread.
 // Returns true if it was deflated and false otherwise.
 //
 // The async deflation protocol sets owner to DEFLATER_MARKER and
 // makes contentions negative as signals to contending threads that
@@ -2186,11 +2045,10 @@
 // Contending threads that see that condition know to retry their operation.
 //
 bool ObjectSynchronizer::deflate_monitor_using_JT(ObjectMonitor* mid,
                                                   ObjectMonitor** free_head_p,
                                                   ObjectMonitor** free_tail_p) {
-  assert(AsyncDeflateIdleMonitors, "sanity check");
   assert(Thread::current()->is_Java_thread(), "precondition");
   // A newly allocated ObjectMonitor should not be seen here so we
   // avoid an endless inflate/deflate cycle.
   assert(mid->is_old(), "must be old: allocation_state=%d",
          (int) mid->allocation_state());
@@ -2275,11 +2133,11 @@
     // to fix the linkages in its context.
     ObjectMonitor* prevtail = *free_tail_p;
     // prevtail should have been cleaned up by the caller:
 #ifdef ASSERT
     ObjectMonitor* l_next_om = unmarked_next(prevtail);
-#endif
+    assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
     assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
     om_lock(prevtail);
     prevtail->set_next_om(mid);  // prevtail now points to mid (and is unlocked)
   }
   *free_tail_p = mid;
@@ -2292,60 +2150,10 @@
   // We leave owner == DEFLATER_MARKER and contentions < 0
   // to force any racing threads to retry.
   return true;  // Success, ObjectMonitor has been deflated.
 }
 
-// Walk a given monitor list, and deflate idle monitors.
-// The given list could be a per-thread list or a global list.
-//
-// In the case of parallel processing of thread local monitor lists,
-// work is done by Threads::parallel_threads_do() which ensures that
-// each Java thread is processed by exactly one worker thread, and
-// thus avoid conflicts that would arise when worker threads would
-// process the same monitor lists concurrently.
-//
-// See also ParallelSPCleanupTask and
-// SafepointSynchronize::do_cleanup_tasks() in safepoint.cpp and
-// Threads::parallel_java_threads_do() in thread.cpp.
-int ObjectSynchronizer::deflate_monitor_list(ObjectMonitor** list_p,
-                                             int* count_p,
-                                             ObjectMonitor** free_head_p,
-                                             ObjectMonitor** free_tail_p) {
-  ObjectMonitor* cur_mid_in_use = NULL;
-  ObjectMonitor* mid = NULL;
-  ObjectMonitor* next = NULL;
-  int deflated_count = 0;
-
-  // This list walk executes at a safepoint and does not race with any
-  // other list walkers.
-
-  for (mid = Atomic::load(list_p); mid != NULL; mid = next) {
-    next = unmarked_next(mid);
-    oop obj = (oop) mid->object();
-    if (obj != NULL && deflate_monitor(mid, obj, free_head_p, free_tail_p)) {
-      // Deflation succeeded and already updated free_head_p and
-      // free_tail_p as needed. Finish the move to the local free list
-      // by unlinking mid from the global or per-thread in-use list.
-      if (cur_mid_in_use == NULL) {
-        // mid is the list head so switch the list head to next:
-        Atomic::store(list_p, next);
-      } else {
-        // Switch cur_mid_in_use's next field to next:
-        cur_mid_in_use->set_next_om(next);
-      }
-      // At this point mid is disconnected from the in-use list.
-      deflated_count++;
-      Atomic::dec(count_p);
-      // mid is current tail in the free_head_p list so NULL terminate it:
-      mid->set_next_om(NULL);
-    } else {
-      cur_mid_in_use = mid;
-    }
-  }
-  return deflated_count;
-}
-
 // Walk a given ObjectMonitor list and deflate idle ObjectMonitors using
 // a JavaThread. Returns the number of deflated ObjectMonitors. The given
 // list could be a per-thread in-use list or the global in-use list.
 // If a safepoint has started, then we save state via saved_mid_in_use_p
 // and return to the caller to honor the safepoint.
@@ -2353,11 +2161,10 @@
 int ObjectSynchronizer::deflate_monitor_list_using_JT(ObjectMonitor** list_p,
                                                       int* count_p,
                                                       ObjectMonitor** free_head_p,
                                                       ObjectMonitor** free_tail_p,
                                                       ObjectMonitor** saved_mid_in_use_p) {
-  assert(AsyncDeflateIdleMonitors, "sanity check");
   JavaThread* self = JavaThread::current();
 
   ObjectMonitor* cur_mid_in_use = NULL;
   ObjectMonitor* mid = NULL;
   ObjectMonitor* next = NULL;
@@ -2483,79 +2290,10 @@
   // no need to save state.
   *saved_mid_in_use_p = NULL;
   return deflated_count;
 }
 
-void ObjectSynchronizer::prepare_deflate_idle_monitors(DeflateMonitorCounters* counters) {
-  counters->n_in_use = 0;              // currently associated with objects
-  counters->n_in_circulation = 0;      // extant
-  counters->n_scavenged = 0;           // reclaimed (global and per-thread)
-  counters->per_thread_scavenged = 0;  // per-thread scavenge total
-  counters->per_thread_times = 0.0;    // per-thread scavenge times
-}
-
-void ObjectSynchronizer::deflate_idle_monitors(DeflateMonitorCounters* counters) {
-  assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
-
-  if (AsyncDeflateIdleMonitors) {
-    // Nothing to do when global idle ObjectMonitors are deflated using
-    // a JavaThread.
-    return;
-  }
-
-  bool deflated = false;
-
-  ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
-  ObjectMonitor* free_tail_p = NULL;
-  elapsedTimer timer;
-
-  if (log_is_enabled(Info, monitorinflation)) {
-    timer.start();
-  }
-
-  // Note: the thread-local monitors lists get deflated in
-  // a separate pass. See deflate_thread_local_monitors().
-
-  // For moribund threads, scan om_list_globals._in_use_list
-  int deflated_count = 0;
-  if (Atomic::load(&om_list_globals._in_use_list) != NULL) {
-    // Update n_in_circulation before om_list_globals._in_use_count is
-    // updated by deflation.
-    Atomic::add(&counters->n_in_circulation,
-                Atomic::load(&om_list_globals._in_use_count));
-
-    deflated_count = deflate_monitor_list(&om_list_globals._in_use_list,
-                                          &om_list_globals._in_use_count,
-                                          &free_head_p, &free_tail_p);
-    Atomic::add(&counters->n_in_use, Atomic::load(&om_list_globals._in_use_count));
-  }
-
-  if (free_head_p != NULL) {
-    // Move the deflated ObjectMonitors back to the global free list.
-    guarantee(free_tail_p != NULL && deflated_count > 0, "invariant");
-#ifdef ASSERT
-    ObjectMonitor* l_next_om = free_tail_p->next_om();
-#endif
-    assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
-    prepend_list_to_global_free_list(free_head_p, free_tail_p, deflated_count);
-    Atomic::add(&counters->n_scavenged, deflated_count);
-  }
-  timer.stop();
-
-  LogStreamHandle(Debug, monitorinflation) lsh_debug;
-  LogStreamHandle(Info, monitorinflation) lsh_info;
-  LogStream* ls = NULL;
-  if (log_is_enabled(Debug, monitorinflation)) {
-    ls = &lsh_debug;
-  } else if (deflated_count != 0 && log_is_enabled(Info, monitorinflation)) {
-    ls = &lsh_info;
-  }
-  if (ls != NULL) {
-    ls->print_cr("deflating global idle monitors, %3.7f secs, %d monitors", timer.seconds(), deflated_count);
-  }
-}
-
 class HandshakeForDeflation : public HandshakeClosure {
  public:
   HandshakeForDeflation() : HandshakeClosure("HandshakeForDeflation") {}
 
   void do_thread(Thread* thread) {
@@ -2563,12 +2301,10 @@
                                 INTPTR_FORMAT, p2i(thread));
   }
 };
 
 void ObjectSynchronizer::deflate_idle_monitors_using_JT() {
-  assert(AsyncDeflateIdleMonitors, "sanity check");
-
   // Deflate any global idle monitors.
   deflate_global_idle_monitors_using_JT();
 
   int count = 0;
   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
@@ -2598,25 +2334,29 @@
   if (Atomic::load(&om_list_globals._wait_count) > 0) {
     // There are deflated ObjectMonitors waiting for a handshake
     // (or a safepoint) for safety.
 
     ObjectMonitor* list = Atomic::load(&om_list_globals._wait_list);
-    ADIM_guarantee(list != NULL, "om_list_globals._wait_list must not be NULL");
+    assert(list != NULL, "om_list_globals._wait_list must not be NULL");
     int count = Atomic::load(&om_list_globals._wait_count);
     Atomic::store(&om_list_globals._wait_count, 0);
     Atomic::store(&om_list_globals._wait_list, (ObjectMonitor*)NULL);
 
     // Find the tail for prepend_list_to_common(). No need to mark
     // ObjectMonitors for this list walk since only the deflater
     // thread manages the wait list.
+#ifdef ASSERT
     int l_count = 0;
+#endif
     ObjectMonitor* tail = NULL;
     for (ObjectMonitor* n = list; n != NULL; n = unmarked_next(n)) {
       tail = n;
+#ifdef ASSERT
       l_count++;
+#endif
     }
-    ADIM_guarantee(count == l_count, "count=%d != l_count=%d", count, l_count);
+    assert(count == l_count, "count=%d != l_count=%d", count, l_count);
 
     // Will execute a safepoint if !ThreadLocalHandshakes:
     HandshakeForDeflation hfd_hc;
     Handshake::execute(&hfd_hc);
 
@@ -2628,21 +2368,19 @@
 }
 
 // Deflate global idle ObjectMonitors using a JavaThread.
 //
 void ObjectSynchronizer::deflate_global_idle_monitors_using_JT() {
-  assert(AsyncDeflateIdleMonitors, "sanity check");
   assert(Thread::current()->is_Java_thread(), "precondition");
   JavaThread* self = JavaThread::current();
 
   deflate_common_idle_monitors_using_JT(true /* is_global */, self);
 }
 
 // Deflate the specified JavaThread's idle ObjectMonitors using a JavaThread.
 //
 void ObjectSynchronizer::deflate_per_thread_idle_monitors_using_JT(JavaThread* target) {
-  assert(AsyncDeflateIdleMonitors, "sanity check");
   assert(Thread::current()->is_Java_thread(), "precondition");
 
   deflate_common_idle_monitors_using_JT(false /* !is_global */, target);
 }
 
@@ -2694,11 +2432,11 @@
       // but the next field in free_tail_p can flicker to marked
       // and then unmarked while prepend_to_common() is sorting it
       // all out.
 #ifdef ASSERT
       ObjectMonitor* l_next_om = unmarked_next(free_tail_p);
-#endif
+      assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
       assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
 
       prepend_list_to_global_wait_list(free_head_p, free_tail_p, local_deflated_count);
 
       OM_PERFDATA_OP(Deflations, inc(local_deflated_count));
@@ -2741,98 +2479,10 @@
       ls->print_cr("jt=" INTPTR_FORMAT ": async-deflating per-thread idle monitors, %3.7f secs, %d monitors", p2i(target), timer.seconds(), deflated_count);
     }
   }
 }
 
-void ObjectSynchronizer::finish_deflate_idle_monitors(DeflateMonitorCounters* counters) {
-  // Report the cumulative time for deflating each thread's idle
-  // monitors. Note: if the work is split among more than one
-  // worker thread, then the reported time will likely be more
-  // than a beginning to end measurement of the phase.
-  log_info(safepoint, cleanup)("deflating per-thread idle monitors, %3.7f secs, monitors=%d", counters->per_thread_times, counters->per_thread_scavenged);
-
-  if (AsyncDeflateIdleMonitors) {
-    // Nothing to do when idle ObjectMonitors are deflated using
-    // a JavaThread.
-    return;
-  }
-
-  if (log_is_enabled(Debug, monitorinflation)) {
-    // exit_globals()'s call to audit_and_print_stats() is done
-    // at the Info level and not at a safepoint.
-    // For async deflation, audit_and_print_stats() is called in
-    // ObjectSynchronizer::do_safepoint_work() at the Debug level
-    // at a safepoint.
-    ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
-  } else if (log_is_enabled(Info, monitorinflation)) {
-    log_info(monitorinflation)("global_population=%d, global_in_use_count=%d, "
-                               "global_free_count=%d, global_wait_count=%d",
-                               Atomic::load(&om_list_globals._population),
-                               Atomic::load(&om_list_globals._in_use_count),
-                               Atomic::load(&om_list_globals._free_count),
-                               Atomic::load(&om_list_globals._wait_count));
-  }
-
-  OM_PERFDATA_OP(Deflations, inc(counters->n_scavenged));
-  OM_PERFDATA_OP(MonExtant, set_value(counters->n_in_circulation));
-
-  GVars.stw_random = os::random();
-  GVars.stw_cycle++;
-}
-
-void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {
-  assert(SafepointSynchronize::is_at_safepoint(), "must be at safepoint");
-
-  if (AsyncDeflateIdleMonitors) {
-    // Nothing to do when per-thread idle ObjectMonitors are deflated
-    // using a JavaThread.
-    return;
-  }
-
-  ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors
-  ObjectMonitor* free_tail_p = NULL;
-  elapsedTimer timer;
-
-  if (log_is_enabled(Info, safepoint, cleanup) ||
-      log_is_enabled(Info, monitorinflation)) {
-    timer.start();
-  }
-
-  // Update n_in_circulation before om_in_use_count is updated by deflation.
-  Atomic::add(&counters->n_in_circulation, Atomic::load(&thread->om_in_use_count));
-
-  int deflated_count = deflate_monitor_list(&thread->om_in_use_list, &thread->om_in_use_count, &free_head_p, &free_tail_p);
-  Atomic::add(&counters->n_in_use, Atomic::load(&thread->om_in_use_count));
-
-  if (free_head_p != NULL) {
-    // Move the deflated ObjectMonitors back to the global free list.
-    guarantee(free_tail_p != NULL && deflated_count > 0, "invariant");
-#ifdef ASSERT
-    ObjectMonitor* l_next_om = free_tail_p->next_om();
-#endif
-    assert(l_next_om == NULL, "must be NULL: _next_om=" INTPTR_FORMAT, p2i(l_next_om));
-    prepend_list_to_global_free_list(free_head_p, free_tail_p, deflated_count);
-    Atomic::add(&counters->n_scavenged, deflated_count);
-    Atomic::add(&counters->per_thread_scavenged, deflated_count);
-  }
-
-  timer.stop();
-  counters->per_thread_times += timer.seconds();
-
-  LogStreamHandle(Debug, monitorinflation) lsh_debug;
-  LogStreamHandle(Info, monitorinflation) lsh_info;
-  LogStream* ls = NULL;
-  if (log_is_enabled(Debug, monitorinflation)) {
-    ls = &lsh_debug;
-  } else if (deflated_count != 0 && log_is_enabled(Info, monitorinflation)) {
-    ls = &lsh_info;
-  }
-  if (ls != NULL) {
-    ls->print_cr("jt=" INTPTR_FORMAT ": deflating per-thread idle monitors, %3.7f secs, %d monitors", p2i(thread), timer.seconds(), deflated_count);
-  }
-}
-
 // Monitor cleanup on JavaThread::exit
 
 // Iterate through monitor cache and attempt to release thread's monitors
 // Gives up on a particular monitor if an exception occurs, but continues
 // the overall iteration, swallowing the exception.
@@ -2854,11 +2504,11 @@
 // all remaining monitors are heavyweight.  All exceptions are swallowed.
 // Scanning the extant monitor list can be time consuming.
 // A simple optimization is to add a per-thread flag that indicates a thread
 // called jni_monitorenter() during its lifetime.
 //
-// Instead of No_Savepoint_Verifier it might be cheaper to
+// Instead of NoSafepointVerifier it might be cheaper to
 // use an idiom of the form:
 //   auto int tmp = SafepointSynchronize::_safepoint_counter ;
 //   <code that must not run at safepoint>
 //   guarantee (((tmp ^ _safepoint_counter) | (tmp & 1)) == 0) ;
 // Since the tests are extremely cheap we could leave them enabled
@@ -2913,12 +2563,10 @@
 //
 // Calls to this function can be added in various places as a debugging
 // aid; pass 'true' for the 'on_exit' parameter to have in-use monitor
 // details logged at the Info level and 'false' for the 'on_exit'
 // parameter to have in-use monitor details logged at the Trace level.
-// deflate_monitor_list() no longer uses spin-locking so be careful
-// when adding audit_and_print_stats() calls at a safepoint.
 //
 void ObjectSynchronizer::audit_and_print_stats(bool on_exit) {
   assert(on_exit || SafepointSynchronize::is_at_safepoint(), "invariant");
 
   LogStreamHandle(Debug, monitorinflation) lsh_debug;
@@ -3012,15 +2660,10 @@
     if (jt != NULL) {
       out->print_cr("ERROR: jt=" INTPTR_FORMAT ", monitor=" INTPTR_FORMAT
                     ": free per-thread monitor must have NULL _header "
                     "field: _header=" INTPTR_FORMAT, p2i(jt), p2i(n),
                     n->header().value());
-      *error_cnt_p = *error_cnt_p + 1;
-    } else if (!AsyncDeflateIdleMonitors) {
-      out->print_cr("ERROR: monitor=" INTPTR_FORMAT ": free global monitor "
-                    "must have NULL _header field: _header=" INTPTR_FORMAT,
-                    p2i(n), n->header().value());
       *error_cnt_p = *error_cnt_p + 1;
     }
   }
   if (n->object() != NULL) {
     if (jt != NULL) {
diff a/src/hotspot/share/runtime/synchronizer.hpp b/src/hotspot/share/runtime/synchronizer.hpp
--- a/src/hotspot/share/runtime/synchronizer.hpp
+++ b/src/hotspot/share/runtime/synchronizer.hpp
@@ -40,18 +40,10 @@
 #define OM_CACHE_LINE_SIZE DEFAULT_CACHE_LINE_SIZE
 #endif
 
 typedef PaddedEnd<ObjectMonitor, OM_CACHE_LINE_SIZE> PaddedObjectMonitor;
 
-struct DeflateMonitorCounters {
-  volatile int n_in_use;              // currently associated with objects
-  volatile int n_in_circulation;      // extant
-  volatile int n_scavenged;           // reclaimed (global and per-thread)
-  volatile int per_thread_scavenged;  // per-thread scavenge total
-           double per_thread_times;   // per-thread scavenge times
-};
-
 class ObjectSynchronizer : AllStatic {
   friend class VMStructs;
  public:
   typedef enum {
     owner_self,
@@ -129,39 +121,26 @@
   static void monitors_iterate(MonitorClosure* m);
 
   // GC: we current use aggressive monitor deflation policy
   // Basically we deflate all monitors that are not busy.
   // An adaptive profile-based deflation policy could be used if needed
-  static void deflate_idle_monitors(DeflateMonitorCounters* counters);
   static void deflate_idle_monitors_using_JT();
   static void deflate_global_idle_monitors_using_JT();
   static void deflate_per_thread_idle_monitors_using_JT(JavaThread* target);
   static void deflate_common_idle_monitors_using_JT(bool is_global, JavaThread* target);
-  static void deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters);
-  static void prepare_deflate_idle_monitors(DeflateMonitorCounters* counters);
-  static void finish_deflate_idle_monitors(DeflateMonitorCounters* counters);
-
-  // For a given monitor list: global or per-thread, deflate idle monitors
-  static int deflate_monitor_list(ObjectMonitor** list_p,
-                                  int* count_p,
-                                  ObjectMonitor** free_head_p,
-                                  ObjectMonitor** free_tail_p);
+
   // For a given in-use monitor list: global or per-thread, deflate idle
   // monitors using a JavaThread.
   static int deflate_monitor_list_using_JT(ObjectMonitor** list_p,
                                            int* count_p,
                                            ObjectMonitor** free_head_p,
                                            ObjectMonitor** free_tail_p,
                                            ObjectMonitor** saved_mid_in_use_p);
-  static bool deflate_monitor(ObjectMonitor* mid, oop obj,
-                              ObjectMonitor** free_head_p,
-                              ObjectMonitor** free_tail_p);
   static bool deflate_monitor_using_JT(ObjectMonitor* mid,
                                        ObjectMonitor** free_head_p,
                                        ObjectMonitor** free_tail_p);
   static bool is_async_deflation_needed();
-  static bool is_safepoint_deflation_needed();
   static bool is_async_deflation_requested() { return _is_async_deflation_requested; }
   static jlong last_async_deflation_time_ns() { return _last_async_deflation_time_ns; }
   static bool request_deflate_idle_monitors();  // for whitebox test support and VM exit logging
   static void set_is_async_deflation_requested(bool new_value) { _is_async_deflation_requested = new_value; }
   static jlong time_since_last_async_deflation_ms();
@@ -189,11 +168,11 @@
                                                  int *error_cnt_p);
   static void log_in_use_monitor_details(outputStream * out);
   static int  log_monitor_list_counts(outputStream * out);
   static int  verify_objmon_isinpool(ObjectMonitor *addr) PRODUCT_RETURN0;
 
-  static void do_safepoint_work(DeflateMonitorCounters* counters);
+  static void do_safepoint_work();
 
  private:
   friend class SynchronizerTest;
 
   enum { _BLOCKSIZE = 128 };
diff a/src/hotspot/share/runtime/vmOperations.cpp b/src/hotspot/share/runtime/vmOperations.cpp
--- a/src/hotspot/share/runtime/vmOperations.cpp
+++ b/src/hotspot/share/runtime/vmOperations.cpp
@@ -429,14 +429,14 @@
     ml.wait(10);
   }
 }
 
 bool VM_Exit::doit_prologue() {
-  if (AsyncDeflateIdleMonitors && log_is_enabled(Info, monitorinflation)) {
-    // AsyncDeflateIdleMonitors does a special deflation in order
-    // to reduce the in-use monitor population that is reported by
-    // ObjectSynchronizer::log_in_use_monitor_details() at VM exit.
+  if (log_is_enabled(Info, monitorinflation)) {
+    // Do a deflation in order to reduce the in-use monitor population
+    // that is reported by ObjectSynchronizer::log_in_use_monitor_details()
+    // at VM exit.
     ObjectSynchronizer::request_deflate_idle_monitors();
   }
   return true;
 }
 
diff a/src/hotspot/share/runtime/vmOperations.hpp b/src/hotspot/share/runtime/vmOperations.hpp
--- a/src/hotspot/share/runtime/vmOperations.hpp
+++ b/src/hotspot/share/runtime/vmOperations.hpp
@@ -77,12 +77,10 @@
   template(JNIFunctionTableCopier)                \
   template(RedefineClasses)                       \
   template(UpdateForPopTopFrame)                  \
   template(SetFramePop)                           \
   template(GetObjectMonitorUsage)                 \
-  template(GetStackTrace)                         \
-  template(GetMultipleStackTraces)                \
   template(GetAllStackTraces)                     \
   template(GetThreadListStackTraces)              \
   template(GetFrameCount)                         \
   template(GetFrameLocation)                      \
   template(ChangeBreakpoints)                     \
diff a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
--- a/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
+++ b/src/java.base/share/classes/java/lang/invoke/MethodHandles.java
@@ -2628,19 +2628,40 @@
                 throw new IllegalArgumentException(targetClass + " is a primitive class");
             if (targetClass.isArray())
                 throw new IllegalArgumentException(targetClass + " is an array class");
 
             if (!VerifyAccess.isClassAccessible(targetClass, lookupClass, prevLookupClass, allowedModes)) {
-                throw new MemberName(targetClass).makeAccessException("access violation", this);
+                throw makeAccessException(targetClass);
             }
-            checkSecurityManager(targetClass, null);
+            checkSecurityManager(targetClass);
 
             // ensure class initialization
             Unsafe.getUnsafe().ensureClassInitialized(targetClass);
             return targetClass;
         }
 
+        /*
+         * Returns IllegalAccessException due to access violation to the given targetClass.
+         *
+         * This method is called by {@link Lookup#accessClass} and {@link Lookup#ensureInitialized}
+         * which verifies access to a class rather a member.
+         */
+        private IllegalAccessException makeAccessException(Class<?> targetClass) {
+            String message = "access violation: "+ targetClass;
+            if (this == MethodHandles.publicLookup()) {
+                message += ", from public Lookup";
+            } else {
+                Module m = lookupClass().getModule();
+                message += ", from " + lookupClass() + " (" + m + ")";
+                if (prevLookupClass != null) {
+                    message += ", previous lookup " +
+                            prevLookupClass.getName() + " (" + prevLookupClass.getModule() + ")";
+                }
+            }
+            return new IllegalAccessException(message);
+        }
+
         /**
          * Determines if a class can be accessed from the lookup context defined by
          * this {@code Lookup} object. The static initializer of the class is not run.
          * <p>
          * If the {@code targetClass} is in the same module as the lookup class,
@@ -2707,13 +2728,13 @@
          * @since 9
          * @see <a href="#cross-module-lookup">Cross-module lookups</a>
          */
         public Class<?> accessClass(Class<?> targetClass) throws IllegalAccessException {
             if (!VerifyAccess.isClassAccessible(targetClass, lookupClass, prevLookupClass, allowedModes)) {
-                throw new MemberName(targetClass).makeAccessException("access violation", this);
+                throw makeAccessException(targetClass);
             }
-            checkSecurityManager(targetClass, null);
+            checkSecurityManager(targetClass);
             return targetClass;
         }
 
         /**
          * Produces an early-bound method handle for a virtual method.
@@ -3538,15 +3559,41 @@
         public boolean hasFullPrivilegeAccess() {
             return (allowedModes & (PRIVATE|MODULE)) == (PRIVATE|MODULE);
         }
 
         /**
-         * Perform necessary <a href="MethodHandles.Lookup.html#secmgr">access checks</a>.
+         * Perform steps 1 and 2b <a href="MethodHandles.Lookup.html#secmgr">access checks</a>
+         * for ensureInitialzed, findClass or accessClass.
+         */
+        void checkSecurityManager(Class<?> refc) {
+            if (allowedModes == TRUSTED)  return;
+
+            SecurityManager smgr = System.getSecurityManager();
+            if (smgr == null)  return;
+
+            // Step 1:
+            boolean fullPowerLookup = hasFullPrivilegeAccess();
+            if (!fullPowerLookup ||
+                !VerifyAccess.classLoaderIsAncestor(lookupClass, refc)) {
+                ReflectUtil.checkPackageAccess(refc);
+            }
+
+            // Step 2b:
+            if (!fullPowerLookup) {
+                smgr.checkPermission(SecurityConstants.GET_CLASSLOADER_PERMISSION);
+            }
+        }
+
+        /**
+         * Perform steps 1, 2a and 3 <a href="MethodHandles.Lookup.html#secmgr">access checks</a>.
          * Determines a trustable caller class to compare with refc, the symbolic reference class.
          * If this lookup object has full privilege access, then the caller class is the lookupClass.
          */
         void checkSecurityManager(Class<?> refc, MemberName m) {
+            Objects.requireNonNull(refc);
+            Objects.requireNonNull(m);
+
             if (allowedModes == TRUSTED)  return;
 
             SecurityManager smgr = System.getSecurityManager();
             if (smgr == null)  return;
 
@@ -3555,18 +3602,10 @@
             if (!fullPowerLookup ||
                 !VerifyAccess.classLoaderIsAncestor(lookupClass, refc)) {
                 ReflectUtil.checkPackageAccess(refc);
             }
 
-            if (m == null) {  // findClass or accessClass
-                // Step 2b:
-                if (!fullPowerLookup) {
-                    smgr.checkPermission(SecurityConstants.GET_CLASSLOADER_PERMISSION);
-                }
-                return;
-            }
-
             // Step 2a:
             if (m.isPublic()) return;
             if (!fullPowerLookup) {
                 smgr.checkPermission(SecurityConstants.CHECK_MEMBER_ACCESS_PERMISSION);
             }
diff a/src/java.base/share/classes/module-info.java b/src/java.base/share/classes/module-info.java
--- a/src/java.base/share/classes/module-info.java
+++ b/src/java.base/share/classes/module-info.java
@@ -190,11 +190,10 @@
         java.management,
         java.naming,
         java.net.http,
         java.rmi,
         java.security.jgss,
-        java.xml,
         jdk.attach,
         jdk.charsets,
         jdk.compiler,
         jdk.jfr,
         jdk.jshell,
diff a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
--- a/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
+++ b/src/jdk.compiler/share/classes/com/sun/tools/javac/comp/TypeEnter.java
@@ -1099,23 +1099,25 @@
             if (implSym == null || (implSym.flags_field & GENERATED_MEMBER) != 0) {
                 /* here we are pushing the annotations present in the corresponding field down to the accessor
                  * it could be that some of those annotations are not applicable to the accessor, they will be striped
                  * away later at Check::validateAnnotation
                  */
+                TreeCopier<JCTree> tc = new TreeCopier<JCTree>(make.at(tree.pos));
                 List<JCAnnotation> originalAnnos = rec.getOriginalAnnos().isEmpty() ?
                         rec.getOriginalAnnos() :
-                        new TreeCopier<JCTree>(make.at(tree.pos)).copy(rec.getOriginalAnnos());
+                        tc.copy(rec.getOriginalAnnos());
+                JCVariableDecl recordField = TreeInfo.recordFields((JCClassDecl) env.tree).stream().filter(rf -> rf.name == tree.name).findAny().get();
                 JCMethodDecl getter = make.at(tree.pos).
                         MethodDef(
                                 make.Modifiers(PUBLIC | Flags.GENERATED_MEMBER, originalAnnos),
                           tree.sym.name,
                           /* we need to special case for the case when the user declared the type as an ident
                            * if we don't do that then we can have issues if type annotations are applied to the
                            * return type: javac issues an error if a type annotation is applied to java.lang.String
                            * but applying a type annotation to String is kosher
                            */
-                          tree.vartype.hasTag(IDENT) ? make.Ident(tree.vartype.type.tsym) : make.Type(tree.sym.type),
+                          tc.copy(recordField.vartype),
                           List.nil(),
                           List.nil(),
                           List.nil(), // thrown
                           null,
                           null);
@@ -1465,14 +1467,15 @@
             for (JCVariableDecl arg : md.params) {
                 /* at this point we are passing all the annotations in the field to the corresponding
                  * parameter in the constructor.
                  */
                 RecordComponent rc = ((ClassSymbol) owner).getRecordComponent(arg.sym);
+                TreeCopier<JCTree> tc = new TreeCopier<JCTree>(make.at(arg.pos));
                 arg.mods.annotations = rc.getOriginalAnnos().isEmpty() ?
                         List.nil() :
-                        new TreeCopier<JCTree>(make.at(arg.pos)).copy(rc.getOriginalAnnos());
-                arg.vartype = tmpRecordFieldDecls.head.vartype;
+                        tc.copy(rc.getOriginalAnnos());
+                arg.vartype = tc.copy(tmpRecordFieldDecls.head.vartype);
                 tmpRecordFieldDecls = tmpRecordFieldDecls.tail;
             }
             return md;
         }
     }
diff a/test/jtreg-ext/requires/VMProps.java b/test/jtreg-ext/requires/VMProps.java
--- a/test/jtreg-ext/requires/VMProps.java
+++ b/test/jtreg-ext/requires/VMProps.java
@@ -266,22 +266,40 @@
      */
     protected String cpuFeatures() {
         return CPUInfo.getFeatures().toString();
     }
 
+    private boolean isGcSupportedByGraal(GC gc) {
+        switch (gc) {
+            case Serial:
+            case Parallel:
+            case G1:
+                return true;
+            case Epsilon:
+            case Z:
+            case Shenandoah:
+                return false;
+            default:
+                throw new IllegalStateException("Unknown GC " + gc.name());
+        }
+    }
+
     /**
      * For all existing GC sets vm.gc.X property.
      * Example vm.gc.G1=true means:
      *    VM supports G1
      *    User either set G1 explicitely (-XX:+UseG1GC) or did not set any GC
+     *    G1 can be selected, i.e. it doesn't conflict with other VM flags
      *
      * @param map - property-value pairs
      */
     protected void vmGC(SafeMap map) {
+        var isGraalEnabled = Compiler.isGraalEnabled();
         for (GC gc: GC.values()) {
             map.put("vm.gc." + gc.name(),
                     () -> "" + (gc.isSupported()
+                            && (!isGraalEnabled || isGcSupportedByGraal(gc))
                             && (gc.isSelected() || GC.isSelectedErgonomically())));
         }
     }
 
     /**

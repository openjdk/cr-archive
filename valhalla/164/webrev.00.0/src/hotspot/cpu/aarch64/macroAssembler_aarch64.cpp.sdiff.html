<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="c1_LIRAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/cpu/aarch64/macroAssembler_aarch64.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  77   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  78     // Load register (literal)
  79     Instruction_aarch64::spatch(branch, 23, 5, offset);
  80   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  81     // Unconditional branch (immediate)
  82     Instruction_aarch64::spatch(branch, 25, 0, offset);
  83   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  84     // Conditional branch (immediate)
  85     Instruction_aarch64::spatch(branch, 23, 5, offset);
  86   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  87     // Compare &amp; branch (immediate)
  88     Instruction_aarch64::spatch(branch, 23, 5, offset);
  89   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  90     // Test &amp; branch (immediate)
  91     Instruction_aarch64::spatch(branch, 18, 5, offset);
  92   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  93     // PC-rel. addressing
  94     offset = target-branch;
  95     int shift = Instruction_aarch64::extract(insn, 31, 31);
  96     if (shift) {
<span class="line-modified">  97       u_int64_t dest = (u_int64_t)target;</span>
  98       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  99       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 100       unsigned offset_lo = dest &amp; 0xfff;
 101       offset = adr_page - pc_page;
 102 
 103       // We handle 4 types of PC relative addressing
 104       //   1 - adrp    Rx, target_page
 105       //       ldr/str Ry, [Rx, #offset_in_page]
 106       //   2 - adrp    Rx, target_page
 107       //       add     Ry, Rx, #offset_in_page
 108       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 109       //       movk    Rx, #imm16&lt;&lt;32
 110       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       // In the first 3 cases we must check that Rx is the same in the adrp and the
 112       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 113       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 114       // to be followed by a random unrelated ldr/str, add or movk instruction.
 115       //
 116       unsigned insn2 = ((unsigned*)branch)[1];
 117       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
</pre>
<hr />
<pre>
 130         Instruction_aarch64::patch(branch + sizeof (unsigned),
 131                                    21, 10, offset_lo);
 132         instructions = 2;
 133       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 134                    Instruction_aarch64::extract(insn, 4, 0) ==
 135                      Instruction_aarch64::extract(insn2, 4, 0)) {
 136         // movk #imm16&lt;&lt;32
 137         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 138         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);
 139         long pc_page = (long)branch &gt;&gt; 12;
 140         long adr_page = (long)dest &gt;&gt; 12;
 141         offset = adr_page - pc_page;
 142         instructions = 2;
 143       }
 144     }
 145     int offset_lo = offset &amp; 3;
 146     offset &gt;&gt;= 2;
 147     Instruction_aarch64::spatch(branch, 23, 5, offset);
 148     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 149   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
<span class="line-modified"> 150     u_int64_t dest = (u_int64_t)target;</span>
 151     // Move wide constant
 152     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 154     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 157     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 158     instructions = 3;
 159   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 160              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 161     // nothing to do
 162     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 163   } else {
 164     ShouldNotReachHere();
 165   }
 166   return instructions * NativeInstruction::instruction_size;
 167 }
 168 
 169 int MacroAssembler::patch_oop(address insn_addr, address o) {
 170   int instructions;
</pre>
<hr />
<pre>
 256         return address(target_page + (byte_offset &lt;&lt; size));
 257       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 258                 Instruction_aarch64::extract(insn, 4, 0) ==
 259                         Instruction_aarch64::extract(insn2, 4, 0)) {
 260         // add (immediate)
 261         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 262         return address(target_page + byte_offset);
 263       } else {
 264         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 265                Instruction_aarch64::extract(insn, 4, 0) ==
 266                  Instruction_aarch64::extract(insn2, 4, 0)) {
 267           target_page = (target_page &amp; 0xffffffff) |
 268                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 269         }
 270         return (address)target_page;
 271       }
 272     } else {
 273       ShouldNotReachHere();
 274     }
 275   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
<span class="line-modified"> 276     u_int32_t *insns = (u_int32_t *)insn_addr;</span>
 277     // Move wide constant: movz, movk, movk.  See movptr().
 278     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 279     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
<span class="line-modified"> 280     return address(u_int64_t(Instruction_aarch64::extract(insns[0], 20, 5))</span>
<span class="line-modified"> 281                    + (u_int64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)</span>
<span class="line-modified"> 282                    + (u_int64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));</span>
 283   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 284              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 285     return 0;
 286   } else {
 287     ShouldNotReachHere();
 288   }
 289   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 290 }
 291 
 292 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 293   ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 294   tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 295 }
 296 
 297 // Just like safepoint_poll, but use an acquiring load for thread-
 298 // local polling.
 299 //
 300 // We need an acquire here to ensure that any subsequent load of the
 301 // global SafepointSynchronize::_state flag is ordered after this load
 302 // of the local Thread::_polling page.  We don&#39;t want this poll to
</pre>
<hr />
<pre>
1520   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
1521 }
1522 
1523 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label&amp; is_flattened_array) {
1524   load_storage_props(temp_reg, oop);
1525   andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);
1526   cbnz(temp_reg, is_flattened_array);
1527 }
1528 
1529 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp; is_null_free_array) {
1530   load_storage_props(temp_reg, oop);
1531   andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);
1532   cbnz(temp_reg, is_null_free_array);
1533 }
1534 
1535 // MacroAssembler protected routines needed to implement
1536 // public methods
1537 
1538 void MacroAssembler::mov(Register r, Address dest) {
1539   code_section()-&gt;relocate(pc(), dest.rspec());
<span class="line-modified">1540   u_int64_t imm64 = (u_int64_t)dest.target();</span>
1541   movptr(r, imm64);
1542 }
1543 
1544 // Move a constant pointer into r.  In AArch64 mode the virtual
1545 // address space is 48 bits in size, so we only need three
1546 // instructions to create a patchable instruction sequence that can
1547 // reach anywhere.
1548 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1549 #ifndef PRODUCT
1550   {
1551     char buffer[64];
1552     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1553     block_comment(buffer);
1554   }
1555 #endif
1556   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1557   movz(r, imm64 &amp; 0xffff);
1558   imm64 &gt;&gt;= 16;
1559   movk(r, imm64 &amp; 0xffff, 16);
1560   imm64 &gt;&gt;= 16;
1561   movk(r, imm64 &amp; 0xffff, 32);
1562 }
1563 
1564 // Macro to mov replicated immediate to vector register.
1565 //  Vd will get the following values for different arrangements in T
1566 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1567 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1568 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1569 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1570 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1571 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1572 //   T1D/T2D: invalid
<span class="line-modified">1573 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, u_int32_t imm32) {</span>
1574   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1575   if (T == T8B || T == T16B) {
1576     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1577     movi(Vd, T, imm32 &amp; 0xff, 0);
1578     return;
1579   }
<span class="line-modified">1580   u_int32_t nimm32 = ~imm32;</span>
1581   if (T == T4H || T == T8H) {
1582     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1583     imm32 &amp;= 0xffff;
1584     nimm32 &amp;= 0xffff;
1585   }
<span class="line-modified">1586   u_int32_t x = imm32;</span>
1587   int movi_cnt = 0;
1588   int movn_cnt = 0;
1589   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1590   x = nimm32;
1591   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1592   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1593   unsigned lsl = 0;
1594   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1595   if (movn_cnt &lt; movi_cnt)
1596     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1597   else
1598     movi(Vd, T, imm32 &amp; 0xff, lsl);
1599   imm32 &gt;&gt;= 8; lsl += 8;
1600   while (imm32) {
1601     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1602     if (movn_cnt &lt; movi_cnt)
1603       bici(Vd, T, imm32 &amp; 0xff, lsl);
1604     else
1605       orri(Vd, T, imm32 &amp; 0xff, lsl);
1606     lsl += 8; imm32 &gt;&gt;= 8;
1607   }
1608 }
1609 
<span class="line-modified">1610 void MacroAssembler::mov_immediate64(Register dst, u_int64_t imm64)</span>
1611 {
1612 #ifndef PRODUCT
1613   {
1614     char buffer[64];
1615     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1616     block_comment(buffer);
1617   }
1618 #endif
1619   if (operand_valid_for_logical_immediate(false, imm64)) {
1620     orr(dst, zr, imm64);
1621   } else {
1622     // we can use a combination of MOVZ or MOVN with
1623     // MOVK to build up the constant
<span class="line-modified">1624     u_int64_t imm_h[4];</span>
1625     int zero_count = 0;
1626     int neg_count = 0;
1627     int i;
1628     for (i = 0; i &lt; 4; i++) {
1629       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1630       if (imm_h[i] == 0) {
1631         zero_count++;
1632       } else if (imm_h[i] == 0xffffL) {
1633         neg_count++;
1634       }
1635     }
1636     if (zero_count == 4) {
1637       // one MOVZ will do
1638       movz(dst, 0);
1639     } else if (neg_count == 4) {
1640       // one MOVN will do
1641       movn(dst, 0);
1642     } else if (zero_count == 3) {
1643       for (i = 0; i &lt; 4; i++) {
1644         if (imm_h[i] != 0L) {
<span class="line-modified">1645           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1646           break;
1647         }
1648       }
1649     } else if (neg_count == 3) {
1650       // one MOVN will do
1651       for (int i = 0; i &lt; 4; i++) {
1652         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1653           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1654           break;
1655         }
1656       }
1657     } else if (zero_count == 2) {
1658       // one MOVZ and one MOVK will do
1659       for (i = 0; i &lt; 3; i++) {
1660         if (imm_h[i] != 0L) {
<span class="line-modified">1661           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1662           i++;
1663           break;
1664         }
1665       }
1666       for (;i &lt; 4; i++) {
1667         if (imm_h[i] != 0L) {
<span class="line-modified">1668           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1669         }
1670       }
1671     } else if (neg_count == 2) {
1672       // one MOVN and one MOVK will do
1673       for (i = 0; i &lt; 4; i++) {
1674         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1675           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1676           i++;
1677           break;
1678         }
1679       }
1680       for (;i &lt; 4; i++) {
1681         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1682           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1683         }
1684       }
1685     } else if (zero_count == 1) {
1686       // one MOVZ and two MOVKs will do
1687       for (i = 0; i &lt; 4; i++) {
1688         if (imm_h[i] != 0L) {
<span class="line-modified">1689           movz(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1690           i++;
1691           break;
1692         }
1693       }
1694       for (;i &lt; 4; i++) {
1695         if (imm_h[i] != 0x0L) {
<span class="line-modified">1696           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1697         }
1698       }
1699     } else if (neg_count == 1) {
1700       // one MOVN and two MOVKs will do
1701       for (i = 0; i &lt; 4; i++) {
1702         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1703           movn(dst, (u_int32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1704           i++;
1705           break;
1706         }
1707       }
1708       for (;i &lt; 4; i++) {
1709         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1710           movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1711         }
1712       }
1713     } else {
1714       // use a MOVZ and 3 MOVKs (makes it easier to debug)
<span class="line-modified">1715       movz(dst, (u_int32_t)imm_h[0], 0);</span>
1716       for (i = 1; i &lt; 4; i++) {
<span class="line-modified">1717         movk(dst, (u_int32_t)imm_h[i], (i &lt;&lt; 4));</span>
1718       }
1719     }
1720   }
1721 }
1722 
<span class="line-modified">1723 void MacroAssembler::mov_immediate32(Register dst, u_int32_t imm32)</span>
1724 {
1725 #ifndef PRODUCT
1726     {
1727       char buffer[64];
1728       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1729       block_comment(buffer);
1730     }
1731 #endif
1732   if (operand_valid_for_logical_immediate(true, imm32)) {
1733     orrw(dst, zr, imm32);
1734   } else {
1735     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1736     // constant
<span class="line-modified">1737     u_int32_t imm_h[2];</span>
1738     imm_h[0] = imm32 &amp; 0xffff;
1739     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1740     if (imm_h[0] == 0) {
1741       movzw(dst, imm_h[1], 16);
1742     } else if (imm_h[0] == 0xffff) {
1743       movnw(dst, imm_h[1] ^ 0xffff, 16);
1744     } else if (imm_h[1] == 0) {
1745       movzw(dst, imm_h[0], 0);
1746     } else if (imm_h[1] == 0xffff) {
1747       movnw(dst, imm_h[0] ^ 0xffff, 0);
1748     } else {
1749       // use a MOVZ and MOVK (makes it easier to debug)
1750       movzw(dst, imm_h[0], 0);
1751       movkw(dst, imm_h[1], 16);
1752     }
1753   }
1754 }
1755 
1756 // Form an address from base + offset in Rd.  Rd may or may
1757 // not actually be used: you must use the Address that is returned.
</pre>
<hr />
<pre>
4883   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
4884     Label l;
4885     tbz(cnt, exact_log2(i), l);
4886     for (int j = 0; j &lt; i; j += 2) {
4887       stp(zr, zr, post(ptr, 16));
4888     }
4889     bind(l);
4890   }
4891   {
4892     Label l;
4893     tbz(cnt, 0, l);
4894     str(zr, Address(ptr));
4895     bind(l);
4896   }
4897   BLOCK_COMMENT(&quot;} zero_words&quot;);
4898 }
4899 
4900 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
4901 // cnt:          Immediate count in HeapWords.
4902 #define SmallArraySize (18 * BytesPerLong)
<span class="line-modified">4903 void MacroAssembler::zero_words(Register base, u_int64_t cnt)</span>
4904 {
4905   BLOCK_COMMENT(&quot;zero_words {&quot;);
4906   int i = cnt &amp; 1;  // store any odd word to start
4907   if (i) str(zr, Address(base));
4908 
4909   if (cnt &lt;= SmallArraySize / BytesPerLong) {
4910     for (; i &lt; (int)cnt; i += 2)
4911       stp(zr, zr, Address(base, i * wordSize));
4912   } else {
4913     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
4914     int remainder = cnt % (2 * unroll);
4915     for (; i &lt; remainder; i += 2)
4916       stp(zr, zr, Address(base, i * wordSize));
4917 
4918     Label loop;
4919     Register cnt_reg = rscratch1;
4920     Register loop_base = rscratch2;
4921     cnt = cnt - remainder;
4922     mov(cnt_reg, cnt);
4923     // adjust base and prebias by -2 * wordSize so we can pre-increment
</pre>
</td>
<td>
<hr />
<pre>
  77   if ((Instruction_aarch64::extract(insn, 29, 24) &amp; 0b111011) == 0b011000) {
  78     // Load register (literal)
  79     Instruction_aarch64::spatch(branch, 23, 5, offset);
  80   } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {
  81     // Unconditional branch (immediate)
  82     Instruction_aarch64::spatch(branch, 25, 0, offset);
  83   } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {
  84     // Conditional branch (immediate)
  85     Instruction_aarch64::spatch(branch, 23, 5, offset);
  86   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {
  87     // Compare &amp; branch (immediate)
  88     Instruction_aarch64::spatch(branch, 23, 5, offset);
  89   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {
  90     // Test &amp; branch (immediate)
  91     Instruction_aarch64::spatch(branch, 18, 5, offset);
  92   } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {
  93     // PC-rel. addressing
  94     offset = target-branch;
  95     int shift = Instruction_aarch64::extract(insn, 31, 31);
  96     if (shift) {
<span class="line-modified">  97       uint64_t dest = (uint64_t)target;</span>
  98       uint64_t pc_page = (uint64_t)branch &gt;&gt; 12;
  99       uint64_t adr_page = (uint64_t)target &gt;&gt; 12;
 100       unsigned offset_lo = dest &amp; 0xfff;
 101       offset = adr_page - pc_page;
 102 
 103       // We handle 4 types of PC relative addressing
 104       //   1 - adrp    Rx, target_page
 105       //       ldr/str Ry, [Rx, #offset_in_page]
 106       //   2 - adrp    Rx, target_page
 107       //       add     Ry, Rx, #offset_in_page
 108       //   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 109       //       movk    Rx, #imm16&lt;&lt;32
 110       //   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)
 111       // In the first 3 cases we must check that Rx is the same in the adrp and the
 112       // subsequent ldr/str, add or movk instruction. Otherwise we could accidentally end
 113       // up treating a type 4 relocation as a type 1, 2 or 3 just because it happened
 114       // to be followed by a random unrelated ldr/str, add or movk instruction.
 115       //
 116       unsigned insn2 = ((unsigned*)branch)[1];
 117       if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &amp;&amp;
</pre>
<hr />
<pre>
 130         Instruction_aarch64::patch(branch + sizeof (unsigned),
 131                                    21, 10, offset_lo);
 132         instructions = 2;
 133       } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &amp;&amp;
 134                    Instruction_aarch64::extract(insn, 4, 0) ==
 135                      Instruction_aarch64::extract(insn2, 4, 0)) {
 136         // movk #imm16&lt;&lt;32
 137         Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target &gt;&gt; 32);
 138         long dest = ((long)target &amp; 0xffffffffL) | ((long)branch &amp; 0xffff00000000L);
 139         long pc_page = (long)branch &gt;&gt; 12;
 140         long adr_page = (long)dest &gt;&gt; 12;
 141         offset = adr_page - pc_page;
 142         instructions = 2;
 143       }
 144     }
 145     int offset_lo = offset &amp; 3;
 146     offset &gt;&gt;= 2;
 147     Instruction_aarch64::spatch(branch, 23, 5, offset);
 148     Instruction_aarch64::patch(branch, 30, 29, offset_lo);
 149   } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {
<span class="line-modified"> 150     uint64_t dest = (uint64_t)target;</span>
 151     // Move wide constant
 152     assert(nativeInstruction_at(branch+4)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 153     assert(nativeInstruction_at(branch+8)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 154     Instruction_aarch64::patch(branch, 20, 5, dest &amp; 0xffff);
 155     Instruction_aarch64::patch(branch+4, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 156     Instruction_aarch64::patch(branch+8, 20, 5, (dest &gt;&gt;= 16) &amp; 0xffff);
 157     assert(target_addr_for_insn(branch) == target, &quot;should be&quot;);
 158     instructions = 3;
 159   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 160              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 161     // nothing to do
 162     assert(target == 0, &quot;did not expect to relocate target for polling page load&quot;);
 163   } else {
 164     ShouldNotReachHere();
 165   }
 166   return instructions * NativeInstruction::instruction_size;
 167 }
 168 
 169 int MacroAssembler::patch_oop(address insn_addr, address o) {
 170   int instructions;
</pre>
<hr />
<pre>
 256         return address(target_page + (byte_offset &lt;&lt; size));
 257       } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &amp;&amp;
 258                 Instruction_aarch64::extract(insn, 4, 0) ==
 259                         Instruction_aarch64::extract(insn2, 4, 0)) {
 260         // add (immediate)
 261         unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);
 262         return address(target_page + byte_offset);
 263       } else {
 264         if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &amp;&amp;
 265                Instruction_aarch64::extract(insn, 4, 0) ==
 266                  Instruction_aarch64::extract(insn2, 4, 0)) {
 267           target_page = (target_page &amp; 0xffffffff) |
 268                          ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) &lt;&lt; 32);
 269         }
 270         return (address)target_page;
 271       }
 272     } else {
 273       ShouldNotReachHere();
 274     }
 275   } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {
<span class="line-modified"> 276     uint32_t *insns = (uint32_t *)insn_addr;</span>
 277     // Move wide constant: movz, movk, movk.  See movptr().
 278     assert(nativeInstruction_at(insns+1)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
 279     assert(nativeInstruction_at(insns+2)-&gt;is_movk(), &quot;wrong insns in patch&quot;);
<span class="line-modified"> 280     return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))</span>
<span class="line-modified"> 281                    + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) &lt;&lt; 16)</span>
<span class="line-modified"> 282                    + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) &lt;&lt; 32));</span>
 283   } else if (Instruction_aarch64::extract(insn, 31, 22) == 0b1011100101 &amp;&amp;
 284              Instruction_aarch64::extract(insn, 4, 0) == 0b11111) {
 285     return 0;
 286   } else {
 287     ShouldNotReachHere();
 288   }
 289   return address(((uint64_t)insn_addr + (offset &lt;&lt; 2)));
 290 }
 291 
 292 void MacroAssembler::safepoint_poll(Label&amp; slow_path) {
 293   ldr(rscratch1, Address(rthread, Thread::polling_page_offset()));
 294   tbnz(rscratch1, exact_log2(SafepointMechanism::poll_bit()), slow_path);
 295 }
 296 
 297 // Just like safepoint_poll, but use an acquiring load for thread-
 298 // local polling.
 299 //
 300 // We need an acquire here to ensure that any subsequent load of the
 301 // global SafepointSynchronize::_state flag is ordered after this load
 302 // of the local Thread::_polling page.  We don&#39;t want this poll to
</pre>
<hr />
<pre>
1520   tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);
1521 }
1522 
1523 void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label&amp; is_flattened_array) {
1524   load_storage_props(temp_reg, oop);
1525   andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);
1526   cbnz(temp_reg, is_flattened_array);
1527 }
1528 
1529 void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&amp; is_null_free_array) {
1530   load_storage_props(temp_reg, oop);
1531   andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);
1532   cbnz(temp_reg, is_null_free_array);
1533 }
1534 
1535 // MacroAssembler protected routines needed to implement
1536 // public methods
1537 
1538 void MacroAssembler::mov(Register r, Address dest) {
1539   code_section()-&gt;relocate(pc(), dest.rspec());
<span class="line-modified">1540   uint64_t imm64 = (uint64_t)dest.target();</span>
1541   movptr(r, imm64);
1542 }
1543 
1544 // Move a constant pointer into r.  In AArch64 mode the virtual
1545 // address space is 48 bits in size, so we only need three
1546 // instructions to create a patchable instruction sequence that can
1547 // reach anywhere.
1548 void MacroAssembler::movptr(Register r, uintptr_t imm64) {
1549 #ifndef PRODUCT
1550   {
1551     char buffer[64];
1552     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1553     block_comment(buffer);
1554   }
1555 #endif
1556   assert(imm64 &lt; (1ul &lt;&lt; 48), &quot;48-bit overflow in address constant&quot;);
1557   movz(r, imm64 &amp; 0xffff);
1558   imm64 &gt;&gt;= 16;
1559   movk(r, imm64 &amp; 0xffff, 16);
1560   imm64 &gt;&gt;= 16;
1561   movk(r, imm64 &amp; 0xffff, 32);
1562 }
1563 
1564 // Macro to mov replicated immediate to vector register.
1565 //  Vd will get the following values for different arrangements in T
1566 //   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh
1567 //   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh
1568 //   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh
1569 //   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh
1570 //   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh
1571 //   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh
1572 //   T1D/T2D: invalid
<span class="line-modified">1573 void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {</span>
1574   assert(T != T1D &amp;&amp; T != T2D, &quot;invalid arrangement&quot;);
1575   if (T == T8B || T == T16B) {
1576     assert((imm32 &amp; ~0xff) == 0, &quot;extraneous bits in unsigned imm32 (T8B/T16B)&quot;);
1577     movi(Vd, T, imm32 &amp; 0xff, 0);
1578     return;
1579   }
<span class="line-modified">1580   uint32_t nimm32 = ~imm32;</span>
1581   if (T == T4H || T == T8H) {
1582     assert((imm32  &amp; ~0xffff) == 0, &quot;extraneous bits in unsigned imm32 (T4H/T8H)&quot;);
1583     imm32 &amp;= 0xffff;
1584     nimm32 &amp;= 0xffff;
1585   }
<span class="line-modified">1586   uint32_t x = imm32;</span>
1587   int movi_cnt = 0;
1588   int movn_cnt = 0;
1589   while (x) { if (x &amp; 0xff) movi_cnt++; x &gt;&gt;= 8; }
1590   x = nimm32;
1591   while (x) { if (x &amp; 0xff) movn_cnt++; x &gt;&gt;= 8; }
1592   if (movn_cnt &lt; movi_cnt) imm32 = nimm32;
1593   unsigned lsl = 0;
1594   while (imm32 &amp;&amp; (imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1595   if (movn_cnt &lt; movi_cnt)
1596     mvni(Vd, T, imm32 &amp; 0xff, lsl);
1597   else
1598     movi(Vd, T, imm32 &amp; 0xff, lsl);
1599   imm32 &gt;&gt;= 8; lsl += 8;
1600   while (imm32) {
1601     while ((imm32 &amp; 0xff) == 0) { lsl += 8; imm32 &gt;&gt;= 8; }
1602     if (movn_cnt &lt; movi_cnt)
1603       bici(Vd, T, imm32 &amp; 0xff, lsl);
1604     else
1605       orri(Vd, T, imm32 &amp; 0xff, lsl);
1606     lsl += 8; imm32 &gt;&gt;= 8;
1607   }
1608 }
1609 
<span class="line-modified">1610 void MacroAssembler::mov_immediate64(Register dst, uint64_t imm64)</span>
1611 {
1612 #ifndef PRODUCT
1613   {
1614     char buffer[64];
1615     snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX64, imm64);
1616     block_comment(buffer);
1617   }
1618 #endif
1619   if (operand_valid_for_logical_immediate(false, imm64)) {
1620     orr(dst, zr, imm64);
1621   } else {
1622     // we can use a combination of MOVZ or MOVN with
1623     // MOVK to build up the constant
<span class="line-modified">1624     uint64_t imm_h[4];</span>
1625     int zero_count = 0;
1626     int neg_count = 0;
1627     int i;
1628     for (i = 0; i &lt; 4; i++) {
1629       imm_h[i] = ((imm64 &gt;&gt; (i * 16)) &amp; 0xffffL);
1630       if (imm_h[i] == 0) {
1631         zero_count++;
1632       } else if (imm_h[i] == 0xffffL) {
1633         neg_count++;
1634       }
1635     }
1636     if (zero_count == 4) {
1637       // one MOVZ will do
1638       movz(dst, 0);
1639     } else if (neg_count == 4) {
1640       // one MOVN will do
1641       movn(dst, 0);
1642     } else if (zero_count == 3) {
1643       for (i = 0; i &lt; 4; i++) {
1644         if (imm_h[i] != 0L) {
<span class="line-modified">1645           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1646           break;
1647         }
1648       }
1649     } else if (neg_count == 3) {
1650       // one MOVN will do
1651       for (int i = 0; i &lt; 4; i++) {
1652         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1653           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1654           break;
1655         }
1656       }
1657     } else if (zero_count == 2) {
1658       // one MOVZ and one MOVK will do
1659       for (i = 0; i &lt; 3; i++) {
1660         if (imm_h[i] != 0L) {
<span class="line-modified">1661           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1662           i++;
1663           break;
1664         }
1665       }
1666       for (;i &lt; 4; i++) {
1667         if (imm_h[i] != 0L) {
<span class="line-modified">1668           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1669         }
1670       }
1671     } else if (neg_count == 2) {
1672       // one MOVN and one MOVK will do
1673       for (i = 0; i &lt; 4; i++) {
1674         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1675           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1676           i++;
1677           break;
1678         }
1679       }
1680       for (;i &lt; 4; i++) {
1681         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1682           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1683         }
1684       }
1685     } else if (zero_count == 1) {
1686       // one MOVZ and two MOVKs will do
1687       for (i = 0; i &lt; 4; i++) {
1688         if (imm_h[i] != 0L) {
<span class="line-modified">1689           movz(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1690           i++;
1691           break;
1692         }
1693       }
1694       for (;i &lt; 4; i++) {
1695         if (imm_h[i] != 0x0L) {
<span class="line-modified">1696           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1697         }
1698       }
1699     } else if (neg_count == 1) {
1700       // one MOVN and two MOVKs will do
1701       for (i = 0; i &lt; 4; i++) {
1702         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1703           movn(dst, (uint32_t)imm_h[i] ^ 0xffffL, (i &lt;&lt; 4));</span>
1704           i++;
1705           break;
1706         }
1707       }
1708       for (;i &lt; 4; i++) {
1709         if (imm_h[i] != 0xffffL) {
<span class="line-modified">1710           movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1711         }
1712       }
1713     } else {
1714       // use a MOVZ and 3 MOVKs (makes it easier to debug)
<span class="line-modified">1715       movz(dst, (uint32_t)imm_h[0], 0);</span>
1716       for (i = 1; i &lt; 4; i++) {
<span class="line-modified">1717         movk(dst, (uint32_t)imm_h[i], (i &lt;&lt; 4));</span>
1718       }
1719     }
1720   }
1721 }
1722 
<span class="line-modified">1723 void MacroAssembler::mov_immediate32(Register dst, uint32_t imm32)</span>
1724 {
1725 #ifndef PRODUCT
1726     {
1727       char buffer[64];
1728       snprintf(buffer, sizeof(buffer), &quot;0x%&quot; PRIX32, imm32);
1729       block_comment(buffer);
1730     }
1731 #endif
1732   if (operand_valid_for_logical_immediate(true, imm32)) {
1733     orrw(dst, zr, imm32);
1734   } else {
1735     // we can use MOVZ, MOVN or two calls to MOVK to build up the
1736     // constant
<span class="line-modified">1737     uint32_t imm_h[2];</span>
1738     imm_h[0] = imm32 &amp; 0xffff;
1739     imm_h[1] = ((imm32 &gt;&gt; 16) &amp; 0xffff);
1740     if (imm_h[0] == 0) {
1741       movzw(dst, imm_h[1], 16);
1742     } else if (imm_h[0] == 0xffff) {
1743       movnw(dst, imm_h[1] ^ 0xffff, 16);
1744     } else if (imm_h[1] == 0) {
1745       movzw(dst, imm_h[0], 0);
1746     } else if (imm_h[1] == 0xffff) {
1747       movnw(dst, imm_h[0] ^ 0xffff, 0);
1748     } else {
1749       // use a MOVZ and MOVK (makes it easier to debug)
1750       movzw(dst, imm_h[0], 0);
1751       movkw(dst, imm_h[1], 16);
1752     }
1753   }
1754 }
1755 
1756 // Form an address from base + offset in Rd.  Rd may or may
1757 // not actually be used: you must use the Address that is returned.
</pre>
<hr />
<pre>
4883   for (int i = zero_words_block_size &gt;&gt; 1; i &gt; 1; i &gt;&gt;= 1) {
4884     Label l;
4885     tbz(cnt, exact_log2(i), l);
4886     for (int j = 0; j &lt; i; j += 2) {
4887       stp(zr, zr, post(ptr, 16));
4888     }
4889     bind(l);
4890   }
4891   {
4892     Label l;
4893     tbz(cnt, 0, l);
4894     str(zr, Address(ptr));
4895     bind(l);
4896   }
4897   BLOCK_COMMENT(&quot;} zero_words&quot;);
4898 }
4899 
4900 // base:         Address of a buffer to be zeroed, 8 bytes aligned.
4901 // cnt:          Immediate count in HeapWords.
4902 #define SmallArraySize (18 * BytesPerLong)
<span class="line-modified">4903 void MacroAssembler::zero_words(Register base, uint64_t cnt)</span>
4904 {
4905   BLOCK_COMMENT(&quot;zero_words {&quot;);
4906   int i = cnt &amp; 1;  // store any odd word to start
4907   if (i) str(zr, Address(base));
4908 
4909   if (cnt &lt;= SmallArraySize / BytesPerLong) {
4910     for (; i &lt; (int)cnt; i += 2)
4911       stp(zr, zr, Address(base, i * wordSize));
4912   } else {
4913     const int unroll = 4; // Number of stp(zr, zr) instructions we&#39;ll unroll
4914     int remainder = cnt % (2 * unroll);
4915     for (; i &lt; remainder; i += 2)
4916       stp(zr, zr, Address(base, i * wordSize));
4917 
4918     Label loop;
4919     Register cnt_reg = rscratch1;
4920     Register loop_base = rscratch2;
4921     cnt = cnt - remainder;
4922     mov(cnt_reg, cnt);
4923     // adjust base and prebias by -2 * wordSize so we can pre-increment
</pre>
</td>
</tr>
</table>
<center><a href="c1_LIRAssembler_aarch64.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macroAssembler_aarch64.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
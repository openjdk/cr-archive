<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/runtime/synchronizer.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/runtime/synchronizer.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 488 
 489 
 490 // The LockNode emitted directly at the synchronization site would have
 491 // been too big if it were to have included support for the cases of inflated
 492 // recursive enter and exit, so they go here instead.
 493 // Note that we can&#39;t safely call AsyncPrintJavaStack() from within
 494 // quick_enter() as our thread state remains _in_Java.
 495 
 496 bool ObjectSynchronizer::quick_enter(oop obj, Thread* self,
 497                                      BasicLock * lock) {
 498   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 499   assert(self-&gt;is_Java_thread(), &quot;invariant&quot;);
 500   assert(((JavaThread *) self)-&gt;thread_state() == _thread_in_Java, &quot;invariant&quot;);
 501   NoSafepointVerifier nsv;
 502   if (obj == NULL) return false;       // Need to throw NPE
 503   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_inline_klass(), &quot;monitor op on inline type&quot;);
 504   const markWord mark = obj-&gt;mark();
 505 
 506   if (mark.has_monitor()) {
 507     ObjectMonitor* const m = mark.monitor();
<span class="line-modified"> 508     if (AsyncDeflateIdleMonitors) {</span>
<span class="line-modified"> 509       // An async deflation can race us before we manage to make the</span>
<span class="line-modified"> 510       // ObjectMonitor busy by setting the owner below. If we detect</span>
<span class="line-modified"> 511       // that race we just bail out to the slow-path here.</span>
<span class="line-modified"> 512       if (m-&gt;object() == NULL) {</span>
<span class="line-removed"> 513         return false;</span>
<span class="line-removed"> 514       }</span>
<span class="line-removed"> 515     } else {</span>
<span class="line-removed"> 516       assert(m-&gt;object() == obj, &quot;invariant&quot;);</span>
 517     }
 518     Thread* const owner = (Thread *) m-&gt;_owner;
 519 
 520     // Lock contention and Transactional Lock Elision (TLE) diagnostics
 521     // and observability
 522     // Case: light contention possibly amenable to TLE
 523     // Case: TLE inimical operations such as nested/recursive synchronization
 524 
 525     if (owner == self) {
 526       m-&gt;_recursions++;
 527       return true;
 528     }
 529 
 530     // This Java Monitor is inflated so obj&#39;s header will never be
 531     // displaced to this thread&#39;s BasicLock. Make the displaced header
 532     // non-NULL so this BasicLock is not seen as recursive nor as
 533     // being locked. We do this unconditionally so that this thread&#39;s
 534     // BasicLock cannot be mis-interpreted by any stack walkers. For
 535     // performance reasons, stack walkers generally first check for
 536     // Biased Locking in the object&#39;s header, the second check is for
</pre>
<hr />
<pre>
1000     self-&gt;_hashStateZ = self-&gt;_hashStateW;
1001     unsigned v = self-&gt;_hashStateW;
1002     v = (v ^ (v &gt;&gt; 19)) ^ (t ^ (t &gt;&gt; 8));
1003     self-&gt;_hashStateW = v;
1004     value = v;
1005   }
1006 
1007   value &amp;= markWord::hash_mask;
1008   if (value == 0) value = 0xBAD;
1009   assert(value != markWord::no_hash, &quot;invariant&quot;);
1010   return value;
1011 }
1012 
1013 intptr_t ObjectSynchronizer::FastHashCode(Thread* self, oop obj) {
1014   if (EnableValhalla &amp;&amp; obj-&gt;klass()-&gt;is_inline_klass()) {
1015     // VM should be calling bootstrap method
1016     ShouldNotReachHere();
1017   }
1018   if (UseBiasedLocking) {
1019     // NOTE: many places throughout the JVM do not expect a safepoint
<span class="line-modified">1020     // to be taken here, in particular most operations on perm gen</span>
<span class="line-modified">1021     // objects. However, we only ever bias Java instances and all of</span>
<span class="line-removed">1022     // the call sites of identity_hash that might revoke biases have</span>
1023     // been checked to make sure they can handle a safepoint. The
1024     // added check of the bias pattern is to avoid useless calls to
1025     // thread-local storage.
1026     if (obj-&gt;mark().has_bias_pattern()) {
1027       // Handle for oop obj in case of STW safepoint
1028       Handle hobj(self, obj);
1029       // Relaxing assertion for bug 6320749.
1030       assert(Universe::verify_in_progress() ||
1031              !SafepointSynchronize::is_at_safepoint(),
1032              &quot;biases should not be seen by VM thread here&quot;);
1033       BiasedLocking::revoke(hobj, JavaThread::current());
1034       obj = hobj();
1035       assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1036     }
1037   }
1038 
1039   // hashCode() is a heap mutator ...
1040   // Relaxing assertion for bug 6320749.
1041   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
1042          !SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
</pre>
<hr />
<pre>
1204   // Possible mark states: neutral, biased, stack-locked, inflated
1205 
1206   if (UseBiasedLocking &amp;&amp; h_obj()-&gt;mark().has_bias_pattern()) {
1207     // CASE: biased
1208     BiasedLocking::revoke(h_obj, self);
1209     assert(!h_obj-&gt;mark().has_bias_pattern(),
1210            &quot;biases should be revoked by now&quot;);
1211   }
1212 
1213   assert(self == JavaThread::current(), &quot;Can only be called on current thread&quot;);
1214   oop obj = h_obj();
1215   markWord mark = read_stable_mark(obj);
1216 
1217   // CASE: stack-locked.  Mark points to a BasicLock on the owner&#39;s stack.
1218   if (mark.has_locker()) {
1219     return self-&gt;is_lock_owned((address)mark.locker()) ?
1220       owner_self : owner_other;
1221   }
1222 
1223   // CASE: inflated. Mark (tagged pointer) points to an ObjectMonitor.
<span class="line-removed">1224   // The Object:ObjectMonitor relationship is stable as long as we&#39;re</span>
<span class="line-removed">1225   // not at a safepoint and AsyncDeflateIdleMonitors is false.</span>
1226   if (mark.has_monitor()) {
1227     // The first stage of async deflation does not affect any field
1228     // used by this comparison so the ObjectMonitor* is usable here.
1229     ObjectMonitor* monitor = mark.monitor();
1230     void* owner = monitor-&gt;owner();
1231     if (owner == NULL) return owner_none;
1232     return (owner == self ||
1233             self-&gt;is_lock_owned((address)owner)) ? owner_self : owner_other;
1234   }
1235 
1236   // CASE: neutral
1237   assert(mark.is_neutral(), &quot;sanity check&quot;);
1238   return owner_none;           // it&#39;s unlocked
1239 }
1240 
1241 // FIXME: jvmti should call this
1242 JavaThread* ObjectSynchronizer::get_lock_owner(ThreadsList * t_list, Handle h_obj) {
1243   if (UseBiasedLocking) {
1244     if (SafepointSynchronize::is_at_safepoint()) {
1245       BiasedLocking::revoke_at_safepoint(h_obj);
</pre>
<hr />
<pre>
1305     // used with block linkage _next_om fields).
1306     block = (PaddedObjectMonitor*)block-&gt;next_om();
1307   }
1308 }
1309 
1310 static bool monitors_used_above_threshold() {
1311   int population = Atomic::load(&amp;om_list_globals._population);
1312   if (population == 0) {
1313     return false;
1314   }
1315   if (MonitorUsedDeflationThreshold &gt; 0) {
1316     int monitors_used = population - Atomic::load(&amp;om_list_globals._free_count) -
1317                         Atomic::load(&amp;om_list_globals._wait_count);
1318     int monitor_usage = (monitors_used * 100LL) / population;
1319     return monitor_usage &gt; MonitorUsedDeflationThreshold;
1320   }
1321   return false;
1322 }
1323 
1324 bool ObjectSynchronizer::is_async_deflation_needed() {
<span class="line-removed">1325   if (!AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">1326     return false;</span>
<span class="line-removed">1327   }</span>
1328   if (is_async_deflation_requested()) {
1329     // Async deflation request.
1330     return true;
1331   }
1332   if (AsyncDeflationInterval &gt; 0 &amp;&amp;
1333       time_since_last_async_deflation_ms() &gt; AsyncDeflationInterval &amp;&amp;
1334       monitors_used_above_threshold()) {
1335     // It&#39;s been longer than our specified deflate interval and there
1336     // are too many monitors in use. We don&#39;t deflate more frequently
1337     // than AsyncDeflationInterval (unless is_async_deflation_requested)
1338     // in order to not swamp the ServiceThread.
1339     return true;
1340   }
1341   return false;
1342 }
1343 
<span class="line-removed">1344 bool ObjectSynchronizer::is_safepoint_deflation_needed() {</span>
<span class="line-removed">1345   return !AsyncDeflateIdleMonitors &amp;&amp;</span>
<span class="line-removed">1346          monitors_used_above_threshold();  // Too many monitors in use.</span>
<span class="line-removed">1347 }</span>
<span class="line-removed">1348 </span>
1349 bool ObjectSynchronizer::request_deflate_idle_monitors() {
1350   bool is_JavaThread = Thread::current()-&gt;is_Java_thread();
1351   bool ret_code = false;
1352 
<span class="line-modified">1353   if (AsyncDeflateIdleMonitors) {</span>
<span class="line-modified">1354     jlong last_time = last_async_deflation_time_ns();</span>
<span class="line-modified">1355     set_is_async_deflation_requested(true);</span>
<span class="line-modified">1356     {</span>
<span class="line-modified">1357       MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-modified">1358       ml.notify_all();</span>
<span class="line-modified">1359     }</span>
<span class="line-modified">1360     const int N_CHECKS = 5;</span>
<span class="line-modified">1361     for (int i = 0; i &lt; N_CHECKS; i++) {  // sleep for at most 5 seconds</span>
<span class="line-modified">1362       if (last_async_deflation_time_ns() &gt; last_time) {</span>
<span class="line-modified">1363         log_info(monitorinflation)(&quot;Async Deflation happened after %d check(s).&quot;, i);</span>
<span class="line-modified">1364         ret_code = true;</span>
<span class="line-removed">1365         break;</span>
<span class="line-removed">1366       }</span>
<span class="line-removed">1367       if (is_JavaThread) {</span>
<span class="line-removed">1368         // JavaThread has to honor the blocking protocol.</span>
<span class="line-removed">1369         ThreadBlockInVM tbivm(JavaThread::current());</span>
<span class="line-removed">1370         os::naked_short_sleep(999);  // sleep for almost 1 second</span>
<span class="line-removed">1371       } else {</span>
<span class="line-removed">1372         os::naked_short_sleep(999);  // sleep for almost 1 second</span>
<span class="line-removed">1373       }</span>
1374     }
<span class="line-modified">1375     if (!ret_code) {</span>
<span class="line-modified">1376       log_info(monitorinflation)(&quot;Async Deflation DID NOT happen after %d checks.&quot;, N_CHECKS);</span>




1377     }
<span class="line-modified">1378   } else {</span>
<span class="line-modified">1379     // Only need to force this safepoint if we are not using async</span>
<span class="line-modified">1380     // deflation. The VMThread won&#39;t call this function before the</span>
<span class="line-removed">1381     // final safepoint if we are not using async deflation so we</span>
<span class="line-removed">1382     // don&#39;t have to reason about the VMThread executing a VM-op here.</span>
<span class="line-removed">1383     VM_ForceSafepoint force_safepoint_op;</span>
<span class="line-removed">1384     VMThread::execute(&amp;force_safepoint_op);</span>
<span class="line-removed">1385     ret_code = true;</span>
1386   }
1387 
1388   return ret_code;
1389 }
1390 
1391 jlong ObjectSynchronizer::time_since_last_async_deflation_ms() {
1392   return (os::javaTimeNanos() - last_async_deflation_time_ns()) / (NANOUNITS / MILLIUNITS);
1393 }
1394 
1395 void ObjectSynchronizer::oops_do(OopClosure* f) {
1396   // We only scan the global used list here (for moribund threads), and
1397   // the thread-local monitors in Thread::oops_do().
1398   global_used_oops_do(f);
1399 }
1400 
1401 void ObjectSynchronizer::global_used_oops_do(OopClosure* f) {
1402   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1403   list_oops_do(Atomic::load(&amp;om_list_globals._in_use_list), f);
1404 }
1405 
</pre>
<hr />
<pre>
1407   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1408   list_oops_do(thread-&gt;om_in_use_list, f);
1409 }
1410 
1411 void ObjectSynchronizer::list_oops_do(ObjectMonitor* list, OopClosure* f) {
1412   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1413   // The oops_do() phase does not overlap with monitor deflation
1414   // so no need to lock ObjectMonitors for the list traversal.
1415   for (ObjectMonitor* mid = list; mid != NULL; mid = unmarked_next(mid)) {
1416     if (mid-&gt;object() != NULL) {
1417       f-&gt;do_oop((oop*)mid-&gt;object_addr());
1418     }
1419   }
1420 }
1421 
1422 
1423 // -----------------------------------------------------------------------------
1424 // ObjectMonitor Lifecycle
1425 // -----------------------
1426 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
<span class="line-modified">1427 // free list and associates them with objects. Deflation -- which occurs at</span>
<span class="line-modified">1428 // STW-time or asynchronously -- disassociates idle monitors from objects.</span>
<span class="line-modified">1429 // Such scavenged monitors are returned to the om_list_globals._free_list.</span>
1430 //
1431 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
1432 //
1433 // Lifecycle:
1434 // --   unassigned and on the om_list_globals._free_list
1435 // --   unassigned and on a per-thread free list
1436 // --   assigned to an object.  The object is inflated and the mark refers
1437 //      to the ObjectMonitor.
1438 
1439 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1440   // A large MAXPRIVATE value reduces both list lock contention
1441   // and list coherency traffic, but also tends to increase the
<span class="line-modified">1442   // number of ObjectMonitors in circulation as well as the STW</span>
1443   // scavenge costs.  As usual, we lean toward time in space-time
1444   // tradeoffs.
1445   const int MAXPRIVATE = 1024;
1446   NoSafepointVerifier nsv;
1447 
1448   for (;;) {
1449     ObjectMonitor* m;
1450 
1451     // 1: try to allocate from the thread&#39;s local om_free_list.
1452     // Threads will attempt to allocate first from their local list, then
1453     // from the global list, and only after those attempts fail will the
1454     // thread attempt to instantiate new monitors. Thread-local free lists
1455     // improve allocation latency, as well as reducing coherency traffic
1456     // on the shared global list.
1457     m = take_from_start_of_om_free_list(self);
1458     if (m != NULL) {
1459       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1460       m-&gt;set_allocation_state(ObjectMonitor::New);
1461       prepend_to_om_in_use_list(self, m);
1462       return m;
1463     }
1464 
1465     // 2: try to allocate from the global om_list_globals._free_list
1466     // If we&#39;re using thread-local free lists then try
1467     // to reprovision the caller&#39;s free list.
1468     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
1469       // Reprovision the thread&#39;s om_free_list.
1470       // Use bulk transfers to reduce the allocation rate and heat
1471       // on various locks.
1472       for (int i = self-&gt;om_free_provision; --i &gt;= 0;) {
1473         ObjectMonitor* take = take_from_start_of_global_free_list();
1474         if (take == NULL) {
1475           break;  // No more are available.
1476         }
1477         guarantee(take-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-modified">1478         if (AsyncDeflateIdleMonitors) {</span>
<span class="line-modified">1479           // We allowed 3 field values to linger during async deflation.</span>
<span class="line-modified">1480           // Clear or restore them as appropriate.</span>
<span class="line-modified">1481           take-&gt;set_header(markWord::zero());</span>
<span class="line-modified">1482           // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-modified">1483           take-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-modified">1484           if (take-&gt;contentions() &lt; 0) {</span>
<span class="line-modified">1485             // Add back max_jint to restore the contentions field to its</span>
<span class="line-modified">1486             // proper value.</span>
<span class="line-removed">1487             take-&gt;add_to_contentions(max_jint);</span>
1488 
1489 #ifdef ASSERT
<span class="line-modified">1490             jint l_contentions = take-&gt;contentions();</span>


1491 #endif
<span class="line-removed">1492             assert(l_contentions &gt;= 0, &quot;must not be negative: l_contentions=%d, contentions=%d&quot;,</span>
<span class="line-removed">1493                    l_contentions, take-&gt;contentions());</span>
<span class="line-removed">1494           }</span>
1495         }
1496         take-&gt;Recycle();
1497         // Since we&#39;re taking from the global free-list, take must be Free.
1498         // om_release() also sets the allocation state to Free because it
1499         // is called from other code paths.
1500         assert(take-&gt;is_free(), &quot;invariant&quot;);
1501         om_release(self, take, false);
1502       }
1503       self-&gt;om_free_provision += 1 + (self-&gt;om_free_provision / 2);
1504       if (self-&gt;om_free_provision &gt; MAXPRIVATE) self-&gt;om_free_provision = MAXPRIVATE;
1505       continue;
1506     }
1507 
1508     // 3: allocate a block of new ObjectMonitors
1509     // Both the local and global free lists are empty -- resort to malloc().
1510     // In the current implementation ObjectMonitors are TSM - immortal.
1511     // Ideally, we&#39;d write &quot;new ObjectMonitor[_BLOCKSIZE], but we want
1512     // each ObjectMonitor to start at the beginning of a cache line,
1513     // so we use align_up().
1514     // A better solution would be to use C++ placement-new.
</pre>
<hr />
<pre>
1540     // Element [0] is reserved for global list linkage
1541     temp[0].set_object(CHAINMARKER);
1542 
1543     // Consider carving out this thread&#39;s current request from the
1544     // block in hand.  This avoids some lock traffic and redundant
1545     // list activity.
1546 
1547     prepend_block_to_lists(temp);
1548   }
1549 }
1550 
1551 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1552 // In practice there&#39;s no need to clamp or limit the number of
1553 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1554 // we&#39;ll call om_release() is to return a monitor to the free list after
1555 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1556 // accumulate on a thread&#39;s free list.
1557 //
1558 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1559 // free list must have their object field set to null. This prevents the
<span class="line-modified">1560 // scavenger -- deflate_monitor_list() or deflate_monitor_list_using_JT()</span>
<span class="line-modified">1561 // -- from reclaiming them while we are trying to release them.</span>
1562 
1563 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1564                                     bool from_per_thread_alloc) {
1565   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1566   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1567   NoSafepointVerifier nsv;
1568 
1569   if ((m-&gt;is_busy() | m-&gt;_recursions) != 0) {
1570     stringStream ss;
1571     fatal(&quot;freeing in-use monitor: %s, recursions=&quot; INTX_FORMAT,
1572           m-&gt;is_busy_to_string(&amp;ss), m-&gt;_recursions);
1573   }
1574   m-&gt;set_allocation_state(ObjectMonitor::Free);
1575   // _next_om is used for both per-thread in-use and free lists so
1576   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1577   if (from_per_thread_alloc) {
1578     // Need to remove &#39;m&#39; from om_in_use_list.
1579     ObjectMonitor* mid = NULL;
1580     ObjectMonitor* next = NULL;
1581 
</pre>
<hr />
<pre>
1650 
1651     // At this point mid is disconnected from the in-use list so
1652     // its lock no longer has any effects on the in-use list.
1653     Atomic::dec(&amp;self-&gt;om_in_use_count);
1654     // Unlock mid, but leave the next value for any lagging list
1655     // walkers. It will get cleaned up when mid is prepended to
1656     // the thread&#39;s free list:
1657     om_unlock(mid);
1658   }
1659 
1660   prepend_to_om_free_list(self, m);
1661   guarantee(m-&gt;is_free(), &quot;invariant&quot;);
1662 }
1663 
1664 // Return ObjectMonitors on a moribund thread&#39;s free and in-use
1665 // lists to the appropriate global lists. The ObjectMonitors on the
1666 // per-thread in-use list may still be in use by other threads.
1667 //
1668 // We currently call om_flush() from Threads::remove() before the
1669 // thread has been excised from the thread list and is no longer a
<span class="line-modified">1670 // mutator. This means that om_flush() cannot run concurrently with</span>
<span class="line-modified">1671 // a safepoint and interleave with deflate_idle_monitors(). In</span>
<span class="line-modified">1672 // particular, this ensures that the thread&#39;s in-use monitors are</span>
<span class="line-modified">1673 // scanned by a GC safepoint, either via Thread::oops_do() (before</span>
<span class="line-removed">1674 // om_flush() is called) or via ObjectSynchronizer::oops_do() (after</span>
<span class="line-removed">1675 // om_flush() is called).</span>
1676 //
<span class="line-modified">1677 // With AsyncDeflateIdleMonitors, deflate_global_idle_monitors_using_JT()</span>
<span class="line-modified">1678 // and deflate_per_thread_idle_monitors_using_JT() (in another thread) can</span>
1679 // run at the same time as om_flush() so we have to follow a careful
1680 // protocol to prevent list corruption.
1681 
1682 void ObjectSynchronizer::om_flush(Thread* self) {
1683   // Process the per-thread in-use list first to be consistent.
1684   int in_use_count = 0;
1685   ObjectMonitor* in_use_list = NULL;
1686   ObjectMonitor* in_use_tail = NULL;
1687   NoSafepointVerifier nsv;
1688 
1689   // This function can race with a list walker or with an async
1690   // deflater thread so we lock the list head to prevent confusion.
1691   // An async deflater thread checks to see if the target thread
1692   // is exiting, but if it has made it past that check before we
1693   // started exiting, then it is racing to get to the in-use list.
1694   if ((in_use_list = get_list_head_locked(&amp;self-&gt;om_in_use_list)) != NULL) {
1695     // At this point, we have locked the in-use list head so a racing
1696     // thread cannot come in after us. However, a racing thread could
1697     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1698     //
</pre>
<hr />
<pre>
1712         while (is_locked(cur_om)) {
1713           os::naked_short_sleep(1);
1714         }
1715         // Refetch the possibly changed next field and try again.
1716         cur_om = unmarked_next(in_use_tail);
1717         continue;
1718       }
1719       if (cur_om-&gt;object() == NULL) {
1720         // cur_om was deflated and the object ref was cleared while it
1721         // was locked. We happened to see it just after it was unlocked
1722         // (and added to the free list). Refetch the possibly changed
1723         // next field and try again.
1724         cur_om = unmarked_next(in_use_tail);
1725         continue;
1726       }
1727       in_use_tail = cur_om;
1728       in_use_count++;
1729       cur_om = unmarked_next(cur_om);
1730     }
1731     guarantee(in_use_tail != NULL, &quot;invariant&quot;);

1732     int l_om_in_use_count = Atomic::load(&amp;self-&gt;om_in_use_count);
<span class="line-modified">1733     ADIM_guarantee(l_om_in_use_count == in_use_count, &quot;in-use counts don&#39;t match: &quot;</span>
<span class="line-modified">1734                    &quot;l_om_in_use_count=%d, in_use_count=%d&quot;, l_om_in_use_count, in_use_count);</span>

1735     Atomic::store(&amp;self-&gt;om_in_use_count, 0);
1736     // Clear the in-use list head (which also unlocks it):
1737     Atomic::store(&amp;self-&gt;om_in_use_list, (ObjectMonitor*)NULL);
1738     om_unlock(in_use_list);
1739   }
1740 
1741   int free_count = 0;
1742   ObjectMonitor* free_list = NULL;
1743   ObjectMonitor* free_tail = NULL;
1744   // This function can race with a list walker thread so we lock the
1745   // list head to prevent confusion.
1746   if ((free_list = get_list_head_locked(&amp;self-&gt;om_free_list)) != NULL) {
1747     // At this point, we have locked the free list head so a racing
1748     // thread cannot come in after us. However, a racing thread could
1749     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1750     //
1751     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1752     // monitor which will be linked to om_list_globals._free_list below.
1753     //
1754     // Account for the free list head before the loop since it is
1755     // already locked (by this thread):
1756     free_tail = free_list;
1757     free_count++;
1758     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1759       if (is_locked(s)) {
1760         // s is locked so there must be a racing walker thread ahead
1761         // of us so we&#39;ll give it a chance to finish.
1762         while (is_locked(s)) {
1763           os::naked_short_sleep(1);
1764         }
1765       }
1766       free_tail = s;
1767       free_count++;
1768       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
1769       if (s-&gt;is_busy()) {
1770         stringStream ss;
1771         fatal(&quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));
1772       }
1773     }
1774     guarantee(free_tail != NULL, &quot;invariant&quot;);

1775     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
<span class="line-modified">1776     ADIM_guarantee(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;</span>
<span class="line-modified">1777                    &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);</span>

1778     Atomic::store(&amp;self-&gt;om_free_count, 0);
1779     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1780     om_unlock(free_list);
1781   }
1782 
1783   if (free_tail != NULL) {
1784     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1785   }
1786 
1787   if (in_use_tail != NULL) {
1788     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1789   }
1790 
1791   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1792   LogStreamHandle(Info, monitorinflation) lsh_info;
1793   LogStream* ls = NULL;
1794   if (log_is_enabled(Debug, monitorinflation)) {
1795     ls = &amp;lsh_debug;
1796   } else if ((free_count != 0 || in_use_count != 0) &amp;&amp;
1797              log_is_enabled(Info, monitorinflation)) {
</pre>
<hr />
<pre>
1840   }
1841 
1842   EventJavaMonitorInflate event;
1843 
1844   for (;;) {
1845     const markWord mark = object-&gt;mark();
1846     assert(!mark.has_bias_pattern(), &quot;invariant&quot;);
1847 
1848     // The mark can be in one of the following states:
1849     // *  Inflated     - just return
1850     // *  Stack-locked - coerce it to inflated
1851     // *  INFLATING    - busy wait for conversion to complete
1852     // *  Neutral      - aggressively inflate the object.
1853     // *  BIASED       - Illegal.  We should never see this
1854 
1855     // CASE: inflated
1856     if (mark.has_monitor()) {
1857       ObjectMonitor* inf = mark.monitor();
1858       markWord dmw = inf-&gt;header();
1859       assert(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());
<span class="line-removed">1860       assert(AsyncDeflateIdleMonitors || inf-&gt;object() == object, &quot;invariant&quot;);</span>
1861       assert(ObjectSynchronizer::verify_objmon_isinpool(inf), &quot;monitor is invalid&quot;);
1862       return inf;
1863     }
1864 
1865     // CASE: inflation in progress - inflating over a stack-lock.
1866     // Some other thread is converting from stack-locked to inflated.
1867     // Only that thread can complete inflation -- other threads must wait.
1868     // The INFLATING value is transient.
1869     // Currently, we spin/yield/park and poll the markword, waiting for inflation to finish.
1870     // We could always eliminate polling by parking the thread on some auxiliary list.
1871     if (mark == markWord::INFLATING()) {
1872       read_stable_mark(object);
1873       continue;
1874     }
1875 
1876     // CASE: stack-locked
1877     // Could be stack-locked either by this thread or by some other thread.
1878     //
1879     // Note that we allocate the objectmonitor speculatively, _before_ attempting
1880     // to install INFLATING into the mark word.  We originally installed INFLATING,
</pre>
<hr />
<pre>
1926       // value from the BasicLock on the owner&#39;s stack to the ObjectMonitor, all
1927       // the while preserving the hashCode stability invariants.  If the owner
1928       // decides to release the lock while the value is 0, the unlock will fail
1929       // and control will eventually pass from slow_exit() to inflate.  The owner
1930       // will then spin, waiting for the 0 value to disappear.   Put another way,
1931       // the 0 causes the owner to stall if the owner happens to try to
1932       // drop the lock (restoring the header from the BasicLock to the object)
1933       // while inflation is in-progress.  This protocol avoids races that might
1934       // would otherwise permit hashCode values to change or &quot;flicker&quot; for an object.
1935       // Critically, while object-&gt;mark is 0 mark.displaced_mark_helper() is stable.
1936       // 0 serves as a &quot;BUSY&quot; inflate-in-progress indicator.
1937 
1938 
1939       // fetch the displaced mark from the owner&#39;s stack.
1940       // The owner can&#39;t die or unwind past the lock while our INFLATING
1941       // object is in the mark.  Furthermore the owner can&#39;t complete
1942       // an unlock on the object, either.
1943       markWord dmw = mark.displaced_mark_helper();
1944       // Catch if the object&#39;s header is not neutral (not locked and
1945       // not marked is what we care about here).
<span class="line-modified">1946       ADIM_guarantee(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());</span>
1947 
1948       // Setup monitor fields to proper values -- prepare the monitor
1949       m-&gt;set_header(dmw);
1950 
1951       // Optimization: if the mark.locker stack address is associated
1952       // with this thread we could simply set m-&gt;_owner = self.
1953       // Note that a thread can inflate an object
1954       // that it has stack-locked -- as might happen in wait() -- directly
1955       // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
<span class="line-modified">1956       if (AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">1957         m-&gt;set_owner_from(NULL, DEFLATER_MARKER, mark.locker());</span>
<span class="line-removed">1958       } else {</span>
<span class="line-removed">1959         m-&gt;set_owner_from(NULL, mark.locker());</span>
<span class="line-removed">1960       }</span>
1961       m-&gt;set_object(object);
1962       // TODO-FIXME: assert BasicLock-&gt;dhw != 0.
1963 
1964       // Must preserve store ordering. The monitor state must
1965       // be stable at the time of publishing the monitor address.
1966       guarantee(object-&gt;mark() == markWord::INFLATING(), &quot;invariant&quot;);
1967       object-&gt;release_set_mark(markWord::encode(m));
1968 
1969       // Once ObjectMonitor is configured and the object is associated
1970       // with the ObjectMonitor, it is safe to allow async deflation:
1971       assert(m-&gt;is_new(), &quot;freshly allocated monitor must be new&quot;);
1972       m-&gt;set_allocation_state(ObjectMonitor::Old);
1973 
1974       // Hopefully the performance counters are allocated on distinct cache lines
1975       // to avoid false sharing on MP systems ...
1976       OM_PERFDATA_OP(Inflations, inc());
1977       if (log_is_enabled(Trace, monitorinflation)) {
1978         ResourceMark rm(self);
1979         lsh.print_cr(&quot;inflate(has_locker): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
1980                      INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
1981                      object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
1982       }
1983       if (event.should_commit()) {
1984         post_monitor_inflate_event(&amp;event, object, cause);
1985       }
1986       return m;
1987     }
1988 
1989     // CASE: neutral
1990     // TODO-FIXME: for entry we currently inflate and then try to CAS _owner.
1991     // If we know we&#39;re inflating for entry it&#39;s better to inflate by swinging a
1992     // pre-locked ObjectMonitor pointer into the object header.   A successful
1993     // CAS inflates the object *and* confers ownership to the inflating thread.
1994     // In the current implementation we use a 2-step mechanism where we CAS()
1995     // to inflate and then CAS() again to try to swing _owner from NULL to self.
1996     // An inflateTry() method that we could call from enter() would be useful.
1997 
1998     // Catch if the object&#39;s header is not neutral (not locked and
1999     // not marked is what we care about here).
<span class="line-modified">2000     ADIM_guarantee(mark.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, mark.value());</span>
2001     ObjectMonitor* m = om_alloc(self);
2002     // prepare m for installation - set monitor to initial state
2003     m-&gt;Recycle();
2004     m-&gt;set_header(mark);
<span class="line-modified">2005     if (AsyncDeflateIdleMonitors) {</span>
<span class="line-modified">2006       // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-removed">2007       m-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-removed">2008     }</span>
2009     m-&gt;set_object(object);
2010     m-&gt;_Responsible  = NULL;
2011     m-&gt;_SpinDuration = ObjectMonitor::Knob_SpinLimit;       // consider: keep metastats by type/class
2012 
2013     if (object-&gt;cas_set_mark(markWord::encode(m), mark) != mark) {
2014       m-&gt;set_header(markWord::zero());
2015       m-&gt;set_object(NULL);
2016       m-&gt;Recycle();
2017       // om_release() will reset the allocation state from New to Free.
2018       om_release(self, m, true);
2019       m = NULL;
2020       continue;
2021       // interference - the markword changed - just retry.
2022       // The state-transitions are one-way, so there&#39;s no chance of
2023       // live-lock -- &quot;Inflated&quot; is an absorbing state.
2024     }
2025 
2026     // Once the ObjectMonitor is configured and object is associated
2027     // with the ObjectMonitor, it is safe to allow async deflation:
2028     assert(m-&gt;is_new(), &quot;freshly allocated monitor must be new&quot;);
2029     m-&gt;set_allocation_state(ObjectMonitor::Old);
2030 
2031     // Hopefully the performance counters are allocated on distinct
2032     // cache lines to avoid false sharing on MP systems ...
2033     OM_PERFDATA_OP(Inflations, inc());
2034     if (log_is_enabled(Trace, monitorinflation)) {
2035       ResourceMark rm(self);
2036       lsh.print_cr(&quot;inflate(neutral): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
2037                    INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
2038                    object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
2039     }
2040     if (event.should_commit()) {
2041       post_monitor_inflate_event(&amp;event, object, cause);
2042     }
2043     return m;
2044   }
2045 }
2046 
2047 
<span class="line-modified">2048 // We maintain a list of in-use monitors for each thread.</span>
<span class="line-modified">2049 //</span>
<span class="line-modified">2050 // For safepoint based deflation:</span>
<span class="line-removed">2051 // deflate_thread_local_monitors() scans a single thread&#39;s in-use list, while</span>
<span class="line-removed">2052 // deflate_idle_monitors() scans only a global list of in-use monitors which</span>
<span class="line-removed">2053 // is populated only as a thread dies (see om_flush()).</span>
<span class="line-removed">2054 //</span>
<span class="line-removed">2055 // These operations are called at all safepoints, immediately after mutators</span>
<span class="line-removed">2056 // are stopped, but before any objects have moved. Collectively they traverse</span>
<span class="line-removed">2057 // the population of in-use monitors, deflating where possible. The scavenged</span>
<span class="line-removed">2058 // monitors are returned to the global monitor free list.</span>
<span class="line-removed">2059 //</span>
<span class="line-removed">2060 // Beware that we scavenge at *every* stop-the-world point. Having a large</span>
<span class="line-removed">2061 // number of monitors in-use could negatively impact performance. We also want</span>
<span class="line-removed">2062 // to minimize the total # of monitors in circulation, as they incur a small</span>
<span class="line-removed">2063 // footprint penalty.</span>
<span class="line-removed">2064 //</span>
<span class="line-removed">2065 // Perversely, the heap size -- and thus the STW safepoint rate --</span>
<span class="line-removed">2066 // typically drives the scavenge rate.  Large heaps can mean infrequent GC,</span>
<span class="line-removed">2067 // which in turn can mean large(r) numbers of ObjectMonitors in circulation.</span>
<span class="line-removed">2068 // This is an unfortunate aspect of this design.</span>
<span class="line-removed">2069 //</span>
<span class="line-removed">2070 // For async deflation:</span>
<span class="line-removed">2071 // If a special deflation request is made, then the safepoint based</span>
<span class="line-removed">2072 // deflation mechanism is used. Otherwise, an async deflation request</span>
<span class="line-removed">2073 // is registered with the ServiceThread and it is notified.</span>
<span class="line-removed">2074 </span>
<span class="line-removed">2075 void ObjectSynchronizer::do_safepoint_work(DeflateMonitorCounters* counters) {</span>
2076   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
2077 
<span class="line-removed">2078   // The per-thread in-use lists are handled in</span>
<span class="line-removed">2079   // ParallelSPCleanupThreadClosure::do_thread().</span>
<span class="line-removed">2080 </span>
<span class="line-removed">2081   if (!AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">2082     // Use the older mechanism for the global in-use list.</span>
<span class="line-removed">2083     ObjectSynchronizer::deflate_idle_monitors(counters);</span>
<span class="line-removed">2084     return;</span>
<span class="line-removed">2085   }</span>
<span class="line-removed">2086 </span>
2087   log_debug(monitorinflation)(&quot;requesting async deflation of idle monitors.&quot;);
2088   // Request deflation of idle monitors by the ServiceThread:
2089   set_is_async_deflation_requested(true);
2090   MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);
2091   ml.notify_all();
2092 
2093   if (log_is_enabled(Debug, monitorinflation)) {
2094     // exit_globals()&#39;s call to audit_and_print_stats() is done
2095     // at the Info level and not at a safepoint.
<span class="line-removed">2096     // For safepoint based deflation, audit_and_print_stats() is called</span>
<span class="line-removed">2097     // in ObjectSynchronizer::finish_deflate_idle_monitors() at the</span>
<span class="line-removed">2098     // Debug level at a safepoint.</span>
2099     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
2100   }
2101 }
2102 
<span class="line-removed">2103 // Deflate a single monitor if not in-use</span>
<span class="line-removed">2104 // Return true if deflated, false if in-use</span>
<span class="line-removed">2105 bool ObjectSynchronizer::deflate_monitor(ObjectMonitor* mid, oop obj,</span>
<span class="line-removed">2106                                          ObjectMonitor** free_head_p,</span>
<span class="line-removed">2107                                          ObjectMonitor** free_tail_p) {</span>
<span class="line-removed">2108   bool deflated;</span>
<span class="line-removed">2109   // Normal case ... The monitor is associated with obj.</span>
<span class="line-removed">2110   const markWord mark = obj-&gt;mark();</span>
<span class="line-removed">2111   guarantee(mark == markWord::encode(mid), &quot;should match: mark=&quot;</span>
<span class="line-removed">2112             INTPTR_FORMAT &quot;, encoded mid=&quot; INTPTR_FORMAT, mark.value(),</span>
<span class="line-removed">2113             markWord::encode(mid).value());</span>
<span class="line-removed">2114   // Make sure that mark.monitor() and markWord::encode() agree:</span>
<span class="line-removed">2115   guarantee(mark.monitor() == mid, &quot;should match: monitor()=&quot; INTPTR_FORMAT</span>
<span class="line-removed">2116             &quot;, mid=&quot; INTPTR_FORMAT, p2i(mark.monitor()), p2i(mid));</span>
<span class="line-removed">2117   const markWord dmw = mid-&gt;header();</span>
<span class="line-removed">2118   guarantee(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());</span>
<span class="line-removed">2119 </span>
<span class="line-removed">2120   if (mid-&gt;is_busy()) {</span>
<span class="line-removed">2121     // Easy checks are first - the ObjectMonitor is busy so no deflation.</span>
<span class="line-removed">2122     deflated = false;</span>
<span class="line-removed">2123   } else {</span>
<span class="line-removed">2124     // Deflate the monitor if it is no longer being used</span>
<span class="line-removed">2125     // It&#39;s idle - scavenge and return to the global free list</span>
<span class="line-removed">2126     // plain old deflation ...</span>
<span class="line-removed">2127     if (log_is_enabled(Trace, monitorinflation)) {</span>
<span class="line-removed">2128       ResourceMark rm;</span>
<span class="line-removed">2129       log_trace(monitorinflation)(&quot;deflate_monitor: &quot;</span>
<span class="line-removed">2130                                   &quot;object=&quot; INTPTR_FORMAT &quot;, mark=&quot;</span>
<span class="line-removed">2131                                   INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(obj),</span>
<span class="line-removed">2132                                   mark.value(), obj-&gt;klass()-&gt;external_name());</span>
<span class="line-removed">2133     }</span>
<span class="line-removed">2134 </span>
<span class="line-removed">2135     // Restore the header back to obj</span>
<span class="line-removed">2136     obj-&gt;release_set_mark(dmw);</span>
<span class="line-removed">2137     if (AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">2138       // clear() expects the owner field to be NULL.</span>
<span class="line-removed">2139       // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-removed">2140       mid-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-removed">2141     }</span>
<span class="line-removed">2142     mid-&gt;clear();</span>
<span class="line-removed">2143 </span>
<span class="line-removed">2144     assert(mid-&gt;object() == NULL, &quot;invariant: object=&quot; INTPTR_FORMAT,</span>
<span class="line-removed">2145            p2i(mid-&gt;object()));</span>
<span class="line-removed">2146     assert(mid-&gt;is_free(), &quot;invariant&quot;);</span>
<span class="line-removed">2147 </span>
<span class="line-removed">2148     // Move the deflated ObjectMonitor to the working free list</span>
<span class="line-removed">2149     // defined by free_head_p and free_tail_p.</span>
<span class="line-removed">2150     if (*free_head_p == NULL) *free_head_p = mid;</span>
<span class="line-removed">2151     if (*free_tail_p != NULL) {</span>
<span class="line-removed">2152       // We append to the list so the caller can use mid-&gt;_next_om</span>
<span class="line-removed">2153       // to fix the linkages in its context.</span>
<span class="line-removed">2154       ObjectMonitor* prevtail = *free_tail_p;</span>
<span class="line-removed">2155       // Should have been cleaned up by the caller:</span>
<span class="line-removed">2156       // Note: Should not have to lock prevtail here since we&#39;re at a</span>
<span class="line-removed">2157       // safepoint and ObjectMonitors on the local free list should</span>
<span class="line-removed">2158       // not be accessed in parallel.</span>
<span class="line-removed">2159 #ifdef ASSERT</span>
<span class="line-removed">2160       ObjectMonitor* l_next_om = prevtail-&gt;next_om();</span>
<span class="line-removed">2161 #endif</span>
<span class="line-removed">2162       assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
<span class="line-removed">2163       prevtail-&gt;set_next_om(mid);</span>
<span class="line-removed">2164     }</span>
<span class="line-removed">2165     *free_tail_p = mid;</span>
<span class="line-removed">2166     // At this point, mid-&gt;_next_om still refers to its current</span>
<span class="line-removed">2167     // value and another ObjectMonitor&#39;s _next_om field still</span>
<span class="line-removed">2168     // refers to this ObjectMonitor. Those linkages have to be</span>
<span class="line-removed">2169     // cleaned up by the caller who has the complete context.</span>
<span class="line-removed">2170     deflated = true;</span>
<span class="line-removed">2171   }</span>
<span class="line-removed">2172   return deflated;</span>
<span class="line-removed">2173 }</span>
<span class="line-removed">2174 </span>
2175 // Deflate the specified ObjectMonitor if not in-use using a JavaThread.
2176 // Returns true if it was deflated and false otherwise.
2177 //
2178 // The async deflation protocol sets owner to DEFLATER_MARKER and
2179 // makes contentions negative as signals to contending threads that
2180 // an async deflation is in progress. There are a number of checks
2181 // as part of the protocol to make sure that the calling thread has
2182 // not lost the race to a contending thread.
2183 //
2184 // The ObjectMonitor has been successfully async deflated when:
2185 //   (contentions &lt; 0)
2186 // Contending threads that see that condition know to retry their operation.
2187 //
2188 bool ObjectSynchronizer::deflate_monitor_using_JT(ObjectMonitor* mid,
2189                                                   ObjectMonitor** free_head_p,
2190                                                   ObjectMonitor** free_tail_p) {
<span class="line-removed">2191   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
2192   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);
2193   // A newly allocated ObjectMonitor should not be seen here so we
2194   // avoid an endless inflate/deflate cycle.
2195   assert(mid-&gt;is_old(), &quot;must be old: allocation_state=%d&quot;,
2196          (int) mid-&gt;allocation_state());
2197 
2198   if (mid-&gt;is_busy()) {
2199     // Easy checks are first - the ObjectMonitor is busy so no deflation.
2200     return false;
2201   }
2202 
2203   // Set a NULL owner to DEFLATER_MARKER to force any contending thread
2204   // through the slow path. This is just the first part of the async
2205   // deflation dance.
2206   if (mid-&gt;try_set_owner_from(NULL, DEFLATER_MARKER) != NULL) {
2207     // The owner field is no longer NULL so we lost the race since the
2208     // ObjectMonitor is now busy.
2209     return false;
2210   }
2211 
</pre>
<hr />
<pre>
2260   mid-&gt;clear_common();
2261 
2262   assert(mid-&gt;object() == NULL, &quot;must be NULL: object=&quot; INTPTR_FORMAT,
2263          p2i(mid-&gt;object()));
2264   assert(mid-&gt;is_free(), &quot;must be free: allocation_state=%d&quot;,
2265          (int)mid-&gt;allocation_state());
2266 
2267   // Move the deflated ObjectMonitor to the working free list
2268   // defined by free_head_p and free_tail_p.
2269   if (*free_head_p == NULL) {
2270     // First one on the list.
2271     *free_head_p = mid;
2272   }
2273   if (*free_tail_p != NULL) {
2274     // We append to the list so the caller can use mid-&gt;_next_om
2275     // to fix the linkages in its context.
2276     ObjectMonitor* prevtail = *free_tail_p;
2277     // prevtail should have been cleaned up by the caller:
2278 #ifdef ASSERT
2279     ObjectMonitor* l_next_om = unmarked_next(prevtail);
<span class="line-modified">2280 #endif</span>
2281     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));
2282     om_lock(prevtail);
2283     prevtail-&gt;set_next_om(mid);  // prevtail now points to mid (and is unlocked)
2284   }
2285   *free_tail_p = mid;
2286 
2287   // At this point, mid-&gt;_next_om still refers to its current
2288   // value and another ObjectMonitor&#39;s _next_om field still
2289   // refers to this ObjectMonitor. Those linkages have to be
2290   // cleaned up by the caller who has the complete context.
2291 
2292   // We leave owner == DEFLATER_MARKER and contentions &lt; 0
2293   // to force any racing threads to retry.
2294   return true;  // Success, ObjectMonitor has been deflated.
2295 }
2296 
<span class="line-removed">2297 // Walk a given monitor list, and deflate idle monitors.</span>
<span class="line-removed">2298 // The given list could be a per-thread list or a global list.</span>
<span class="line-removed">2299 //</span>
<span class="line-removed">2300 // In the case of parallel processing of thread local monitor lists,</span>
<span class="line-removed">2301 // work is done by Threads::parallel_threads_do() which ensures that</span>
<span class="line-removed">2302 // each Java thread is processed by exactly one worker thread, and</span>
<span class="line-removed">2303 // thus avoid conflicts that would arise when worker threads would</span>
<span class="line-removed">2304 // process the same monitor lists concurrently.</span>
<span class="line-removed">2305 //</span>
<span class="line-removed">2306 // See also ParallelSPCleanupTask and</span>
<span class="line-removed">2307 // SafepointSynchronize::do_cleanup_tasks() in safepoint.cpp and</span>
<span class="line-removed">2308 // Threads::parallel_java_threads_do() in thread.cpp.</span>
<span class="line-removed">2309 int ObjectSynchronizer::deflate_monitor_list(ObjectMonitor** list_p,</span>
<span class="line-removed">2310                                              int* count_p,</span>
<span class="line-removed">2311                                              ObjectMonitor** free_head_p,</span>
<span class="line-removed">2312                                              ObjectMonitor** free_tail_p) {</span>
<span class="line-removed">2313   ObjectMonitor* cur_mid_in_use = NULL;</span>
<span class="line-removed">2314   ObjectMonitor* mid = NULL;</span>
<span class="line-removed">2315   ObjectMonitor* next = NULL;</span>
<span class="line-removed">2316   int deflated_count = 0;</span>
<span class="line-removed">2317 </span>
<span class="line-removed">2318   // This list walk executes at a safepoint and does not race with any</span>
<span class="line-removed">2319   // other list walkers.</span>
<span class="line-removed">2320 </span>
<span class="line-removed">2321   for (mid = Atomic::load(list_p); mid != NULL; mid = next) {</span>
<span class="line-removed">2322     next = unmarked_next(mid);</span>
<span class="line-removed">2323     oop obj = (oop) mid-&gt;object();</span>
<span class="line-removed">2324     if (obj != NULL &amp;&amp; deflate_monitor(mid, obj, free_head_p, free_tail_p)) {</span>
<span class="line-removed">2325       // Deflation succeeded and already updated free_head_p and</span>
<span class="line-removed">2326       // free_tail_p as needed. Finish the move to the local free list</span>
<span class="line-removed">2327       // by unlinking mid from the global or per-thread in-use list.</span>
<span class="line-removed">2328       if (cur_mid_in_use == NULL) {</span>
<span class="line-removed">2329         // mid is the list head so switch the list head to next:</span>
<span class="line-removed">2330         Atomic::store(list_p, next);</span>
<span class="line-removed">2331       } else {</span>
<span class="line-removed">2332         // Switch cur_mid_in_use&#39;s next field to next:</span>
<span class="line-removed">2333         cur_mid_in_use-&gt;set_next_om(next);</span>
<span class="line-removed">2334       }</span>
<span class="line-removed">2335       // At this point mid is disconnected from the in-use list.</span>
<span class="line-removed">2336       deflated_count++;</span>
<span class="line-removed">2337       Atomic::dec(count_p);</span>
<span class="line-removed">2338       // mid is current tail in the free_head_p list so NULL terminate it:</span>
<span class="line-removed">2339       mid-&gt;set_next_om(NULL);</span>
<span class="line-removed">2340     } else {</span>
<span class="line-removed">2341       cur_mid_in_use = mid;</span>
<span class="line-removed">2342     }</span>
<span class="line-removed">2343   }</span>
<span class="line-removed">2344   return deflated_count;</span>
<span class="line-removed">2345 }</span>
<span class="line-removed">2346 </span>
2347 // Walk a given ObjectMonitor list and deflate idle ObjectMonitors using
2348 // a JavaThread. Returns the number of deflated ObjectMonitors. The given
2349 // list could be a per-thread in-use list or the global in-use list.
2350 // If a safepoint has started, then we save state via saved_mid_in_use_p
2351 // and return to the caller to honor the safepoint.
2352 //
2353 int ObjectSynchronizer::deflate_monitor_list_using_JT(ObjectMonitor** list_p,
2354                                                       int* count_p,
2355                                                       ObjectMonitor** free_head_p,
2356                                                       ObjectMonitor** free_tail_p,
2357                                                       ObjectMonitor** saved_mid_in_use_p) {
<span class="line-removed">2358   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
2359   JavaThread* self = JavaThread::current();
2360 
2361   ObjectMonitor* cur_mid_in_use = NULL;
2362   ObjectMonitor* mid = NULL;
2363   ObjectMonitor* next = NULL;
2364   ObjectMonitor* next_next = NULL;
2365   int deflated_count = 0;
2366   NoSafepointVerifier nsv;
2367 
2368   // We use the more complicated lock-cur_mid_in_use-and-mid-as-we-go
2369   // protocol because om_release() can do list deletions in parallel;
2370   // this also prevents races with a list walker thread. We also
2371   // lock-next-next-as-we-go to prevent an om_flush() that is behind
2372   // this thread from passing us.
2373   if (*saved_mid_in_use_p == NULL) {
2374     // No saved state so start at the beginning.
2375     // Lock the list head so we can possibly deflate it:
2376     if ((mid = get_list_head_locked(list_p)) == NULL) {
2377       return 0;  // The list is empty so nothing to deflate.
2378     }
</pre>
<hr />
<pre>
2468         }
2469         return deflated_count;
2470       }
2471     }
2472     if (mid == NULL) {
2473       if (cur_mid_in_use != NULL) {
2474         om_unlock(cur_mid_in_use);
2475       }
2476       break;  // Reached end of the list so nothing more to deflate.
2477     }
2478 
2479     // The current mid&#39;s next field is locked at this point. If we have
2480     // a cur_mid_in_use, then it is also locked at this point.
2481   }
2482   // We finished the list without a safepoint starting so there&#39;s
2483   // no need to save state.
2484   *saved_mid_in_use_p = NULL;
2485   return deflated_count;
2486 }
2487 
<span class="line-removed">2488 void ObjectSynchronizer::prepare_deflate_idle_monitors(DeflateMonitorCounters* counters) {</span>
<span class="line-removed">2489   counters-&gt;n_in_use = 0;              // currently associated with objects</span>
<span class="line-removed">2490   counters-&gt;n_in_circulation = 0;      // extant</span>
<span class="line-removed">2491   counters-&gt;n_scavenged = 0;           // reclaimed (global and per-thread)</span>
<span class="line-removed">2492   counters-&gt;per_thread_scavenged = 0;  // per-thread scavenge total</span>
<span class="line-removed">2493   counters-&gt;per_thread_times = 0.0;    // per-thread scavenge times</span>
<span class="line-removed">2494 }</span>
<span class="line-removed">2495 </span>
<span class="line-removed">2496 void ObjectSynchronizer::deflate_idle_monitors(DeflateMonitorCounters* counters) {</span>
<span class="line-removed">2497   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);</span>
<span class="line-removed">2498 </span>
<span class="line-removed">2499   if (AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">2500     // Nothing to do when global idle ObjectMonitors are deflated using</span>
<span class="line-removed">2501     // a JavaThread.</span>
<span class="line-removed">2502     return;</span>
<span class="line-removed">2503   }</span>
<span class="line-removed">2504 </span>
<span class="line-removed">2505   bool deflated = false;</span>
<span class="line-removed">2506 </span>
<span class="line-removed">2507   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors</span>
<span class="line-removed">2508   ObjectMonitor* free_tail_p = NULL;</span>
<span class="line-removed">2509   elapsedTimer timer;</span>
<span class="line-removed">2510 </span>
<span class="line-removed">2511   if (log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-removed">2512     timer.start();</span>
<span class="line-removed">2513   }</span>
<span class="line-removed">2514 </span>
<span class="line-removed">2515   // Note: the thread-local monitors lists get deflated in</span>
<span class="line-removed">2516   // a separate pass. See deflate_thread_local_monitors().</span>
<span class="line-removed">2517 </span>
<span class="line-removed">2518   // For moribund threads, scan om_list_globals._in_use_list</span>
<span class="line-removed">2519   int deflated_count = 0;</span>
<span class="line-removed">2520   if (Atomic::load(&amp;om_list_globals._in_use_list) != NULL) {</span>
<span class="line-removed">2521     // Update n_in_circulation before om_list_globals._in_use_count is</span>
<span class="line-removed">2522     // updated by deflation.</span>
<span class="line-removed">2523     Atomic::add(&amp;counters-&gt;n_in_circulation,</span>
<span class="line-removed">2524                 Atomic::load(&amp;om_list_globals._in_use_count));</span>
<span class="line-removed">2525 </span>
<span class="line-removed">2526     deflated_count = deflate_monitor_list(&amp;om_list_globals._in_use_list,</span>
<span class="line-removed">2527                                           &amp;om_list_globals._in_use_count,</span>
<span class="line-removed">2528                                           &amp;free_head_p, &amp;free_tail_p);</span>
<span class="line-removed">2529     Atomic::add(&amp;counters-&gt;n_in_use, Atomic::load(&amp;om_list_globals._in_use_count));</span>
<span class="line-removed">2530   }</span>
<span class="line-removed">2531 </span>
<span class="line-removed">2532   if (free_head_p != NULL) {</span>
<span class="line-removed">2533     // Move the deflated ObjectMonitors back to the global free list.</span>
<span class="line-removed">2534     guarantee(free_tail_p != NULL &amp;&amp; deflated_count &gt; 0, &quot;invariant&quot;);</span>
<span class="line-removed">2535 #ifdef ASSERT</span>
<span class="line-removed">2536     ObjectMonitor* l_next_om = free_tail_p-&gt;next_om();</span>
<span class="line-removed">2537 #endif</span>
<span class="line-removed">2538     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
<span class="line-removed">2539     prepend_list_to_global_free_list(free_head_p, free_tail_p, deflated_count);</span>
<span class="line-removed">2540     Atomic::add(&amp;counters-&gt;n_scavenged, deflated_count);</span>
<span class="line-removed">2541   }</span>
<span class="line-removed">2542   timer.stop();</span>
<span class="line-removed">2543 </span>
<span class="line-removed">2544   LogStreamHandle(Debug, monitorinflation) lsh_debug;</span>
<span class="line-removed">2545   LogStreamHandle(Info, monitorinflation) lsh_info;</span>
<span class="line-removed">2546   LogStream* ls = NULL;</span>
<span class="line-removed">2547   if (log_is_enabled(Debug, monitorinflation)) {</span>
<span class="line-removed">2548     ls = &amp;lsh_debug;</span>
<span class="line-removed">2549   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-removed">2550     ls = &amp;lsh_info;</span>
<span class="line-removed">2551   }</span>
<span class="line-removed">2552   if (ls != NULL) {</span>
<span class="line-removed">2553     ls-&gt;print_cr(&quot;deflating global idle monitors, %3.7f secs, %d monitors&quot;, timer.seconds(), deflated_count);</span>
<span class="line-removed">2554   }</span>
<span class="line-removed">2555 }</span>
<span class="line-removed">2556 </span>
2557 class HandshakeForDeflation : public HandshakeClosure {
2558  public:
2559   HandshakeForDeflation() : HandshakeClosure(&quot;HandshakeForDeflation&quot;) {}
2560 
2561   void do_thread(Thread* thread) {
2562     log_trace(monitorinflation)(&quot;HandshakeForDeflation::do_thread: thread=&quot;
2563                                 INTPTR_FORMAT, p2i(thread));
2564   }
2565 };
2566 
2567 void ObjectSynchronizer::deflate_idle_monitors_using_JT() {
<span class="line-removed">2568   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
<span class="line-removed">2569 </span>
2570   // Deflate any global idle monitors.
2571   deflate_global_idle_monitors_using_JT();
2572 
2573   int count = 0;
2574   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
2575     if (Atomic::load(&amp;jt-&gt;om_in_use_count) &gt; 0 &amp;&amp; !jt-&gt;is_exiting()) {
2576       // This JavaThread is using ObjectMonitors so deflate any that
2577       // are idle unless this JavaThread is exiting; do not race with
2578       // ObjectSynchronizer::om_flush().
2579       deflate_per_thread_idle_monitors_using_JT(jt);
2580       count++;
2581     }
2582   }
2583   if (count &gt; 0) {
2584     log_debug(monitorinflation)(&quot;did async deflation of idle monitors for %d thread(s).&quot;, count);
2585   }
2586 
2587   log_info(monitorinflation)(&quot;async global_population=%d, global_in_use_count=%d, &quot;
2588                              &quot;global_free_count=%d, global_wait_count=%d&quot;,
2589                              Atomic::load(&amp;om_list_globals._population),
2590                              Atomic::load(&amp;om_list_globals._in_use_count),
2591                              Atomic::load(&amp;om_list_globals._free_count),
2592                              Atomic::load(&amp;om_list_globals._wait_count));
2593 
2594   // The ServiceThread&#39;s async deflation request has been processed.
2595   _last_async_deflation_time_ns = os::javaTimeNanos();
2596   set_is_async_deflation_requested(false);
2597 
2598   if (Atomic::load(&amp;om_list_globals._wait_count) &gt; 0) {
2599     // There are deflated ObjectMonitors waiting for a handshake
2600     // (or a safepoint) for safety.
2601 
2602     ObjectMonitor* list = Atomic::load(&amp;om_list_globals._wait_list);
<span class="line-modified">2603     ADIM_guarantee(list != NULL, &quot;om_list_globals._wait_list must not be NULL&quot;);</span>
2604     int count = Atomic::load(&amp;om_list_globals._wait_count);
2605     Atomic::store(&amp;om_list_globals._wait_count, 0);
2606     Atomic::store(&amp;om_list_globals._wait_list, (ObjectMonitor*)NULL);
2607 
2608     // Find the tail for prepend_list_to_common(). No need to mark
2609     // ObjectMonitors for this list walk since only the deflater
2610     // thread manages the wait list.

2611     int l_count = 0;

2612     ObjectMonitor* tail = NULL;
2613     for (ObjectMonitor* n = list; n != NULL; n = unmarked_next(n)) {
2614       tail = n;

2615       l_count++;

2616     }
<span class="line-modified">2617     ADIM_guarantee(count == l_count, &quot;count=%d != l_count=%d&quot;, count, l_count);</span>
2618 
2619     // Will execute a safepoint if !ThreadLocalHandshakes:
2620     HandshakeForDeflation hfd_hc;
2621     Handshake::execute(&amp;hfd_hc);
2622 
2623     prepend_list_to_common(list, tail, count, &amp;om_list_globals._free_list,
2624                            &amp;om_list_globals._free_count);
2625 
2626     log_info(monitorinflation)(&quot;moved %d idle monitors from global waiting list to global free list&quot;, count);
2627   }
2628 }
2629 
2630 // Deflate global idle ObjectMonitors using a JavaThread.
2631 //
2632 void ObjectSynchronizer::deflate_global_idle_monitors_using_JT() {
<span class="line-removed">2633   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
2634   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);
2635   JavaThread* self = JavaThread::current();
2636 
2637   deflate_common_idle_monitors_using_JT(true /* is_global */, self);
2638 }
2639 
2640 // Deflate the specified JavaThread&#39;s idle ObjectMonitors using a JavaThread.
2641 //
2642 void ObjectSynchronizer::deflate_per_thread_idle_monitors_using_JT(JavaThread* target) {
<span class="line-removed">2643   assert(AsyncDeflateIdleMonitors, &quot;sanity check&quot;);</span>
2644   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);
2645 
2646   deflate_common_idle_monitors_using_JT(false /* !is_global */, target);
2647 }
2648 
2649 // Deflate global or per-thread idle ObjectMonitors using a JavaThread.
2650 //
2651 void ObjectSynchronizer::deflate_common_idle_monitors_using_JT(bool is_global, JavaThread* target) {
2652   JavaThread* self = JavaThread::current();
2653 
2654   int deflated_count = 0;
2655   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged ObjectMonitors
2656   ObjectMonitor* free_tail_p = NULL;
2657   ObjectMonitor* saved_mid_in_use_p = NULL;
2658   elapsedTimer timer;
2659 
2660   if (log_is_enabled(Info, monitorinflation)) {
2661     timer.start();
2662   }
2663 
</pre>
<hr />
<pre>
2679       local_deflated_count =
2680           deflate_monitor_list_using_JT(&amp;target-&gt;om_in_use_list,
2681                                         &amp;target-&gt;om_in_use_count, &amp;free_head_p,
2682                                         &amp;free_tail_p, &amp;saved_mid_in_use_p);
2683     }
2684     deflated_count += local_deflated_count;
2685 
2686     if (free_head_p != NULL) {
2687       // Move the deflated ObjectMonitors to the global free list.
2688       guarantee(free_tail_p != NULL &amp;&amp; local_deflated_count &gt; 0, &quot;free_tail_p=&quot; INTPTR_FORMAT &quot;, local_deflated_count=%d&quot;, p2i(free_tail_p), local_deflated_count);
2689       // Note: The target thread can be doing an om_alloc() that
2690       // is trying to prepend an ObjectMonitor on its in-use list
2691       // at the same time that we have deflated the current in-use
2692       // list head and put it on the local free list. prepend_to_common()
2693       // will detect the race and retry which avoids list corruption,
2694       // but the next field in free_tail_p can flicker to marked
2695       // and then unmarked while prepend_to_common() is sorting it
2696       // all out.
2697 #ifdef ASSERT
2698       ObjectMonitor* l_next_om = unmarked_next(free_tail_p);
<span class="line-modified">2699 #endif</span>
2700       assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));
2701 
2702       prepend_list_to_global_wait_list(free_head_p, free_tail_p, local_deflated_count);
2703 
2704       OM_PERFDATA_OP(Deflations, inc(local_deflated_count));
2705     }
2706 
2707     if (saved_mid_in_use_p != NULL) {
2708       // deflate_monitor_list_using_JT() detected a safepoint starting.
2709       timer.stop();
2710       {
2711         if (is_global) {
2712           log_debug(monitorinflation)(&quot;pausing deflation of global idle monitors for a safepoint.&quot;);
2713         } else {
2714           log_debug(monitorinflation)(&quot;jt=&quot; INTPTR_FORMAT &quot;: pausing deflation of per-thread idle monitors for a safepoint.&quot;, p2i(target));
2715         }
2716         assert(SafepointMechanism::should_block(self), &quot;sanity check&quot;);
2717         ThreadBlockInVM blocker(self);
2718       }
2719       // Prepare for another loop after the safepoint.
</pre>
<hr />
<pre>
2726   } while (saved_mid_in_use_p != NULL);
2727   timer.stop();
2728 
2729   LogStreamHandle(Debug, monitorinflation) lsh_debug;
2730   LogStreamHandle(Info, monitorinflation) lsh_info;
2731   LogStream* ls = NULL;
2732   if (log_is_enabled(Debug, monitorinflation)) {
2733     ls = &amp;lsh_debug;
2734   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {
2735     ls = &amp;lsh_info;
2736   }
2737   if (ls != NULL) {
2738     if (is_global) {
2739       ls-&gt;print_cr(&quot;async-deflating global idle monitors, %3.7f secs, %d monitors&quot;, timer.seconds(), deflated_count);
2740     } else {
2741       ls-&gt;print_cr(&quot;jt=&quot; INTPTR_FORMAT &quot;: async-deflating per-thread idle monitors, %3.7f secs, %d monitors&quot;, p2i(target), timer.seconds(), deflated_count);
2742     }
2743   }
2744 }
2745 
<span class="line-removed">2746 void ObjectSynchronizer::finish_deflate_idle_monitors(DeflateMonitorCounters* counters) {</span>
<span class="line-removed">2747   // Report the cumulative time for deflating each thread&#39;s idle</span>
<span class="line-removed">2748   // monitors. Note: if the work is split among more than one</span>
<span class="line-removed">2749   // worker thread, then the reported time will likely be more</span>
<span class="line-removed">2750   // than a beginning to end measurement of the phase.</span>
<span class="line-removed">2751   log_info(safepoint, cleanup)(&quot;deflating per-thread idle monitors, %3.7f secs, monitors=%d&quot;, counters-&gt;per_thread_times, counters-&gt;per_thread_scavenged);</span>
<span class="line-removed">2752 </span>
<span class="line-removed">2753   if (AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">2754     // Nothing to do when idle ObjectMonitors are deflated using</span>
<span class="line-removed">2755     // a JavaThread.</span>
<span class="line-removed">2756     return;</span>
<span class="line-removed">2757   }</span>
<span class="line-removed">2758 </span>
<span class="line-removed">2759   if (log_is_enabled(Debug, monitorinflation)) {</span>
<span class="line-removed">2760     // exit_globals()&#39;s call to audit_and_print_stats() is done</span>
<span class="line-removed">2761     // at the Info level and not at a safepoint.</span>
<span class="line-removed">2762     // For async deflation, audit_and_print_stats() is called in</span>
<span class="line-removed">2763     // ObjectSynchronizer::do_safepoint_work() at the Debug level</span>
<span class="line-removed">2764     // at a safepoint.</span>
<span class="line-removed">2765     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);</span>
<span class="line-removed">2766   } else if (log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-removed">2767     log_info(monitorinflation)(&quot;global_population=%d, global_in_use_count=%d, &quot;</span>
<span class="line-removed">2768                                &quot;global_free_count=%d, global_wait_count=%d&quot;,</span>
<span class="line-removed">2769                                Atomic::load(&amp;om_list_globals._population),</span>
<span class="line-removed">2770                                Atomic::load(&amp;om_list_globals._in_use_count),</span>
<span class="line-removed">2771                                Atomic::load(&amp;om_list_globals._free_count),</span>
<span class="line-removed">2772                                Atomic::load(&amp;om_list_globals._wait_count));</span>
<span class="line-removed">2773   }</span>
<span class="line-removed">2774 </span>
<span class="line-removed">2775   OM_PERFDATA_OP(Deflations, inc(counters-&gt;n_scavenged));</span>
<span class="line-removed">2776   OM_PERFDATA_OP(MonExtant, set_value(counters-&gt;n_in_circulation));</span>
<span class="line-removed">2777 </span>
<span class="line-removed">2778   GVars.stw_random = os::random();</span>
<span class="line-removed">2779   GVars.stw_cycle++;</span>
<span class="line-removed">2780 }</span>
<span class="line-removed">2781 </span>
<span class="line-removed">2782 void ObjectSynchronizer::deflate_thread_local_monitors(Thread* thread, DeflateMonitorCounters* counters) {</span>
<span class="line-removed">2783   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);</span>
<span class="line-removed">2784 </span>
<span class="line-removed">2785   if (AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">2786     // Nothing to do when per-thread idle ObjectMonitors are deflated</span>
<span class="line-removed">2787     // using a JavaThread.</span>
<span class="line-removed">2788     return;</span>
<span class="line-removed">2789   }</span>
<span class="line-removed">2790 </span>
<span class="line-removed">2791   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged monitors</span>
<span class="line-removed">2792   ObjectMonitor* free_tail_p = NULL;</span>
<span class="line-removed">2793   elapsedTimer timer;</span>
<span class="line-removed">2794 </span>
<span class="line-removed">2795   if (log_is_enabled(Info, safepoint, cleanup) ||</span>
<span class="line-removed">2796       log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-removed">2797     timer.start();</span>
<span class="line-removed">2798   }</span>
<span class="line-removed">2799 </span>
<span class="line-removed">2800   // Update n_in_circulation before om_in_use_count is updated by deflation.</span>
<span class="line-removed">2801   Atomic::add(&amp;counters-&gt;n_in_circulation, Atomic::load(&amp;thread-&gt;om_in_use_count));</span>
<span class="line-removed">2802 </span>
<span class="line-removed">2803   int deflated_count = deflate_monitor_list(&amp;thread-&gt;om_in_use_list, &amp;thread-&gt;om_in_use_count, &amp;free_head_p, &amp;free_tail_p);</span>
<span class="line-removed">2804   Atomic::add(&amp;counters-&gt;n_in_use, Atomic::load(&amp;thread-&gt;om_in_use_count));</span>
<span class="line-removed">2805 </span>
<span class="line-removed">2806   if (free_head_p != NULL) {</span>
<span class="line-removed">2807     // Move the deflated ObjectMonitors back to the global free list.</span>
<span class="line-removed">2808     guarantee(free_tail_p != NULL &amp;&amp; deflated_count &gt; 0, &quot;invariant&quot;);</span>
<span class="line-removed">2809 #ifdef ASSERT</span>
<span class="line-removed">2810     ObjectMonitor* l_next_om = free_tail_p-&gt;next_om();</span>
<span class="line-removed">2811 #endif</span>
<span class="line-removed">2812     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
<span class="line-removed">2813     prepend_list_to_global_free_list(free_head_p, free_tail_p, deflated_count);</span>
<span class="line-removed">2814     Atomic::add(&amp;counters-&gt;n_scavenged, deflated_count);</span>
<span class="line-removed">2815     Atomic::add(&amp;counters-&gt;per_thread_scavenged, deflated_count);</span>
<span class="line-removed">2816   }</span>
<span class="line-removed">2817 </span>
<span class="line-removed">2818   timer.stop();</span>
<span class="line-removed">2819   counters-&gt;per_thread_times += timer.seconds();</span>
<span class="line-removed">2820 </span>
<span class="line-removed">2821   LogStreamHandle(Debug, monitorinflation) lsh_debug;</span>
<span class="line-removed">2822   LogStreamHandle(Info, monitorinflation) lsh_info;</span>
<span class="line-removed">2823   LogStream* ls = NULL;</span>
<span class="line-removed">2824   if (log_is_enabled(Debug, monitorinflation)) {</span>
<span class="line-removed">2825     ls = &amp;lsh_debug;</span>
<span class="line-removed">2826   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {</span>
<span class="line-removed">2827     ls = &amp;lsh_info;</span>
<span class="line-removed">2828   }</span>
<span class="line-removed">2829   if (ls != NULL) {</span>
<span class="line-removed">2830     ls-&gt;print_cr(&quot;jt=&quot; INTPTR_FORMAT &quot;: deflating per-thread idle monitors, %3.7f secs, %d monitors&quot;, p2i(thread), timer.seconds(), deflated_count);</span>
<span class="line-removed">2831   }</span>
<span class="line-removed">2832 }</span>
<span class="line-removed">2833 </span>
2834 // Monitor cleanup on JavaThread::exit
2835 
2836 // Iterate through monitor cache and attempt to release thread&#39;s monitors
2837 // Gives up on a particular monitor if an exception occurs, but continues
2838 // the overall iteration, swallowing the exception.
2839 class ReleaseJavaMonitorsClosure: public MonitorClosure {
2840  private:
2841   TRAPS;
2842 
2843  public:
2844   ReleaseJavaMonitorsClosure(Thread* thread) : THREAD(thread) {}
2845   void do_monitor(ObjectMonitor* mid) {
2846     if (mid-&gt;owner() == THREAD) {
2847       (void)mid-&gt;complete_exit(CHECK);
2848     }
2849   }
2850 };
2851 
2852 // Release all inflated monitors owned by THREAD.  Lightweight monitors are
2853 // ignored.  This is meant to be called during JNI thread detach which assumes
2854 // all remaining monitors are heavyweight.  All exceptions are swallowed.
2855 // Scanning the extant monitor list can be time consuming.
2856 // A simple optimization is to add a per-thread flag that indicates a thread
2857 // called jni_monitorenter() during its lifetime.
2858 //
<span class="line-modified">2859 // Instead of No_Savepoint_Verifier it might be cheaper to</span>
2860 // use an idiom of the form:
2861 //   auto int tmp = SafepointSynchronize::_safepoint_counter ;
2862 //   &lt;code that must not run at safepoint&gt;
2863 //   guarantee (((tmp ^ _safepoint_counter) | (tmp &amp; 1)) == 0) ;
2864 // Since the tests are extremely cheap we could leave them enabled
2865 // for normal product builds.
2866 
2867 void ObjectSynchronizer::release_monitors_owned_by_thread(TRAPS) {
2868   assert(THREAD == JavaThread::current(), &quot;must be current Java thread&quot;);
2869   NoSafepointVerifier nsv;
2870   ReleaseJavaMonitorsClosure rjmc(THREAD);
2871   ObjectSynchronizer::monitors_iterate(&amp;rjmc);
2872   THREAD-&gt;clear_pending_exception();
2873 }
2874 
2875 const char* ObjectSynchronizer::inflate_cause_name(const InflateCause cause) {
2876   switch (cause) {
2877     case inflate_cause_vm_internal:    return &quot;VM Internal&quot;;
2878     case inflate_cause_monitor_enter:  return &quot;Monitor Enter&quot;;
2879     case inflate_cause_wait:           return &quot;Monitor Wait&quot;;
</pre>
<hr />
<pre>
2898   return (u_char*)&amp;GVars.hc_sequence;
2899 }
2900 
2901 size_t ObjectSynchronizer::get_gvars_size() {
2902   return sizeof(SharedGlobals);
2903 }
2904 
2905 u_char* ObjectSynchronizer::get_gvars_stw_random_addr() {
2906   return (u_char*)&amp;GVars.stw_random;
2907 }
2908 
2909 // This function can be called at a safepoint or it can be called when
2910 // we are trying to exit the VM. When we are trying to exit the VM, the
2911 // list walker functions can run in parallel with the other list
2912 // operations so spin-locking is used for safety.
2913 //
2914 // Calls to this function can be added in various places as a debugging
2915 // aid; pass &#39;true&#39; for the &#39;on_exit&#39; parameter to have in-use monitor
2916 // details logged at the Info level and &#39;false&#39; for the &#39;on_exit&#39;
2917 // parameter to have in-use monitor details logged at the Trace level.
<span class="line-removed">2918 // deflate_monitor_list() no longer uses spin-locking so be careful</span>
<span class="line-removed">2919 // when adding audit_and_print_stats() calls at a safepoint.</span>
2920 //
2921 void ObjectSynchronizer::audit_and_print_stats(bool on_exit) {
2922   assert(on_exit || SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
2923 
2924   LogStreamHandle(Debug, monitorinflation) lsh_debug;
2925   LogStreamHandle(Info, monitorinflation) lsh_info;
2926   LogStreamHandle(Trace, monitorinflation) lsh_trace;
2927   LogStream* ls = NULL;
2928   if (log_is_enabled(Trace, monitorinflation)) {
2929     ls = &amp;lsh_trace;
2930   } else if (log_is_enabled(Debug, monitorinflation)) {
2931     ls = &amp;lsh_debug;
2932   } else if (log_is_enabled(Info, monitorinflation)) {
2933     ls = &amp;lsh_info;
2934   }
2935   assert(ls != NULL, &quot;sanity check&quot;);
2936 
2937   // Log counts for the global and per-thread monitor lists:
2938   int chk_om_population = log_monitor_list_counts(ls);
2939   int error_cnt = 0;
</pre>
<hr />
<pre>
2997 void ObjectSynchronizer::chk_free_entry(JavaThread* jt, ObjectMonitor* n,
2998                                         outputStream * out, int *error_cnt_p) {
2999   stringStream ss;
3000   if (n-&gt;is_busy()) {
3001     if (jt != NULL) {
3002       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
3003                     &quot;: free per-thread monitor must not be busy: %s&quot;, p2i(jt),
3004                     p2i(n), n-&gt;is_busy_to_string(&amp;ss));
3005     } else {
3006       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
3007                     &quot;must not be busy: %s&quot;, p2i(n), n-&gt;is_busy_to_string(&amp;ss));
3008     }
3009     *error_cnt_p = *error_cnt_p + 1;
3010   }
3011   if (n-&gt;header().value() != 0) {
3012     if (jt != NULL) {
3013       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
3014                     &quot;: free per-thread monitor must have NULL _header &quot;
3015                     &quot;field: _header=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
3016                     n-&gt;header().value());
<span class="line-removed">3017       *error_cnt_p = *error_cnt_p + 1;</span>
<span class="line-removed">3018     } else if (!AsyncDeflateIdleMonitors) {</span>
<span class="line-removed">3019       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;</span>
<span class="line-removed">3020                     &quot;must have NULL _header field: _header=&quot; INTPTR_FORMAT,</span>
<span class="line-removed">3021                     p2i(n), n-&gt;header().value());</span>
3022       *error_cnt_p = *error_cnt_p + 1;
3023     }
3024   }
3025   if (n-&gt;object() != NULL) {
3026     if (jt != NULL) {
3027       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
3028                     &quot;: free per-thread monitor must have NULL _object &quot;
3029                     &quot;field: _object=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
3030                     p2i(n-&gt;object()));
3031     } else {
3032       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
3033                     &quot;must have NULL _object field: _object=&quot; INTPTR_FORMAT,
3034                     p2i(n), p2i(n-&gt;object()));
3035     }
3036     *error_cnt_p = *error_cnt_p + 1;
3037   }
3038 }
3039 
3040 // Lock the next ObjectMonitor for traversal and unlock the current
3041 // ObjectMonitor. Returns the next ObjectMonitor if there is one.
</pre>
</td>
<td>
<hr />
<pre>
 488 
 489 
 490 // The LockNode emitted directly at the synchronization site would have
 491 // been too big if it were to have included support for the cases of inflated
 492 // recursive enter and exit, so they go here instead.
 493 // Note that we can&#39;t safely call AsyncPrintJavaStack() from within
 494 // quick_enter() as our thread state remains _in_Java.
 495 
 496 bool ObjectSynchronizer::quick_enter(oop obj, Thread* self,
 497                                      BasicLock * lock) {
 498   assert(!SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
 499   assert(self-&gt;is_Java_thread(), &quot;invariant&quot;);
 500   assert(((JavaThread *) self)-&gt;thread_state() == _thread_in_Java, &quot;invariant&quot;);
 501   NoSafepointVerifier nsv;
 502   if (obj == NULL) return false;       // Need to throw NPE
 503   assert(!EnableValhalla || !obj-&gt;klass()-&gt;is_inline_klass(), &quot;monitor op on inline type&quot;);
 504   const markWord mark = obj-&gt;mark();
 505 
 506   if (mark.has_monitor()) {
 507     ObjectMonitor* const m = mark.monitor();
<span class="line-modified"> 508     // An async deflation can race us before we manage to make the</span>
<span class="line-modified"> 509     // ObjectMonitor busy by setting the owner below. If we detect</span>
<span class="line-modified"> 510     // that race we just bail out to the slow-path here.</span>
<span class="line-modified"> 511     if (m-&gt;object() == NULL) {</span>
<span class="line-modified"> 512       return false;</span>




 513     }
 514     Thread* const owner = (Thread *) m-&gt;_owner;
 515 
 516     // Lock contention and Transactional Lock Elision (TLE) diagnostics
 517     // and observability
 518     // Case: light contention possibly amenable to TLE
 519     // Case: TLE inimical operations such as nested/recursive synchronization
 520 
 521     if (owner == self) {
 522       m-&gt;_recursions++;
 523       return true;
 524     }
 525 
 526     // This Java Monitor is inflated so obj&#39;s header will never be
 527     // displaced to this thread&#39;s BasicLock. Make the displaced header
 528     // non-NULL so this BasicLock is not seen as recursive nor as
 529     // being locked. We do this unconditionally so that this thread&#39;s
 530     // BasicLock cannot be mis-interpreted by any stack walkers. For
 531     // performance reasons, stack walkers generally first check for
 532     // Biased Locking in the object&#39;s header, the second check is for
</pre>
<hr />
<pre>
 996     self-&gt;_hashStateZ = self-&gt;_hashStateW;
 997     unsigned v = self-&gt;_hashStateW;
 998     v = (v ^ (v &gt;&gt; 19)) ^ (t ^ (t &gt;&gt; 8));
 999     self-&gt;_hashStateW = v;
1000     value = v;
1001   }
1002 
1003   value &amp;= markWord::hash_mask;
1004   if (value == 0) value = 0xBAD;
1005   assert(value != markWord::no_hash, &quot;invariant&quot;);
1006   return value;
1007 }
1008 
1009 intptr_t ObjectSynchronizer::FastHashCode(Thread* self, oop obj) {
1010   if (EnableValhalla &amp;&amp; obj-&gt;klass()-&gt;is_inline_klass()) {
1011     // VM should be calling bootstrap method
1012     ShouldNotReachHere();
1013   }
1014   if (UseBiasedLocking) {
1015     // NOTE: many places throughout the JVM do not expect a safepoint
<span class="line-modified">1016     // to be taken here. However, we only ever bias Java instances and all</span>
<span class="line-modified">1017     // of the call sites of identity_hash that might revoke biases have</span>

1018     // been checked to make sure they can handle a safepoint. The
1019     // added check of the bias pattern is to avoid useless calls to
1020     // thread-local storage.
1021     if (obj-&gt;mark().has_bias_pattern()) {
1022       // Handle for oop obj in case of STW safepoint
1023       Handle hobj(self, obj);
1024       // Relaxing assertion for bug 6320749.
1025       assert(Universe::verify_in_progress() ||
1026              !SafepointSynchronize::is_at_safepoint(),
1027              &quot;biases should not be seen by VM thread here&quot;);
1028       BiasedLocking::revoke(hobj, JavaThread::current());
1029       obj = hobj();
1030       assert(!obj-&gt;mark().has_bias_pattern(), &quot;biases should be revoked by now&quot;);
1031     }
1032   }
1033 
1034   // hashCode() is a heap mutator ...
1035   // Relaxing assertion for bug 6320749.
1036   assert(Universe::verify_in_progress() || DumpSharedSpaces ||
1037          !SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
</pre>
<hr />
<pre>
1199   // Possible mark states: neutral, biased, stack-locked, inflated
1200 
1201   if (UseBiasedLocking &amp;&amp; h_obj()-&gt;mark().has_bias_pattern()) {
1202     // CASE: biased
1203     BiasedLocking::revoke(h_obj, self);
1204     assert(!h_obj-&gt;mark().has_bias_pattern(),
1205            &quot;biases should be revoked by now&quot;);
1206   }
1207 
1208   assert(self == JavaThread::current(), &quot;Can only be called on current thread&quot;);
1209   oop obj = h_obj();
1210   markWord mark = read_stable_mark(obj);
1211 
1212   // CASE: stack-locked.  Mark points to a BasicLock on the owner&#39;s stack.
1213   if (mark.has_locker()) {
1214     return self-&gt;is_lock_owned((address)mark.locker()) ?
1215       owner_self : owner_other;
1216   }
1217 
1218   // CASE: inflated. Mark (tagged pointer) points to an ObjectMonitor.


1219   if (mark.has_monitor()) {
1220     // The first stage of async deflation does not affect any field
1221     // used by this comparison so the ObjectMonitor* is usable here.
1222     ObjectMonitor* monitor = mark.monitor();
1223     void* owner = monitor-&gt;owner();
1224     if (owner == NULL) return owner_none;
1225     return (owner == self ||
1226             self-&gt;is_lock_owned((address)owner)) ? owner_self : owner_other;
1227   }
1228 
1229   // CASE: neutral
1230   assert(mark.is_neutral(), &quot;sanity check&quot;);
1231   return owner_none;           // it&#39;s unlocked
1232 }
1233 
1234 // FIXME: jvmti should call this
1235 JavaThread* ObjectSynchronizer::get_lock_owner(ThreadsList * t_list, Handle h_obj) {
1236   if (UseBiasedLocking) {
1237     if (SafepointSynchronize::is_at_safepoint()) {
1238       BiasedLocking::revoke_at_safepoint(h_obj);
</pre>
<hr />
<pre>
1298     // used with block linkage _next_om fields).
1299     block = (PaddedObjectMonitor*)block-&gt;next_om();
1300   }
1301 }
1302 
1303 static bool monitors_used_above_threshold() {
1304   int population = Atomic::load(&amp;om_list_globals._population);
1305   if (population == 0) {
1306     return false;
1307   }
1308   if (MonitorUsedDeflationThreshold &gt; 0) {
1309     int monitors_used = population - Atomic::load(&amp;om_list_globals._free_count) -
1310                         Atomic::load(&amp;om_list_globals._wait_count);
1311     int monitor_usage = (monitors_used * 100LL) / population;
1312     return monitor_usage &gt; MonitorUsedDeflationThreshold;
1313   }
1314   return false;
1315 }
1316 
1317 bool ObjectSynchronizer::is_async_deflation_needed() {



1318   if (is_async_deflation_requested()) {
1319     // Async deflation request.
1320     return true;
1321   }
1322   if (AsyncDeflationInterval &gt; 0 &amp;&amp;
1323       time_since_last_async_deflation_ms() &gt; AsyncDeflationInterval &amp;&amp;
1324       monitors_used_above_threshold()) {
1325     // It&#39;s been longer than our specified deflate interval and there
1326     // are too many monitors in use. We don&#39;t deflate more frequently
1327     // than AsyncDeflationInterval (unless is_async_deflation_requested)
1328     // in order to not swamp the ServiceThread.
1329     return true;
1330   }
1331   return false;
1332 }
1333 





1334 bool ObjectSynchronizer::request_deflate_idle_monitors() {
1335   bool is_JavaThread = Thread::current()-&gt;is_Java_thread();
1336   bool ret_code = false;
1337 
<span class="line-modified">1338   jlong last_time = last_async_deflation_time_ns();</span>
<span class="line-modified">1339   set_is_async_deflation_requested(true);</span>
<span class="line-modified">1340   {</span>
<span class="line-modified">1341     MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);</span>
<span class="line-modified">1342     ml.notify_all();</span>
<span class="line-modified">1343   }</span>
<span class="line-modified">1344   const int N_CHECKS = 5;</span>
<span class="line-modified">1345   for (int i = 0; i &lt; N_CHECKS; i++) {  // sleep for at most 5 seconds</span>
<span class="line-modified">1346     if (last_async_deflation_time_ns() &gt; last_time) {</span>
<span class="line-modified">1347       log_info(monitorinflation)(&quot;Async Deflation happened after %d check(s).&quot;, i);</span>
<span class="line-modified">1348       ret_code = true;</span>
<span class="line-modified">1349       break;</span>









1350     }
<span class="line-modified">1351     if (is_JavaThread) {</span>
<span class="line-modified">1352       // JavaThread has to honor the blocking protocol.</span>
<span class="line-added">1353       ThreadBlockInVM tbivm(JavaThread::current());</span>
<span class="line-added">1354       os::naked_short_sleep(999);  // sleep for almost 1 second</span>
<span class="line-added">1355     } else {</span>
<span class="line-added">1356       os::naked_short_sleep(999);  // sleep for almost 1 second</span>
1357     }
<span class="line-modified">1358   }</span>
<span class="line-modified">1359   if (!ret_code) {</span>
<span class="line-modified">1360     log_info(monitorinflation)(&quot;Async Deflation DID NOT happen after %d checks.&quot;, N_CHECKS);</span>





1361   }
1362 
1363   return ret_code;
1364 }
1365 
1366 jlong ObjectSynchronizer::time_since_last_async_deflation_ms() {
1367   return (os::javaTimeNanos() - last_async_deflation_time_ns()) / (NANOUNITS / MILLIUNITS);
1368 }
1369 
1370 void ObjectSynchronizer::oops_do(OopClosure* f) {
1371   // We only scan the global used list here (for moribund threads), and
1372   // the thread-local monitors in Thread::oops_do().
1373   global_used_oops_do(f);
1374 }
1375 
1376 void ObjectSynchronizer::global_used_oops_do(OopClosure* f) {
1377   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1378   list_oops_do(Atomic::load(&amp;om_list_globals._in_use_list), f);
1379 }
1380 
</pre>
<hr />
<pre>
1382   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1383   list_oops_do(thread-&gt;om_in_use_list, f);
1384 }
1385 
1386 void ObjectSynchronizer::list_oops_do(ObjectMonitor* list, OopClosure* f) {
1387   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
1388   // The oops_do() phase does not overlap with monitor deflation
1389   // so no need to lock ObjectMonitors for the list traversal.
1390   for (ObjectMonitor* mid = list; mid != NULL; mid = unmarked_next(mid)) {
1391     if (mid-&gt;object() != NULL) {
1392       f-&gt;do_oop((oop*)mid-&gt;object_addr());
1393     }
1394   }
1395 }
1396 
1397 
1398 // -----------------------------------------------------------------------------
1399 // ObjectMonitor Lifecycle
1400 // -----------------------
1401 // Inflation unlinks monitors from om_list_globals._free_list or a per-thread
<span class="line-modified">1402 // free list and associates them with objects. Async deflation disassociates</span>
<span class="line-modified">1403 // idle monitors from objects. Such scavenged monitors are returned to the</span>
<span class="line-modified">1404 // om_list_globals._free_list.</span>
1405 //
1406 // ObjectMonitors reside in type-stable memory (TSM) and are immortal.
1407 //
1408 // Lifecycle:
1409 // --   unassigned and on the om_list_globals._free_list
1410 // --   unassigned and on a per-thread free list
1411 // --   assigned to an object.  The object is inflated and the mark refers
1412 //      to the ObjectMonitor.
1413 
1414 ObjectMonitor* ObjectSynchronizer::om_alloc(Thread* self) {
1415   // A large MAXPRIVATE value reduces both list lock contention
1416   // and list coherency traffic, but also tends to increase the
<span class="line-modified">1417   // number of ObjectMonitors in circulation as well as the</span>
1418   // scavenge costs.  As usual, we lean toward time in space-time
1419   // tradeoffs.
1420   const int MAXPRIVATE = 1024;
1421   NoSafepointVerifier nsv;
1422 
1423   for (;;) {
1424     ObjectMonitor* m;
1425 
1426     // 1: try to allocate from the thread&#39;s local om_free_list.
1427     // Threads will attempt to allocate first from their local list, then
1428     // from the global list, and only after those attempts fail will the
1429     // thread attempt to instantiate new monitors. Thread-local free lists
1430     // improve allocation latency, as well as reducing coherency traffic
1431     // on the shared global list.
1432     m = take_from_start_of_om_free_list(self);
1433     if (m != NULL) {
1434       guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1435       m-&gt;set_allocation_state(ObjectMonitor::New);
1436       prepend_to_om_in_use_list(self, m);
1437       return m;
1438     }
1439 
1440     // 2: try to allocate from the global om_list_globals._free_list
1441     // If we&#39;re using thread-local free lists then try
1442     // to reprovision the caller&#39;s free list.
1443     if (Atomic::load(&amp;om_list_globals._free_list) != NULL) {
1444       // Reprovision the thread&#39;s om_free_list.
1445       // Use bulk transfers to reduce the allocation rate and heat
1446       // on various locks.
1447       for (int i = self-&gt;om_free_provision; --i &gt;= 0;) {
1448         ObjectMonitor* take = take_from_start_of_global_free_list();
1449         if (take == NULL) {
1450           break;  // No more are available.
1451         }
1452         guarantee(take-&gt;object() == NULL, &quot;invariant&quot;);
<span class="line-modified">1453         // We allowed 3 field values to linger during async deflation.</span>
<span class="line-modified">1454         // Clear or restore them as appropriate.</span>
<span class="line-modified">1455         take-&gt;set_header(markWord::zero());</span>
<span class="line-modified">1456         // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-modified">1457         take-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>
<span class="line-modified">1458         if (take-&gt;contentions() &lt; 0) {</span>
<span class="line-modified">1459           // Add back max_jint to restore the contentions field to its</span>
<span class="line-modified">1460           // proper value.</span>
<span class="line-modified">1461           take-&gt;add_to_contentions(max_jint);</span>

1462 
1463 #ifdef ASSERT
<span class="line-modified">1464           jint l_contentions = take-&gt;contentions();</span>
<span class="line-added">1465           assert(l_contentions &gt;= 0, &quot;must not be negative: l_contentions=%d, contentions=%d&quot;,</span>
<span class="line-added">1466                  l_contentions, take-&gt;contentions());</span>
1467 #endif



1468         }
1469         take-&gt;Recycle();
1470         // Since we&#39;re taking from the global free-list, take must be Free.
1471         // om_release() also sets the allocation state to Free because it
1472         // is called from other code paths.
1473         assert(take-&gt;is_free(), &quot;invariant&quot;);
1474         om_release(self, take, false);
1475       }
1476       self-&gt;om_free_provision += 1 + (self-&gt;om_free_provision / 2);
1477       if (self-&gt;om_free_provision &gt; MAXPRIVATE) self-&gt;om_free_provision = MAXPRIVATE;
1478       continue;
1479     }
1480 
1481     // 3: allocate a block of new ObjectMonitors
1482     // Both the local and global free lists are empty -- resort to malloc().
1483     // In the current implementation ObjectMonitors are TSM - immortal.
1484     // Ideally, we&#39;d write &quot;new ObjectMonitor[_BLOCKSIZE], but we want
1485     // each ObjectMonitor to start at the beginning of a cache line,
1486     // so we use align_up().
1487     // A better solution would be to use C++ placement-new.
</pre>
<hr />
<pre>
1513     // Element [0] is reserved for global list linkage
1514     temp[0].set_object(CHAINMARKER);
1515 
1516     // Consider carving out this thread&#39;s current request from the
1517     // block in hand.  This avoids some lock traffic and redundant
1518     // list activity.
1519 
1520     prepend_block_to_lists(temp);
1521   }
1522 }
1523 
1524 // Place &quot;m&quot; on the caller&#39;s private per-thread om_free_list.
1525 // In practice there&#39;s no need to clamp or limit the number of
1526 // monitors on a thread&#39;s om_free_list as the only non-allocation time
1527 // we&#39;ll call om_release() is to return a monitor to the free list after
1528 // a CAS attempt failed. This doesn&#39;t allow unbounded #s of monitors to
1529 // accumulate on a thread&#39;s free list.
1530 //
1531 // Key constraint: all ObjectMonitors on a thread&#39;s free list and the global
1532 // free list must have their object field set to null. This prevents the
<span class="line-modified">1533 // scavenger -- deflate_monitor_list_using_JT() -- from reclaiming them</span>
<span class="line-modified">1534 // while we are trying to release them.</span>
1535 
1536 void ObjectSynchronizer::om_release(Thread* self, ObjectMonitor* m,
1537                                     bool from_per_thread_alloc) {
1538   guarantee(m-&gt;header().value() == 0, &quot;invariant&quot;);
1539   guarantee(m-&gt;object() == NULL, &quot;invariant&quot;);
1540   NoSafepointVerifier nsv;
1541 
1542   if ((m-&gt;is_busy() | m-&gt;_recursions) != 0) {
1543     stringStream ss;
1544     fatal(&quot;freeing in-use monitor: %s, recursions=&quot; INTX_FORMAT,
1545           m-&gt;is_busy_to_string(&amp;ss), m-&gt;_recursions);
1546   }
1547   m-&gt;set_allocation_state(ObjectMonitor::Free);
1548   // _next_om is used for both per-thread in-use and free lists so
1549   // we have to remove &#39;m&#39; from the in-use list first (as needed).
1550   if (from_per_thread_alloc) {
1551     // Need to remove &#39;m&#39; from om_in_use_list.
1552     ObjectMonitor* mid = NULL;
1553     ObjectMonitor* next = NULL;
1554 
</pre>
<hr />
<pre>
1623 
1624     // At this point mid is disconnected from the in-use list so
1625     // its lock no longer has any effects on the in-use list.
1626     Atomic::dec(&amp;self-&gt;om_in_use_count);
1627     // Unlock mid, but leave the next value for any lagging list
1628     // walkers. It will get cleaned up when mid is prepended to
1629     // the thread&#39;s free list:
1630     om_unlock(mid);
1631   }
1632 
1633   prepend_to_om_free_list(self, m);
1634   guarantee(m-&gt;is_free(), &quot;invariant&quot;);
1635 }
1636 
1637 // Return ObjectMonitors on a moribund thread&#39;s free and in-use
1638 // lists to the appropriate global lists. The ObjectMonitors on the
1639 // per-thread in-use list may still be in use by other threads.
1640 //
1641 // We currently call om_flush() from Threads::remove() before the
1642 // thread has been excised from the thread list and is no longer a
<span class="line-modified">1643 // mutator. In particular, this ensures that the thread&#39;s in-use</span>
<span class="line-modified">1644 // monitors are scanned by a GC safepoint, either via Thread::oops_do()</span>
<span class="line-modified">1645 // (before om_flush() is called) or via ObjectSynchronizer::oops_do()</span>
<span class="line-modified">1646 // (after om_flush() is called).</span>


1647 //
<span class="line-modified">1648 // deflate_global_idle_monitors_using_JT() and</span>
<span class="line-modified">1649 // deflate_per_thread_idle_monitors_using_JT() (in another thread) can</span>
1650 // run at the same time as om_flush() so we have to follow a careful
1651 // protocol to prevent list corruption.
1652 
1653 void ObjectSynchronizer::om_flush(Thread* self) {
1654   // Process the per-thread in-use list first to be consistent.
1655   int in_use_count = 0;
1656   ObjectMonitor* in_use_list = NULL;
1657   ObjectMonitor* in_use_tail = NULL;
1658   NoSafepointVerifier nsv;
1659 
1660   // This function can race with a list walker or with an async
1661   // deflater thread so we lock the list head to prevent confusion.
1662   // An async deflater thread checks to see if the target thread
1663   // is exiting, but if it has made it past that check before we
1664   // started exiting, then it is racing to get to the in-use list.
1665   if ((in_use_list = get_list_head_locked(&amp;self-&gt;om_in_use_list)) != NULL) {
1666     // At this point, we have locked the in-use list head so a racing
1667     // thread cannot come in after us. However, a racing thread could
1668     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1669     //
</pre>
<hr />
<pre>
1683         while (is_locked(cur_om)) {
1684           os::naked_short_sleep(1);
1685         }
1686         // Refetch the possibly changed next field and try again.
1687         cur_om = unmarked_next(in_use_tail);
1688         continue;
1689       }
1690       if (cur_om-&gt;object() == NULL) {
1691         // cur_om was deflated and the object ref was cleared while it
1692         // was locked. We happened to see it just after it was unlocked
1693         // (and added to the free list). Refetch the possibly changed
1694         // next field and try again.
1695         cur_om = unmarked_next(in_use_tail);
1696         continue;
1697       }
1698       in_use_tail = cur_om;
1699       in_use_count++;
1700       cur_om = unmarked_next(cur_om);
1701     }
1702     guarantee(in_use_tail != NULL, &quot;invariant&quot;);
<span class="line-added">1703 #ifdef ASSERT</span>
1704     int l_om_in_use_count = Atomic::load(&amp;self-&gt;om_in_use_count);
<span class="line-modified">1705     assert(l_om_in_use_count == in_use_count, &quot;in-use counts don&#39;t match: &quot;</span>
<span class="line-modified">1706            &quot;l_om_in_use_count=%d, in_use_count=%d&quot;, l_om_in_use_count, in_use_count);</span>
<span class="line-added">1707 #endif</span>
1708     Atomic::store(&amp;self-&gt;om_in_use_count, 0);
1709     // Clear the in-use list head (which also unlocks it):
1710     Atomic::store(&amp;self-&gt;om_in_use_list, (ObjectMonitor*)NULL);
1711     om_unlock(in_use_list);
1712   }
1713 
1714   int free_count = 0;
1715   ObjectMonitor* free_list = NULL;
1716   ObjectMonitor* free_tail = NULL;
1717   // This function can race with a list walker thread so we lock the
1718   // list head to prevent confusion.
1719   if ((free_list = get_list_head_locked(&amp;self-&gt;om_free_list)) != NULL) {
1720     // At this point, we have locked the free list head so a racing
1721     // thread cannot come in after us. However, a racing thread could
1722     // be ahead of us; we&#39;ll detect that and delay to let it finish.
1723     //
1724     // The thread is going away. Set &#39;free_tail&#39; to the last per-thread free
1725     // monitor which will be linked to om_list_globals._free_list below.
1726     //
1727     // Account for the free list head before the loop since it is
1728     // already locked (by this thread):
1729     free_tail = free_list;
1730     free_count++;
1731     for (ObjectMonitor* s = unmarked_next(free_list); s != NULL; s = unmarked_next(s)) {
1732       if (is_locked(s)) {
1733         // s is locked so there must be a racing walker thread ahead
1734         // of us so we&#39;ll give it a chance to finish.
1735         while (is_locked(s)) {
1736           os::naked_short_sleep(1);
1737         }
1738       }
1739       free_tail = s;
1740       free_count++;
1741       guarantee(s-&gt;object() == NULL, &quot;invariant&quot;);
1742       if (s-&gt;is_busy()) {
1743         stringStream ss;
1744         fatal(&quot;must be !is_busy: %s&quot;, s-&gt;is_busy_to_string(&amp;ss));
1745       }
1746     }
1747     guarantee(free_tail != NULL, &quot;invariant&quot;);
<span class="line-added">1748 #ifdef ASSERT</span>
1749     int l_om_free_count = Atomic::load(&amp;self-&gt;om_free_count);
<span class="line-modified">1750     assert(l_om_free_count == free_count, &quot;free counts don&#39;t match: &quot;</span>
<span class="line-modified">1751            &quot;l_om_free_count=%d, free_count=%d&quot;, l_om_free_count, free_count);</span>
<span class="line-added">1752 #endif</span>
1753     Atomic::store(&amp;self-&gt;om_free_count, 0);
1754     Atomic::store(&amp;self-&gt;om_free_list, (ObjectMonitor*)NULL);
1755     om_unlock(free_list);
1756   }
1757 
1758   if (free_tail != NULL) {
1759     prepend_list_to_global_free_list(free_list, free_tail, free_count);
1760   }
1761 
1762   if (in_use_tail != NULL) {
1763     prepend_list_to_global_in_use_list(in_use_list, in_use_tail, in_use_count);
1764   }
1765 
1766   LogStreamHandle(Debug, monitorinflation) lsh_debug;
1767   LogStreamHandle(Info, monitorinflation) lsh_info;
1768   LogStream* ls = NULL;
1769   if (log_is_enabled(Debug, monitorinflation)) {
1770     ls = &amp;lsh_debug;
1771   } else if ((free_count != 0 || in_use_count != 0) &amp;&amp;
1772              log_is_enabled(Info, monitorinflation)) {
</pre>
<hr />
<pre>
1815   }
1816 
1817   EventJavaMonitorInflate event;
1818 
1819   for (;;) {
1820     const markWord mark = object-&gt;mark();
1821     assert(!mark.has_bias_pattern(), &quot;invariant&quot;);
1822 
1823     // The mark can be in one of the following states:
1824     // *  Inflated     - just return
1825     // *  Stack-locked - coerce it to inflated
1826     // *  INFLATING    - busy wait for conversion to complete
1827     // *  Neutral      - aggressively inflate the object.
1828     // *  BIASED       - Illegal.  We should never see this
1829 
1830     // CASE: inflated
1831     if (mark.has_monitor()) {
1832       ObjectMonitor* inf = mark.monitor();
1833       markWord dmw = inf-&gt;header();
1834       assert(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());

1835       assert(ObjectSynchronizer::verify_objmon_isinpool(inf), &quot;monitor is invalid&quot;);
1836       return inf;
1837     }
1838 
1839     // CASE: inflation in progress - inflating over a stack-lock.
1840     // Some other thread is converting from stack-locked to inflated.
1841     // Only that thread can complete inflation -- other threads must wait.
1842     // The INFLATING value is transient.
1843     // Currently, we spin/yield/park and poll the markword, waiting for inflation to finish.
1844     // We could always eliminate polling by parking the thread on some auxiliary list.
1845     if (mark == markWord::INFLATING()) {
1846       read_stable_mark(object);
1847       continue;
1848     }
1849 
1850     // CASE: stack-locked
1851     // Could be stack-locked either by this thread or by some other thread.
1852     //
1853     // Note that we allocate the objectmonitor speculatively, _before_ attempting
1854     // to install INFLATING into the mark word.  We originally installed INFLATING,
</pre>
<hr />
<pre>
1900       // value from the BasicLock on the owner&#39;s stack to the ObjectMonitor, all
1901       // the while preserving the hashCode stability invariants.  If the owner
1902       // decides to release the lock while the value is 0, the unlock will fail
1903       // and control will eventually pass from slow_exit() to inflate.  The owner
1904       // will then spin, waiting for the 0 value to disappear.   Put another way,
1905       // the 0 causes the owner to stall if the owner happens to try to
1906       // drop the lock (restoring the header from the BasicLock to the object)
1907       // while inflation is in-progress.  This protocol avoids races that might
1908       // would otherwise permit hashCode values to change or &quot;flicker&quot; for an object.
1909       // Critically, while object-&gt;mark is 0 mark.displaced_mark_helper() is stable.
1910       // 0 serves as a &quot;BUSY&quot; inflate-in-progress indicator.
1911 
1912 
1913       // fetch the displaced mark from the owner&#39;s stack.
1914       // The owner can&#39;t die or unwind past the lock while our INFLATING
1915       // object is in the mark.  Furthermore the owner can&#39;t complete
1916       // an unlock on the object, either.
1917       markWord dmw = mark.displaced_mark_helper();
1918       // Catch if the object&#39;s header is not neutral (not locked and
1919       // not marked is what we care about here).
<span class="line-modified">1920       assert(dmw.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, dmw.value());</span>
1921 
1922       // Setup monitor fields to proper values -- prepare the monitor
1923       m-&gt;set_header(dmw);
1924 
1925       // Optimization: if the mark.locker stack address is associated
1926       // with this thread we could simply set m-&gt;_owner = self.
1927       // Note that a thread can inflate an object
1928       // that it has stack-locked -- as might happen in wait() -- directly
1929       // with CAS.  That is, we can avoid the xchg-NULL .... ST idiom.
<span class="line-modified">1930       m-&gt;set_owner_from(NULL, DEFLATER_MARKER, mark.locker());</span>




1931       m-&gt;set_object(object);
1932       // TODO-FIXME: assert BasicLock-&gt;dhw != 0.
1933 
1934       // Must preserve store ordering. The monitor state must
1935       // be stable at the time of publishing the monitor address.
1936       guarantee(object-&gt;mark() == markWord::INFLATING(), &quot;invariant&quot;);
1937       object-&gt;release_set_mark(markWord::encode(m));
1938 
1939       // Once ObjectMonitor is configured and the object is associated
1940       // with the ObjectMonitor, it is safe to allow async deflation:
1941       assert(m-&gt;is_new(), &quot;freshly allocated monitor must be new&quot;);
1942       m-&gt;set_allocation_state(ObjectMonitor::Old);
1943 
1944       // Hopefully the performance counters are allocated on distinct cache lines
1945       // to avoid false sharing on MP systems ...
1946       OM_PERFDATA_OP(Inflations, inc());
1947       if (log_is_enabled(Trace, monitorinflation)) {
1948         ResourceMark rm(self);
1949         lsh.print_cr(&quot;inflate(has_locker): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
1950                      INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
1951                      object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
1952       }
1953       if (event.should_commit()) {
1954         post_monitor_inflate_event(&amp;event, object, cause);
1955       }
1956       return m;
1957     }
1958 
1959     // CASE: neutral
1960     // TODO-FIXME: for entry we currently inflate and then try to CAS _owner.
1961     // If we know we&#39;re inflating for entry it&#39;s better to inflate by swinging a
1962     // pre-locked ObjectMonitor pointer into the object header.   A successful
1963     // CAS inflates the object *and* confers ownership to the inflating thread.
1964     // In the current implementation we use a 2-step mechanism where we CAS()
1965     // to inflate and then CAS() again to try to swing _owner from NULL to self.
1966     // An inflateTry() method that we could call from enter() would be useful.
1967 
1968     // Catch if the object&#39;s header is not neutral (not locked and
1969     // not marked is what we care about here).
<span class="line-modified">1970     assert(mark.is_neutral(), &quot;invariant: header=&quot; INTPTR_FORMAT, mark.value());</span>
1971     ObjectMonitor* m = om_alloc(self);
1972     // prepare m for installation - set monitor to initial state
1973     m-&gt;Recycle();
1974     m-&gt;set_header(mark);
<span class="line-modified">1975     // DEFLATER_MARKER is the only non-NULL value we should see here.</span>
<span class="line-modified">1976     m-&gt;try_set_owner_from(DEFLATER_MARKER, NULL);</span>


1977     m-&gt;set_object(object);
1978     m-&gt;_Responsible  = NULL;
1979     m-&gt;_SpinDuration = ObjectMonitor::Knob_SpinLimit;       // consider: keep metastats by type/class
1980 
1981     if (object-&gt;cas_set_mark(markWord::encode(m), mark) != mark) {
1982       m-&gt;set_header(markWord::zero());
1983       m-&gt;set_object(NULL);
1984       m-&gt;Recycle();
1985       // om_release() will reset the allocation state from New to Free.
1986       om_release(self, m, true);
1987       m = NULL;
1988       continue;
1989       // interference - the markword changed - just retry.
1990       // The state-transitions are one-way, so there&#39;s no chance of
1991       // live-lock -- &quot;Inflated&quot; is an absorbing state.
1992     }
1993 
1994     // Once the ObjectMonitor is configured and object is associated
1995     // with the ObjectMonitor, it is safe to allow async deflation:
1996     assert(m-&gt;is_new(), &quot;freshly allocated monitor must be new&quot;);
1997     m-&gt;set_allocation_state(ObjectMonitor::Old);
1998 
1999     // Hopefully the performance counters are allocated on distinct
2000     // cache lines to avoid false sharing on MP systems ...
2001     OM_PERFDATA_OP(Inflations, inc());
2002     if (log_is_enabled(Trace, monitorinflation)) {
2003       ResourceMark rm(self);
2004       lsh.print_cr(&quot;inflate(neutral): object=&quot; INTPTR_FORMAT &quot;, mark=&quot;
2005                    INTPTR_FORMAT &quot;, type=&#39;%s&#39;&quot;, p2i(object),
2006                    object-&gt;mark().value(), object-&gt;klass()-&gt;external_name());
2007     }
2008     if (event.should_commit()) {
2009       post_monitor_inflate_event(&amp;event, object, cause);
2010     }
2011     return m;
2012   }
2013 }
2014 
2015 
<span class="line-modified">2016 // An async deflation request is registered with the ServiceThread</span>
<span class="line-modified">2017 // and it is notified.</span>
<span class="line-modified">2018 void ObjectSynchronizer::do_safepoint_work() {</span>

























2019   assert(SafepointSynchronize::is_at_safepoint(), &quot;must be at safepoint&quot;);
2020 









2021   log_debug(monitorinflation)(&quot;requesting async deflation of idle monitors.&quot;);
2022   // Request deflation of idle monitors by the ServiceThread:
2023   set_is_async_deflation_requested(true);
2024   MonitorLocker ml(Service_lock, Mutex::_no_safepoint_check_flag);
2025   ml.notify_all();
2026 
2027   if (log_is_enabled(Debug, monitorinflation)) {
2028     // exit_globals()&#39;s call to audit_and_print_stats() is done
2029     // at the Info level and not at a safepoint.



2030     ObjectSynchronizer::audit_and_print_stats(false /* on_exit */);
2031   }
2032 }
2033 








































































2034 // Deflate the specified ObjectMonitor if not in-use using a JavaThread.
2035 // Returns true if it was deflated and false otherwise.
2036 //
2037 // The async deflation protocol sets owner to DEFLATER_MARKER and
2038 // makes contentions negative as signals to contending threads that
2039 // an async deflation is in progress. There are a number of checks
2040 // as part of the protocol to make sure that the calling thread has
2041 // not lost the race to a contending thread.
2042 //
2043 // The ObjectMonitor has been successfully async deflated when:
2044 //   (contentions &lt; 0)
2045 // Contending threads that see that condition know to retry their operation.
2046 //
2047 bool ObjectSynchronizer::deflate_monitor_using_JT(ObjectMonitor* mid,
2048                                                   ObjectMonitor** free_head_p,
2049                                                   ObjectMonitor** free_tail_p) {

2050   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);
2051   // A newly allocated ObjectMonitor should not be seen here so we
2052   // avoid an endless inflate/deflate cycle.
2053   assert(mid-&gt;is_old(), &quot;must be old: allocation_state=%d&quot;,
2054          (int) mid-&gt;allocation_state());
2055 
2056   if (mid-&gt;is_busy()) {
2057     // Easy checks are first - the ObjectMonitor is busy so no deflation.
2058     return false;
2059   }
2060 
2061   // Set a NULL owner to DEFLATER_MARKER to force any contending thread
2062   // through the slow path. This is just the first part of the async
2063   // deflation dance.
2064   if (mid-&gt;try_set_owner_from(NULL, DEFLATER_MARKER) != NULL) {
2065     // The owner field is no longer NULL so we lost the race since the
2066     // ObjectMonitor is now busy.
2067     return false;
2068   }
2069 
</pre>
<hr />
<pre>
2118   mid-&gt;clear_common();
2119 
2120   assert(mid-&gt;object() == NULL, &quot;must be NULL: object=&quot; INTPTR_FORMAT,
2121          p2i(mid-&gt;object()));
2122   assert(mid-&gt;is_free(), &quot;must be free: allocation_state=%d&quot;,
2123          (int)mid-&gt;allocation_state());
2124 
2125   // Move the deflated ObjectMonitor to the working free list
2126   // defined by free_head_p and free_tail_p.
2127   if (*free_head_p == NULL) {
2128     // First one on the list.
2129     *free_head_p = mid;
2130   }
2131   if (*free_tail_p != NULL) {
2132     // We append to the list so the caller can use mid-&gt;_next_om
2133     // to fix the linkages in its context.
2134     ObjectMonitor* prevtail = *free_tail_p;
2135     // prevtail should have been cleaned up by the caller:
2136 #ifdef ASSERT
2137     ObjectMonitor* l_next_om = unmarked_next(prevtail);
<span class="line-modified">2138     assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
2139 #endif
2140     om_lock(prevtail);
2141     prevtail-&gt;set_next_om(mid);  // prevtail now points to mid (and is unlocked)
2142   }
2143   *free_tail_p = mid;
2144 
2145   // At this point, mid-&gt;_next_om still refers to its current
2146   // value and another ObjectMonitor&#39;s _next_om field still
2147   // refers to this ObjectMonitor. Those linkages have to be
2148   // cleaned up by the caller who has the complete context.
2149 
2150   // We leave owner == DEFLATER_MARKER and contentions &lt; 0
2151   // to force any racing threads to retry.
2152   return true;  // Success, ObjectMonitor has been deflated.
2153 }
2154 


















































2155 // Walk a given ObjectMonitor list and deflate idle ObjectMonitors using
2156 // a JavaThread. Returns the number of deflated ObjectMonitors. The given
2157 // list could be a per-thread in-use list or the global in-use list.
2158 // If a safepoint has started, then we save state via saved_mid_in_use_p
2159 // and return to the caller to honor the safepoint.
2160 //
2161 int ObjectSynchronizer::deflate_monitor_list_using_JT(ObjectMonitor** list_p,
2162                                                       int* count_p,
2163                                                       ObjectMonitor** free_head_p,
2164                                                       ObjectMonitor** free_tail_p,
2165                                                       ObjectMonitor** saved_mid_in_use_p) {

2166   JavaThread* self = JavaThread::current();
2167 
2168   ObjectMonitor* cur_mid_in_use = NULL;
2169   ObjectMonitor* mid = NULL;
2170   ObjectMonitor* next = NULL;
2171   ObjectMonitor* next_next = NULL;
2172   int deflated_count = 0;
2173   NoSafepointVerifier nsv;
2174 
2175   // We use the more complicated lock-cur_mid_in_use-and-mid-as-we-go
2176   // protocol because om_release() can do list deletions in parallel;
2177   // this also prevents races with a list walker thread. We also
2178   // lock-next-next-as-we-go to prevent an om_flush() that is behind
2179   // this thread from passing us.
2180   if (*saved_mid_in_use_p == NULL) {
2181     // No saved state so start at the beginning.
2182     // Lock the list head so we can possibly deflate it:
2183     if ((mid = get_list_head_locked(list_p)) == NULL) {
2184       return 0;  // The list is empty so nothing to deflate.
2185     }
</pre>
<hr />
<pre>
2275         }
2276         return deflated_count;
2277       }
2278     }
2279     if (mid == NULL) {
2280       if (cur_mid_in_use != NULL) {
2281         om_unlock(cur_mid_in_use);
2282       }
2283       break;  // Reached end of the list so nothing more to deflate.
2284     }
2285 
2286     // The current mid&#39;s next field is locked at this point. If we have
2287     // a cur_mid_in_use, then it is also locked at this point.
2288   }
2289   // We finished the list without a safepoint starting so there&#39;s
2290   // no need to save state.
2291   *saved_mid_in_use_p = NULL;
2292   return deflated_count;
2293 }
2294 





































































2295 class HandshakeForDeflation : public HandshakeClosure {
2296  public:
2297   HandshakeForDeflation() : HandshakeClosure(&quot;HandshakeForDeflation&quot;) {}
2298 
2299   void do_thread(Thread* thread) {
2300     log_trace(monitorinflation)(&quot;HandshakeForDeflation::do_thread: thread=&quot;
2301                                 INTPTR_FORMAT, p2i(thread));
2302   }
2303 };
2304 
2305 void ObjectSynchronizer::deflate_idle_monitors_using_JT() {


2306   // Deflate any global idle monitors.
2307   deflate_global_idle_monitors_using_JT();
2308 
2309   int count = 0;
2310   for (JavaThreadIteratorWithHandle jtiwh; JavaThread *jt = jtiwh.next(); ) {
2311     if (Atomic::load(&amp;jt-&gt;om_in_use_count) &gt; 0 &amp;&amp; !jt-&gt;is_exiting()) {
2312       // This JavaThread is using ObjectMonitors so deflate any that
2313       // are idle unless this JavaThread is exiting; do not race with
2314       // ObjectSynchronizer::om_flush().
2315       deflate_per_thread_idle_monitors_using_JT(jt);
2316       count++;
2317     }
2318   }
2319   if (count &gt; 0) {
2320     log_debug(monitorinflation)(&quot;did async deflation of idle monitors for %d thread(s).&quot;, count);
2321   }
2322 
2323   log_info(monitorinflation)(&quot;async global_population=%d, global_in_use_count=%d, &quot;
2324                              &quot;global_free_count=%d, global_wait_count=%d&quot;,
2325                              Atomic::load(&amp;om_list_globals._population),
2326                              Atomic::load(&amp;om_list_globals._in_use_count),
2327                              Atomic::load(&amp;om_list_globals._free_count),
2328                              Atomic::load(&amp;om_list_globals._wait_count));
2329 
2330   // The ServiceThread&#39;s async deflation request has been processed.
2331   _last_async_deflation_time_ns = os::javaTimeNanos();
2332   set_is_async_deflation_requested(false);
2333 
2334   if (Atomic::load(&amp;om_list_globals._wait_count) &gt; 0) {
2335     // There are deflated ObjectMonitors waiting for a handshake
2336     // (or a safepoint) for safety.
2337 
2338     ObjectMonitor* list = Atomic::load(&amp;om_list_globals._wait_list);
<span class="line-modified">2339     assert(list != NULL, &quot;om_list_globals._wait_list must not be NULL&quot;);</span>
2340     int count = Atomic::load(&amp;om_list_globals._wait_count);
2341     Atomic::store(&amp;om_list_globals._wait_count, 0);
2342     Atomic::store(&amp;om_list_globals._wait_list, (ObjectMonitor*)NULL);
2343 
2344     // Find the tail for prepend_list_to_common(). No need to mark
2345     // ObjectMonitors for this list walk since only the deflater
2346     // thread manages the wait list.
<span class="line-added">2347 #ifdef ASSERT</span>
2348     int l_count = 0;
<span class="line-added">2349 #endif</span>
2350     ObjectMonitor* tail = NULL;
2351     for (ObjectMonitor* n = list; n != NULL; n = unmarked_next(n)) {
2352       tail = n;
<span class="line-added">2353 #ifdef ASSERT</span>
2354       l_count++;
<span class="line-added">2355 #endif</span>
2356     }
<span class="line-modified">2357     assert(count == l_count, &quot;count=%d != l_count=%d&quot;, count, l_count);</span>
2358 
2359     // Will execute a safepoint if !ThreadLocalHandshakes:
2360     HandshakeForDeflation hfd_hc;
2361     Handshake::execute(&amp;hfd_hc);
2362 
2363     prepend_list_to_common(list, tail, count, &amp;om_list_globals._free_list,
2364                            &amp;om_list_globals._free_count);
2365 
2366     log_info(monitorinflation)(&quot;moved %d idle monitors from global waiting list to global free list&quot;, count);
2367   }
2368 }
2369 
2370 // Deflate global idle ObjectMonitors using a JavaThread.
2371 //
2372 void ObjectSynchronizer::deflate_global_idle_monitors_using_JT() {

2373   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);
2374   JavaThread* self = JavaThread::current();
2375 
2376   deflate_common_idle_monitors_using_JT(true /* is_global */, self);
2377 }
2378 
2379 // Deflate the specified JavaThread&#39;s idle ObjectMonitors using a JavaThread.
2380 //
2381 void ObjectSynchronizer::deflate_per_thread_idle_monitors_using_JT(JavaThread* target) {

2382   assert(Thread::current()-&gt;is_Java_thread(), &quot;precondition&quot;);
2383 
2384   deflate_common_idle_monitors_using_JT(false /* !is_global */, target);
2385 }
2386 
2387 // Deflate global or per-thread idle ObjectMonitors using a JavaThread.
2388 //
2389 void ObjectSynchronizer::deflate_common_idle_monitors_using_JT(bool is_global, JavaThread* target) {
2390   JavaThread* self = JavaThread::current();
2391 
2392   int deflated_count = 0;
2393   ObjectMonitor* free_head_p = NULL;  // Local SLL of scavenged ObjectMonitors
2394   ObjectMonitor* free_tail_p = NULL;
2395   ObjectMonitor* saved_mid_in_use_p = NULL;
2396   elapsedTimer timer;
2397 
2398   if (log_is_enabled(Info, monitorinflation)) {
2399     timer.start();
2400   }
2401 
</pre>
<hr />
<pre>
2417       local_deflated_count =
2418           deflate_monitor_list_using_JT(&amp;target-&gt;om_in_use_list,
2419                                         &amp;target-&gt;om_in_use_count, &amp;free_head_p,
2420                                         &amp;free_tail_p, &amp;saved_mid_in_use_p);
2421     }
2422     deflated_count += local_deflated_count;
2423 
2424     if (free_head_p != NULL) {
2425       // Move the deflated ObjectMonitors to the global free list.
2426       guarantee(free_tail_p != NULL &amp;&amp; local_deflated_count &gt; 0, &quot;free_tail_p=&quot; INTPTR_FORMAT &quot;, local_deflated_count=%d&quot;, p2i(free_tail_p), local_deflated_count);
2427       // Note: The target thread can be doing an om_alloc() that
2428       // is trying to prepend an ObjectMonitor on its in-use list
2429       // at the same time that we have deflated the current in-use
2430       // list head and put it on the local free list. prepend_to_common()
2431       // will detect the race and retry which avoids list corruption,
2432       // but the next field in free_tail_p can flicker to marked
2433       // and then unmarked while prepend_to_common() is sorting it
2434       // all out.
2435 #ifdef ASSERT
2436       ObjectMonitor* l_next_om = unmarked_next(free_tail_p);
<span class="line-modified">2437       assert(l_next_om == NULL, &quot;must be NULL: _next_om=&quot; INTPTR_FORMAT, p2i(l_next_om));</span>
2438 #endif
2439 
2440       prepend_list_to_global_wait_list(free_head_p, free_tail_p, local_deflated_count);
2441 
2442       OM_PERFDATA_OP(Deflations, inc(local_deflated_count));
2443     }
2444 
2445     if (saved_mid_in_use_p != NULL) {
2446       // deflate_monitor_list_using_JT() detected a safepoint starting.
2447       timer.stop();
2448       {
2449         if (is_global) {
2450           log_debug(monitorinflation)(&quot;pausing deflation of global idle monitors for a safepoint.&quot;);
2451         } else {
2452           log_debug(monitorinflation)(&quot;jt=&quot; INTPTR_FORMAT &quot;: pausing deflation of per-thread idle monitors for a safepoint.&quot;, p2i(target));
2453         }
2454         assert(SafepointMechanism::should_block(self), &quot;sanity check&quot;);
2455         ThreadBlockInVM blocker(self);
2456       }
2457       // Prepare for another loop after the safepoint.
</pre>
<hr />
<pre>
2464   } while (saved_mid_in_use_p != NULL);
2465   timer.stop();
2466 
2467   LogStreamHandle(Debug, monitorinflation) lsh_debug;
2468   LogStreamHandle(Info, monitorinflation) lsh_info;
2469   LogStream* ls = NULL;
2470   if (log_is_enabled(Debug, monitorinflation)) {
2471     ls = &amp;lsh_debug;
2472   } else if (deflated_count != 0 &amp;&amp; log_is_enabled(Info, monitorinflation)) {
2473     ls = &amp;lsh_info;
2474   }
2475   if (ls != NULL) {
2476     if (is_global) {
2477       ls-&gt;print_cr(&quot;async-deflating global idle monitors, %3.7f secs, %d monitors&quot;, timer.seconds(), deflated_count);
2478     } else {
2479       ls-&gt;print_cr(&quot;jt=&quot; INTPTR_FORMAT &quot;: async-deflating per-thread idle monitors, %3.7f secs, %d monitors&quot;, p2i(target), timer.seconds(), deflated_count);
2480     }
2481   }
2482 }
2483 
























































































2484 // Monitor cleanup on JavaThread::exit
2485 
2486 // Iterate through monitor cache and attempt to release thread&#39;s monitors
2487 // Gives up on a particular monitor if an exception occurs, but continues
2488 // the overall iteration, swallowing the exception.
2489 class ReleaseJavaMonitorsClosure: public MonitorClosure {
2490  private:
2491   TRAPS;
2492 
2493  public:
2494   ReleaseJavaMonitorsClosure(Thread* thread) : THREAD(thread) {}
2495   void do_monitor(ObjectMonitor* mid) {
2496     if (mid-&gt;owner() == THREAD) {
2497       (void)mid-&gt;complete_exit(CHECK);
2498     }
2499   }
2500 };
2501 
2502 // Release all inflated monitors owned by THREAD.  Lightweight monitors are
2503 // ignored.  This is meant to be called during JNI thread detach which assumes
2504 // all remaining monitors are heavyweight.  All exceptions are swallowed.
2505 // Scanning the extant monitor list can be time consuming.
2506 // A simple optimization is to add a per-thread flag that indicates a thread
2507 // called jni_monitorenter() during its lifetime.
2508 //
<span class="line-modified">2509 // Instead of NoSafepointVerifier it might be cheaper to</span>
2510 // use an idiom of the form:
2511 //   auto int tmp = SafepointSynchronize::_safepoint_counter ;
2512 //   &lt;code that must not run at safepoint&gt;
2513 //   guarantee (((tmp ^ _safepoint_counter) | (tmp &amp; 1)) == 0) ;
2514 // Since the tests are extremely cheap we could leave them enabled
2515 // for normal product builds.
2516 
2517 void ObjectSynchronizer::release_monitors_owned_by_thread(TRAPS) {
2518   assert(THREAD == JavaThread::current(), &quot;must be current Java thread&quot;);
2519   NoSafepointVerifier nsv;
2520   ReleaseJavaMonitorsClosure rjmc(THREAD);
2521   ObjectSynchronizer::monitors_iterate(&amp;rjmc);
2522   THREAD-&gt;clear_pending_exception();
2523 }
2524 
2525 const char* ObjectSynchronizer::inflate_cause_name(const InflateCause cause) {
2526   switch (cause) {
2527     case inflate_cause_vm_internal:    return &quot;VM Internal&quot;;
2528     case inflate_cause_monitor_enter:  return &quot;Monitor Enter&quot;;
2529     case inflate_cause_wait:           return &quot;Monitor Wait&quot;;
</pre>
<hr />
<pre>
2548   return (u_char*)&amp;GVars.hc_sequence;
2549 }
2550 
2551 size_t ObjectSynchronizer::get_gvars_size() {
2552   return sizeof(SharedGlobals);
2553 }
2554 
2555 u_char* ObjectSynchronizer::get_gvars_stw_random_addr() {
2556   return (u_char*)&amp;GVars.stw_random;
2557 }
2558 
2559 // This function can be called at a safepoint or it can be called when
2560 // we are trying to exit the VM. When we are trying to exit the VM, the
2561 // list walker functions can run in parallel with the other list
2562 // operations so spin-locking is used for safety.
2563 //
2564 // Calls to this function can be added in various places as a debugging
2565 // aid; pass &#39;true&#39; for the &#39;on_exit&#39; parameter to have in-use monitor
2566 // details logged at the Info level and &#39;false&#39; for the &#39;on_exit&#39;
2567 // parameter to have in-use monitor details logged at the Trace level.


2568 //
2569 void ObjectSynchronizer::audit_and_print_stats(bool on_exit) {
2570   assert(on_exit || SafepointSynchronize::is_at_safepoint(), &quot;invariant&quot;);
2571 
2572   LogStreamHandle(Debug, monitorinflation) lsh_debug;
2573   LogStreamHandle(Info, monitorinflation) lsh_info;
2574   LogStreamHandle(Trace, monitorinflation) lsh_trace;
2575   LogStream* ls = NULL;
2576   if (log_is_enabled(Trace, monitorinflation)) {
2577     ls = &amp;lsh_trace;
2578   } else if (log_is_enabled(Debug, monitorinflation)) {
2579     ls = &amp;lsh_debug;
2580   } else if (log_is_enabled(Info, monitorinflation)) {
2581     ls = &amp;lsh_info;
2582   }
2583   assert(ls != NULL, &quot;sanity check&quot;);
2584 
2585   // Log counts for the global and per-thread monitor lists:
2586   int chk_om_population = log_monitor_list_counts(ls);
2587   int error_cnt = 0;
</pre>
<hr />
<pre>
2645 void ObjectSynchronizer::chk_free_entry(JavaThread* jt, ObjectMonitor* n,
2646                                         outputStream * out, int *error_cnt_p) {
2647   stringStream ss;
2648   if (n-&gt;is_busy()) {
2649     if (jt != NULL) {
2650       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2651                     &quot;: free per-thread monitor must not be busy: %s&quot;, p2i(jt),
2652                     p2i(n), n-&gt;is_busy_to_string(&amp;ss));
2653     } else {
2654       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
2655                     &quot;must not be busy: %s&quot;, p2i(n), n-&gt;is_busy_to_string(&amp;ss));
2656     }
2657     *error_cnt_p = *error_cnt_p + 1;
2658   }
2659   if (n-&gt;header().value() != 0) {
2660     if (jt != NULL) {
2661       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2662                     &quot;: free per-thread monitor must have NULL _header &quot;
2663                     &quot;field: _header=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
2664                     n-&gt;header().value());





2665       *error_cnt_p = *error_cnt_p + 1;
2666     }
2667   }
2668   if (n-&gt;object() != NULL) {
2669     if (jt != NULL) {
2670       out-&gt;print_cr(&quot;ERROR: jt=&quot; INTPTR_FORMAT &quot;, monitor=&quot; INTPTR_FORMAT
2671                     &quot;: free per-thread monitor must have NULL _object &quot;
2672                     &quot;field: _object=&quot; INTPTR_FORMAT, p2i(jt), p2i(n),
2673                     p2i(n-&gt;object()));
2674     } else {
2675       out-&gt;print_cr(&quot;ERROR: monitor=&quot; INTPTR_FORMAT &quot;: free global monitor &quot;
2676                     &quot;must have NULL _object field: _object=&quot; INTPTR_FORMAT,
2677                     p2i(n), p2i(n-&gt;object()));
2678     }
2679     *error_cnt_p = *error_cnt_p + 1;
2680   }
2681 }
2682 
2683 // Lock the next ObjectMonitor for traversal and unlock the current
2684 // ObjectMonitor. Returns the next ObjectMonitor if there is one.
</pre>
</td>
</tr>
</table>
<center><a href="safepoint.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="synchronizer.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
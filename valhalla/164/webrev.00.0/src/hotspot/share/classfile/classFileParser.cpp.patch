diff a/src/hotspot/share/classfile/classFileParser.cpp b/src/hotspot/share/classfile/classFileParser.cpp
--- a/src/hotspot/share/classfile/classFileParser.cpp
+++ b/src/hotspot/share/classfile/classFileParser.cpp
@@ -4179,47 +4179,10 @@
     }
   }
   return super_klass;
 }
 
-#ifndef PRODUCT
-static void print_field_layout(const Symbol* name,
-                               Array<u2>* fields,
-                               ConstantPool* cp,
-                               int instance_size,
-                               int instance_fields_start,
-                               int instance_fields_end,
-                               int static_fields_end) {
-
-  assert(name != NULL, "invariant");
-
-  tty->print("%s: field layout\n", name->as_klass_external_name());
-  tty->print("  @%3d %s\n", instance_fields_start, "--- instance fields start ---");
-  for (AllFieldStream fs(fields, cp); !fs.done(); fs.next()) {
-    if (!fs.access_flags().is_static()) {
-      tty->print("  @%3d \"%s\" %s\n",
-        fs.offset(),
-        fs.name()->as_klass_external_name(),
-        fs.signature()->as_klass_external_name());
-    }
-  }
-  tty->print("  @%3d %s\n", instance_fields_end, "--- instance fields end ---");
-  tty->print("  @%3d %s\n", instance_size * wordSize, "--- instance ends ---");
-  tty->print("  @%3d %s\n", InstanceMirrorKlass::offset_of_static_fields(), "--- static fields start ---");
-  for (AllFieldStream fs(fields, cp); !fs.done(); fs.next()) {
-    if (fs.access_flags().is_static()) {
-      tty->print("  @%3d \"%s\" %s\n",
-        fs.offset(),
-        fs.name()->as_klass_external_name(),
-        fs.signature()->as_klass_external_name());
-    }
-  }
-  tty->print("  @%3d %s\n", static_fields_end, "--- static fields end ---");
-  tty->print("\n");
-}
-#endif
-
 OopMapBlocksBuilder::OopMapBlocksBuilder(unsigned int max_blocks) {
   _max_nonstatic_oop_maps = max_blocks;
   _nonstatic_oop_map_count = 0;
   if (max_blocks == 0) {
     _nonstatic_oop_maps = NULL;
@@ -4351,622 +4314,10 @@
         "\"%s\" sig: \"%s\" class: %s - %s", name->as_C_string(), sig->as_C_string(),
         _class_name->as_C_string(), msg);
   }
 }
 
-// Layout fields and fill in FieldLayoutInfo.  Could use more refactoring!
-void ClassFileParser::layout_fields(ConstantPool* cp,
-                                    const FieldAllocationCount* fac,
-                                    const ClassAnnotationCollector* parsed_annotations,
-                                    FieldLayoutInfo* info,
-                                    TRAPS) {
-
-  assert(cp != NULL, "invariant");
-
-  // Field size and offset computation
-  int nonstatic_field_size = _super_klass == NULL ? 0 :
-                               _super_klass->nonstatic_field_size();
-  int next_nonstatic_inline_type_offset = 0;
-  int first_nonstatic_inline_type_offset = 0;
-
-  // Fields that are inline types are handled differently depending if they are static or not:
-  // - static fields are oops
-  // - non-static fields are embedded
-
-  // Count the contended fields by type.
-  //
-  // We ignore static fields, because @Contended is not supported for them.
-  // The layout code below will also ignore the static fields.
-  int nonstatic_contended_count = 0;
-  FieldAllocationCount fac_contended;
-  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-    FieldAllocationType atype = (FieldAllocationType) fs.allocation_type();
-    if (fs.is_contended()) {
-      fac_contended.count[atype]++;
-      if (!fs.access_flags().is_static()) {
-        nonstatic_contended_count++;
-      }
-    }
-  }
-
-
-  // Calculate the starting byte offsets
-  int next_static_oop_offset    = InstanceMirrorKlass::offset_of_static_fields();
-  // Inline types in static fields are not embedded, they are handled with oops
-  int next_static_double_offset = next_static_oop_offset +
-                                  ((fac->count[STATIC_OOP] + fac->count[STATIC_INLINE]) * heapOopSize);
-  if (fac->count[STATIC_DOUBLE]) {
-    next_static_double_offset = align_up(next_static_double_offset, BytesPerLong);
-  }
-
-  int next_static_word_offset   = next_static_double_offset +
-                                    ((fac->count[STATIC_DOUBLE]) * BytesPerLong);
-  int next_static_short_offset  = next_static_word_offset +
-                                    ((fac->count[STATIC_WORD]) * BytesPerInt);
-  int next_static_byte_offset   = next_static_short_offset +
-                                  ((fac->count[STATIC_SHORT]) * BytesPerShort);
-
-  int nonstatic_fields_start  = instanceOopDesc::base_offset_in_bytes() +
-                                nonstatic_field_size * heapOopSize;
-
-  // First field of inline types is aligned on a long boundary in order to ease
-  // in-lining of inline types (with header removal) in packed arrays and
-  // inlined fields
-  int initial_inline_type_padding = 0;
-  if (is_inline_type()) {
-    int old = nonstatic_fields_start;
-    nonstatic_fields_start = align_up(nonstatic_fields_start, BytesPerLong);
-    initial_inline_type_padding = nonstatic_fields_start - old;
-  }
-
-  int next_nonstatic_field_offset = nonstatic_fields_start;
-
-  const bool is_contended_class     = parsed_annotations->is_contended();
-
-  // Class is contended, pad before all the fields
-  if (is_contended_class) {
-    next_nonstatic_field_offset += ContendedPaddingWidth;
-  }
-
-  // Temporary inline types restrictions
-  if (is_inline_type()) {
-    if (is_contended_class) {
-      throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support @Contended annotation yet");
-      return;
-    }
-  }
-
-  // Compute the non-contended fields count.
-  // The packing code below relies on these counts to determine if some field
-  // can be squeezed into the alignment gap. Contended fields are obviously
-  // exempt from that.
-  unsigned int nonstatic_double_count = fac->count[NONSTATIC_DOUBLE] - fac_contended.count[NONSTATIC_DOUBLE];
-  unsigned int nonstatic_word_count   = fac->count[NONSTATIC_WORD]   - fac_contended.count[NONSTATIC_WORD];
-  unsigned int nonstatic_short_count  = fac->count[NONSTATIC_SHORT]  - fac_contended.count[NONSTATIC_SHORT];
-  unsigned int nonstatic_byte_count   = fac->count[NONSTATIC_BYTE]   - fac_contended.count[NONSTATIC_BYTE];
-  unsigned int nonstatic_oop_count    = fac->count[NONSTATIC_OOP]    - fac_contended.count[NONSTATIC_OOP];
-
-  int static_inline_type_count = 0;
-  int nonstatic_inline_type_count = 0;
-  int* nonstatic_inline_type_indexes = NULL;
-  Klass** nonstatic_inline_type_klasses = NULL;
-  unsigned int inline_type_oop_map_count = 0;
-  int inline_types_not_inlined = 0;
-  int not_atomic_inline_types = 0;
-
-  int max_nonstatic_inline_type = fac->count[NONSTATIC_INLINE] + 1;
-
-  nonstatic_inline_type_indexes = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, int,
-                                                               max_nonstatic_inline_type);
-  for (int i = 0; i < max_nonstatic_inline_type; i++) {
-    nonstatic_inline_type_indexes[i] = -1;
-  }
-  nonstatic_inline_type_klasses = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, Klass*,
-                                                               max_nonstatic_inline_type);
-
-  for (AllFieldStream fs(_fields, _cp); !fs.done(); fs.next()) {
-    if (fs.allocation_type() == STATIC_INLINE) {
-      ResourceMark rm;
-      if (!fs.signature()->is_Q_signature()) {
-        THROW(vmSymbols::java_lang_ClassFormatError());
-      }
-      static_inline_type_count++;
-    } else if (fs.allocation_type() == NONSTATIC_INLINE) {
-      // Pre-resolve the inline field and check for inline type circularity issues.
-      ResourceMark rm;
-      if (!fs.signature()->is_Q_signature()) {
-        THROW(vmSymbols::java_lang_ClassFormatError());
-      }
-      Klass* klass =
-        SystemDictionary::resolve_inline_type_field_or_fail(&fs,
-                                                            Handle(THREAD, _loader_data->class_loader()),
-                                                            _protection_domain, true, CHECK);
-      assert(klass != NULL, "Sanity check");
-      if (!klass->access_flags().is_inline_type()) {
-        THROW(vmSymbols::java_lang_IncompatibleClassChangeError());
-      }
-      InlineKlass* vk = InlineKlass::cast(klass);
-      // Conditions to apply flattening or not should be defined in a single place
-      bool too_big_to_allocate_inline = (InlineFieldMaxFlatSize >= 0 &&
-                                 (vk->size_helper() * HeapWordSize) > InlineFieldMaxFlatSize);
-      bool too_atomic_to_allocate_inline = vk->is_declared_atomic();
-      bool too_volatile_to_allocate_inline = fs.access_flags().is_volatile();
-      if (vk->is_naturally_atomic()) {
-        too_atomic_to_allocate_inline = false;
-        // too_volatile_to_allocate_inline = false; //FIXME
-        // volatile fields are currently never inlined, this could change in the future
-      }
-      if (!(too_big_to_allocate_inline | too_atomic_to_allocate_inline | too_volatile_to_allocate_inline)) {
-        nonstatic_inline_type_indexes[nonstatic_inline_type_count] = fs.index();
-        nonstatic_inline_type_klasses[nonstatic_inline_type_count] = klass;
-        nonstatic_inline_type_count++;
-
-        InlineKlass* vklass = InlineKlass::cast(klass);
-        if (vklass->contains_oops()) {
-          inline_type_oop_map_count += vklass->nonstatic_oop_map_count();
-        }
-        fs.set_inlined(true);
-        if (!vk->is_atomic()) {  // flat and non-atomic: take note
-          not_atomic_inline_types++;
-        }
-      } else {
-        inline_types_not_inlined++;
-        fs.set_inlined(false);
-      }
-    }
-  }
-
-  // Adjusting non_static_oop_count to take into account inline types fields not inlined;
-  nonstatic_oop_count += inline_types_not_inlined;
-
-  // Total non-static fields count, including every contended field
-  unsigned int nonstatic_fields_count = fac->count[NONSTATIC_DOUBLE] + fac->count[NONSTATIC_WORD] +
-                                        fac->count[NONSTATIC_SHORT] + fac->count[NONSTATIC_BYTE] +
-                                        fac->count[NONSTATIC_OOP] + fac->count[NONSTATIC_INLINE];
-
-  const bool super_has_nonstatic_fields =
-          (_super_klass != NULL && _super_klass->has_nonstatic_fields());
-  const bool has_nonstatic_fields =
-    super_has_nonstatic_fields || (nonstatic_fields_count != 0);
-  const bool has_nonstatic_inline_fields = nonstatic_inline_type_count > 0;
-
-  if (is_inline_type() && (!has_nonstatic_fields)) {
-    // There are a number of fixes required throughout the type system and JIT
-    throwInlineTypeLimitation(THREAD_AND_LOCATION, "Inline Types do not support zero instance size yet");
-    return;
-  }
-
-  // Prepare list of oops for oop map generation.
-  //
-  // "offset" and "count" lists are describing the set of contiguous oop
-  // regions. offset[i] is the start of the i-th region, which then has
-  // count[i] oops following. Before we know how many regions are required,
-  // we pessimistically allocate the maps to fit all the oops into the
-  // distinct regions.
-  //
-  int super_oop_map_count = (_super_klass == NULL) ? 0 :_super_klass->nonstatic_oop_map_count();
-  int max_oop_map_count =
-      super_oop_map_count +
-      fac->count[NONSTATIC_OOP] +
-      inline_type_oop_map_count +
-      inline_types_not_inlined;
-
-  OopMapBlocksBuilder* nonstatic_oop_maps = new OopMapBlocksBuilder(max_oop_map_count);
-  if (super_oop_map_count > 0) {
-    nonstatic_oop_maps->initialize_inherited_blocks(_super_klass->start_of_nonstatic_oop_maps(),
-                                                    _super_klass->nonstatic_oop_map_count());
-  }
-
-  int first_nonstatic_oop_offset = 0; // will be set for first oop field
-
-  bool compact_fields  = true;
-  bool allocate_oops_first = false;
-
-  int next_nonstatic_oop_offset = 0;
-  int next_nonstatic_double_offset = 0;
-
-  // Rearrange fields for a given allocation style
-  if (allocate_oops_first) {
-    // Fields order: oops, longs/doubles, ints, shorts/chars, bytes, padded fields
-    next_nonstatic_oop_offset    = next_nonstatic_field_offset;
-    next_nonstatic_double_offset = next_nonstatic_oop_offset +
-                                    (nonstatic_oop_count * heapOopSize);
-  } else {
-    // Fields order: longs/doubles, ints, shorts/chars, bytes, oops, padded fields
-    next_nonstatic_double_offset = next_nonstatic_field_offset;
-  }
-
-  int nonstatic_oop_space_count   = 0;
-  int nonstatic_word_space_count  = 0;
-  int nonstatic_short_space_count = 0;
-  int nonstatic_byte_space_count  = 0;
-  int nonstatic_oop_space_offset = 0;
-  int nonstatic_word_space_offset = 0;
-  int nonstatic_short_space_offset = 0;
-  int nonstatic_byte_space_offset = 0;
-
-  // Try to squeeze some of the fields into the gaps due to
-  // long/double alignment.
-  if (nonstatic_double_count > 0) {
-    int offset = next_nonstatic_double_offset;
-    next_nonstatic_double_offset = align_up(offset, BytesPerLong);
-    if (compact_fields && offset != next_nonstatic_double_offset) {
-      // Allocate available fields into the gap before double field.
-      int length = next_nonstatic_double_offset - offset;
-      assert(length == BytesPerInt, "");
-      nonstatic_word_space_offset = offset;
-      if (nonstatic_word_count > 0) {
-        nonstatic_word_count      -= 1;
-        nonstatic_word_space_count = 1; // Only one will fit
-        length -= BytesPerInt;
-        offset += BytesPerInt;
-      }
-      nonstatic_short_space_offset = offset;
-      while (length >= BytesPerShort && nonstatic_short_count > 0) {
-        nonstatic_short_count       -= 1;
-        nonstatic_short_space_count += 1;
-        length -= BytesPerShort;
-        offset += BytesPerShort;
-      }
-      nonstatic_byte_space_offset = offset;
-      while (length > 0 && nonstatic_byte_count > 0) {
-        nonstatic_byte_count       -= 1;
-        nonstatic_byte_space_count += 1;
-        length -= 1;
-      }
-      // Allocate oop field in the gap if there are no other fields for that.
-      nonstatic_oop_space_offset = offset;
-      if (length >= heapOopSize && nonstatic_oop_count > 0 &&
-          !allocate_oops_first) { // when oop fields not first
-        nonstatic_oop_count      -= 1;
-        nonstatic_oop_space_count = 1; // Only one will fit
-        length -= heapOopSize;
-        offset += heapOopSize;
-      }
-    }
-  }
-
-  int next_nonstatic_word_offset = next_nonstatic_double_offset +
-                                     (nonstatic_double_count * BytesPerLong);
-  int next_nonstatic_short_offset = next_nonstatic_word_offset +
-                                      (nonstatic_word_count * BytesPerInt);
-  int next_nonstatic_byte_offset = next_nonstatic_short_offset +
-                                     (nonstatic_short_count * BytesPerShort);
-  int next_nonstatic_padded_offset = next_nonstatic_byte_offset +
-                                       nonstatic_byte_count;
-
-  // let oops jump before padding with this allocation style
-  if (!allocate_oops_first) {
-    next_nonstatic_oop_offset = next_nonstatic_padded_offset;
-    if( nonstatic_oop_count > 0 ) {
-      next_nonstatic_oop_offset = align_up(next_nonstatic_oop_offset, heapOopSize);
-    }
-    next_nonstatic_padded_offset = next_nonstatic_oop_offset + (nonstatic_oop_count * heapOopSize);
-  }
-
-  // Aligning embedded inline types
-  // bug below, the current algorithm to layout embedded inline types always put them at the
-  // end of the layout, which doesn't match the different allocation policies the VM is
-  // supposed to provide => FixMe
-  // Note also that the current alignment policy is to make each inline type starting on a
-  // 64 bits boundary. This could be optimized later. For instance, it could be nice to
-  // align inline types according to their most constrained internal type.
-  next_nonstatic_inline_type_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
-  int next_inline_type_index = 0;
-
-  // Iterate over fields again and compute correct offsets.
-  // The field allocation type was temporarily stored in the offset slot.
-  // oop fields are located before non-oop fields (static and non-static).
-  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-
-    // skip already laid out fields
-    if (fs.is_offset_set()) continue;
-
-    // contended instance fields are handled below
-    if (fs.is_contended() && !fs.access_flags().is_static()) continue;
-
-    int real_offset = 0;
-    const FieldAllocationType atype = (const FieldAllocationType) fs.allocation_type();
-
-    // pack the rest of the fields
-    switch (atype) {
-      // Inline types in static fields are handled with oops
-      case STATIC_INLINE:   // Fallthrough
-      case STATIC_OOP:
-        real_offset = next_static_oop_offset;
-        next_static_oop_offset += heapOopSize;
-        break;
-      case STATIC_BYTE:
-        real_offset = next_static_byte_offset;
-        next_static_byte_offset += 1;
-        break;
-      case STATIC_SHORT:
-        real_offset = next_static_short_offset;
-        next_static_short_offset += BytesPerShort;
-        break;
-      case STATIC_WORD:
-        real_offset = next_static_word_offset;
-        next_static_word_offset += BytesPerInt;
-        break;
-      case STATIC_DOUBLE:
-        real_offset = next_static_double_offset;
-        next_static_double_offset += BytesPerLong;
-        break;
-      case NONSTATIC_INLINE:
-        if (fs.is_inlined()) {
-          Klass* klass = nonstatic_inline_type_klasses[next_inline_type_index];
-          assert(klass != NULL, "Klass should have been loaded and resolved earlier");
-          assert(klass->access_flags().is_inline_type(),"Must be an inline type");
-          InlineKlass* vklass = InlineKlass::cast(klass);
-          real_offset = next_nonstatic_inline_type_offset;
-          next_nonstatic_inline_type_offset += (vklass->size_helper()) * wordSize - vklass->first_field_offset();
-          // aligning next inline type on a 64 bits boundary
-          next_nonstatic_inline_type_offset = align_up(next_nonstatic_inline_type_offset, BytesPerLong);
-          next_inline_type_index += 1;
-
-          if (vklass->contains_oops()) { // add flatten oop maps
-            int diff = real_offset - vklass->first_field_offset();
-            const OopMapBlock* map = vklass->start_of_nonstatic_oop_maps();
-            const OopMapBlock* const last_map = map + vklass->nonstatic_oop_map_count();
-            while (map < last_map) {
-              nonstatic_oop_maps->add(map->offset() + diff, map->count());
-              map++;
-            }
-          }
-          break;
-        } else {
-          // Fall through
-        }
-      case NONSTATIC_OOP:
-        if( nonstatic_oop_space_count > 0 ) {
-          real_offset = nonstatic_oop_space_offset;
-          nonstatic_oop_space_offset += heapOopSize;
-          nonstatic_oop_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_oop_offset;
-          next_nonstatic_oop_offset += heapOopSize;
-        }
-        nonstatic_oop_maps->add(real_offset, 1);
-        break;
-      case NONSTATIC_BYTE:
-        if( nonstatic_byte_space_count > 0 ) {
-          real_offset = nonstatic_byte_space_offset;
-          nonstatic_byte_space_offset += 1;
-          nonstatic_byte_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_byte_offset;
-          next_nonstatic_byte_offset += 1;
-        }
-        break;
-      case NONSTATIC_SHORT:
-        if( nonstatic_short_space_count > 0 ) {
-          real_offset = nonstatic_short_space_offset;
-          nonstatic_short_space_offset += BytesPerShort;
-          nonstatic_short_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_short_offset;
-          next_nonstatic_short_offset += BytesPerShort;
-        }
-        break;
-      case NONSTATIC_WORD:
-        if( nonstatic_word_space_count > 0 ) {
-          real_offset = nonstatic_word_space_offset;
-          nonstatic_word_space_offset += BytesPerInt;
-          nonstatic_word_space_count  -= 1;
-        } else {
-          real_offset = next_nonstatic_word_offset;
-          next_nonstatic_word_offset += BytesPerInt;
-        }
-        break;
-      case NONSTATIC_DOUBLE:
-        real_offset = next_nonstatic_double_offset;
-        next_nonstatic_double_offset += BytesPerLong;
-        break;
-      default:
-        ShouldNotReachHere();
-    }
-    fs.set_offset(real_offset);
-  }
-
-
-  // Handle the contended cases.
-  //
-  // Each contended field should not intersect the cache line with another contended field.
-  // In the absence of alignment information, we end up with pessimistically separating
-  // the fields with full-width padding.
-  //
-  // Additionally, this should not break alignment for the fields, so we round the alignment up
-  // for each field.
-  if (nonstatic_contended_count > 0) {
-
-    // if there is at least one contended field, we need to have pre-padding for them
-    next_nonstatic_padded_offset += ContendedPaddingWidth;
-
-    // collect all contended groups
-    ResourceBitMap bm(cp->size());
-    for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-      // skip already laid out fields
-      if (fs.is_offset_set()) continue;
-
-      if (fs.is_contended()) {
-        bm.set_bit(fs.contended_group());
-      }
-    }
-
-    int current_group = -1;
-    while ((current_group = (int)bm.get_next_one_offset(current_group + 1)) != (int)bm.size()) {
-
-      for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {
-
-        // skip already laid out fields
-        if (fs.is_offset_set()) continue;
-
-        // skip non-contended fields and fields from different group
-        if (!fs.is_contended() || (fs.contended_group() != current_group)) continue;
-
-        // handle statics below
-        if (fs.access_flags().is_static()) continue;
-
-        int real_offset = 0;
-        FieldAllocationType atype = (FieldAllocationType) fs.allocation_type();
-
-        switch (atype) {
-          case NONSTATIC_BYTE:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, 1);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += 1;
-            break;
-
-          case NONSTATIC_SHORT:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerShort);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += BytesPerShort;
-            break;
-
-          case NONSTATIC_WORD:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerInt);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += BytesPerInt;
-            break;
-
-          case NONSTATIC_DOUBLE:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, BytesPerLong);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += BytesPerLong;
-            break;
-
-            // Inline types in static fields are handled with oops
-          case NONSTATIC_INLINE:
-            throwInlineTypeLimitation(THREAD_AND_LOCATION,
-                                      "@Contended annotation not supported for inline types yet", fs.name(), fs.signature());
-            return;
-
-          case NONSTATIC_OOP:
-            next_nonstatic_padded_offset = align_up(next_nonstatic_padded_offset, heapOopSize);
-            real_offset = next_nonstatic_padded_offset;
-            next_nonstatic_padded_offset += heapOopSize;
-            nonstatic_oop_maps->add(real_offset, 1);
-            break;
-
-          default:
-            ShouldNotReachHere();
-        }
-
-        if (fs.contended_group() == 0) {
-          // Contended group defines the equivalence class over the fields:
-          // the fields within the same contended group are not inter-padded.
-          // The only exception is default group, which does not incur the
-          // equivalence, and so requires intra-padding.
-          next_nonstatic_padded_offset += ContendedPaddingWidth;
-        }
-
-        fs.set_offset(real_offset);
-      } // for
-
-      // Start laying out the next group.
-      // Note that this will effectively pad the last group in the back;
-      // this is expected to alleviate memory contention effects for
-      // subclass fields and/or adjacent object.
-      // If this was the default group, the padding is already in place.
-      if (current_group != 0) {
-        next_nonstatic_padded_offset += ContendedPaddingWidth;
-      }
-    }
-
-    // handle static fields
-  }
-
-  // Entire class is contended, pad in the back.
-  // This helps to alleviate memory contention effects for subclass fields
-  // and/or adjacent object.
-  if (is_contended_class) {
-    assert(!is_inline_type(), "@Contended not supported for inline types yet");
-    next_nonstatic_padded_offset += ContendedPaddingWidth;
-  }
-
-  int notaligned_nonstatic_fields_end;
-  if (nonstatic_inline_type_count != 0) {
-    notaligned_nonstatic_fields_end = next_nonstatic_inline_type_offset;
-  } else {
-    notaligned_nonstatic_fields_end = next_nonstatic_padded_offset;
-  }
-
-  int nonstatic_field_sz_align = heapOopSize;
-  if (is_inline_type()) {
-    if ((notaligned_nonstatic_fields_end - nonstatic_fields_start) > heapOopSize) {
-      nonstatic_field_sz_align = BytesPerLong; // value copy of fields only uses jlong copy
-    }
-  }
-  int nonstatic_fields_end      = align_up(notaligned_nonstatic_fields_end, nonstatic_field_sz_align);
-  int instance_end              = align_up(notaligned_nonstatic_fields_end, wordSize);
-  int static_fields_end         = align_up(next_static_byte_offset, wordSize);
-
-  int static_field_size         = (static_fields_end -
-                                   InstanceMirrorKlass::offset_of_static_fields()) / wordSize;
-  nonstatic_field_size          = nonstatic_field_size +
-                                  (nonstatic_fields_end - nonstatic_fields_start) / heapOopSize;
-
-  int instance_size             = align_object_size(instance_end / wordSize);
-
-  assert(instance_size == align_object_size(align_up(
-         (instanceOopDesc::base_offset_in_bytes() + nonstatic_field_size*heapOopSize)
-         + initial_inline_type_padding, wordSize) / wordSize), "consistent layout helper value");
-
-
-  // Invariant: nonstatic_field end/start should only change if there are
-  // nonstatic fields in the class, or if the class is contended. We compare
-  // against the non-aligned value, so that end alignment will not fail the
-  // assert without actually having the fields.
-  assert((notaligned_nonstatic_fields_end == nonstatic_fields_start) ||
-         is_contended_class ||
-         (nonstatic_fields_count > 0), "double-check nonstatic start/end");
-
-  // Number of non-static oop map blocks allocated at end of klass.
-  nonstatic_oop_maps->compact();
-
-#ifndef PRODUCT
-  if ((PrintFieldLayout && !is_inline_type()) ||
-      (PrintInlineLayout && (is_inline_type() || has_nonstatic_inline_fields))) {
-    print_field_layout(_class_name,
-          _fields,
-          cp,
-          instance_size,
-          nonstatic_fields_start,
-          nonstatic_fields_end,
-          static_fields_end);
-    nonstatic_oop_maps->print_on(tty);
-    tty->print("\n");
-    tty->print_cr("Instance size = %d", instance_size);
-    tty->print_cr("Nonstatic_field_size = %d", nonstatic_field_size);
-    tty->print_cr("Static_field_size = %d", static_field_size);
-    tty->print_cr("Has nonstatic fields = %d", has_nonstatic_fields);
-    tty->print_cr("---");
-  }
-
-#endif
-  // Pass back information needed for InstanceKlass creation
-  info->oop_map_blocks = nonstatic_oop_maps;
-  info->_instance_size = instance_size;
-  info->_static_field_size = static_field_size;
-  info->_nonstatic_field_size = nonstatic_field_size;
-  info->_has_nonstatic_fields = has_nonstatic_fields;
-  info->_has_inline_fields = nonstatic_inline_type_count > 0;
-
-  // An inline type is naturally atomic if it has just one field, and
-  // that field is simple enough.
-  info->_is_naturally_atomic = (is_inline_type() &&
-                                !super_has_nonstatic_fields &&
-                                (nonstatic_fields_count <= 1) &&
-                                (not_atomic_inline_types == 0) &&
-                                (nonstatic_contended_count == 0));
-  // This may be too restrictive, since if all the fields fit in 64
-  // bits we could make the decision to align instances of this class
-  // to 64-bit boundaries, and load and store them as single words.
-  // And on machines which supported larger atomics we could similarly
-  // allow larger values to be atomic, if properly aligned.
-}
-
 void ClassFileParser::set_precomputed_flags(InstanceKlass* ik) {
   assert(ik != NULL, "invariant");
 
   const Klass* const super = ik->super();
 
@@ -6453,17 +5804,13 @@
     }
   }
 
   if (is_inline_type()) {
     InlineKlass* vk = InlineKlass::cast(ik);
-    if (UseNewFieldLayout) {
-      vk->set_alignment(_alignment);
-      vk->set_first_field_offset(_first_field_offset);
-      vk->set_exact_size_in_bytes(_exact_size_in_bytes);
-    } else {
-      vk->set_first_field_offset(vk->first_field_offset_old());
-    }
+    vk->set_alignment(_alignment);
+    vk->set_first_field_offset(_first_field_offset);
+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);
     InlineKlass::cast(ik)->initialize_calling_convention(CHECK);
   }
 
   ClassLoadingService::notify_class_loaded(ik, false /* not shared class */);
 
@@ -7290,22 +6637,18 @@
       assert(klass->access_flags().is_inline_type(), "Value type expected");
     }
   }
 
   _field_info = new FieldLayoutInfo();
-  if (UseNewFieldLayout) {
-    FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
-        _parsed_annotations->is_contended(), is_inline_type(),
-        loader_data(), _protection_domain, _field_info);
-    lb.build_layout(CHECK);
-    if (is_inline_type()) {
-      _alignment = lb.get_alignment();
-      _first_field_offset = lb.get_first_field_offset();
-      _exact_size_in_bytes = lb.get_exact_size_in_byte();
-    }
-  } else {
-    layout_fields(cp, _fac, _parsed_annotations, _field_info, CHECK);
+  FieldLayoutBuilder lb(class_name(), super_klass(), _cp, _fields,
+      _parsed_annotations->is_contended(), is_inline_type(),
+      loader_data(), _protection_domain, _field_info);
+  lb.build_layout(CHECK);
+  if (is_inline_type()) {
+    _alignment = lb.get_alignment();
+    _first_field_offset = lb.get_first_field_offset();
+    _exact_size_in_bytes = lb.get_exact_size_in_byte();
   }
   _has_inline_type_fields = _field_info->_has_inline_fields;
 
   // Compute reference type
   _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();

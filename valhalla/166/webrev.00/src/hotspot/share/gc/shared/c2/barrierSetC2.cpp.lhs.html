<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/gc/shared/c2/barrierSetC2.cpp</title>
    <link rel="stylesheet" href="../../../../../../style.css" />
    <script type="text/javascript" src="../../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 #include &quot;gc/shared/c2/barrierSetC2.hpp&quot;
 27 #include &quot;opto/arraycopynode.hpp&quot;
 28 #include &quot;opto/convertnode.hpp&quot;
 29 #include &quot;opto/graphKit.hpp&quot;
 30 #include &quot;opto/idealKit.hpp&quot;
 31 #include &quot;opto/macro.hpp&quot;
 32 #include &quot;opto/narrowptrnode.hpp&quot;
 33 #include &quot;opto/runtime.hpp&quot;
 34 #include &quot;utilities/macros.hpp&quot;
 35 
 36 // By default this is a no-op.
 37 void BarrierSetC2::resolve_address(C2Access&amp; access) const { }
 38 
 39 void* C2ParseAccess::barrier_set_state() const {
 40   return _kit-&gt;barrier_set_state();
 41 }
 42 
 43 PhaseGVN&amp; C2ParseAccess::gvn() const { return _kit-&gt;gvn(); }
 44 
 45 Node* C2ParseAccess::control() const {
 46   return _ctl == NULL ? _kit-&gt;control() : _ctl;
 47 }
 48 
 49 bool C2Access::needs_cpu_membar() const {
 50   bool mismatched   = (_decorators &amp; C2_MISMATCHED) != 0;
 51   bool is_unordered = (_decorators &amp; MO_UNORDERED) != 0;
 52 
 53   bool anonymous = (_decorators &amp; C2_UNSAFE_ACCESS) != 0;
 54   bool in_heap   = (_decorators &amp; IN_HEAP) != 0;
 55   bool in_native = (_decorators &amp; IN_NATIVE) != 0;
 56   bool is_mixed  = !in_heap &amp;&amp; !in_native;
 57 
 58   bool is_write  = (_decorators &amp; C2_WRITE_ACCESS) != 0;
 59   bool is_read   = (_decorators &amp; C2_READ_ACCESS) != 0;
 60   bool is_atomic = is_read &amp;&amp; is_write;
 61 
 62   if (is_atomic) {
 63     // Atomics always need to be wrapped in CPU membars
 64     return true;
 65   }
 66 
 67   if (anonymous) {
 68     // We will need memory barriers unless we can determine a unique
 69     // alias category for this reference.  (Note:  If for some reason
 70     // the barriers get omitted and the unsafe reference begins to &quot;pollute&quot;
 71     // the alias analysis of the rest of the graph, either Compile::can_alias
 72     // or Compile::must_alias will throw a diagnostic assert.)
 73     if (is_mixed || !is_unordered || (mismatched &amp;&amp; !_addr.type()-&gt;isa_aryptr())) {
 74       return true;
 75     }
 76   } else {
 77     assert(!is_mixed, &quot;not unsafe&quot;);
 78   }
 79 
 80   return false;
 81 }
 82 
 83 Node* BarrierSetC2::store_at_resolved(C2Access&amp; access, C2AccessValue&amp; val) const {
 84   DecoratorSet decorators = access.decorators();
 85 
 86   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
 87   bool unaligned = (decorators &amp; C2_UNALIGNED) != 0;
 88   bool unsafe = (decorators &amp; C2_UNSAFE_ACCESS) != 0;
 89   bool requires_atomic_access = (decorators &amp; MO_UNORDERED) == 0;
 90 
 91   bool in_native = (decorators &amp; IN_NATIVE) != 0;
 92   assert(!in_native || (unsafe &amp;&amp; !access.is_oop()), &quot;not supported yet&quot;);
 93 
 94   MemNode::MemOrd mo = access.mem_node_mo();
 95 
 96   Node* store;
 97   if (access.is_parse_access()) {
 98     C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
 99 
100     GraphKit* kit = parse_access.kit();
101     if (access.type() == T_DOUBLE) {
102       Node* new_val = kit-&gt;dstore_rounding(val.node());
103       val.set_node(new_val);
104     }
105 
106     store = kit-&gt;store_to_memory(kit-&gt;control(), access.addr().node(), val.node(), access.type(),
107                                      access.addr().type(), mo, requires_atomic_access, unaligned, mismatched, unsafe);
108   } else {
109     assert(!requires_atomic_access, &quot;not yet supported&quot;);
110     assert(access.is_opt_access(), &quot;either parse or opt access&quot;);
111     C2OptAccess&amp; opt_access = static_cast&lt;C2OptAccess&amp;&gt;(access);
112     Node* ctl = opt_access.ctl();
113     MergeMemNode* mm = opt_access.mem();
114     PhaseGVN&amp; gvn = opt_access.gvn();
115     const TypePtr* adr_type = access.addr().type();
116     int alias = gvn.C-&gt;get_alias_index(adr_type);
117     Node* mem = mm-&gt;memory_at(alias);
118 
119     StoreNode* st = StoreNode::make(gvn, ctl, mem, access.addr().node(), adr_type, val.node(), access.type(), mo);
120     if (unaligned) {
121       st-&gt;set_unaligned_access();
122     }
123     if (mismatched) {
124       st-&gt;set_mismatched_access();
125     }
126     store = gvn.transform(st);
127     if (store == st) {
128       mm-&gt;set_memory_at(alias, st);
129     }
130   }
131   access.set_raw_access(store);
132 
133   return store;
134 }
135 
136 Node* BarrierSetC2::load_at_resolved(C2Access&amp; access, const Type* val_type) const {
137   DecoratorSet decorators = access.decorators();
138 
139   Node* adr = access.addr().node();
140   const TypePtr* adr_type = access.addr().type();
141 
142   bool mismatched = (decorators &amp; C2_MISMATCHED) != 0;
143   bool requires_atomic_access = (decorators &amp; MO_UNORDERED) == 0;
144   bool unaligned = (decorators &amp; C2_UNALIGNED) != 0;
145   bool control_dependent = (decorators &amp; C2_CONTROL_DEPENDENT_LOAD) != 0;
146   bool unknown_control = (decorators &amp; C2_UNKNOWN_CONTROL_LOAD) != 0;
147   bool unsafe = (decorators &amp; C2_UNSAFE_ACCESS) != 0;
148 
149   bool in_native = (decorators &amp; IN_NATIVE) != 0;
150 
151   MemNode::MemOrd mo = access.mem_node_mo();
152   LoadNode::ControlDependency dep = unknown_control ? LoadNode::UnknownControl : LoadNode::DependsOnlyOnTest;
153 
154   Node* load;
155   if (access.is_parse_access()) {
156     C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
157     GraphKit* kit = parse_access.kit();
158     Node* control = control_dependent ? parse_access.control() : NULL;
159 
160     if (in_native) {
161       load = kit-&gt;make_load(control, adr, val_type, access.type(), mo, dep,
162                             requires_atomic_access, unaligned,
163                             mismatched, unsafe, access.barrier_data());
164     } else {
165       load = kit-&gt;make_load(control, adr, val_type, access.type(), adr_type, mo,
166                             dep, requires_atomic_access, unaligned, mismatched, unsafe,
167                             access.barrier_data());
168     }
169   } else {
170     assert(!requires_atomic_access, &quot;not yet supported&quot;);
171     assert(access.is_opt_access(), &quot;either parse or opt access&quot;);
172     C2OptAccess&amp; opt_access = static_cast&lt;C2OptAccess&amp;&gt;(access);
173     Node* control = control_dependent ? opt_access.ctl() : NULL;
174     MergeMemNode* mm = opt_access.mem();
175     PhaseGVN&amp; gvn = opt_access.gvn();
176     Node* mem = mm-&gt;memory_at(gvn.C-&gt;get_alias_index(adr_type));
177     load = LoadNode::make(gvn, control, mem, adr, adr_type, val_type, access.type(), mo,
178                           dep, unaligned, mismatched, unsafe, access.barrier_data());
179     load = gvn.transform(load);
180   }
181   access.set_raw_access(load);
182 
183   return load;
184 }
185 
186 class C2AccessFence: public StackObj {
187   C2Access&amp; _access;
188   Node* _leading_membar;
189 
190 public:
191   C2AccessFence(C2Access&amp; access) :
192     _access(access), _leading_membar(NULL) {
193     GraphKit* kit = NULL;
194     if (access.is_parse_access()) {
195       C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
196       kit = parse_access.kit();
197     }
198     DecoratorSet decorators = access.decorators();
199 
200     bool is_write = (decorators &amp; C2_WRITE_ACCESS) != 0;
201     bool is_read = (decorators &amp; C2_READ_ACCESS) != 0;
202     bool is_atomic = is_read &amp;&amp; is_write;
203 
204     bool is_volatile = (decorators &amp; MO_SEQ_CST) != 0;
205     bool is_release = (decorators &amp; MO_RELEASE) != 0;
206 
207     if (is_atomic) {
208       assert(kit != NULL, &quot;unsupported at optimization time&quot;);
209       // Memory-model-wise, a LoadStore acts like a little synchronized
210       // block, so needs barriers on each side.  These don&#39;t translate
211       // into actual barriers on most machines, but we still need rest of
212       // compiler to respect ordering.
213       if (is_release) {
214         _leading_membar = kit-&gt;insert_mem_bar(Op_MemBarRelease);
215       } else if (is_volatile) {
216         if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
217           _leading_membar = kit-&gt;insert_mem_bar(Op_MemBarVolatile);
218         } else {
219           _leading_membar = kit-&gt;insert_mem_bar(Op_MemBarRelease);
220         }
221       }
222     } else if (is_write) {
223       // If reference is volatile, prevent following memory ops from
224       // floating down past the volatile write.  Also prevents commoning
225       // another volatile read.
226       if (is_volatile || is_release) {
227         assert(kit != NULL, &quot;unsupported at optimization time&quot;);
228         _leading_membar = kit-&gt;insert_mem_bar(Op_MemBarRelease);
229       }
230     } else {
231       // Memory barrier to prevent normal and &#39;unsafe&#39; accesses from
232       // bypassing each other.  Happens after null checks, so the
233       // exception paths do not take memory state from the memory barrier,
234       // so there&#39;s no problems making a strong assert about mixing users
235       // of safe &amp; unsafe memory.
236       if (is_volatile &amp;&amp; support_IRIW_for_not_multiple_copy_atomic_cpu) {
237         assert(kit != NULL, &quot;unsupported at optimization time&quot;);
238         _leading_membar = kit-&gt;insert_mem_bar(Op_MemBarVolatile);
239       }
240     }
241 
242     if (access.needs_cpu_membar()) {
243       assert(kit != NULL, &quot;unsupported at optimization time&quot;);
244       kit-&gt;insert_mem_bar(Op_MemBarCPUOrder);
245     }
246 
247     if (is_atomic) {
248       // 4984716: MemBars must be inserted before this
249       //          memory node in order to avoid a false
250       //          dependency which will confuse the scheduler.
251       access.set_memory();
252     }
253   }
254 
255   ~C2AccessFence() {
256     GraphKit* kit = NULL;
257     if (_access.is_parse_access()) {
258       C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(_access);
259       kit = parse_access.kit();
260     }
261     DecoratorSet decorators = _access.decorators();
262 
263     bool is_write = (decorators &amp; C2_WRITE_ACCESS) != 0;
264     bool is_read = (decorators &amp; C2_READ_ACCESS) != 0;
265     bool is_atomic = is_read &amp;&amp; is_write;
266 
267     bool is_volatile = (decorators &amp; MO_SEQ_CST) != 0;
268     bool is_acquire = (decorators &amp; MO_ACQUIRE) != 0;
269 
270     // If reference is volatile, prevent following volatiles ops from
271     // floating up before the volatile access.
272     if (_access.needs_cpu_membar()) {
273       kit-&gt;insert_mem_bar(Op_MemBarCPUOrder);
274     }
275 
276     if (is_atomic) {
277       assert(kit != NULL, &quot;unsupported at optimization time&quot;);
278       if (is_acquire || is_volatile) {
279         Node* n = _access.raw_access();
280         Node* mb = kit-&gt;insert_mem_bar(Op_MemBarAcquire, n);
281         if (_leading_membar != NULL) {
282           MemBarNode::set_load_store_pair(_leading_membar-&gt;as_MemBar(), mb-&gt;as_MemBar());
283         }
284       }
285     } else if (is_write) {
286       // If not multiple copy atomic, we do the MemBarVolatile before the load.
287       if (is_volatile &amp;&amp; !support_IRIW_for_not_multiple_copy_atomic_cpu) {
288         assert(kit != NULL, &quot;unsupported at optimization time&quot;);
289         Node* n = _access.raw_access();
290         Node* mb = kit-&gt;insert_mem_bar(Op_MemBarVolatile, n); // Use fat membar
291         if (_leading_membar != NULL) {
292           MemBarNode::set_store_pair(_leading_membar-&gt;as_MemBar(), mb-&gt;as_MemBar());
293         }
294       }
295     } else {
296       if (is_volatile || is_acquire) {
297         assert(kit != NULL, &quot;unsupported at optimization time&quot;);
298         Node* n = _access.raw_access();
299         assert(_leading_membar == NULL || support_IRIW_for_not_multiple_copy_atomic_cpu, &quot;no leading membar expected&quot;);
300         Node* mb = kit-&gt;insert_mem_bar(Op_MemBarAcquire, n);
301         mb-&gt;as_MemBar()-&gt;set_trailing_load();
302       }
303     }
304   }
305 };
306 
307 Node* BarrierSetC2::store_at(C2Access&amp; access, C2AccessValue&amp; val) const {
308   C2AccessFence fence(access);
309   resolve_address(access);
310   return store_at_resolved(access, val);
311 }
312 
313 Node* BarrierSetC2::load_at(C2Access&amp; access, const Type* val_type) const {
314   C2AccessFence fence(access);
315   resolve_address(access);
316   return load_at_resolved(access, val_type);
317 }
318 
319 MemNode::MemOrd C2Access::mem_node_mo() const {
320   bool is_write = (_decorators &amp; C2_WRITE_ACCESS) != 0;
321   bool is_read = (_decorators &amp; C2_READ_ACCESS) != 0;
322   if ((_decorators &amp; MO_SEQ_CST) != 0) {
323     if (is_write &amp;&amp; is_read) {
324       // For atomic operations
325       return MemNode::seqcst;
326     } else if (is_write) {
327       return MemNode::release;
328     } else {
329       assert(is_read, &quot;what else?&quot;);
330       return MemNode::acquire;
331     }
332   } else if ((_decorators &amp; MO_RELEASE) != 0) {
333     return MemNode::release;
334   } else if ((_decorators &amp; MO_ACQUIRE) != 0) {
335     return MemNode::acquire;
336   } else if (is_write) {
337     // Volatile fields need releasing stores.
338     // Non-volatile fields also need releasing stores if they hold an
339     // object reference, because the object reference might point to
340     // a freshly created object.
341     // Conservatively release stores of object references.
342     return StoreNode::release_if_reference(_type);
343   } else {
344     return MemNode::unordered;
345   }
346 }
347 
348 void C2Access::fixup_decorators() {
349   bool default_mo = (_decorators &amp; MO_DECORATOR_MASK) == 0;
350   bool is_unordered = (_decorators &amp; MO_UNORDERED) != 0 || default_mo;
351   bool anonymous = (_decorators &amp; C2_UNSAFE_ACCESS) != 0;
352 
353   bool is_read = (_decorators &amp; C2_READ_ACCESS) != 0;
354   bool is_write = (_decorators &amp; C2_WRITE_ACCESS) != 0;
355 
356   if (AlwaysAtomicAccesses &amp;&amp; is_unordered) {
357     _decorators &amp;= ~MO_DECORATOR_MASK; // clear the MO bits
358     _decorators |= MO_RELAXED; // Force the MO_RELAXED decorator with AlwaysAtomicAccess
359   }
360 
361   _decorators = AccessInternal::decorator_fixup(_decorators);
362 
363   if (is_read &amp;&amp; !is_write &amp;&amp; anonymous) {
364     // To be valid, unsafe loads may depend on other conditions than
365     // the one that guards them: pin the Load node
366     _decorators |= C2_CONTROL_DEPENDENT_LOAD;
367     _decorators |= C2_UNKNOWN_CONTROL_LOAD;
368     const TypePtr* adr_type = _addr.type();
369     Node* adr = _addr.node();
370     if (!needs_cpu_membar() &amp;&amp; adr_type-&gt;isa_instptr()) {
371       assert(adr_type-&gt;meet(TypePtr::NULL_PTR) != adr_type-&gt;remove_speculative(), &quot;should be not null&quot;);
372       intptr_t offset = Type::OffsetBot;
373       AddPNode::Ideal_base_and_offset(adr, &amp;gvn(), offset);
374       if (offset &gt;= 0) {
375         int s = Klass::layout_helper_size_in_bytes(adr_type-&gt;isa_instptr()-&gt;klass()-&gt;layout_helper());
376         if (offset &lt; s) {
377           // Guaranteed to be a valid access, no need to pin it
378           _decorators ^= C2_CONTROL_DEPENDENT_LOAD;
379           _decorators ^= C2_UNKNOWN_CONTROL_LOAD;
380         }
381       }
382     }
383   }
384 }
385 
386 //--------------------------- atomic operations---------------------------------
387 
388 void BarrierSetC2::pin_atomic_op(C2AtomicParseAccess&amp; access) const {
389   if (!access.needs_pinning()) {
390     return;
391   }
392   // SCMemProjNodes represent the memory state of a LoadStore. Their
393   // main role is to prevent LoadStore nodes from being optimized away
394   // when their results aren&#39;t used.
395   assert(access.is_parse_access(), &quot;entry not supported at optimization time&quot;);
396   C2ParseAccess&amp; parse_access = static_cast&lt;C2ParseAccess&amp;&gt;(access);
397   GraphKit* kit = parse_access.kit();
398   Node* load_store = access.raw_access();
399   assert(load_store != NULL, &quot;must pin atomic op&quot;);
400   Node* proj = kit-&gt;gvn().transform(new SCMemProjNode(load_store));
401   kit-&gt;set_memory(proj, access.alias_idx());
402 }
403 
404 void C2AtomicParseAccess::set_memory() {
405   Node *mem = _kit-&gt;memory(_alias_idx);
406   _memory = mem;
407 }
408 
409 Node* BarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
410                                                    Node* new_val, const Type* value_type) const {
411   GraphKit* kit = access.kit();
412   MemNode::MemOrd mo = access.mem_node_mo();
413   Node* mem = access.memory();
414 
415   Node* adr = access.addr().node();
416   const TypePtr* adr_type = access.addr().type();
417 
418   Node* load_store = NULL;
419 
420   if (access.is_oop()) {
421 #ifdef _LP64
422     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
423       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
424       Node *oldval_enc = kit-&gt;gvn().transform(new EncodePNode(expected_val, expected_val-&gt;bottom_type()-&gt;make_narrowoop()));
425       load_store = new CompareAndExchangeNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, adr_type, value_type-&gt;make_narrowoop(), mo);
426     } else
427 #endif
428     {
429       load_store = new CompareAndExchangePNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, value_type-&gt;is_oopptr(), mo);
430     }
431   } else {
432     switch (access.type()) {
433       case T_BYTE: {
434         load_store = new CompareAndExchangeBNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);
435         break;
436       }
437       case T_SHORT: {
438         load_store = new CompareAndExchangeSNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);
439         break;
440       }
441       case T_INT: {
442         load_store = new CompareAndExchangeINode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);
443         break;
444       }
445       case T_LONG: {
446         load_store = new CompareAndExchangeLNode(kit-&gt;control(), mem, adr, new_val, expected_val, adr_type, mo);
447         break;
448       }
449       default:
450         ShouldNotReachHere();
451     }
452   }
453 
454   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());
455   load_store = kit-&gt;gvn().transform(load_store);
456 
457   access.set_raw_access(load_store);
458   pin_atomic_op(access);
459 
460 #ifdef _LP64
461   if (access.is_oop() &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
462     return kit-&gt;gvn().transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
463   }
464 #endif
465 
466   return load_store;
467 }
468 
469 Node* BarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess&amp; access, Node* expected_val,
470                                                     Node* new_val, const Type* value_type) const {
471   GraphKit* kit = access.kit();
472   DecoratorSet decorators = access.decorators();
473   MemNode::MemOrd mo = access.mem_node_mo();
474   Node* mem = access.memory();
475   bool is_weak_cas = (decorators &amp; C2_WEAK_CMPXCHG) != 0;
476   Node* load_store = NULL;
477   Node* adr = access.addr().node();
478 
479   if (access.is_oop()) {
480 #ifdef _LP64
481     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
482       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
483       Node *oldval_enc = kit-&gt;gvn().transform(new EncodePNode(expected_val, expected_val-&gt;bottom_type()-&gt;make_narrowoop()));
484       if (is_weak_cas) {
485         load_store = new WeakCompareAndSwapNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, mo);
486       } else {
487         load_store = new CompareAndSwapNNode(kit-&gt;control(), mem, adr, newval_enc, oldval_enc, mo);
488       }
489     } else
490 #endif
491     {
492       if (is_weak_cas) {
493         load_store = new WeakCompareAndSwapPNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
494       } else {
495         load_store = new CompareAndSwapPNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
496       }
497     }
498   } else {
499     switch(access.type()) {
500       case T_BYTE: {
501         if (is_weak_cas) {
502           load_store = new WeakCompareAndSwapBNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
503         } else {
504           load_store = new CompareAndSwapBNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
505         }
506         break;
507       }
508       case T_SHORT: {
509         if (is_weak_cas) {
510           load_store = new WeakCompareAndSwapSNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
511         } else {
512           load_store = new CompareAndSwapSNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
513         }
514         break;
515       }
516       case T_INT: {
517         if (is_weak_cas) {
518           load_store = new WeakCompareAndSwapINode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
519         } else {
520           load_store = new CompareAndSwapINode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
521         }
522         break;
523       }
524       case T_LONG: {
525         if (is_weak_cas) {
526           load_store = new WeakCompareAndSwapLNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
527         } else {
528           load_store = new CompareAndSwapLNode(kit-&gt;control(), mem, adr, new_val, expected_val, mo);
529         }
530         break;
531       }
532       default:
533         ShouldNotReachHere();
534     }
535   }
536 
537   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());
538   load_store = kit-&gt;gvn().transform(load_store);
539 
540   access.set_raw_access(load_store);
541   pin_atomic_op(access);
542 
543   return load_store;
544 }
545 
546 Node* BarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
547   GraphKit* kit = access.kit();
548   Node* mem = access.memory();
549   Node* adr = access.addr().node();
550   const TypePtr* adr_type = access.addr().type();
551   Node* load_store = NULL;
552 
553   if (access.is_oop()) {
554 #ifdef _LP64
555     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
556       Node *newval_enc = kit-&gt;gvn().transform(new EncodePNode(new_val, new_val-&gt;bottom_type()-&gt;make_narrowoop()));
557       load_store = kit-&gt;gvn().transform(new GetAndSetNNode(kit-&gt;control(), mem, adr, newval_enc, adr_type, value_type-&gt;make_narrowoop()));
558     } else
559 #endif
560     {
561       load_store = new GetAndSetPNode(kit-&gt;control(), mem, adr, new_val, adr_type, value_type-&gt;is_oopptr());
562     }
563   } else  {
564     switch (access.type()) {
565       case T_BYTE:
566         load_store = new GetAndSetBNode(kit-&gt;control(), mem, adr, new_val, adr_type);
567         break;
568       case T_SHORT:
569         load_store = new GetAndSetSNode(kit-&gt;control(), mem, adr, new_val, adr_type);
570         break;
571       case T_INT:
572         load_store = new GetAndSetINode(kit-&gt;control(), mem, adr, new_val, adr_type);
573         break;
574       case T_LONG:
575         load_store = new GetAndSetLNode(kit-&gt;control(), mem, adr, new_val, adr_type);
576         break;
577       default:
578         ShouldNotReachHere();
579     }
580   }
581 
582   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());
583   load_store = kit-&gt;gvn().transform(load_store);
584 
585   access.set_raw_access(load_store);
586   pin_atomic_op(access);
587 
588 #ifdef _LP64
589   if (access.is_oop() &amp;&amp; adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
590     return kit-&gt;gvn().transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
591   }
592 #endif
593 
594   return load_store;
595 }
596 
597 Node* BarrierSetC2::atomic_add_at_resolved(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
598   Node* load_store = NULL;
599   GraphKit* kit = access.kit();
600   Node* adr = access.addr().node();
601   const TypePtr* adr_type = access.addr().type();
602   Node* mem = access.memory();
603 
604   switch(access.type()) {
605     case T_BYTE:
606       load_store = new GetAndAddBNode(kit-&gt;control(), mem, adr, new_val, adr_type);
607       break;
608     case T_SHORT:
609       load_store = new GetAndAddSNode(kit-&gt;control(), mem, adr, new_val, adr_type);
610       break;
611     case T_INT:
612       load_store = new GetAndAddINode(kit-&gt;control(), mem, adr, new_val, adr_type);
613       break;
614     case T_LONG:
615       load_store = new GetAndAddLNode(kit-&gt;control(), mem, adr, new_val, adr_type);
616       break;
617     default:
618       ShouldNotReachHere();
619   }
620 
621   load_store-&gt;as_LoadStore()-&gt;set_barrier_data(access.barrier_data());
622   load_store = kit-&gt;gvn().transform(load_store);
623 
624   access.set_raw_access(load_store);
625   pin_atomic_op(access);
626 
627   return load_store;
628 }
629 
630 Node* BarrierSetC2::atomic_cmpxchg_val_at(C2AtomicParseAccess&amp; access, Node* expected_val,
631                                           Node* new_val, const Type* value_type) const {
632   C2AccessFence fence(access);
633   resolve_address(access);
634   return atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, value_type);
635 }
636 
637 Node* BarrierSetC2::atomic_cmpxchg_bool_at(C2AtomicParseAccess&amp; access, Node* expected_val,
638                                            Node* new_val, const Type* value_type) const {
639   C2AccessFence fence(access);
640   resolve_address(access);
641   return atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);
642 }
643 
644 Node* BarrierSetC2::atomic_xchg_at(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
645   C2AccessFence fence(access);
646   resolve_address(access);
647   return atomic_xchg_at_resolved(access, new_val, value_type);
648 }
649 
650 Node* BarrierSetC2::atomic_add_at(C2AtomicParseAccess&amp; access, Node* new_val, const Type* value_type) const {
651   C2AccessFence fence(access);
652   resolve_address(access);
653   return atomic_add_at_resolved(access, new_val, value_type);
654 }
655 
656 int BarrierSetC2::arraycopy_payload_base_offset(bool is_array) {
657   // Exclude the header but include array length to copy by 8 bytes words.
658   // Can&#39;t use base_offset_in_bytes(bt) since basic type is unknown.
659   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
660                             instanceOopDesc::base_offset_in_bytes();
661   // base_off:
662   // 8  - 32-bit VM
663   // 12 - 64-bit VM, compressed klass
664   // 16 - 64-bit VM, normal klass
665   if (base_off % BytesPerLong != 0) {
666     assert(UseCompressedClassPointers, &quot;&quot;);
667     if (is_array) {
668       // Exclude length to copy by 8 bytes words.
669       base_off += sizeof(int);
670     } else {
671       // Include klass to copy by 8 bytes words.
672       base_off = instanceOopDesc::klass_offset_in_bytes();
673     }
674     assert(base_off % BytesPerLong == 0, &quot;expect 8 bytes alignment&quot;);
675   }
676   return base_off;
677 }
678 
679 void BarrierSetC2::clone(GraphKit* kit, Node* src_base, Node* dst_base, Node* countx, bool is_array) const {
680 #ifdef ASSERT
681   intptr_t src_offset;
682   Node* src = AddPNode::Ideal_base_and_offset(src_base, &amp;kit-&gt;gvn(), src_offset);
683   intptr_t dst_offset;
684   Node* dst = AddPNode::Ideal_base_and_offset(dst_base, &amp;kit-&gt;gvn(), dst_offset);
685   assert(src == NULL || (src_offset % BytesPerLong == 0), &quot;expect 8 bytes alignment&quot;);
686   assert(dst == NULL || (dst_offset % BytesPerLong == 0), &quot;expect 8 bytes alignment&quot;);
687 #endif
688   int base_off = arraycopy_payload_base_offset(is_array);
689   Node* offset = kit-&gt;MakeConX(base_off);
690   ArrayCopyNode* ac = ArrayCopyNode::make(kit, false, src_base, offset, dst_base, offset, countx, true, false);
691   if (is_array) {
692     ac-&gt;set_clone_array();
693   } else {
694     ac-&gt;set_clone_inst();
695   }
696   Node* n = kit-&gt;gvn().transform(ac);
697   if (n == ac) {
698     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
699     ac-&gt;_adr_type = TypeRawPtr::BOTTOM;
700     kit-&gt;set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
701   } else {
702     kit-&gt;set_all_memory(n);
703   }
704 }
705 
706 Node* BarrierSetC2::obj_allocate(PhaseMacroExpand* macro, Node* ctrl, Node* mem, Node* toobig_false, Node* size_in_bytes,
707                                  Node*&amp; i_o, Node*&amp; needgc_ctrl,
708                                  Node*&amp; fast_oop_ctrl, Node*&amp; fast_oop_rawmem,
709                                  intx prefetch_lines) const {
710 
711   Node* eden_top_adr;
712   Node* eden_end_adr;
713 
714   macro-&gt;set_eden_pointers(eden_top_adr, eden_end_adr);
715 
716   // Load Eden::end.  Loop invariant and hoisted.
717   //
718   // Note: We set the control input on &quot;eden_end&quot; and &quot;old_eden_top&quot; when using
719   //       a TLAB to work around a bug where these values were being moved across
720   //       a safepoint.  These are not oops, so they cannot be include in the oop
721   //       map, but they can be changed by a GC.   The proper way to fix this would
722   //       be to set the raw memory state when generating a  SafepointNode.  However
723   //       this will require extensive changes to the loop optimization in order to
724   //       prevent a degradation of the optimization.
725   //       See comment in memnode.hpp, around line 227 in class LoadPNode.
726   Node *eden_end = macro-&gt;make_load(ctrl, mem, eden_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);
727 
728   // We need a Region for the loop-back contended case.
729   enum { fall_in_path = 1, contended_loopback_path = 2 };
730   Node *contended_region;
731   Node *contended_phi_rawmem;
732   if (UseTLAB) {
733     contended_region = toobig_false;
734     contended_phi_rawmem = mem;
735   } else {
736     contended_region = new RegionNode(3);
737     contended_phi_rawmem = new PhiNode(contended_region, Type::MEMORY, TypeRawPtr::BOTTOM);
738     // Now handle the passing-too-big test.  We fall into the contended
739     // loop-back merge point.
740     contended_region    -&gt;init_req(fall_in_path, toobig_false);
741     contended_phi_rawmem-&gt;init_req(fall_in_path, mem);
742     macro-&gt;transform_later(contended_region);
743     macro-&gt;transform_later(contended_phi_rawmem);
744   }
745 
746   // Load(-locked) the heap top.
747   // See note above concerning the control input when using a TLAB
748   Node *old_eden_top = UseTLAB
749     ? new LoadPNode      (ctrl, contended_phi_rawmem, eden_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered)
750     : new LoadPLockedNode(contended_region, contended_phi_rawmem, eden_top_adr, MemNode::acquire);
751 
752   macro-&gt;transform_later(old_eden_top);
753   // Add to heap top to get a new heap top
754   Node *new_eden_top = new AddPNode(macro-&gt;top(), old_eden_top, size_in_bytes);
755   macro-&gt;transform_later(new_eden_top);
756   // Check for needing a GC; compare against heap end
757   Node *needgc_cmp = new CmpPNode(new_eden_top, eden_end);
758   macro-&gt;transform_later(needgc_cmp);
759   Node *needgc_bol = new BoolNode(needgc_cmp, BoolTest::ge);
760   macro-&gt;transform_later(needgc_bol);
761   IfNode *needgc_iff = new IfNode(contended_region, needgc_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);
762   macro-&gt;transform_later(needgc_iff);
763 
764   // Plug the failing-heap-space-need-gc test into the slow-path region
765   Node *needgc_true = new IfTrueNode(needgc_iff);
766   macro-&gt;transform_later(needgc_true);
767   needgc_ctrl = needgc_true;
768 
769   // No need for a GC.  Setup for the Store-Conditional
770   Node *needgc_false = new IfFalseNode(needgc_iff);
771   macro-&gt;transform_later(needgc_false);
772 
773   i_o = macro-&gt;prefetch_allocation(i_o, needgc_false, contended_phi_rawmem,
774                                    old_eden_top, new_eden_top, prefetch_lines);
775 
776   Node* fast_oop = old_eden_top;
777 
778   // Store (-conditional) the modified eden top back down.
779   // StorePConditional produces flags for a test PLUS a modified raw
780   // memory state.
781   if (UseTLAB) {
782     Node* store_eden_top =
783       new StorePNode(needgc_false, contended_phi_rawmem, eden_top_adr,
784                      TypeRawPtr::BOTTOM, new_eden_top, MemNode::unordered);
785     macro-&gt;transform_later(store_eden_top);
786     fast_oop_ctrl = needgc_false; // No contention, so this is the fast path
787     fast_oop_rawmem = store_eden_top;
788   } else {
789     Node* store_eden_top =
790       new StorePConditionalNode(needgc_false, contended_phi_rawmem, eden_top_adr,
791                                 new_eden_top, fast_oop/*old_eden_top*/);
792     macro-&gt;transform_later(store_eden_top);
793     Node *contention_check = new BoolNode(store_eden_top, BoolTest::ne);
794     macro-&gt;transform_later(contention_check);
795     store_eden_top = new SCMemProjNode(store_eden_top);
796     macro-&gt;transform_later(store_eden_top);
797 
798     // If not using TLABs, check to see if there was contention.
799     IfNode *contention_iff = new IfNode (needgc_false, contention_check, PROB_MIN, COUNT_UNKNOWN);
800     macro-&gt;transform_later(contention_iff);
801     Node *contention_true = new IfTrueNode(contention_iff);
802     macro-&gt;transform_later(contention_true);
803     // If contention, loopback and try again.
804     contended_region-&gt;init_req(contended_loopback_path, contention_true);
805     contended_phi_rawmem-&gt;init_req(contended_loopback_path, store_eden_top);
806 
807     // Fast-path succeeded with no contention!
808     Node *contention_false = new IfFalseNode(contention_iff);
809     macro-&gt;transform_later(contention_false);
810     fast_oop_ctrl = contention_false;
811 
812     // Bump total allocated bytes for this thread
813     Node* thread = new ThreadLocalNode();
814     macro-&gt;transform_later(thread);
815     Node* alloc_bytes_adr = macro-&gt;basic_plus_adr(macro-&gt;top()/*not oop*/, thread,
816                                                   in_bytes(JavaThread::allocated_bytes_offset()));
817     Node* alloc_bytes = macro-&gt;make_load(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
818                                          0, TypeLong::LONG, T_LONG);
819 #ifdef _LP64
820     Node* alloc_size = size_in_bytes;
821 #else
822     Node* alloc_size = new ConvI2LNode(size_in_bytes);
823     macro-&gt;transform_later(alloc_size);
824 #endif
825     Node* new_alloc_bytes = new AddLNode(alloc_bytes, alloc_size);
826     macro-&gt;transform_later(new_alloc_bytes);
827     fast_oop_rawmem = macro-&gt;make_store(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,
828                                         0, new_alloc_bytes, T_LONG);
829   }
830   return fast_oop;
831 }
832 
833 #define XTOP LP64_ONLY(COMMA phase-&gt;top())
834 
835 void BarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {
836   Node* ctrl = ac-&gt;in(TypeFunc::Control);
837   Node* mem = ac-&gt;in(TypeFunc::Memory);
838   Node* src = ac-&gt;in(ArrayCopyNode::Src);
839   Node* src_offset = ac-&gt;in(ArrayCopyNode::SrcPos);
840   Node* dest = ac-&gt;in(ArrayCopyNode::Dest);
841   Node* dest_offset = ac-&gt;in(ArrayCopyNode::DestPos);
842   Node* length = ac-&gt;in(ArrayCopyNode::Length);
843 
844   Node* payload_src = phase-&gt;basic_plus_adr(src, src_offset);
845   Node* payload_dst = phase-&gt;basic_plus_adr(dest, dest_offset);
846 
847   const char* copyfunc_name = &quot;arraycopy&quot;;
848   address     copyfunc_addr = phase-&gt;basictype2arraycopy(T_LONG, NULL, NULL, true, copyfunc_name, true);
849 
850   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
851   const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();
852 
853   Node* call = phase-&gt;make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);
854   phase-&gt;transform_later(call);
855 
<a name="1" id="anc1"></a><span class="line-modified">856   phase-&gt;igvn().replace_node(ac, call);</span>
857 }
<a name="2" id="anc2"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="2" type="hidden" />
</body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/compile.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="castnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="escape.cpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/compile.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
1296       } else {                  // Random constant offset into array body
1297         offset = Type::OffsetBot;   // Flatten constant access into array body
1298         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1299       }
1300     }
1301     // Arrays of fixed size alias with arrays of unknown size.
1302     if (ta-&gt;size() != TypeInt::POS) {
1303       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1304       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1305     }
1306     // Arrays of known objects become arrays of unknown objects.
1307     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1308       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1309       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1310     }
1311     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1312       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1313       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1314     }
1315     // Initially all flattened array accesses share a single slice
<span class="line-modified">1316     if (ta-&gt;elem()-&gt;isa_inlinetype() &amp;&amp; ta-&gt;elem() != TypeInlineType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
1317       const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta-&gt;size());
1318       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));
1319     }
1320     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1321     // cannot be distinguished by bytecode alone.
1322     if (ta-&gt;elem() == TypeInt::BOOL) {
1323       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1324       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1325       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());
1326     }
1327     // During the 2nd round of IterGVN, NotNull castings are removed.
1328     // Make sure the Bottom and NotNull variants alias the same.
1329     // Also, make sure exact and non-exact variants alias the same.
1330     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1331       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1332     }
1333   }
1334 
1335   // Oop pointers need some flattening
1336   const TypeInstPtr *to = tj-&gt;isa_instptr();
1337   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1338     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1339     if( ptr == TypePtr::Constant ) {
1340       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1341           offset &lt; k-&gt;size_helper() * wordSize) {
1342         // No constant oop pointers (such as Strings); they alias with
1343         // unknown strings.
1344         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1345         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1346       }
1347     } else if( is_known_inst ) {
1348       tj = to; // Keep NotNull and klass_is_exact for instance type
1349     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1350       // During the 2nd round of IterGVN, NotNull castings are removed.
1351       // Make sure the Bottom and NotNull variants alias the same.
1352       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1353       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset), to-&gt;klass()-&gt;flatten_array());</span>
1354     }
1355     if (to-&gt;speculative() != NULL) {
1356       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),Type::Offset(to-&gt;offset()), to-&gt;klass()-&gt;flatten_array(), to-&gt;instance_id());
1357     }
1358     // Canonicalize the holder of this field
1359     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1360       // First handle header references such as a LoadKlassNode, even if the
1361       // object&#39;s klass is unloaded at compile time (4965979).
1362       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1363         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, Type::Offset(offset), false);</span>
1364       }
1365     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1366       // Static fields are in the space above the normal instance
1367       // fields in the java.lang.Class instance.
1368       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1369         to = NULL;
1370         tj = TypeOopPtr::BOTTOM;
1371         offset = tj-&gt;offset();
1372       }
1373     } else {
1374       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1375       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1376         if( is_known_inst ) {
1377           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array(), to-&gt;instance_id());
1378         } else {
<span class="line-modified">1379           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array());</span>
1380         }
1381       }
1382     }
1383   }
1384 
1385   // Klass pointers to object array klasses need some flattening
1386   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1387   if( tk ) {
1388     // If we are referencing a field within a Klass, we need
1389     // to assume the worst case of an Object.  Both exact and
1390     // inexact types must flatten to the same alias class so
1391     // use NotNull as the PTR.
1392     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1393 
1394       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1395                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1396                                    Type::Offset(offset),</span>
<span class="line-removed">1397                                    false);</span>
1398     }
1399 
1400     ciKlass* klass = tk-&gt;klass();
1401     if (klass != NULL &amp;&amp; klass-&gt;is_obj_array_klass()) {
1402       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1403       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1404         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1405       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset), false);</span>
1406     }
1407 
1408     // Check for precise loads from the primary supertype array and force them
1409     // to the supertype cache alias index.  Check for generic array loads from
1410     // the primary supertype array and also force them to the supertype cache
1411     // alias index.  Since the same load can reach both, we need to merge
1412     // these 2 disparate memories into the same alias class.  Since the
1413     // primary supertype array is read-only, there&#39;s no chance of confusion
1414     // where we bypass an array load and an array store.
1415     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1416     if (offset == Type::OffsetBot ||
1417         (offset &gt;= primary_supers_offset &amp;&amp;
1418          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1419         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1420       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1421       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk-&gt;klass(), Type::Offset(offset), tk-&gt;flat_array());</span>
1422     }
1423   }
1424 
1425   // Flatten all Raw pointers together.
1426   if (tj-&gt;base() == Type::RawPtr)
1427     tj = TypeRawPtr::BOTTOM;
1428 
1429   if (tj-&gt;base() == Type::AnyPtr)
1430     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1431 
1432   // Flatten all to bottom for now
1433   switch( _AliasLevel ) {
1434   case 0:
1435     tj = TypePtr::BOTTOM;
1436     break;
1437   case 1:                       // Flatten to: oop, static, field or array
1438     switch (tj-&gt;base()) {
1439     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1440     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1441     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1665         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
1666       } else if (tinst-&gt;klass()-&gt;is_inlinetype()) {
1667         // Inline type field
1668         ciInlineKlass* vk = tinst-&gt;inline_klass();
1669         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1670       } else {
1671         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();
1672         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1673       }
1674     }
1675     assert(field == NULL ||
1676            original_field == NULL ||
1677            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1678             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1679             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);
1680     // Set field() and is_rewritable() attributes.
1681     if (field != NULL) {
1682       alias_type(idx)-&gt;set_field(field);
1683       if (flat-&gt;isa_aryptr()) {
1684         // Fields of flat arrays are rewritable although they are declared final
<span class="line-modified">1685         assert(flat-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype(), &quot;must be a flat array&quot;);</span>
1686         alias_type(idx)-&gt;set_rewritable(true);
1687       }
1688     }
1689   }
1690 
1691   // Fill the cache for next time.
1692   if (!uncached) {
1693     ace-&gt;_adr_type = adr_type;
1694     ace-&gt;_index    = idx;
1695     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);
1696 
1697     // Might as well try to fill the cache for the flattened version, too.
1698     AliasCacheEntry* face = probe_alias_cache(flat);
1699     if (face-&gt;_adr_type == NULL) {
1700       face-&gt;_adr_type = flat;
1701       face-&gt;_index    = idx;
1702       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);
1703     }
1704   }
1705 
</pre>
<hr />
<pre>
1984       }
1985     }
1986     for (uint j = 0; j &lt; n-&gt;req(); j++) {
1987       Node* m = n-&gt;in(j);
1988       if (m != NULL) {
1989         wq.push(m);
1990       }
1991     }
1992   }
1993 
1994   if (memnodes.size() &gt; 0) {
1995     _flattened_accesses_share_alias = false;
1996 
1997     // We are going to change the slice for the flattened array
1998     // accesses so we need to clear the cache entries that refer to
1999     // them.
2000     for (uint i = 0; i &lt; AliasCacheSize; i++) {
2001       AliasCacheEntry* ace = &amp;_alias_cache[i];
2002       if (ace-&gt;_adr_type != NULL &amp;&amp;
2003           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;
<span class="line-modified">2004           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2005         ace-&gt;_adr_type = NULL;
2006         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop
2007       }
2008     }
2009 
2010     // Find what aliases we are going to add
2011     int start_alias = num_alias_types()-1;
2012     int stop_alias = 0;
2013 
2014     for (uint i = 0; i &lt; memnodes.size(); i++) {
2015       Node* m = memnodes.at(i);
2016       const TypePtr* adr_type = NULL;
2017       if (m-&gt;Opcode() == Op_StoreCM) {
2018         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();
2019         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),
2020                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),
2021                                       get_alias_index(adr_type));
2022         igvn.register_new_node_with_optimizer(clone);
2023         igvn.replace_node(m, clone);
2024       } else {
</pre>
<hr />
<pre>
2107           // nodes.
2108           mm = MergeMemNode::make(n);
2109           igvn.register_new_node_with_optimizer(mm);
2110           while (stack.size() &gt; 0) {
2111             Node* m = stack.node();
2112             uint idx = stack.index();
2113             if (m-&gt;is_Mem()) {
2114               // Move memory node to its new slice
2115               const TypePtr* adr_type = m-&gt;adr_type();
2116               int alias = get_alias_index(adr_type);
2117               Node* prev = mm-&gt;memory_at(alias);
2118               igvn.replace_input_of(m, MemNode::Memory, prev);
2119               mm-&gt;set_memory_at(alias, m);
2120             } else if (m-&gt;is_Phi()) {
2121               // We need as many new phis as there are new aliases
2122               igvn.replace_input_of(m, idx, mm);
2123               if (idx == m-&gt;req()-1) {
2124                 Node* r = m-&gt;in(0);
2125                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2126                   const Type* adr_type = get_adr_type(j);
<span class="line-modified">2127                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2128                     continue;
2129                   }
2130                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));
2131                   igvn.register_new_node_with_optimizer(phi);
2132                   for (uint k = 1; k &lt; m-&gt;req(); k++) {
2133                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));
2134                   }
2135                   mm-&gt;set_memory_at(j, phi);
2136                 }
2137                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);
2138                 igvn.register_new_node_with_optimizer(base_phi);
2139                 for (uint k = 1; k &lt; m-&gt;req(); k++) {
2140                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());
2141                 }
2142                 mm-&gt;set_base_memory(base_phi);
2143               }
2144             } else {
2145               // This is a MemBarCPUOrder node from
2146               // Parse::array_load()/Parse::array_store(), in the
2147               // branch that handles flattened arrays hidden under
2148               // an Object[] array. We also need one new membar per
2149               // new alias to keep the unknown access that the
2150               // membars protect properly ordered with accesses to
2151               // known flattened array.
2152               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);
2153               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);
2154               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());
2155               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2156                 const Type* adr_type = get_adr_type(j);
<span class="line-modified">2157                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2158                   continue;
2159                 }
2160                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);
2161                 igvn.register_new_node_with_optimizer(mb);
2162                 Node* mem = mm-&gt;memory_at(j);
2163                 mb-&gt;init_req(TypeFunc::Control, ctrl);
2164                 mb-&gt;init_req(TypeFunc::Memory, mem);
2165                 ctrl = new ProjNode(mb, TypeFunc::Control);
2166                 igvn.register_new_node_with_optimizer(ctrl);
2167                 mem = new ProjNode(mb, TypeFunc::Memory);
2168                 igvn.register_new_node_with_optimizer(mem);
2169                 mm-&gt;set_memory_at(j, mem);
2170               }
2171               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);
2172             }
2173             if (idx &lt; m-&gt;req()-1) {
2174               idx += 1;
2175               stack.set_index(idx);
2176               n = m-&gt;in(idx);
2177               break;
</pre>
<hr />
<pre>
2180             if (m-&gt;has_out_with(Op_Node)) {
2181               Node* place_holder = m-&gt;find_out_with(Op_Node);
2182               if (place_holder != NULL) {
2183                 Node* mm_clone = mm-&gt;clone();
2184                 igvn.register_new_node_with_optimizer(mm_clone);
2185                 Node* hook = new Node(1);
2186                 hook-&gt;init_req(0, mm);
2187                 igvn.replace_node(place_holder, mm_clone);
2188                 hook-&gt;destruct();
2189               }
2190               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);
2191             }
2192             stack.pop();
2193           }
2194         }
2195       } while(stack.size() &gt; 0);
2196       // Fix the memory state at the MergeMem we started from
2197       igvn.rehash_node_delayed(current);
2198       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2199         const Type* adr_type = get_adr_type(j);
<span class="line-modified">2200         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_inlinetype()) {</span>
2201           continue;
2202         }
2203         current-&gt;set_memory_at(j, mm);
2204       }
2205       current-&gt;set_memory_at(index, current-&gt;base_memory());
2206     }
2207     igvn.optimize();
2208   }
2209   print_method(PHASE_SPLIT_INLINES_ARRAY, 2);
2210 }
2211 
2212 
2213 // StringOpts and late inlining of string methods
2214 void Compile::inline_string_calls(bool parse_time) {
2215   {
2216     // remove useless nodes to make the usage analysis simpler
2217     ResourceMark rm;
2218     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2219   }
2220 
</pre>
</td>
<td>
<hr />
<pre>
1296       } else {                  // Random constant offset into array body
1297         offset = Type::OffsetBot;   // Flatten constant access into array body
1298         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1299       }
1300     }
1301     // Arrays of fixed size alias with arrays of unknown size.
1302     if (ta-&gt;size() != TypeInt::POS) {
1303       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1304       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1305     }
1306     // Arrays of known objects become arrays of unknown objects.
1307     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1308       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1309       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1310     }
1311     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1312       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1313       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), ta-&gt;field_offset());
1314     }
1315     // Initially all flattened array accesses share a single slice
<span class="line-modified">1316     if (ta-&gt;is_flat() &amp;&amp; ta-&gt;elem() != TypeInlineType::BOTTOM &amp;&amp; _flattened_accesses_share_alias) {</span>
1317       const TypeAry *tary = TypeAry::make(TypeInlineType::BOTTOM, ta-&gt;size());
1318       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,Type::Offset(offset), Type::Offset(Type::OffsetBot));
1319     }
1320     // Arrays of bytes and of booleans both use &#39;bastore&#39; and &#39;baload&#39; so
1321     // cannot be distinguished by bytecode alone.
1322     if (ta-&gt;elem() == TypeInt::BOOL) {
1323       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1324       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1325       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,Type::Offset(offset), ta-&gt;field_offset());
1326     }
1327     // During the 2nd round of IterGVN, NotNull castings are removed.
1328     // Make sure the Bottom and NotNull variants alias the same.
1329     // Also, make sure exact and non-exact variants alias the same.
1330     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1331       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,Type::Offset(offset), ta-&gt;field_offset());
1332     }
1333   }
1334 
1335   // Oop pointers need some flattening
1336   const TypeInstPtr *to = tj-&gt;isa_instptr();
1337   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1338     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1339     if( ptr == TypePtr::Constant ) {
1340       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1341           offset &lt; k-&gt;size_helper() * wordSize) {
1342         // No constant oop pointers (such as Strings); they alias with
1343         // unknown strings.
1344         assert(!is_known_inst, &quot;not scalarizable allocation&quot;);
<span class="line-modified">1345         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset));</span>
1346       }
1347     } else if( is_known_inst ) {
1348       tj = to; // Keep NotNull and klass_is_exact for instance type
1349     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1350       // During the 2nd round of IterGVN, NotNull castings are removed.
1351       // Make sure the Bottom and NotNull variants alias the same.
1352       // Also, make sure exact and non-exact variants alias the same.
<span class="line-modified">1353       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,Type::Offset(offset));</span>
1354     }
1355     if (to-&gt;speculative() != NULL) {
1356       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),Type::Offset(to-&gt;offset()), to-&gt;klass()-&gt;flatten_array(), to-&gt;instance_id());
1357     }
1358     // Canonicalize the holder of this field
1359     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1360       // First handle header references such as a LoadKlassNode, even if the
1361       // object&#39;s klass is unloaded at compile time (4965979).
1362       if (!is_known_inst) { // Do it only for non-instance types
<span class="line-modified">1363         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, Type::Offset(offset));</span>
1364       }
1365     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1366       // Static fields are in the space above the normal instance
1367       // fields in the java.lang.Class instance.
1368       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1369         to = NULL;
1370         tj = TypeOopPtr::BOTTOM;
1371         offset = tj-&gt;offset();
1372       }
1373     } else {
1374       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1375       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1376         if( is_known_inst ) {
1377           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, Type::Offset(offset), canonical_holder-&gt;flatten_array(), to-&gt;instance_id());
1378         } else {
<span class="line-modified">1379           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, Type::Offset(offset));</span>
1380         }
1381       }
1382     }
1383   }
1384 
1385   // Klass pointers to object array klasses need some flattening
1386   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1387   if( tk ) {
1388     // If we are referencing a field within a Klass, we need
1389     // to assume the worst case of an Object.  Both exact and
1390     // inexact types must flatten to the same alias class so
1391     // use NotNull as the PTR.
1392     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1393 
1394       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1395                                    TypeKlassPtr::OBJECT-&gt;klass(),
<span class="line-modified">1396                                    Type::Offset(offset));</span>

1397     }
1398 
1399     ciKlass* klass = tk-&gt;klass();
1400     if (klass != NULL &amp;&amp; klass-&gt;is_obj_array_klass()) {
1401       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1402       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1403         k = TypeInstPtr::BOTTOM-&gt;klass();
<span class="line-modified">1404       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));</span>
1405     }
1406 
1407     // Check for precise loads from the primary supertype array and force them
1408     // to the supertype cache alias index.  Check for generic array loads from
1409     // the primary supertype array and also force them to the supertype cache
1410     // alias index.  Since the same load can reach both, we need to merge
1411     // these 2 disparate memories into the same alias class.  Since the
1412     // primary supertype array is read-only, there&#39;s no chance of confusion
1413     // where we bypass an array load and an array store.
1414     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1415     if (offset == Type::OffsetBot ||
1416         (offset &gt;= primary_supers_offset &amp;&amp;
1417          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1418         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1419       offset = in_bytes(Klass::secondary_super_cache_offset());
<span class="line-modified">1420       tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk-&gt;klass(), Type::Offset(offset));</span>
1421     }
1422   }
1423 
1424   // Flatten all Raw pointers together.
1425   if (tj-&gt;base() == Type::RawPtr)
1426     tj = TypeRawPtr::BOTTOM;
1427 
1428   if (tj-&gt;base() == Type::AnyPtr)
1429     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1430 
1431   // Flatten all to bottom for now
1432   switch( _AliasLevel ) {
1433   case 0:
1434     tj = TypePtr::BOTTOM;
1435     break;
1436   case 1:                       // Flatten to: oop, static, field or array
1437     switch (tj-&gt;base()) {
1438     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1439     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1440     case Type::AryPtr:   // do not distinguish arrays at all
</pre>
<hr />
<pre>
1664         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
1665       } else if (tinst-&gt;klass()-&gt;is_inlinetype()) {
1666         // Inline type field
1667         ciInlineKlass* vk = tinst-&gt;inline_klass();
1668         field = vk-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1669       } else {
1670         ciInstanceKlass* k = tinst-&gt;klass()-&gt;as_instance_klass();
1671         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1672       }
1673     }
1674     assert(field == NULL ||
1675            original_field == NULL ||
1676            (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1677             field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1678             field-&gt;is_static() == original_field-&gt;is_static()), &quot;wrong field?&quot;);
1679     // Set field() and is_rewritable() attributes.
1680     if (field != NULL) {
1681       alias_type(idx)-&gt;set_field(field);
1682       if (flat-&gt;isa_aryptr()) {
1683         // Fields of flat arrays are rewritable although they are declared final
<span class="line-modified">1684         assert(flat-&gt;is_aryptr()-&gt;is_flat(), &quot;must be a flat array&quot;);</span>
1685         alias_type(idx)-&gt;set_rewritable(true);
1686       }
1687     }
1688   }
1689 
1690   // Fill the cache for next time.
1691   if (!uncached) {
1692     ace-&gt;_adr_type = adr_type;
1693     ace-&gt;_index    = idx;
1694     assert(alias_type(adr_type) == alias_type(idx),  &quot;type must be installed&quot;);
1695 
1696     // Might as well try to fill the cache for the flattened version, too.
1697     AliasCacheEntry* face = probe_alias_cache(flat);
1698     if (face-&gt;_adr_type == NULL) {
1699       face-&gt;_adr_type = flat;
1700       face-&gt;_index    = idx;
1701       assert(alias_type(flat) == alias_type(idx), &quot;flat type must work too&quot;);
1702     }
1703   }
1704 
</pre>
<hr />
<pre>
1983       }
1984     }
1985     for (uint j = 0; j &lt; n-&gt;req(); j++) {
1986       Node* m = n-&gt;in(j);
1987       if (m != NULL) {
1988         wq.push(m);
1989       }
1990     }
1991   }
1992 
1993   if (memnodes.size() &gt; 0) {
1994     _flattened_accesses_share_alias = false;
1995 
1996     // We are going to change the slice for the flattened array
1997     // accesses so we need to clear the cache entries that refer to
1998     // them.
1999     for (uint i = 0; i &lt; AliasCacheSize; i++) {
2000       AliasCacheEntry* ace = &amp;_alias_cache[i];
2001       if (ace-&gt;_adr_type != NULL &amp;&amp;
2002           ace-&gt;_adr_type-&gt;isa_aryptr() &amp;&amp;
<span class="line-modified">2003           ace-&gt;_adr_type-&gt;is_aryptr()-&gt;is_flat()) {</span>
2004         ace-&gt;_adr_type = NULL;
2005         ace-&gt;_index = (i != 0) ? 0 : AliasIdxTop; // Make sure the NULL adr_type resolves to AliasIdxTop
2006       }
2007     }
2008 
2009     // Find what aliases we are going to add
2010     int start_alias = num_alias_types()-1;
2011     int stop_alias = 0;
2012 
2013     for (uint i = 0; i &lt; memnodes.size(); i++) {
2014       Node* m = memnodes.at(i);
2015       const TypePtr* adr_type = NULL;
2016       if (m-&gt;Opcode() == Op_StoreCM) {
2017         adr_type = m-&gt;in(MemNode::OopStore)-&gt;adr_type();
2018         Node* clone = new StoreCMNode(m-&gt;in(MemNode::Control), m-&gt;in(MemNode::Memory), m-&gt;in(MemNode::Address),
2019                                       m-&gt;adr_type(), m-&gt;in(MemNode::ValueIn), m-&gt;in(MemNode::OopStore),
2020                                       get_alias_index(adr_type));
2021         igvn.register_new_node_with_optimizer(clone);
2022         igvn.replace_node(m, clone);
2023       } else {
</pre>
<hr />
<pre>
2106           // nodes.
2107           mm = MergeMemNode::make(n);
2108           igvn.register_new_node_with_optimizer(mm);
2109           while (stack.size() &gt; 0) {
2110             Node* m = stack.node();
2111             uint idx = stack.index();
2112             if (m-&gt;is_Mem()) {
2113               // Move memory node to its new slice
2114               const TypePtr* adr_type = m-&gt;adr_type();
2115               int alias = get_alias_index(adr_type);
2116               Node* prev = mm-&gt;memory_at(alias);
2117               igvn.replace_input_of(m, MemNode::Memory, prev);
2118               mm-&gt;set_memory_at(alias, m);
2119             } else if (m-&gt;is_Phi()) {
2120               // We need as many new phis as there are new aliases
2121               igvn.replace_input_of(m, idx, mm);
2122               if (idx == m-&gt;req()-1) {
2123                 Node* r = m-&gt;in(0);
2124                 for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2125                   const Type* adr_type = get_adr_type(j);
<span class="line-modified">2126                   if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;is_flat()) {</span>
2127                     continue;
2128                   }
2129                   Node* phi = new PhiNode(r, Type::MEMORY, get_adr_type(j));
2130                   igvn.register_new_node_with_optimizer(phi);
2131                   for (uint k = 1; k &lt; m-&gt;req(); k++) {
2132                     phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;memory_at(j));
2133                   }
2134                   mm-&gt;set_memory_at(j, phi);
2135                 }
2136                 Node* base_phi = new PhiNode(r, Type::MEMORY, TypePtr::BOTTOM);
2137                 igvn.register_new_node_with_optimizer(base_phi);
2138                 for (uint k = 1; k &lt; m-&gt;req(); k++) {
2139                   base_phi-&gt;init_req(k, m-&gt;in(k)-&gt;as_MergeMem()-&gt;base_memory());
2140                 }
2141                 mm-&gt;set_base_memory(base_phi);
2142               }
2143             } else {
2144               // This is a MemBarCPUOrder node from
2145               // Parse::array_load()/Parse::array_store(), in the
2146               // branch that handles flattened arrays hidden under
2147               // an Object[] array. We also need one new membar per
2148               // new alias to keep the unknown access that the
2149               // membars protect properly ordered with accesses to
2150               // known flattened array.
2151               assert(m-&gt;is_Proj(), &quot;projection expected&quot;);
2152               Node* ctrl = m-&gt;in(0)-&gt;in(TypeFunc::Control);
2153               igvn.replace_input_of(m-&gt;in(0), TypeFunc::Control, top());
2154               for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2155                 const Type* adr_type = get_adr_type(j);
<span class="line-modified">2156                 if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;is_flat()) {</span>
2157                   continue;
2158                 }
2159                 MemBarNode* mb = new MemBarCPUOrderNode(this, j, NULL);
2160                 igvn.register_new_node_with_optimizer(mb);
2161                 Node* mem = mm-&gt;memory_at(j);
2162                 mb-&gt;init_req(TypeFunc::Control, ctrl);
2163                 mb-&gt;init_req(TypeFunc::Memory, mem);
2164                 ctrl = new ProjNode(mb, TypeFunc::Control);
2165                 igvn.register_new_node_with_optimizer(ctrl);
2166                 mem = new ProjNode(mb, TypeFunc::Memory);
2167                 igvn.register_new_node_with_optimizer(mem);
2168                 mm-&gt;set_memory_at(j, mem);
2169               }
2170               igvn.replace_node(m-&gt;in(0)-&gt;as_Multi()-&gt;proj_out(TypeFunc::Control), ctrl);
2171             }
2172             if (idx &lt; m-&gt;req()-1) {
2173               idx += 1;
2174               stack.set_index(idx);
2175               n = m-&gt;in(idx);
2176               break;
</pre>
<hr />
<pre>
2179             if (m-&gt;has_out_with(Op_Node)) {
2180               Node* place_holder = m-&gt;find_out_with(Op_Node);
2181               if (place_holder != NULL) {
2182                 Node* mm_clone = mm-&gt;clone();
2183                 igvn.register_new_node_with_optimizer(mm_clone);
2184                 Node* hook = new Node(1);
2185                 hook-&gt;init_req(0, mm);
2186                 igvn.replace_node(place_holder, mm_clone);
2187                 hook-&gt;destruct();
2188               }
2189               assert(!m-&gt;has_out_with(Op_Node), &quot;place holder should be gone now&quot;);
2190             }
2191             stack.pop();
2192           }
2193         }
2194       } while(stack.size() &gt; 0);
2195       // Fix the memory state at the MergeMem we started from
2196       igvn.rehash_node_delayed(current);
2197       for (uint j = (uint)start_alias; j &lt;= (uint)stop_alias; j++) {
2198         const Type* adr_type = get_adr_type(j);
<span class="line-modified">2199         if (!adr_type-&gt;isa_aryptr() || !adr_type-&gt;is_aryptr()-&gt;is_flat()) {</span>
2200           continue;
2201         }
2202         current-&gt;set_memory_at(j, mm);
2203       }
2204       current-&gt;set_memory_at(index, current-&gt;base_memory());
2205     }
2206     igvn.optimize();
2207   }
2208   print_method(PHASE_SPLIT_INLINES_ARRAY, 2);
2209 }
2210 
2211 
2212 // StringOpts and late inlining of string methods
2213 void Compile::inline_string_calls(bool parse_time) {
2214   {
2215     // remove useless nodes to make the usage analysis simpler
2216     ResourceMark rm;
2217     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2218   }
2219 
</pre>
</td>
</tr>
</table>
<center><a href="castnode.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="escape.cpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>
<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/gc/shared/genCollectedHeap.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
  </head>
<body>
<center><a href="../../../os/linux/os_linux.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="../../utilities/globalDefinitions_gcc.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/gc/shared/genCollectedHeap.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
  46 #include &quot;gc/shared/genCollectedHeap.hpp&quot;
  47 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
  48 #include &quot;gc/shared/generationSpec.hpp&quot;
  49 #include &quot;gc/shared/gcInitLogger.hpp&quot;
  50 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  51 #include &quot;gc/shared/oopStorage.inline.hpp&quot;
  52 #include &quot;gc/shared/oopStorageSet.inline.hpp&quot;
  53 #include &quot;gc/shared/oopStorageParState.inline.hpp&quot;
  54 #include &quot;gc/shared/scavengableNMethods.hpp&quot;
  55 #include &quot;gc/shared/space.hpp&quot;
  56 #include &quot;gc/shared/strongRootsScope.hpp&quot;
  57 #include &quot;gc/shared/weakProcessor.hpp&quot;
  58 #include &quot;gc/shared/workgroup.hpp&quot;
  59 #include &quot;memory/filemap.hpp&quot;
  60 #include &quot;memory/iterator.hpp&quot;
  61 #include &quot;memory/metaspaceCounters.hpp&quot;
  62 #include &quot;memory/resourceArea.hpp&quot;
  63 #include &quot;memory/universe.hpp&quot;
  64 #include &quot;oops/oop.inline.hpp&quot;
  65 #include &quot;runtime/biasedLocking.hpp&quot;
<span class="line-removed">  66 #include &quot;runtime/flags/flagSetting.hpp&quot;</span>
  67 #include &quot;runtime/handles.hpp&quot;
  68 #include &quot;runtime/handles.inline.hpp&quot;
  69 #include &quot;runtime/java.hpp&quot;
  70 #include &quot;runtime/vmThread.hpp&quot;
<span class="line-modified">  71 #include &quot;services/management.hpp&quot;</span>
  72 #include &quot;services/memoryService.hpp&quot;
  73 #include &quot;utilities/debug.hpp&quot;
  74 #include &quot;utilities/formatBuffer.hpp&quot;
  75 #include &quot;utilities/macros.hpp&quot;
  76 #include &quot;utilities/stack.inline.hpp&quot;
  77 #include &quot;utilities/vmError.hpp&quot;
  78 #if INCLUDE_JVMCI
  79 #include &quot;jvmci/jvmci.hpp&quot;
  80 #endif
  81 
  82 GenCollectedHeap::GenCollectedHeap(Generation::Name young,
  83                                    Generation::Name old,
  84                                    const char* policy_counters_name) :
  85   CollectedHeap(),
  86   _young_gen(NULL),
  87   _old_gen(NULL),
  88   _young_gen_spec(new GenerationSpec(young,
  89                                      NewSize,
  90                                      MaxNewSize,
  91                                      GenAlignment)),
</pre>
<hr />
<pre>
 301     if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 302       result = _young_gen-&gt;expand_and_allocate(size, is_tlab);
 303     }
 304   }
 305   assert(result == NULL || is_in_reserved(result), &quot;result not in heap&quot;);
 306   return result;
 307 }
 308 
 309 HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,
 310                                               bool is_tlab,
 311                                               bool* gc_overhead_limit_was_exceeded) {
 312   // In general gc_overhead_limit_was_exceeded should be false so
 313   // set it so here and reset it to true only if the gc time
 314   // limit is being exceeded as checked below.
 315   *gc_overhead_limit_was_exceeded = false;
 316 
 317   HeapWord* result = NULL;
 318 
 319   // Loop until the allocation is satisfied, or unsatisfied after GC.
 320   for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {
<span class="line-removed"> 321     HandleMark hm; // Discard any handles allocated in each iteration.</span>
 322 
 323     // First allocation attempt is lock-free.
 324     Generation *young = _young_gen;
 325     assert(young-&gt;supports_inline_contig_alloc(),
 326       &quot;Otherwise, must do alloc within heap lock&quot;);
 327     if (young-&gt;should_allocate(size, is_tlab)) {
 328       result = young-&gt;par_allocate(size, is_tlab);
 329       if (result != NULL) {
 330         assert(is_in_reserved(result), &quot;result not in heap&quot;);
 331         return result;
 332       }
 333     }
 334     uint gc_count_before;  // Read inside the Heap_lock locked region.
 335     {
 336       MutexLocker ml(Heap_lock);
 337       log_trace(gc, alloc)(&quot;GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation&quot;);
 338       // Note that only large objects get a shot at being
 339       // allocated in later generations.
 340       bool first_only = !should_try_older_generation_allocation(size);
 341 
</pre>
<hr />
<pre>
 461 
 462 void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,
 463                                           bool is_tlab, bool run_verification, bool clear_soft_refs,
 464                                           bool restore_marks_for_biased_locking) {
 465   FormatBuffer&lt;&gt; title(&quot;Collect gen: %s&quot;, gen-&gt;short_name());
 466   GCTraceTime(Trace, gc, phases) t1(title);
 467   TraceCollectorStats tcs(gen-&gt;counters());
 468   TraceMemoryManagerStats tmms(gen-&gt;gc_manager(), gc_cause());
 469 
 470   gen-&gt;stat_record()-&gt;invocations++;
 471   gen-&gt;stat_record()-&gt;accumulated_time.start();
 472 
 473   // Must be done anew before each collection because
 474   // a previous collection will do mangling and will
 475   // change top of some spaces.
 476   record_gen_tops_before_GC();
 477 
 478   log_trace(gc)(&quot;%s invoke=%d size=&quot; SIZE_FORMAT, heap()-&gt;is_young_gen(gen) ? &quot;Young&quot; : &quot;Old&quot;, gen-&gt;stat_record()-&gt;invocations, size * HeapWordSize);
 479 
 480   if (run_verification &amp;&amp; VerifyBeforeGC) {
<span class="line-removed"> 481     HandleMark hm;  // Discard invalid handles created during verification</span>
 482     Universe::verify(&quot;Before GC&quot;);
 483   }
 484   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());
 485 
 486   if (restore_marks_for_biased_locking) {
 487     // We perform this mark word preservation work lazily
 488     // because it&#39;s only at this point that we know whether we
 489     // absolutely have to do it; we want to avoid doing it for
 490     // scavenge-only collections where it&#39;s unnecessary
 491     BiasedLocking::preserve_marks();
 492   }
 493 
 494   // Do collection work
 495   {
 496     // Note on ref discovery: For what appear to be historical reasons,
 497     // GCH enables and disabled (by enqueing) refs discovery.
 498     // In the future this should be moved into the generation&#39;s
 499     // collect method so that ref discovery and enqueueing concerns
 500     // are local to a generation. The collect method could return
 501     // an appropriate indication in the case that notification on
 502     // the ref lock was needed. This will make the treatment of
 503     // weak refs more uniform (and indeed remove such concerns
 504     // from GCH). XXX
 505 
<span class="line-removed"> 506     HandleMark hm;  // Discard invalid handles created during gc</span>
 507     save_marks();   // save marks for all gens
 508     // We want to discover references, but not process them yet.
 509     // This mode is disabled in process_discovered_references if the
 510     // generation does some collection work, or in
 511     // enqueue_discovered_references if the generation returns
 512     // without doing any work.
 513     ReferenceProcessor* rp = gen-&gt;ref_processor();
 514     // If the discovery of (&quot;weak&quot;) refs in this generation is
 515     // atomic wrt other collectors in this configuration, we
 516     // are guaranteed to have empty discovered ref lists.
 517     if (rp-&gt;discovery_is_atomic()) {
 518       rp-&gt;enable_discovery();
 519       rp-&gt;setup_policy(clear_soft_refs);
 520     } else {
 521       // collect() below will enable discovery as appropriate
 522     }
 523     gen-&gt;collect(full, clear_soft_refs, size, is_tlab);
 524     if (!rp-&gt;enqueuing_is_done()) {
 525       rp-&gt;disable_discovery();
 526     } else {
 527       rp-&gt;set_enqueuing_is_done(false);
 528     }
 529     rp-&gt;verify_no_references_recorded();
 530   }
 531 
 532   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());
 533 
 534   gen-&gt;stat_record()-&gt;accumulated_time.stop();
 535 
 536   update_gc_stats(gen, full);
 537 
 538   if (run_verification &amp;&amp; VerifyAfterGC) {
<span class="line-removed"> 539     HandleMark hm;  // Discard invalid handles created during verification</span>
 540     Universe::verify(&quot;After GC&quot;);
 541   }
 542 }
 543 
 544 void GenCollectedHeap::do_collection(bool           full,
 545                                      bool           clear_all_soft_refs,
 546                                      size_t         size,
 547                                      bool           is_tlab,
 548                                      GenerationType max_generation) {
 549   ResourceMark rm;
 550   DEBUG_ONLY(Thread* my_thread = Thread::current();)
 551 
 552   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
 553   assert(my_thread-&gt;is_VM_thread() ||
 554          my_thread-&gt;is_ConcurrentGC_thread(),
 555          &quot;incorrect thread type capability&quot;);
 556   assert(Heap_lock-&gt;is_locked(),
 557          &quot;the requesting thread should have the Heap_lock&quot;);
 558   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
 559 
 560   if (GCLocker::check_active_before_gc()) {
 561     return; // GC is disabled (e.g. JNI GetXXXCritical operation)
 562   }
 563 
 564   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
 565                           soft_ref_policy()-&gt;should_clear_all_soft_refs();
 566 
 567   ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());
 568 
<span class="line-modified"> 569   FlagSetting fl(_is_gc_active, true);</span>
 570 
 571   bool complete = full &amp;&amp; (max_generation == OldGen);
 572   bool old_collects_young = complete &amp;&amp; !ScavengeBeforeFullGC;
 573   bool do_young_collection = !old_collects_young &amp;&amp; _young_gen-&gt;should_collect(full, size, is_tlab);
 574 
 575   const PreGenGCValues pre_gc_values = get_pre_gc_values();
 576 
 577   bool run_verification = total_collections() &gt;= VerifyGCStartAt;
 578   bool prepared_for_verification = false;
 579   bool do_full_collection = false;
 580 
 581   if (do_young_collection) {
 582     GCIdMark gc_id_mark;
 583     GCTraceCPUTime tcpu;
 584     GCTraceTime(Info, gc) t(&quot;Pause Young&quot;, NULL, gc_cause(), true);
 585 
 586     print_heap_before_gc();
 587 
 588     if (run_verification &amp;&amp; VerifyGCLevel &lt;= 0 &amp;&amp; VerifyBeforeGC) {
 589       prepare_for_verify();
</pre>
<hr />
<pre>
 805                                      CLDClosure* strong_cld_closure,
 806                                      CLDClosure* weak_cld_closure,
 807                                      CodeBlobToOopClosure* code_roots) {
 808   // General roots.
 809   assert(code_roots != NULL, &quot;code root closure should always be set&quot;);
 810   // _n_termination for _process_strong_tasks should be set up stream
 811   // in a method not running in a GC worker.  Otherwise the GC worker
 812   // could be trying to change the termination condition while the task
 813   // is executing in another GC worker.
 814 
 815   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ClassLoaderDataGraph_oops_do)) {
 816     ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);
 817   }
 818 
 819   // Only process code roots from thread stacks if we aren&#39;t visiting the entire CodeCache anyway
 820   CodeBlobToOopClosure* roots_from_code_p = (so &amp; SO_AllCodeCache) ? NULL : code_roots;
 821 
 822   bool is_par = scope-&gt;n_threads() &gt; 1;
 823   Threads::possibly_parallel_oops_do(is_par, strong_roots, roots_from_code_p);
 824 
<span class="line-removed"> 825   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_Universe_oops_do)) {</span>
<span class="line-removed"> 826     Universe::oops_do(strong_roots);</span>
<span class="line-removed"> 827   }</span>
<span class="line-removed"> 828 </span>
 829   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ObjectSynchronizer_oops_do)) {
 830     ObjectSynchronizer::oops_do(strong_roots);
 831   }
<span class="line-removed"> 832   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_Management_oops_do)) {</span>
<span class="line-removed"> 833     Management::oops_do(strong_roots);</span>
<span class="line-removed"> 834   }</span>
<span class="line-removed"> 835   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_jvmti_oops_do)) {</span>
<span class="line-removed"> 836     JvmtiExport::oops_do(strong_roots);</span>
<span class="line-removed"> 837   }</span>
 838 #if INCLUDE_AOT
 839   if (UseAOT &amp;&amp; _process_strong_tasks-&gt;try_claim_task(GCH_PS_aot_oops_do)) {
 840     AOTLoader::oops_do(strong_roots);
 841   }
 842 #endif
 843   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_OopStorageSet_oops_do)) {
 844     OopStorageSet::strong_oops_do(strong_roots);
 845   }
 846 
 847   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_CodeCache_oops_do)) {
 848     if (so &amp; SO_ScavengeCodeCache) {
 849       assert(code_roots != NULL, &quot;must supply closure for code cache&quot;);
 850 
 851       // We only visit parts of the CodeCache when scavenging.
 852       ScavengableNMethods::nmethods_do(code_roots);
 853     }
 854     if (so &amp; SO_AllCodeCache) {
 855       assert(code_roots != NULL, &quot;must supply closure for code cache&quot;);
 856 
 857       // CMSCollector uses this to do intermediate-strength collections.
</pre>
<hr />
<pre>
1352 void GenCollectedHeap::ensure_parsability(bool retire_tlabs) {
1353   CollectedHeap::ensure_parsability(retire_tlabs);
1354   GenEnsureParsabilityClosure ep_cl;
1355   generation_iterate(&amp;ep_cl, false);
1356 }
1357 
1358 oop GenCollectedHeap::handle_failed_promotion(Generation* old_gen,
1359                                               oop obj,
1360                                               size_t obj_size) {
1361   guarantee(old_gen == _old_gen, &quot;We only get here with an old generation&quot;);
1362   assert(obj_size == (size_t)obj-&gt;size(), &quot;bad obj_size passed in&quot;);
1363   HeapWord* result = NULL;
1364 
1365   result = old_gen-&gt;expand_and_allocate(obj_size, false);
1366 
1367   if (result != NULL) {
1368     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(obj), result, obj_size);
1369   }
1370   return oop(result);
1371 }
<span class="line-removed">1372 </span>
<span class="line-removed">1373 class GenTimeOfLastGCClosure: public GenCollectedHeap::GenClosure {</span>
<span class="line-removed">1374   jlong _time;   // in ms</span>
<span class="line-removed">1375   jlong _now;    // in ms</span>
<span class="line-removed">1376 </span>
<span class="line-removed">1377  public:</span>
<span class="line-removed">1378   GenTimeOfLastGCClosure(jlong now) : _time(now), _now(now) { }</span>
<span class="line-removed">1379 </span>
<span class="line-removed">1380   jlong time() { return _time; }</span>
<span class="line-removed">1381 </span>
<span class="line-removed">1382   void do_generation(Generation* gen) {</span>
<span class="line-removed">1383     _time = MIN2(_time, gen-&gt;time_of_last_gc(_now));</span>
<span class="line-removed">1384   }</span>
<span class="line-removed">1385 };</span>
<span class="line-removed">1386 </span>
<span class="line-removed">1387 jlong GenCollectedHeap::millis_since_last_gc() {</span>
<span class="line-removed">1388   // javaTimeNanos() is guaranteed to be monotonically non-decreasing</span>
<span class="line-removed">1389   // provided the underlying platform provides such a time source</span>
<span class="line-removed">1390   // (and it is bug free). So we still have to guard against getting</span>
<span class="line-removed">1391   // back a time later than &#39;now&#39;.</span>
<span class="line-removed">1392   jlong now = os::javaTimeNanos() / NANOSECS_PER_MILLISEC;</span>
<span class="line-removed">1393   GenTimeOfLastGCClosure tolgc_cl(now);</span>
<span class="line-removed">1394   // iterate over generations getting the oldest</span>
<span class="line-removed">1395   // time that a generation was collected</span>
<span class="line-removed">1396   generation_iterate(&amp;tolgc_cl, false);</span>
<span class="line-removed">1397 </span>
<span class="line-removed">1398   jlong retVal = now - tolgc_cl.time();</span>
<span class="line-removed">1399   if (retVal &lt; 0) {</span>
<span class="line-removed">1400     log_warning(gc)(&quot;millis_since_last_gc() would return : &quot; JLONG_FORMAT</span>
<span class="line-removed">1401        &quot;. returning zero instead.&quot;, retVal);</span>
<span class="line-removed">1402     return 0;</span>
<span class="line-removed">1403   }</span>
<span class="line-removed">1404   return retVal;</span>
<span class="line-removed">1405 }</span>
</pre>
</td>
<td>
<hr />
<pre>
  46 #include &quot;gc/shared/genCollectedHeap.hpp&quot;
  47 #include &quot;gc/shared/genOopClosures.inline.hpp&quot;
  48 #include &quot;gc/shared/generationSpec.hpp&quot;
  49 #include &quot;gc/shared/gcInitLogger.hpp&quot;
  50 #include &quot;gc/shared/locationPrinter.inline.hpp&quot;
  51 #include &quot;gc/shared/oopStorage.inline.hpp&quot;
  52 #include &quot;gc/shared/oopStorageSet.inline.hpp&quot;
  53 #include &quot;gc/shared/oopStorageParState.inline.hpp&quot;
  54 #include &quot;gc/shared/scavengableNMethods.hpp&quot;
  55 #include &quot;gc/shared/space.hpp&quot;
  56 #include &quot;gc/shared/strongRootsScope.hpp&quot;
  57 #include &quot;gc/shared/weakProcessor.hpp&quot;
  58 #include &quot;gc/shared/workgroup.hpp&quot;
  59 #include &quot;memory/filemap.hpp&quot;
  60 #include &quot;memory/iterator.hpp&quot;
  61 #include &quot;memory/metaspaceCounters.hpp&quot;
  62 #include &quot;memory/resourceArea.hpp&quot;
  63 #include &quot;memory/universe.hpp&quot;
  64 #include &quot;oops/oop.inline.hpp&quot;
  65 #include &quot;runtime/biasedLocking.hpp&quot;

  66 #include &quot;runtime/handles.hpp&quot;
  67 #include &quot;runtime/handles.inline.hpp&quot;
  68 #include &quot;runtime/java.hpp&quot;
  69 #include &quot;runtime/vmThread.hpp&quot;
<span class="line-modified">  70 #include &quot;services/memoryService.hpp&quot;</span>
  71 #include &quot;utilities/autoRestore.hpp&quot;
  72 #include &quot;utilities/debug.hpp&quot;
  73 #include &quot;utilities/formatBuffer.hpp&quot;
  74 #include &quot;utilities/macros.hpp&quot;
  75 #include &quot;utilities/stack.inline.hpp&quot;
  76 #include &quot;utilities/vmError.hpp&quot;
  77 #if INCLUDE_JVMCI
  78 #include &quot;jvmci/jvmci.hpp&quot;
  79 #endif
  80 
  81 GenCollectedHeap::GenCollectedHeap(Generation::Name young,
  82                                    Generation::Name old,
  83                                    const char* policy_counters_name) :
  84   CollectedHeap(),
  85   _young_gen(NULL),
  86   _old_gen(NULL),
  87   _young_gen_spec(new GenerationSpec(young,
  88                                      NewSize,
  89                                      MaxNewSize,
  90                                      GenAlignment)),
</pre>
<hr />
<pre>
 300     if (_young_gen-&gt;should_allocate(size, is_tlab)) {
 301       result = _young_gen-&gt;expand_and_allocate(size, is_tlab);
 302     }
 303   }
 304   assert(result == NULL || is_in_reserved(result), &quot;result not in heap&quot;);
 305   return result;
 306 }
 307 
 308 HeapWord* GenCollectedHeap::mem_allocate_work(size_t size,
 309                                               bool is_tlab,
 310                                               bool* gc_overhead_limit_was_exceeded) {
 311   // In general gc_overhead_limit_was_exceeded should be false so
 312   // set it so here and reset it to true only if the gc time
 313   // limit is being exceeded as checked below.
 314   *gc_overhead_limit_was_exceeded = false;
 315 
 316   HeapWord* result = NULL;
 317 
 318   // Loop until the allocation is satisfied, or unsatisfied after GC.
 319   for (uint try_count = 1, gclocker_stalled_count = 0; /* return or throw */; try_count += 1) {

 320 
 321     // First allocation attempt is lock-free.
 322     Generation *young = _young_gen;
 323     assert(young-&gt;supports_inline_contig_alloc(),
 324       &quot;Otherwise, must do alloc within heap lock&quot;);
 325     if (young-&gt;should_allocate(size, is_tlab)) {
 326       result = young-&gt;par_allocate(size, is_tlab);
 327       if (result != NULL) {
 328         assert(is_in_reserved(result), &quot;result not in heap&quot;);
 329         return result;
 330       }
 331     }
 332     uint gc_count_before;  // Read inside the Heap_lock locked region.
 333     {
 334       MutexLocker ml(Heap_lock);
 335       log_trace(gc, alloc)(&quot;GenCollectedHeap::mem_allocate_work: attempting locked slow path allocation&quot;);
 336       // Note that only large objects get a shot at being
 337       // allocated in later generations.
 338       bool first_only = !should_try_older_generation_allocation(size);
 339 
</pre>
<hr />
<pre>
 459 
 460 void GenCollectedHeap::collect_generation(Generation* gen, bool full, size_t size,
 461                                           bool is_tlab, bool run_verification, bool clear_soft_refs,
 462                                           bool restore_marks_for_biased_locking) {
 463   FormatBuffer&lt;&gt; title(&quot;Collect gen: %s&quot;, gen-&gt;short_name());
 464   GCTraceTime(Trace, gc, phases) t1(title);
 465   TraceCollectorStats tcs(gen-&gt;counters());
 466   TraceMemoryManagerStats tmms(gen-&gt;gc_manager(), gc_cause());
 467 
 468   gen-&gt;stat_record()-&gt;invocations++;
 469   gen-&gt;stat_record()-&gt;accumulated_time.start();
 470 
 471   // Must be done anew before each collection because
 472   // a previous collection will do mangling and will
 473   // change top of some spaces.
 474   record_gen_tops_before_GC();
 475 
 476   log_trace(gc)(&quot;%s invoke=%d size=&quot; SIZE_FORMAT, heap()-&gt;is_young_gen(gen) ? &quot;Young&quot; : &quot;Old&quot;, gen-&gt;stat_record()-&gt;invocations, size * HeapWordSize);
 477 
 478   if (run_verification &amp;&amp; VerifyBeforeGC) {

 479     Universe::verify(&quot;Before GC&quot;);
 480   }
 481   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::clear());
 482 
 483   if (restore_marks_for_biased_locking) {
 484     // We perform this mark word preservation work lazily
 485     // because it&#39;s only at this point that we know whether we
 486     // absolutely have to do it; we want to avoid doing it for
 487     // scavenge-only collections where it&#39;s unnecessary
 488     BiasedLocking::preserve_marks();
 489   }
 490 
 491   // Do collection work
 492   {
 493     // Note on ref discovery: For what appear to be historical reasons,
 494     // GCH enables and disabled (by enqueing) refs discovery.
 495     // In the future this should be moved into the generation&#39;s
 496     // collect method so that ref discovery and enqueueing concerns
 497     // are local to a generation. The collect method could return
 498     // an appropriate indication in the case that notification on
 499     // the ref lock was needed. This will make the treatment of
 500     // weak refs more uniform (and indeed remove such concerns
 501     // from GCH). XXX
 502 

 503     save_marks();   // save marks for all gens
 504     // We want to discover references, but not process them yet.
 505     // This mode is disabled in process_discovered_references if the
 506     // generation does some collection work, or in
 507     // enqueue_discovered_references if the generation returns
 508     // without doing any work.
 509     ReferenceProcessor* rp = gen-&gt;ref_processor();
 510     // If the discovery of (&quot;weak&quot;) refs in this generation is
 511     // atomic wrt other collectors in this configuration, we
 512     // are guaranteed to have empty discovered ref lists.
 513     if (rp-&gt;discovery_is_atomic()) {
 514       rp-&gt;enable_discovery();
 515       rp-&gt;setup_policy(clear_soft_refs);
 516     } else {
 517       // collect() below will enable discovery as appropriate
 518     }
 519     gen-&gt;collect(full, clear_soft_refs, size, is_tlab);
 520     if (!rp-&gt;enqueuing_is_done()) {
 521       rp-&gt;disable_discovery();
 522     } else {
 523       rp-&gt;set_enqueuing_is_done(false);
 524     }
 525     rp-&gt;verify_no_references_recorded();
 526   }
 527 
 528   COMPILER2_OR_JVMCI_PRESENT(DerivedPointerTable::update_pointers());
 529 
 530   gen-&gt;stat_record()-&gt;accumulated_time.stop();
 531 
 532   update_gc_stats(gen, full);
 533 
 534   if (run_verification &amp;&amp; VerifyAfterGC) {

 535     Universe::verify(&quot;After GC&quot;);
 536   }
 537 }
 538 
 539 void GenCollectedHeap::do_collection(bool           full,
 540                                      bool           clear_all_soft_refs,
 541                                      size_t         size,
 542                                      bool           is_tlab,
 543                                      GenerationType max_generation) {
 544   ResourceMark rm;
 545   DEBUG_ONLY(Thread* my_thread = Thread::current();)
 546 
 547   assert(SafepointSynchronize::is_at_safepoint(), &quot;should be at safepoint&quot;);
 548   assert(my_thread-&gt;is_VM_thread() ||
 549          my_thread-&gt;is_ConcurrentGC_thread(),
 550          &quot;incorrect thread type capability&quot;);
 551   assert(Heap_lock-&gt;is_locked(),
 552          &quot;the requesting thread should have the Heap_lock&quot;);
 553   guarantee(!is_gc_active(), &quot;collection is not reentrant&quot;);
 554 
 555   if (GCLocker::check_active_before_gc()) {
 556     return; // GC is disabled (e.g. JNI GetXXXCritical operation)
 557   }
 558 
 559   const bool do_clear_all_soft_refs = clear_all_soft_refs ||
 560                           soft_ref_policy()-&gt;should_clear_all_soft_refs();
 561 
 562   ClearedAllSoftRefs casr(do_clear_all_soft_refs, soft_ref_policy());
 563 
<span class="line-modified"> 564   AutoModifyRestore&lt;bool&gt; temporarily(_is_gc_active, true);</span>
 565 
 566   bool complete = full &amp;&amp; (max_generation == OldGen);
 567   bool old_collects_young = complete &amp;&amp; !ScavengeBeforeFullGC;
 568   bool do_young_collection = !old_collects_young &amp;&amp; _young_gen-&gt;should_collect(full, size, is_tlab);
 569 
 570   const PreGenGCValues pre_gc_values = get_pre_gc_values();
 571 
 572   bool run_verification = total_collections() &gt;= VerifyGCStartAt;
 573   bool prepared_for_verification = false;
 574   bool do_full_collection = false;
 575 
 576   if (do_young_collection) {
 577     GCIdMark gc_id_mark;
 578     GCTraceCPUTime tcpu;
 579     GCTraceTime(Info, gc) t(&quot;Pause Young&quot;, NULL, gc_cause(), true);
 580 
 581     print_heap_before_gc();
 582 
 583     if (run_verification &amp;&amp; VerifyGCLevel &lt;= 0 &amp;&amp; VerifyBeforeGC) {
 584       prepare_for_verify();
</pre>
<hr />
<pre>
 800                                      CLDClosure* strong_cld_closure,
 801                                      CLDClosure* weak_cld_closure,
 802                                      CodeBlobToOopClosure* code_roots) {
 803   // General roots.
 804   assert(code_roots != NULL, &quot;code root closure should always be set&quot;);
 805   // _n_termination for _process_strong_tasks should be set up stream
 806   // in a method not running in a GC worker.  Otherwise the GC worker
 807   // could be trying to change the termination condition while the task
 808   // is executing in another GC worker.
 809 
 810   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ClassLoaderDataGraph_oops_do)) {
 811     ClassLoaderDataGraph::roots_cld_do(strong_cld_closure, weak_cld_closure);
 812   }
 813 
 814   // Only process code roots from thread stacks if we aren&#39;t visiting the entire CodeCache anyway
 815   CodeBlobToOopClosure* roots_from_code_p = (so &amp; SO_AllCodeCache) ? NULL : code_roots;
 816 
 817   bool is_par = scope-&gt;n_threads() &gt; 1;
 818   Threads::possibly_parallel_oops_do(is_par, strong_roots, roots_from_code_p);
 819 




 820   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_ObjectSynchronizer_oops_do)) {
 821     ObjectSynchronizer::oops_do(strong_roots);
 822   }






 823 #if INCLUDE_AOT
 824   if (UseAOT &amp;&amp; _process_strong_tasks-&gt;try_claim_task(GCH_PS_aot_oops_do)) {
 825     AOTLoader::oops_do(strong_roots);
 826   }
 827 #endif
 828   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_OopStorageSet_oops_do)) {
 829     OopStorageSet::strong_oops_do(strong_roots);
 830   }
 831 
 832   if (_process_strong_tasks-&gt;try_claim_task(GCH_PS_CodeCache_oops_do)) {
 833     if (so &amp; SO_ScavengeCodeCache) {
 834       assert(code_roots != NULL, &quot;must supply closure for code cache&quot;);
 835 
 836       // We only visit parts of the CodeCache when scavenging.
 837       ScavengableNMethods::nmethods_do(code_roots);
 838     }
 839     if (so &amp; SO_AllCodeCache) {
 840       assert(code_roots != NULL, &quot;must supply closure for code cache&quot;);
 841 
 842       // CMSCollector uses this to do intermediate-strength collections.
</pre>
<hr />
<pre>
1337 void GenCollectedHeap::ensure_parsability(bool retire_tlabs) {
1338   CollectedHeap::ensure_parsability(retire_tlabs);
1339   GenEnsureParsabilityClosure ep_cl;
1340   generation_iterate(&amp;ep_cl, false);
1341 }
1342 
1343 oop GenCollectedHeap::handle_failed_promotion(Generation* old_gen,
1344                                               oop obj,
1345                                               size_t obj_size) {
1346   guarantee(old_gen == _old_gen, &quot;We only get here with an old generation&quot;);
1347   assert(obj_size == (size_t)obj-&gt;size(), &quot;bad obj_size passed in&quot;);
1348   HeapWord* result = NULL;
1349 
1350   result = old_gen-&gt;expand_and_allocate(obj_size, false);
1351 
1352   if (result != NULL) {
1353     Copy::aligned_disjoint_words(cast_from_oop&lt;HeapWord*&gt;(obj), result, obj_size);
1354   }
1355   return oop(result);
1356 }


































</pre>
</td>
</tr>
</table>
<center><a href="../../../os/linux/os_linux.cpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../../index.html" target="_top">index</a> <a href="../../utilities/globalDefinitions_gcc.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>